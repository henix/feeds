<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>学界 | NYU联合Google Brain提出结合工作记忆的视觉推理架构和数据集</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1523489533&amp;src=3&amp;ver=1&amp;signature=DceMtFD8-qHjoR-Cgo60qJcNfZG95jNMDq1gpBdkGcsW6lkvXzblweZqDbTKDbpFIgf19053F9z9xfPTsxClihu4HT8VKm3PaLLeySxTyitNuY7iLA5neEOYOtf1T6-rXtLkJYAVIISJ6k1VnVbe5fLIlRT8rK1kfUMEO*qJ-t8=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    学界 | NYU联合Google Brain提出结合工作记忆的视觉推理架构和数据集                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-02</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="font-size: 14px;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="font-size: 16px;padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：Guangyu Robert Yang等</strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">参与：</span></strong><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">Nurhachu Null、刘晓坤</span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p><br></p><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">现有的视觉推理数据集都避开了时间和记忆的复杂性，而这两者都是现实应用中不可或缺的因素。为突破这个局限性，受认知心理学启发，纽约大学联合 Google Brain 开发了新的视觉问答数据集 ( COG ) 以及对应的网络架构。该架构能利用多模态信息和动态注意、记忆机制执行推理，初步分析表明，它能以人类可理解的方式完成任务。</span></p></blockquote><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">1. 简介</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">人工智能的一个主要目标就是构建能够对感官环境进行强有力并且灵活地推理的系统 [1]。视觉提供了一个极其丰富和高度实用的领域，我们可以在其中通过建立系统对复杂的刺激执行逻辑推理 [2,3,4,5]。研究视觉推理的一个途径是对视觉问答 ( VQA ) 数据集进行建模，模型可以从中学习正确地回答关于静态图像的挑战性自然语言问题 [6,7,8,9]。尽管这些多模态数据集已经有了很大进步，但是目前的方法还存在几个局限性。首先，与推理一个问题的逻辑组成不一样，在 VQA 数据集上训练的模型刚好遵循图像中的固有统计特性的程度是不确定的 [10,11,12,13]。其次，这些数据集都避开了时间和记忆的复杂性，而这两者都是智能体设计 [1,14,15,16] 与视频分析、总结 [17,18,19] 中不可或缺的因素。</span></p><p><br></p><p><img class="" data-ratio="0.33553500660501984" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicGH8NGqG3ibicubd8SydPfqKAekOW0ibBiaaN7hAZl1zZwxoIohq1ssrZ2OGGqsghfWk46lYkLiaG6V3Q/640?wx_fmt=png" data-type="png" data-w="1514" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 1. COG 数据集中的样本序列和指令。COG 数据集中的任务是测试目标识别、关系理解以及为解决问题而进行的记忆操作和适应。所有问题都可能涉及到当前图像和之前图像中的目标。注意在最后一个例子中，指令涉及到最后一个但不是最近一个「b」。前者排除了在当前图像中寻找「b」。白色箭头表示每个图像的目标响应。</span></em></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了解决 VQA 数据集中与空间关系的逻辑推理相关的缺点，Johnson 等人 [10] 最近提出了 CLEVER 来直接用于基本视觉推理模型的测试，以便与其他 VQA 数据集相结合 (例如，[6,7,8,9])。CLEVR 数据集提供了人工静态图像和关于这些图像的自然语言问题，让模型学习执行逻辑和视觉推理。最近研究中人们开发出来的网络能够达到几乎完美的准确率 [5,4,20]。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在这项工作中，研究者解决了视觉推理中的第二项限制，即关于时间和记忆的限制。推理智能体必须记住它的视觉历史中相关的片段，忽略不相关的细节，基于新的信息来更新和操作记忆，以及在后面的时间里利用这些记忆来作出决策。作者的方法就是创建一个人工的数据集，它具有时变数据中存在的很多复杂性，同时也避免了处理视频时的很多视觉复杂性和技术难题（例如，视频解码、时间平滑帧之间的冗余）。特别是，作者从认知心理学 [21,22,23,24,25] 和现代系统神经科学 [26,27,28,29,30,31] 最近几十年的研究中得到启发。这些领域有着基于空间和逻辑推理、记忆组成和语义理解将视觉推理分解为核心组件的悠久研究传统。为此，作者建立了一个称为 COG 的人工数据集，它也能用于人类的认知实验 [32,33,34]，并能够及时地训练视觉推理。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">COG 数据集是基于一种能够构建三元组任务集的编程语言开发的：三元组包含图像序列、语言指令以及正确答案的序列。这些随机生成的三元组能够在大量的任务序列中训练视觉推理，解决它们需要对文本的语义理解，对图像序列中每张图像的视觉认知，以及决定时变答案的工作记忆（图 3）。研究者在编程语言中特别强调了几个参数，开发者可以通过这些参数来从易到难地设定挑战性环境，从而对问题难度进行调制。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">最后，作者引入了用于有记忆视觉推理的多模态循环架构。该网络将语义、视觉模块与状态控制器相结合，状态控制器调节视觉注意力和记忆，以便正确执行视觉任务。他们证明了该模型在 CLEVER 数据集上取得当前最佳的性能。此外，该网络还提供了稳健的基线，其可以在 COG 数据集的一系列设置中实现良好的性能。通过控制变量研究和对网络的动态分析，他们发现网络采用人类可解释的注意力机制来解决这些视觉推理任务。作者希望 COG 数据集、与之对应的网络架构和相关的基线结果能够为研究时变视觉刺激下的推理提供一个有用的基准。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">3.COG 数据集</span></strong></p><p><br></p><p><img class="" data-ratio="0.46908602150537637" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicGH8NGqG3ibicubd8SydPfqKL7j9QGbH9x6PibSL8mIZT7fk6eEJOH8lHBAahGdDVkD1DgwpZbmFydQ/640?wx_fmt=png" data-type="png" data-w="1488" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 2. 生成综合的 COG 数据集。COG 数据集基于一系列的运算符（A）, 这些运算符被组合以形成各种任务图 ( B )。( C ) 通过在任务图中指定所有运算符的属性来实例化任务。任务实例用于生成图像序列和语义任务指令。( D ) 正向传递图形和图像序列以用于正常任务执行。( E ) 生成一致的、偏差最小化的图像序列需要以反向拓扑顺序向后传递任务图，并且以反向时间顺序向后传递图像序列。</span></em></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">4. 网络</span></strong></p><p><br></p><p><img class="" data-ratio="0.3812754409769335" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicGH8NGqG3ibicubd8SydPfqKYzyID3Lwl0Qyobrfv1aah7iaER8qvBcy2PIuRwCcdgavGexrzbxrfPg/640?wx_fmt=png" data-type="png" data-w="1474" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 3. 本文提出的网络。图像序列被用来作为卷积神经网络 ( 绿色部分) 的输入。英语文本形式的指令被输入到顺序嵌入网络 (红色) 中。视觉短期记忆 ( vSTM ) 网络及时保存视觉空间信息并提供指向输出 ( 蓝绿色 )。vSTM 模块可以被认为是具有外部门控的卷积 LSTM 网络。状态控制器 (蓝色部分) 直接或间接提供所有注意和门控信号。网络的输出是离散的 (语言) 或 2D 连续的 (指向的)。</span></em></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">5. 结果</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.30926430517711173" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicGH8NGqG3ibicubd8SydPfqKpj2cYuGpZzVhmt7PNziaV2ewZlbFkeH5XunoibD9R0dOXhK3Hkkm5kpA/640?wx_fmt=png" data-type="png" data-w="1468" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">表 1. CLEVER 上的测试准确率：人类、基线、仅靠训练中的任务指令和像素输入的性能顶尖模型，以及本文提出的模型。（*）代表的是所用的预训练模型。</span></em></span></p><p><br></p><p style="text-align: center;"><img class="" data-ratio="0.4053156146179402" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicGH8NGqG3ibicubd8SydPfqK9HdyIbuLlboXOvDe45ibLAgtM55TYTTsJZpt7ic2XvDu4tzEF4ALuzQw/640?wx_fmt=png" data-type="png" data-w="1204" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 4. 本文提出网络的工作时的思想过程，通过可视化单个 CLEVER 样本的注意力和输出来展示。( A ) 来自 CLEVER 验证集的示例问题和图像。( B ) 每个思考步骤的有效特征注意图。(C) 相关的空间注意力图。(D) 语义注意力。( E ) 排名前 5 的语词输出。红色和蓝色分别表示较强和较弱。在同时特征注意到「小金属球」和空间注意到「位于红色橡胶目标之后」，被关注物体的颜色 (黄色) 反映在语词输出中。在后来的思考过程中，网络特征注意的是「大亚光球」，而正确的答案 (是) 出现在语词输出中。</span></em></span></p><p><br></p><p><img class="" data-ratio="0.36363636363636365" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicGH8NGqG3ibicubd8SydPfqKqibDxHG4TzI6qEwZx3HgvH5Z7McHJNyic4N7LOibm7Rrib6QicIGiaPht94g/640?wx_fmt=png" data-type="png" data-w="1474" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 5. 控制变量研究。CLEVER 测试集在不同的模型上的总体准确率； A 和 B 分别是 CLEVER 数据集和 COG 数据集：CLEVR 数据集的相关模型中未包含任何 vSTM 模块。（C）基于输出类型、是否涉及空间推理、操作符的数量以及任务图中的最后一个操作符来分析 COG 的准确率。</span></em></span></p><p><br></p><p><img class="" data-ratio="0.38133333333333336" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicGH8NGqG3ibicubd8SydPfqKyKOUribIgVpuQUpFQTZswf2K7Sspkw7FOD90sUcz15heoC6n2arNEYw/640?wx_fmt=png" data-type="png" data-w="1500" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 7. 本文提出的网络可以零样本地推广到新任务。用 44 个任务中的 43 个任务训练了 44 个网络。如图所示是 43 个已训练任务 (灰色) 的最大性能，迁移到一个没有经过训练的任务 (蓝色) 的最大性能，以及在这个任务上的机会水平（红色）。</span></em></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">论文：A dataset and architecture for visual reasoning with a working memory</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.48026315789473684" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicGH8NGqG3ibicubd8SydPfqKXqib4mH50zAIFK2KQLXbzpanHSkhJe0o6d4CEDdCFEMozVEF3kv6tVw/640?wx_fmt=png" data-type="png" data-w="1520" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：：https://arxiv.org/pdf/1803.06092.pdf</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">摘要：人工智能中存在一个令人烦恼的问题，就是对复杂的、不断变化的视觉刺激中发生的事件进行推理 (如视频分析或游戏)。受认知心理学和神经科学中丰富的视觉推理和记忆的传统研究所启发，我们开发了一个人工的、可配置的视觉问答数据集 ( COG )，这个数据集可用于人类和动物的实验。尽管 COG 比视频分析的一般问题简单得多，但它解决了许多与视觉、逻辑推理以及记忆相关的问题，这些问题对现代深度学习架构来说仍然具有挑战性。此外，我们还提出了一种深度学习架构，该架构在其他诊断 VQA 数据集 (即 CLEVER) 以及 COG 数据集的简单设置上具有竞争力。但是，COG 的某些设置可以令数据集的学习越来越困难。经过训练，该网络可以零样本地泛化到许多新任务。对在 COG 上训练的网络架构的初步分析表明，该网络以人类可理解的方式完成任务。<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);font-size: 14px;text-align: justify;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;width: 48px !important;visibility: visible !important;" width="48px"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;"></span></em></span></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="1307771004" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
