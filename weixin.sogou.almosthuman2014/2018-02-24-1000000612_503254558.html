<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>前沿 | 剧本自动生成电影：杜克大学提出AI视频生成新方法</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1520302001&amp;src=3&amp;ver=1&amp;signature=W6Fo7aDHiJtK4ecUcnSJ4jytVNZnenAKNQPwbhoaQJopBAViUCy5T-5GI*0c6*G9IjIYpt*CHAlUd-xL9PD7B8xX7IsVEFJXedN2WcKqNIcelz3Ct8E-L2RjXq70MkyDCibe2YwUw4esJ0kSmxXu-Z*4Zjmi7py5RkSQEaO4nKY=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    AAAI 2018 | 腾讯AI Lab现场陈述论文：训练L1稀疏模型的象限性消极下降算法                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-02-24</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="color: rgb(62, 62, 62);text-align: center;margin-top: -1.2em;max-width: 100%;min-height: 1em;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心发布</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(127, 127, 127);font-size: 12px;"><strong>演讲者：<span style="color: rgb(136, 136, 136);text-align: justify;font-size: 12px;">王倪剑桥</span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">腾讯 AI Lab 共有 12 篇论文入选在美国新奥尔良举行的国际人工智能领域顶级学术会议 AAAI 2018。腾讯技术工程官方号独家编译了论文《训练 L1 稀疏模型的象限性消极下降算法》(Training L1-Regularized Models with Orthant-Wise Passive Descent Algorithms)，该论文被 AAAI 2018 录用为现场陈述论文 (Oral Presentation)，由腾讯 AI Lab 独立完成，王倪剑桥为论文唯一作者。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/abs/1704.07987</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.23314606741573032" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2UAF9HpZdt2Wkjh1bmknwOr7C5BbppzVp1dk2IhOogB37EOF3EMbNww/640?wx_fmt=png" data-type="png" data-w="1068" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">中文概要</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">L1 范数正则模型是一种常用的高维数据的分析方法。对于现代大规模互联网数据上的该模型，研究其优化算法可以提高其收敛速度，进而在有限时间内显著其模型准确率，或者降低对服务器资源的依赖。经典的随机梯度下降 (SGD) 虽然可以适用于神经网络等多种模型，但对于 L1 范数不可导性并不适用。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在本文中，我们提出了一种新的随机优化方法，随机象限性消极下降算法 (OPDA)。本算法的出发点是 L1 范数函数在任何一个象限内是连续可导的，因此，模型的参数在每一次更新之后被投影到上一次迭代时所在的象限。我们使用随机方差缩减梯度 (SVRG) 方法产生梯度方向，并结合伪牛顿法 (Quasi-Newton) 来使用损失函数的二阶信息。我们提出了一种新的象限投影方法，使得该算法收敛于一个更加稀疏的最优点。我们在理论上证明了，在强凸和光滑的损失函数上，该算法可以线性收敛。在 RCV1 等典型稀疏数据集上，我们测试了不同参数下 L1/L2 范数约束 Logistic 回归下该算法性能，其结果显著超越了已有的线性收敛算法 Proximal-SVRG，并且在卷积神经网络 (CNN) 的实验上超越 Proximal-SGD 等算法，证明了该算法在凸函数和非凸函数上均有很好的表现。</span></p><p><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>演讲</strong></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">大家好，我是王倪剑桥，来自腾讯 AI Lab。</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.6100478468899522" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2IiaiccFyf1eqibTED0DMstqGeWbz3oYavLdvG5AjBVG86PF43x9QtDBlQ/640?wx_fmt=png" data-type="png" data-w="836" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">引言：</span></strong><span style="font-size: 14px;">学习稀疏表征一直都是一个非常重要的数据分析任务。比如说，在生物学领域，为了对单个个体进行基因分析，常常涉及到数百万个基因，然而其中真正对某任务，比如帮助预测某种稀有癌症，有用的基因片段并不多。在金融序列预测和网络广告等领域，也有很多数据数量甚至比数据维度还小的情况。这本身是一个病态 (ill-condition) 的问题，然而如果对解有一个稀疏先验的话，问题则是可解的。通信领域的压缩感知中的核心部分也是如何高效求解稀疏模型。目前，这些方法包括 Lasso (Robert Tibshirani), Dantzig Selector (Emmanuel Candes, Terrence Tao), OMP (Joel A. Tropp, Tong Zhang), FoBa (Tong Zhang) 等。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先，我们给一个直观的例子，为什么 L1 范数正则项（绝对值的和）适用于求解稀疏模型。下图中，蓝色区域是约束内的可行解区域，左边是 L1 范数球（norm ball），右边是 L2 范数球，红色圈是均方损失函数（average-of-square-loss）的等高线。这些球和等高线之间的交点是模型的解。我们可以看到 L1 正则化模型的解接近 Y 轴，这意味着其 X 维元素更接近于 0。这是一个简单的例子，可以看出来 L1 正则项比 L2 正则项更适合于学习稀疏模型。</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7707317073170732" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2QAG5fcR7g1nZ4IAza0F5BZVkjoiaFKgtpngP8IDXJwvtaX1fzicjAgqw/640?wx_fmt=png" data-type="png" data-w="820" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们可以把大部分问题统一为最小化一个正则化函数 P(x)= F(x)+R(x)，其中 F(x) 是 N 个损失函数的平均，其中每个都依赖于一个数据样本，R 是 L1 正则项。我们还假设每个损失函数都满足二次可导 (twice differentiable)、强凸 (strongly-convex) 和光滑 (smooth)。由于该 L1 范数是不可导的（在零点），目前最通用的优化方法是近端方法 (proximal method)，通过迭代式地采取梯度下降步骤，然后在当前点上优化一个 proximal 问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7527010804321729" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2VZmAGYbLIUn5Uia7QxAxyuyFghIlskE8Uoq6mZsOdVrUVHLKPkdk8fQ/640?wx_fmt=png" data-type="png" data-w="833" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">相关文献介绍：</span></strong><span style="font-size: 14px;">我们的主要参考方法是象限性伪牛顿法 (OWL-QN: Galen Andrew, Jianfeng Gao)，这种方法基于 L-BFGS (Jorge Nocedal)，一种最常用的伪牛顿法。该方法 OWL-QN 将更新后的参数限制在一个特定的象限内，因为在每个单个象限中，其绝对值函数实际上是可微分的。OWL-QN 的一个关键创新点与在零点的次梯度 (subgradient) 有关。这里 L1 正则项 R(x) 的次梯度既可以是正λ，也可以是负 λ，那么如何选择次梯度会影响收敛速度。以下面大括号内第三个分支为例：我们研究的是当前点 X 的第 i 维 X_i，和梯度 V_i。如果 X_i 等于 0 且 X_i 加上 λ 为负，那么该次梯度就设为 V_i 加上正 λ，因为在减去这个次梯度之后，X_i 将会是一个正值，那么 R(x) 的次梯度将仍然为正 λ，这会使 L1 范数的次梯度在一次迭代中保持一致。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7539203860072377" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2a6toxQiaoUTiczsjSTQ6jco4UO7SiaGKUk1tQb41FkxQX3W2hvTAjyFPg/640?wx_fmt=png" data-type="png" data-w="829" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了处理大规模互联网数据，研究者提出了很多用于加速训练过程的优化方法，比如随机梯度下降法 (SGD)。但是，SGD 通常需要衰减步长才能收敛，而且它的收敛率是次线性的 (sublinear)。近期出现了 SVRG (Rie Johnson, Tong Zhang) 和 SAGA (Aaron Defazio, Francis Bach) 等一些随机方差缩减方法，可以无需降低步长就能收敛，而且可以在光滑和强凸的模型上实现线性收敛率 (linear convergence rate)。在 SGD 方法中，第 k 步的下降方向 v_k 是在该数据集的一个随机子集 S_k 上进行评估的。在 SVRG 方法中，我们需要周期性地在一些参考点（比如 tilde-X）上计算一个准确梯度。这个准确梯度构成了 SVRG 的 v_k 的第三项，然后我们必须通过在 tilde-X 上减去一个随机梯度（这是在同一子集 S_k 上计算的）来平衡它的期望。</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7535885167464115" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2xoXrW705kktPQiaI1t6ibHfNj9PQ2vn1ROQ8sSzEuhmUZk1oAF2w60Ng/640?wx_fmt=png" data-type="png" data-w="836" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">受 SVRG 和 OWL-QN 的启发，我们开发了一种针对 L1 正则化模型的随机优化方法。在第一步时，我们计算损失函数 F(x) 的 SVRG，然后我们使用来自 OWL-QN 的思想来计算一个参考方向，尽量使下降方向以在一次迭代后维持同一个象限。而我们的实际下降方向 V_k 是用这里的第三个等式计算的。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.749400479616307" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2fF6PreGCOBUgf25HN01ZymqWI96PJ7l70PYsVfcNT3wGOYz7LURZ2Q/640?wx_fmt=png" data-type="png" data-w="834" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">现在我们可以计算解前进的方向了。我们可以通过计算当前点上的 Hessian 矩阵或估计近似的 Hessian 矩阵（伪牛顿法）来使用该损失函数的二阶信息，从而加速其收敛。然后我们可以通过在当前点附近的 Taylor 二次展开来计算最优下降方向 D_k。如果我们使用 L-BFGS，我们可以绕开耗时的矩阵求逆运算，只通过快速的矩阵向量乘法就可以做到。我们也可以直接将 V_k 分配给 D_k，这样就是一个典型的一阶方法。在这个步骤之后，方向 D_k 的象限必须与参考方向保持一致，这意味着：如果 D_k 的某些维度与参考方向的符号不同，它们就必须被分配为 0，我们将这个对齐后的版本记为 P_k。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7612121212121212" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2cNFibURJ2YuouXqB1bLCfhbrUS9vbZA8mibSlvbDMeYcgJiadpQrqibatA/640?wx_fmt=png" data-type="png" data-w="825" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">除了对齐参考，上述计算梯度并没有涉及到 L1 正则化 R(x) 的偏导数，因为我们要避免对随机梯度的引入额外方差。为了使解是稀疏的，我们提出了一种全新的对齐算子来激励零元素。这个算子作用在 X,Y 两个元素上，如果 X 和 Y 符号不同或 X 的绝对值小于一个阈值，就强制 X 为零。通过这个对齐算子，每次在我们完成了之前的计算之后，我们就检查下一个点是否与当前点在同一个象限，如果不在，下一个点的某些维度就会被强制为零。在这一步之后，显然 X_k 的更多维度应该为零，而不是绝对值很小的非零值。</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7490996398559424" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2b3q9qRstQiadKLZ50sckvtandmzYwYNyI6PwkTibDo1FdnTmia4d7eJTA/640?wx_fmt=png" data-type="png" data-w="833" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">收敛性分析：在这篇论文中，我们证明在平滑性和强凸性的假设下，我们的方法将以一个线性速率收敛。</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7621359223300971" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2lldvfuryvFlzbdUbMHJL5SY9MbrzYnG7HorEPEblRdVcXuouFSxbQA/640?wx_fmt=png" data-type="png" data-w="824" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了进行可视化，我们在这张图中绘制了在一个简单的二维合成函数的优化轨迹。我们的方法 OPDA 用红线标识，作为基准的 Proximal-Gradient Descent 算法用蓝线标识。在迭代了同样多次数之后，我们看到 OPDA 用更快的速度收敛到了等高线中更低的区域。</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7619047619047619" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2Rx2yIBc1BgohFzPS7KvNN8xBal8zYibWpoLGyHzSzlprHxBPSYGtveQ/640?wx_fmt=png" data-type="png" data-w="819" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在凸函数上的实验：我们还使用 L2/L1 regularized Logistic 回归上进行了一些实验。在这一部分，我们将我们的方法与一个线性收敛算法 Prox-SVRG (Lin Xiao, Tong Zhang) 进行了比较。在这张图中，Y 轴表示解的次优性 (suboptimality)，即当前目标函数与最小值之间的间隔；X 轴表示完整扫过数据集的次数。我们发现使用不同的步长以及 L2 和 L1 正则化系数时，OPDA 的速度稳定地超越了 Prox-SVRG 算法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7539015606242497" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2ST95OPvOSHfvz2taBQKcA81HwwnxBoqGibiccybHZMjx8LU5d6CCeWgg/640?wx_fmt=png" data-type="png" data-w="833" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.6020408163265306" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2N9V6ia3ncOYia7mpIias4IZASGCYHiaxedytdaas5m5TMknPesYI1ZwsHA/640?wx_fmt=png" data-type="png" data-w="1078" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在深度学习上的实验：我们使用 L1 正则化稀疏卷积神经网络 (sparse-CNN) 进行了实验，以表明我们的方法在非凸函数的有效性。下图中红线表示我们的方法，蓝线表示近端 SVRG。我们测试了不同规模的 L1 正则化。我们看到 OPDA 收敛速度比近 Prox-SGD 更快。而且在 L1 正则项更强的情况下，这种差异更大。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.7547846889952153" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9m4ennicIoM1tibCveiaQzyr2ia2GiaT9kCxWhLBP9kbict7ZUX8Ztn8da7lNIJtMzJdEpPJ6890XaY0rA/640?wx_fmt=png" data-type="png" data-w="836" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">总结：</span></strong><span style="font-size: 14px;">我们提出的 OPDA 算法可以适用于快速优化 L1 正则的稀疏模型。实际上，由于我们的方法的象限性本质，下降方向和更新后的点的许多维度都会在对齐过程中被强制变为零，这会使等效步长变小很多；但是，在同等步长条件下，我们方法的速度仍然远远快于近端方法。这证明我们提出的对齐算子确实能将方向调校得更好，使其整体框架更为有效，而且在每次迭代中所需额外运算量接近可以忽略不计。 <span style="color: rgb(62, 62, 62);font-size: 14px;background-color: rgb(255, 255, 255);"> </span><img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicKczlUNeEAmUicw7MXcFnte6PhPpD5AHeEVvqImFdQz3ziaXsDmtjAp2icXupgXc6j3OpGl8dvHEicvQ/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);white-space: normal;font-size: 16px;text-align: justify;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 51px !important;" width="43px"></span></p><p><br></p><p><br></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><img class="" data-ratio="1.3654970760233918" src="https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gW9cWqUGib1I8LicEna2GCqsE2zaRCjXkkQxUj4xSMwic51nHPBia7CbbDHcJTkShoT3z4RZCh1v4KPYGA/640?wx_fmt=jpeg" data-type="jpeg" data-w="1026" width="auto" style="font-size: 16px;white-space: normal;color: rgb(62, 62, 62);text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: auto !important;"></p>
                </div>
                <script nonce="1902842951" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <a class="media_tool_meta meta_primary" id="js_view_source" href="##">阅读原文</a>
                                <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                <a class="media_tool_meta meta_primary" href="https://mp.weixin.qq.com/mp/homepage?__biz=MzA3MzI4MjgzMw==&amp;hid=11&amp;sn=a2c8b0d29b504f0b5c30cd6c86eb34d7#wechat_redirect" target="_blank">阅读原文</a>
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
