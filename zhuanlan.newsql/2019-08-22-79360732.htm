<div class="title-image"><img src="https://pic3.zhimg.com/v2-4613bc3df9c7e4d064fe5a0b8c66eca2_b.jpg" alt=""></div><p>作者：satoru</p><p>在 <a href="https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-binlog-source-code-reading-3/" class=" wrap external" target="_blank" rel="nofollow noreferrer">上篇文章</a> 中，我们介绍了 TiDB 如何通过 Pump client 将 binlog 发往 Pump，本文将继续介绍 Pump server 的实现，对应的源码主要集中在 TiDB Binlog 仓库的 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go" class=" wrap external" target="_blank" rel="nofollow noreferrer">pump/server.go</a></code> 文件中。</p><h2>启动 Pump Server</h2><p>Server 的启动主要由两个函数实现：<code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L106" class=" wrap external" target="_blank" rel="nofollow noreferrer">NewServer</a></code> 和 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L317" class=" wrap external" target="_blank" rel="nofollow noreferrer">(*Server).Start</a></code>。</p><p><code>NewServer</code> 依照传入的配置项创建 Server 实例，初始化 Server 运行所必需的字段，以下简单说明部分重要字段：</p><ol><li><code>metrics</code>：一个 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pkg/util/p8s.go%23L36" class=" wrap external" target="_blank" rel="nofollow noreferrer">MetricClient</a></code>，用于定时向 Prometheus Pushgateway 推送 metrics。</li><li><code>clusterID</code>：每个 TiDB 集群都有一个 ID，连接到同一个 TiDB 集群的服务可以通过这个 ID 识别其他服务是否属于同个集群。</li><li><code>pdCli</code>：<a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/pd" class=" wrap external" target="_blank" rel="nofollow noreferrer">PD</a> Client，用于注册、发现服务，获取 Timestamp Oracle。</li><li><code>tiStore</code>：用于连接 TiDB storage engine，在这里主要用于查询事务相关的信息（可以通过 TiDB 中的对应 <a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/blob/v3.0.1/kv/kv.go%23L259" class=" wrap external" target="_blank" rel="nofollow noreferrer">interface 描述</a> 了解它的功能）。</li><li><code>storage</code>：Pump 的存储实现，从 TiDB 发过来的 binlog 就是通过它保存的，下一篇文章将会重点介绍。</li></ol><p>Server 初始化以后，就可以用 <code>(*Server).Start</code> 启动服务。为了避免丢失 binlog，在开始对外提供 binlog 写入服务之前，<a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L323-L337" class=" wrap external" target="_blank" rel="nofollow noreferrer">它会将当前 Server 注册到 PD 上，确保所有运行中的 Drainer 都已经观察到新增的 Pump 节点</a>。这一步除了启动对外的服务，还开启了一些 Pump 正常运作所必须的辅助机制，下文会有更详细的介绍。</p><h2>Pump Server API</h2><p>Pump Server 通过 gRPC 暴露出一些服务，这些接口定义在 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tipb/blob/master/go-binlog/pump.pb.go%23L312" class=" wrap external" target="_blank" rel="nofollow noreferrer">tipb/pump.pb.go</a></code>，包含两个接口 <code>WriteBinlog</code>、 <code>PullBinlogs</code>。</p><h3>WriteBinlog</h3><p>顾名思义，这是用于写入 binlog 的接口，上篇文章中 Pump client 调用的就是这个。客户端传入的请求，是以下的格式：</p><div class="highlight"><pre><code class="language-text">type WriteBinlogReq struct {
  // The identifier of tidb-cluster, which is given at tidb startup.
  // Must specify the clusterID for each binlog to write.
  ClusterID uint64 `protobuf:&#34;varint,1,opt,name=clusterID,proto3&#34; json:&#34;clusterID,omitempty&#34;`
  // Payload bytes can be decoded back to binlog struct by the protobuf.
  Payload []byte `protobuf:&#34;bytes,2,opt,name=payload,proto3&#34; json:&#34;payload,omitempty&#34;`
}</code></pre></div><p>其中 <code>Payload</code> 是一个用 <code>Protobuf</code> 序列化的 <a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tipb/blob/master/go-binlog/binlog.pb.go%23L223" class=" wrap external" target="_blank" rel="nofollow noreferrer">binlog</a>，WriteBinlog 的 <a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L213-L227" class=" wrap external" target="_blank" rel="nofollow noreferrer">主要流程</a> 就是将请求中的 <code>Payload</code> 解析成 binlog 实例，然后调用 <code>storage.WriteBinlog</code> 保存下来。<code>storage.WriteBinlog</code> 将 binlog 持久化存储，并对 binlog 按 <code>start TS</code> / <code>commit TS</code> 进行排序，详细的实现将在下章展开讨论。</p><h3>PullBinlogs</h3><p>PullBinlogs 是为 Drainer 提供的接口，用于按顺序获取 binlog。这是一个 streaming 接口，客户端请求后得到一个 stream，可以从中不断读取 binlog。请求的格式如下：</p><div class="highlight"><pre><code class="language-text">type PullBinlogReq struct {
  // Specifies which clusterID of binlog to pull.
  ClusterID uint64 `protobuf:&#34;varint,1,opt,name=clusterID,proto3&#34; json:&#34;clusterID,omitempty&#34;`
  // The position from which the binlog will be sent.
  StartFrom Pos `protobuf:&#34;bytes,2,opt,name=startFrom&#34; json:&#34;startFrom&#34;`
}

// Binlogs are stored in a number of sequential files in a directory.
// The Pos describes the position of a binlog.
type Pos struct {
  // The suffix of binlog file, like .000001 .000002
  Suffix uint64 `protobuf:&#34;varint,1,opt,name=suffix,proto3&#34; json:&#34;suffix,omitempty&#34;`
  // The binlog offset in a file.
  Offset int64 `protobuf:&#34;varint,2,opt,name=offset,proto3&#34; json:&#34;offset,omitempty&#34;`
}</code></pre></div><p>从名字可以看出，这个请求指定了 Drainer 要从什么时间点的 binlog 开始同步。虽然 Pos 中有 <code>Suffix</code> 和 <code>Offset</code> 两个字段，目前只有 <code>Offset</code> 字段是有效的，我们把它用作一个 <code>commit TS</code>，表示只拉取这个时间以后的 binlog。</p><p>PullBinlogs 的 <a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L275-L286" class=" wrap external" target="_blank" rel="nofollow noreferrer">主要流程</a>，是调用 <code>storage.PullCommitBinlogs</code> 得到一个可以获取序列化 binlog 的 channel，将这些 binlog 通过 <code>stream.Send</code> 接口逐个发送给客户端。</p><h2>辅助机制</h2><p>上文提到 Pump 的正常运作需要一些辅助机制，本节将逐一介绍这些机制。</p><h3>fake binlog</h3><p>在 <a href="https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/" class=" wrap external" target="_blank" rel="nofollow noreferrer">《TiDB-Binlog 架构演进与实现原理》</a> 一文中，对 fake binlog 机制有以下说明：</p><blockquote>“Pump 会定时（默认三秒）向本地存储中写入一条数据为空的 binlog，在生成该 binlog 前，会向 PD 中获取一个 tso，作为该 binlog 的 <code>start_ts</code> 与 <code>commit_ts</code>，这种 binlog 我们叫作 fake binlog。<br/>……Drainer 通过如上所示的方式对 binlog 进行归并排序，并推进同步的位置。那么可能会存在这种情况：某个 Pump 由于一些特殊的原因一直没有收到 binlog 数据，那么 Drainer 中的归并排序就无法继续下去，正如我们用两条腿走路，其中一只腿不动就不能继续前进。我们使用 Pump 一节中提到的 fake binlog 的机制来避免这种问题，Pump 每隔指定的时间就生成一条 fake binlog，即使某些 Pump 一直没有数据写入，也可以保证归并排序正常向前推进。”</blockquote><p><code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L460" class=" wrap external" target="_blank" rel="nofollow noreferrer">genForwardBinlog</a></code> 实现了这个机制，它里面是一个定时循环，每隔一段时间（默认 3 秒，可通过 <code>gen-binlog-interval</code> 选项配置）检查一下是否有新的 binlog 写入，如果没有，就调用 <code>writeFakeBinlog</code> 写一条假的 binlog。</p><p>判断是否有新的 binlog 写入，是通过 <code>lastWriteBinlogUnixNano</code> 这个变量，每次有新的写入都会 <a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L193" class=" wrap external" target="_blank" rel="nofollow noreferrer">将这个变量设置为当前时间</a>。</p><h3>垃圾回收</h3><p>由于存储容量限制，显然 Pump 不能无限制地存储收到的 binlog，因此需要有一个 GC (Garbage Collection) 机制来清理没用的 binlog 释放空间，<code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L527" class=" wrap external" target="_blank" rel="nofollow noreferrer">gcBinlogFile</a></code> 就负责 GC 的调度。有两个值会影响 GC 的调度：</p><ol><li><code>gcInterval</code>：控制 GC 检查的周期，目前写死在代码里的设置是 <a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L56" class=" wrap external" target="_blank" rel="nofollow noreferrer">1 小时</a></li><li><code>gcDuration</code>：binlog 的保存时长，每次 GC 检查就是 <a href="&lt;code">&#34;https://github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go#L544-L545&#34;&gt;通过当前时间和 gcDuration 计算出 GC 时间点</a>，在这个时间点之前的 binlog 将被 GC 在 <code>gcBinlogFile</code> 的循环中，用 select 监控着 3 种情况：<br/>select { case &lt;-s.ctx.Done(): <a href="https://link.zhihu.com/?target=http%3A//log.Info" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">log.Info</span><span class="invisible"></span></a>(&#34;gcBinlogFile exit&#34;) return case &lt;-s.triggerGC: <a href="https://link.zhihu.com/?target=http%3A//log.Info" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">log.Info</span><span class="invisible"></span></a>(&#34;trigger gc now&#34;) case &lt;-time.After(gcInterval): }</li></ol><p>3 个 case 分别对应：server 退出，外部触发 GC，定时检查这三种情况。其中 server 退出的情况我们直接退出循环。另外两种情况都会继续，计算 GC 时间点，交由 <code>storage.GC</code> 执行。</p><h3>Heartbeat</h3><p>心跳机制用于定时（默认两秒）向 PD 发送 Server 最新状态，由 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/node.go%23L211" class=" wrap external" target="_blank" rel="nofollow noreferrer">(*pumpNode).HeartBeat</a></code> 实现。状态是由 JSON 编码的 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pkg/node/node.go%23L84" class=" wrap external" target="_blank" rel="nofollow noreferrer">Status</a></code> 实例，主要记录 <code>NodeID</code>、<code>MaxCommitTS</code> 之类的信息。</p><h2>HTTP API 实现</h2><p>Pump Server 通过 HTTP 方式暴露出一些 API，主要提供运维相关的接口。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-af38acfc360e589fc573ca66844fc4ef_b.jpg" data-caption="" data-size="normal" data-rawwidth="1468" data-rawheight="594" class="origin_image zh-lightbox-thumb" width="1468" data-original="https://pic4.zhimg.com/v2-af38acfc360e589fc573ca66844fc4ef_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-af38acfc360e589fc573ca66844fc4ef_b.jpg" data-caption="" data-size="normal" data-rawwidth="1468" data-rawheight="594" class="origin_image zh-lightbox-thumb lazy" width="1468" data-original="https://pic4.zhimg.com/v2-af38acfc360e589fc573ca66844fc4ef_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-af38acfc360e589fc573ca66844fc4ef_b.jpg"/></figure><h2>下线 Pump Server</h2><p>下线一个 Pump server 的流程通常由 <code>binlogctl</code> 命令发起，例如：</p><div class="highlight"><pre><code class="language-text">bin/binlogctl -pd-urls=localhost:2379 -cmd offline-pump -node-id=My-Host:8240</code></pre></div><p><code>binlogctl</code> 先通过 <code>nodeID</code> 在 PD 发现的 Pump 节点中找到指定的节点，然后调用上一小节中提到的接口 <code>PUT /state/{nodeID}/close</code>。</p><p>在 Server 端，<code>ApplyAction</code> 收到 close 后会将节点状态置为 Closing（Heartbeat 进程会定时将这类状态更新到 PD），然后另起一个 goroutine 调用 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L834" class=" wrap external" target="_blank" rel="nofollow noreferrer">Close</a></code>。<code>Close</code> 首先调用 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L121" class=" wrap external" target="_blank" rel="nofollow noreferrer">cancel</a></code>，通过 <code>context</code> 将关停信号发往协作的 goroutine，这些 goroutine 主要就是上文提到的辅助机制运行的 goroutine，例如在 <code>genForwardBinlog</code> 中设计了在 <code>context</code> 被 cancel 时退出：</p><div class="highlight"><pre><code class="language-text">for {
  select {
  case &lt;-s.ctx.Done():
     log.Info(&#34;genFakeBinlog exit&#34;)
     return</code></pre></div><p><code>Close</code> 用 <code>waitGroup</code> 等待这些 goroutine 全部退出。这时 Pump 仍然能正常提供 PullBinlogs 服务，但是写入功能 <a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L221" class=" wrap external" target="_blank" rel="nofollow noreferrer">已经停止</a>。<code>Close</code> 下一行调用了 <code>commitStatus</code>，这时节点的状态是 Closing，对应的分支调用了 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.1/pump/server.go%23L769" class=" wrap external" target="_blank" rel="nofollow noreferrer">waitSafeToOffline</a></code>来确保到目前为止写入的 binlog 都已经被所有的 Drainer 读到了。<code>waitSafeToOffline</code> 先往 storage 中写入一条 fake binlog，由于此时写入功能已经停止，可以确定这将是这个 Pump 最后的一条 binlog。之后就是在循环中定时检查所有 Drainer 已经读到的 Binlog 时间信息，<a href="https://link.zhihu.com/?target=https%3A//github.c%3Ccode%3Eom/pingc%3C/code%3Eap/tidb-binlog/blob/v3.0.1/pump/server.go%23L795" class=" wrap external" target="_blank" rel="nofollow noreferrer">直到这个时间已经大于 fake binlog 的 CommitTS</a>。</p><p><code>waitSafeToOffline</code> 等待结束后，就可以关停 gRPC 服务，释放其他资源。</p><h2>小结</h2><p>本文介绍了 Pump server 的启动、gRPC API 实现、辅助机制的设计以及下线服务的流程，希望能帮助大家在阅读源码时有一个更清晰的思路。在上面的介绍中，我们多次提到 <code>storage</code> 这个实体，用来存储和查询 binlog 的逻辑主要封装在这个模块内，这部分内容将在下篇文章为大家作详细介绍。</p><p><b>原文阅读：</b></p><a href="https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-binlog-source-code-reading-4/" data-draft-node="block" data-draft-type="link-card" data-image="https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg" data-image-width="1200" data-image-height="1200" class=" wrap external" target="_blank" rel="nofollow noreferrer">TiDB Binlog 源码阅读系列文章（四）Pump server 介绍 | PingCAP</a><p><b>更多 TiDB Binlog 源码阅读：</b></p><a href="https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23TiDB-Binlog-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB" data-draft-node="block" data-draft-type="link-card" data-image="https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg" data-image-width="1200" data-image-height="1200" class=" wrap external" target="_blank" rel="nofollow noreferrer">Blog-cns | PingCAP</a><p></p>