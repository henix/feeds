<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Sat, 26 Jan 2019 04:16:03 +0800</lastBuildDate>
<item>
<title>刘奇：我们最喜欢听用户说的话是「你们搞得定吗？」 | TiDB DevCon 2019</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-25-55728943.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/55728943&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-28382092f2192a8a933108bcc8a73c23_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;1 月 19 日 &lt;b&gt;TiDB DevCon 2019&lt;/b&gt; 在北京圆满落幕，&lt;b&gt;超过 750 位&lt;/b&gt;热情的社区伙伴参加了此次大会。在本次大会上，我们首次全面展示了全新存储引擎 Titan、新生态工具 TiFlash 以及 TiDB 在云上的进展，同时宣布 TiDB-Lightning Toolset &amp;amp; TiDB-DM 两大生态工具开源，并分享了  TiDB 3.0 的特性与未来规划，描述了我们眼中未来数据库的模样。&lt;br&gt;此外，更有 &lt;b&gt;11 位&lt;/b&gt;来自一线的 TiDB 用户为大家分享了实践经验与踩过的「坑」，获得了现场观众的阵阵掌声。同时，我们也为新晋 TiDB Committer 授予了证书，并为 2018 年最佳社区贡献个人、最佳社区贡献团队颁发了荣誉奖杯。&lt;br&gt;我们将挑选部分精彩实录分享给大家，敬请期待哦～&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-be16a754f66708ffcf2d99eff6b6d586_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1920&quot; data-rawheight=&quot;1280&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1920&quot; data-original=&quot;https://pic3.zhimg.com/v2-be16a754f66708ffcf2d99eff6b6d586_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-be16a754f66708ffcf2d99eff6b6d586_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1920&quot; data-rawheight=&quot;1280&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1920&quot; data-original=&quot;https://pic3.zhimg.com/v2-be16a754f66708ffcf2d99eff6b6d586_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-be16a754f66708ffcf2d99eff6b6d586_b.jpg&quot;&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3b56b71eae061d20cf43f10b381b9db7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2000&quot; data-rawheight=&quot;1500&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;2000&quot; data-original=&quot;https://pic4.zhimg.com/v2-3b56b71eae061d20cf43f10b381b9db7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3b56b71eae061d20cf43f10b381b9db7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2000&quot; data-rawheight=&quot;1500&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;2000&quot; data-original=&quot;https://pic4.zhimg.com/v2-3b56b71eae061d20cf43f10b381b9db7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-3b56b71eae061d20cf43f10b381b9db7_b.jpg&quot;&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5e2723a6626816bff697648396b4f3d7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1620&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1620&quot; data-original=&quot;https://pic4.zhimg.com/v2-5e2723a6626816bff697648396b4f3d7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5e2723a6626816bff697648396b4f3d7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1620&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1620&quot; data-original=&quot;https://pic4.zhimg.com/v2-5e2723a6626816bff697648396b4f3d7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-5e2723a6626816bff697648396b4f3d7_b.jpg&quot;&gt;&lt;figcaption&gt;开场前十分钟，场内座位全部坐满&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;以下为我司 CEO 刘奇在 TiDB DevCon 2019 上的 Opening Keynote 实录。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-916bc91a394589cc09b1620bda54aff4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1920&quot; data-rawheight=&quot;1280&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1920&quot; data-original=&quot;https://pic1.zhimg.com/v2-916bc91a394589cc09b1620bda54aff4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-916bc91a394589cc09b1620bda54aff4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1920&quot; data-rawheight=&quot;1280&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1920&quot; data-original=&quot;https://pic1.zhimg.com/v2-916bc91a394589cc09b1620bda54aff4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-916bc91a394589cc09b1620bda54aff4_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;首先我想特别感谢每一位来参加 TiDB DevCon 2019 的 Contributor 和用户，还有对 TiDB 保持好奇的人。今天我主要想跟大家分享一下我们过去一年的一些发展情况，以及我们对于未来的一些想法。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Growth&lt;/b&gt;&lt;/h2&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-562624cde1e3bf8264ad479447a87f09_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-562624cde1e3bf8264ad479447a87f09_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-562624cde1e3bf8264ad479447a87f09_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-562624cde1e3bf8264ad479447a87f09_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-562624cde1e3bf8264ad479447a87f09_b.jpg&quot;&gt;&lt;figcaption&gt;图 1 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;从图 1 大家可以很清楚的看到 TiDB 在过去一年的增长。如果大家去对比一下 TiDB 增长曲线和其他同类产品、或者是上一代 NoSQL 产品的增长曲线会发现，TiDB 是遥遥领先的。看完我们的 Contributor 增长和我们在 GitHub 上面的各种状态，在这里也特别感谢我们所有的那些正在使用 TiDB 的用户。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d0e398e9ae2f002b0847ebfd6149315_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5d0e398e9ae2f002b0847ebfd6149315_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d0e398e9ae2f002b0847ebfd6149315_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5d0e398e9ae2f002b0847ebfd6149315_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5d0e398e9ae2f002b0847ebfd6149315_b.jpg&quot;&gt;&lt;figcaption&gt;图 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图 2 是过去一年，我们用户自己去分享自己使用 TiDB 的一些经验。我记得我们在筹办这个会的时候，我说我有特别多想讲的东西，掏心窝子的话特别多，能不能让我多讲讲。我们市场的同学不太同意，说我们只有一天时间，我们应该把更多的时间交给我们用户，让他们来分享他们自己的经验，交给在一线的同学。大家如果特别有兴趣的话，可以去翻一翻我们用户使用 TiDB 的一些经验（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/cases-cn/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;），里面有一些他们的踩坑经验，也有一些他们比较欣慰的点，还有一些用户吐槽的东西，所以我们在 2018 年年底的时候，搞了一次吐槽大会，请几个用户过去疯狂吐槽一下我们的产品。我们定了几个原则，比如，只允许说缺点，不允许说优点。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-334d3d76c42b5dae0e2b696c37a5cf12_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-334d3d76c42b5dae0e2b696c37a5cf12_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-334d3d76c42b5dae0e2b696c37a5cf12_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-334d3d76c42b5dae0e2b696c37a5cf12_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-334d3d76c42b5dae0e2b696c37a5cf12_b.jpg&quot;&gt;&lt;figcaption&gt;图 3 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这是海外的一些媒体对我们的报道（图 3），大家可能也知道，我们去年拿了 InfoWorld 评选的 Bossie Awards 最佳开源软件奖，接下来的分享 Morgan 会介绍我们在海外的一些发展情况和我们的海外团队。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;HTAP Rocks!&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在过去一年，我们最喜欢听用户讲的一句话是什么？&lt;b&gt;我们最喜欢听的一句话是：你们搞得定吗？&lt;/b&gt;我觉得这句话太好了，很多时候，我们突然会去跟用户去讲，你这是 OLAP，你这是 OLTP。其实用户关心的是，你能不能搞定我的问题，而不是说你过来派了一堆专家，告诉我该怎么干。&lt;/p&gt;&lt;p&gt;在过去一年里，用户在用 TiDB 的过程中，也会遇到很多的问题。比如说，OLTP 和 OLAP 的隔离怎么去做。&lt;b&gt;所以我们在今年启用了一个全新的 Design，在这个 Design 里面是彻底地隔离 OLAP 和 OLTP 的 Workload。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们曾经见到很多争论，也见到很多论文，说到底是行存好，还是列存好。如果大家去看知乎的话，这个讨论现在还没有休止：到底数据库是应该使用行存还是使用列存。而在我们现在的 Design 里面，我们会搁置这个争议——为什么我们不能都有？&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8fc533e31fab5dd7221912a7dfa451d5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-8fc533e31fab5dd7221912a7dfa451d5_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8fc533e31fab5dd7221912a7dfa451d5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-8fc533e31fab5dd7221912a7dfa451d5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-8fc533e31fab5dd7221912a7dfa451d5_b.jpg&quot;&gt;&lt;figcaption&gt;图 4 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;大家想一想，在多少年前，我们家里面还有固定电话，我们还在看纸质书，我们听歌用 MP3，我们可能想看一下自己今天跑了多少步，还得用一个专门的硬件或者运动设备。但是到今天，一部手机就搞定这一切。&lt;/p&gt;&lt;p&gt;在之前，我们可能线上需要用 MySQL，或者用 PG，中间我们可能需要用消息队列，去把 binlog 或者 change feeds 都给弄出来，然后再弄到一个 Data ware house 里面，在 Data ware house 里面去  Run。最终我们丧失了实时性，我们丧失了一致性。但是如果我们重新去想一下这个事情，这事儿就像当初的手机和 MP3、纸质书一样的。&lt;b&gt;到今天技术进步到一定程度的时候，为什么我不能 OLTP/OLAP All in one ，我只是一个普通的用户，我不想接受那么一堆东西，同时我要实时性，我现在要的，马上就要。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-77ed7f259a529b5dfc39a329e5e84c57_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-77ed7f259a529b5dfc39a329e5e84c57_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-77ed7f259a529b5dfc39a329e5e84c57_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-77ed7f259a529b5dfc39a329e5e84c57_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-77ed7f259a529b5dfc39a329e5e84c57_b.jpg&quot;&gt;&lt;figcaption&gt;图 5&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当然大的氛围下面，吹牛的很多，但如果我不知道他是怎么 Work 的，通常我是不太放心的，所以我们用一个简单的图（图 5）说一下，到底 OLAP 和 OLTP 的 Workload 是怎么隔离的。 在我们全新的 Design 里面，TiDB 的 engine——TiKV  ，但是我们通过 Raft 协议，通过 learner 把数据复制出来一份，这份协议是实时通过 Raft 做复制，但是用列式来存储。&lt;b&gt;如果我们的优化器变得更加聪明，当一个查询过来的时候，它不用再去纠结，而是会根据这个 Query 的特点、自动根据这个 SQL 去选择到底是使用行存，还是使用列存，还是一部分使用行存，一部分使用列存，这样就会带来很多额外的好处。&lt;/b&gt;在这个图上（图 5）可以看到，Workload 是整个物理上是隔离的，是完全跑在不同的 Server 上面的。&lt;/p&gt;&lt;p&gt;这样带来的好处就非常明显。我们就能够同时去 Join 两个不同格式的数据，同时能得到所有的 OLAP 和 OLTP 的系统的好处，能得到一个更神奇的结果，就是它可以比 OLTP 系统跑的快；你可以在一个 OLTP 的系统，在传统上面不可想象的、在下面去跑一个报表。&lt;b&gt;所以今天我们也非常高兴的去向大家推出我们新的 Design 和对应的产品，这个产品叫 TiFlash&lt;/b&gt;。看过美剧 The Flash 的同学知道闪电侠，这个名称是为了形容它的速度，因为它又是 TiDB 的 Ti 系列家族中的一员，所以我们给他取名叫 TiFlash，下午会有一个非常非常 Amazing 的环节会去展示整个 TiFlash。大家可以保持期待，这个一定不会让大家失望。我昨天看了一下演示，非常震撼。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 3.0 Beta &lt;/b&gt;&lt;/h2&gt;&lt;p&gt;有关注我们微信公众号的同学会发现，在今天早上（1 月 19 日）我们发布了 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/docs-cn/releases/3.0beta/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;3.0 Bata 版本&lt;/a&gt;&lt;/u&gt;，在 3.0 里面，我们发布了大量的新特性，比如去年在 DevCon 上面，我承诺给大家的，我们会&lt;b&gt;支持 Window Fuction、支持 View、支持 Partition，这些东西现在统统都有了&lt;/b&gt;。同时我们还有一些新的东西是之前没有的，比如说 &lt;b&gt;Plan binding&lt;/b&gt;，就是绑定执行计划，这里也是特别感谢美团同学的 Contribution，让我们能够支持到新的特性。这些特性，稍后申砾老师会给大家分享详细的细节，这边我就先跳过。（申砾老师的演讲实录正在整理中，后续会分享给大家～）&lt;/p&gt;&lt;p&gt;同时在 3.0 里面，我们还做了大量的改进。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1e7eb99f2c428056080dca50a970a0d3_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-1e7eb99f2c428056080dca50a970a0d3_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1e7eb99f2c428056080dca50a970a0d3_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-1e7eb99f2c428056080dca50a970a0d3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-1e7eb99f2c428056080dca50a970a0d3_b.jpg&quot;&gt;&lt;figcaption&gt;图 6 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;大家知道，过去一年有那么多 TiDB 用户，其实他们也有头疼的地方。就是 TiDB 的执行计划跟 TiDB 的统计信息是高度相关的，有时候会遇到执行计划产生变化。所以 2019 年的 Q1，我们将会花大量的时间，去让这个执行计划变的更加稳定。 同时为了便于大家去查看这些慢查询，我们做了一个非常漂亮的 &lt;b&gt;Query Tracing&lt;/b&gt; 的界面，上午申砾的分享也会去介绍这个非常漂亮的界面，让大家看到，一个复杂的 Query 下去，最终在每一步耗了多长时间，还有个非常漂亮的树形图。&lt;/p&gt;&lt;p&gt;&lt;b&gt;然后我们也解决了过去一年，我们 Raft Store 是一个单线程的问题。&lt;/b&gt;我觉得这个需要消耗大量的时间和精力。我记得我们当初做 Region split 的时候好像没花多久，分裂可能做一个月，然后 merge 做了一年，多线程这个也差不多做了一年。&lt;/p&gt;&lt;p&gt;前一阵大家可能也知道业内出现过删库跑路的事情。当时我们也非常震惊，我就想从我们的层面上能做哪些事情？所以，&lt;b&gt;我们在 3.0 里面提供了一个新的功能，叫 Admin restore table，&lt;/b&gt;如果你一不小心把一个数据库，或者把一个 table 给删了，只要还没有对数据做垃圾收集、没有彻底丧失之前，你还可以一个命令，马上恢复这个数据库。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5cd48855a5724e8fd92e3efe0e1ad81d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5cd48855a5724e8fd92e3efe0e1ad81d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5cd48855a5724e8fd92e3efe0e1ad81d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5cd48855a5724e8fd92e3efe0e1ad81d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5cd48855a5724e8fd92e3efe0e1ad81d_b.jpg&quot;&gt;&lt;figcaption&gt;图 7 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当然通常聊到一个新版本的时候，大家最关心的就是，不服跑个分。所以呢，我们也在最简单最基础的环境下跑了个分，图 7 是 3.0 版本与 2.1 版本的对比。大家知道我们在前不久发布了 2.1，大家可以看到，&lt;b&gt;整体的 Performance 的提升非常的明显，基本上直接 Double 了。&lt;/b&gt;大家在实测的过程中，应该会测出比 Double 更高的性能。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3e1a1d3e09f428fe70222ca2883b1ae3_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-3e1a1d3e09f428fe70222ca2883b1ae3_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3e1a1d3e09f428fe70222ca2883b1ae3_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-3e1a1d3e09f428fe70222ca2883b1ae3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-3e1a1d3e09f428fe70222ca2883b1ae3_b.jpg&quot;&gt;&lt;figcaption&gt;图 8&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;当然这个 Performance 的提升，里面有很大一部分是我们一个新的 Storage 的贡献。新的 Storage 叫 Titan。&lt;/b&gt;我们也是非常有意思的和美图基于 TiKV 开发的一个 Redis 的实现，使用了一样的名字。大家对于这个希腊神话的热爱，其实是一样的。程序员在选名字的时候，也都有自己的特点，所以大家就重名了，重名之后，我们还讨论了一下，觉得这个名字要不要考虑改一下，后来大家觉得既然都挺喜欢，要不然都用这个吧，我们觉得这也挺好。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-61ce2d53a80dd83226c58df58482c7ce_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-61ce2d53a80dd83226c58df58482c7ce_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-61ce2d53a80dd83226c58df58482c7ce_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-61ce2d53a80dd83226c58df58482c7ce_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-61ce2d53a80dd83226c58df58482c7ce_b.jpg&quot;&gt;&lt;figcaption&gt;图 9&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;然后整个新的存储引擎的 Design 是这样（图 9），我们把 Key 和 Value 做了分离。大家知道，去年我们在做论文分享的时候，有一次专门分享了 《&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247486286%26idx%3D1%26sn%3Deb25fa3a77cb8da74f014258396820b8%26chksm%3Deb162c24dc61a5328f373a3a7fde4baf2280cb28e0500f14fa1a653210d4ecefddebdd06cce1%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;WiscKey: Separating Keys from Values in SSD-conscious Storage&lt;/a&gt;&lt;/u&gt;》 这篇论文，也是非常感谢这篇论文。Titan 整体上是基于 RocksDB 去做的一个修改或者是一个优化，更多的是在 RocksDB 的外围实现了 Key Value 分离，主要是适应于更大的 Value。&lt;/p&gt;&lt;p&gt;&lt;b&gt;下面是 Titan 的 Performance 跑分。大家看到整体的提升都会非常的明显，从两倍到 N 倍吧，这个 N 的多少，取决于 Value 最终有多大，Value 越大的话，N 会越大&lt;/b&gt;（延伸阅读：《&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/titan-design-and-implementation/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Titan 的设计与实现&lt;/a&gt;&lt;/u&gt;》）&lt;b&gt;。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5ee464583a28e2c48a5c71ff357cb18f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;712&quot; data-rawheight=&quot;283&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;712&quot; data-original=&quot;https://pic4.zhimg.com/v2-5ee464583a28e2c48a5c71ff357cb18f_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5ee464583a28e2c48a5c71ff357cb18f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;712&quot; data-rawheight=&quot;283&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;712&quot; data-original=&quot;https://pic4.zhimg.com/v2-5ee464583a28e2c48a5c71ff357cb18f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-5ee464583a28e2c48a5c71ff357cb18f_b.jpg&quot;&gt;&lt;figcaption&gt;图 10 Titan Data Loading Performance &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3864cfb47089804ee123411923513bcd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;760&quot; data-rawheight=&quot;304&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;760&quot; data-original=&quot;https://pic2.zhimg.com/v2-3864cfb47089804ee123411923513bcd_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3864cfb47089804ee123411923513bcd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;760&quot; data-rawheight=&quot;304&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;760&quot; data-original=&quot;https://pic2.zhimg.com/v2-3864cfb47089804ee123411923513bcd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-3864cfb47089804ee123411923513bcd_b.jpg&quot;&gt;&lt;figcaption&gt;图 11  Titan Update Performance &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-36c5f6c982ff3e29c97cf7eee3b931fe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-36c5f6c982ff3e29c97cf7eee3b931fe_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-36c5f6c982ff3e29c97cf7eee3b931fe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-36c5f6c982ff3e29c97cf7eee3b931fe_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-36c5f6c982ff3e29c97cf7eee3b931fe_b.jpg&quot;&gt;&lt;figcaption&gt;图 12  Titan Random Key Lookup Performance &lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-703d920f0b0cb74cb56172aa96df86a5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-703d920f0b0cb74cb56172aa96df86a5_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-703d920f0b0cb74cb56172aa96df86a5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-703d920f0b0cb74cb56172aa96df86a5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-703d920f0b0cb74cb56172aa96df86a5_b.jpg&quot;&gt;&lt;figcaption&gt;图 13  Titan Sorted Range Performance &lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;TiDB on Cloud&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;说了这么多，那么在一个云的时代我们到底是怎样去拥抱云的。&lt;/p&gt;&lt;p&gt;大家知道 TiDB 在最初 Design 的时候，就是为 Cloud 做了大量的优化，同时在三年前我们就相信 Kubernetes 是未来，然后 TiDB 整个就 All-in Kubernetes 了。所以我们一直在等着 Cloud 出一个功能，就是 Cloud 能不能够支持 Native  Kubernetes  Engine，后来我们看到了 Google 发布了他们的 Kubernetes  Engine。&lt;b&gt;所以我们第一时间和 Google 的 K8s 做了一个集成，同时大家现在也可以去访问 Google 云平台（Google Cloud Platform），去试用我们的产品&lt;/b&gt;（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/tidb-cloud/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/tidb-cloud/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;）&lt;b&gt;，在那上面真的是一键就可以跑起一个集群，然后都可以由我们来 maintain 整个 TiDB，相当于我们现在有一个 TiDB On Cloud。&lt;/b&gt;接下来也会支持 AWS 和 Azure。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-40e78b8e8f3cb98dc026845fcbe2b490_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-40e78b8e8f3cb98dc026845fcbe2b490_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-40e78b8e8f3cb98dc026845fcbe2b490_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-40e78b8e8f3cb98dc026845fcbe2b490_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-40e78b8e8f3cb98dc026845fcbe2b490_b.jpg&quot;&gt;&lt;figcaption&gt;图 14&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其实之前有部分同学都提过，TiDB  做得挺好的为什么不做一套漂亮的界面，然后它的易用性会更佳，更重要的是支持多租户。美团今天也会分享他们在使用 TiDB 的经验，当我们一个集群，两个集群，十个集群，二十个集群，一百个集群的时候怎么办，那么多集群，我怎么用一个简单的方式去维护，那这个时候就需要一套&lt;b&gt;Database as Service&lt;/b&gt;的东西，能够去帮我管理整个公司的所有 TiDB 集群。所以对于多租户的支持就变得非常有用。同时也会做自动的 Backup 和 Recover。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What’s Next&lt;/b&gt;&lt;/h2&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-57d2063136de80067aa11f474335a2a4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-57d2063136de80067aa11f474335a2a4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-57d2063136de80067aa11f474335a2a4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-57d2063136de80067aa11f474335a2a4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-57d2063136de80067aa11f474335a2a4_b.jpg&quot;&gt;&lt;figcaption&gt;图 15&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;那我们下一步会有什么不一样的地方？我们刚才提到 3.0 版本有这么多让人非常兴奋的功能，有这么多的巨大改进，什么时候能够把他用到产品里面，是大家接下来关心的一个问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;首先我们会在今年的 6 月份发布第一个 3.0 的 GA。&lt;/b&gt;目前正在不同的用户场景下做大量的测试，通过不同的 Workload  做测试。&lt;/p&gt;&lt;p&gt;另外，大家知道，我们去年写了一个 24 章经——就是 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读系列文章&lt;/a&gt;，我们写了 24 篇，如果熟悉金庸先生的话应该知道 42 章经，&lt;b&gt;今年我们开始为 TiKV 准备 24 章经，会去详细解读 TiKV 源码的实现。&lt;/b&gt;著名 IT 作家、译者侯捷大师说：「源码面前，了无秘密」。我希望大家对于 TiDB 的理解能够深入骨髓。能够自己随意去 Hack 我们的东西，能为整个 TiDB Community 贡献更多东西。&lt;/p&gt;&lt;p&gt;&lt;b&gt;同时我们也会提供更加智能的、基于机器学习的功能。&lt;/b&gt;如果大家之前有关注我们的&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D1%26sn%3D72d9d52558e83eb97cd709c67b5a4149%26chksm%3Deb1628e0dc61a1f60bb99ffe2fe42fafe91570159094fc5e3d46039b5490bd0c391ee500b8d6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;黑客马拉松&lt;/a&gt;&lt;/u&gt;，会发现我们实现第一个 prototype，是用贝叶斯模型做智能的热点的调度。大家以后应该会跟“人工看热点调度，再人工 split ”这事儿 say goodbye 了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后，当我们有大量的用户，有大量的使用场景，有大量的经验的时候，我们需要一个更加强大的 Community 和一个更加强大的 Ecosystem。&lt;/b&gt;今天崔秋老师也会去讲我们整个 Community 的运转并为新晋 Committer 授予证书。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;社区的相聚让我们度过了兴奋而充实的一天，感谢大家对 TiDB 社区的贡献和热情，未来我们继续携手同行！社区实践专场和 Lighting Talk 环节的部分 PPT&lt;/b&gt; &lt;b&gt;可以在微信公号（PingCAP）后台回复“2019”获取。&lt;/b&gt;&lt;br&gt;TiDB DevCon 是 PingCAP 团队面向 TiDB 社区推出的技术会议。 本届 TiDB DevCon 2019 以 “Powered by Contributors” 为主题，聚焦 TiDB 项目核心技术的最新进展和未来规划，以及来自社区一线用户的最佳实践经验，展示 TiDB 在海内外的最新动态。旨在帮助社区更好的理解 TiDB 的技术理念，汇总用户从技术选型到应用落地各阶段的实操中总结出的经验和坑，挖掘 TiDB 场景适配的更多可能性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-25-55728943</guid>
<pubDate>Fri, 25 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>Titan 的设计与实现</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-23-55521489.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/55521489&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-579b54e6a23d0d19a9cd2950355a72af_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：郑志铨&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/rocksdb/tree/titan-5.15&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Titan&lt;/a&gt; 是由 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PingCAP&lt;/a&gt; 研发的一个基于 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/facebook/rocksdb&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RocksDB&lt;/a&gt; 的高性能单机 key-value 存储引擎，其主要设计灵感来源于 USENIX FAST 2016 上发表的一篇论文 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;WiscKey&lt;/a&gt;。&lt;code&gt;WiscKey&lt;/code&gt; 提出了一种高度基于 SSD 优化的设计，利用 SSD 高效的随机读写性能，通过将 value 分离出 &lt;code&gt;LSM-tree&lt;/code&gt; 的方法来达到降低写放大的目的。&lt;/p&gt;&lt;p&gt;我们的基准测试结果显示，当 value 较大的时候，Titan 在写、更新和点读等场景下性能都优于 RocksDB。但是根据 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//daslab.seas.harvard.edu/rum-conjecture/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RUM Conjecture&lt;/a&gt;&lt;/code&gt;，通常某些方面的提升往往是以牺牲其他方面为代价而取得的。Titan 便是以牺牲硬盘空间和范围查询的性能为代价，来取得更高的写性能。随着 SSD 价格的降低，我们认为这种取舍的意义会越来越明显。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;设计目标&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Titan 作为 TiKV 的一个子项目，首要的设计目标便是兼容 RocksDB。因为 TiKV 使用 RocksDB 作为其底层的存储引擎，而 TiKV 作为一个成熟项目已经拥有庞大的用户群体，所以我们需要考虑已有的用户也可以将已有的基于 RocksDB 的 TiKV 平滑地升级到基于 Titan 的 TiKV。&lt;/p&gt;&lt;p&gt;因此，我们总结了四点主要的设计目标：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持将 value 从 &lt;code&gt;LSM-tree&lt;/code&gt; 中分离出来单独存储，以降低写放大。&lt;/li&gt;&lt;li&gt;已有 RocksDB 实例可以平滑地升级到 Titan，这意味着升级过程不需要人工干预，并且不会影响线上服务。&lt;/li&gt;&lt;li&gt;100% 兼容目前 TiKV 所使用的所有 RocksDB 的特性。&lt;/li&gt;&lt;li&gt;尽量减少对 RocksDB 的侵入性改动，保证 Titan 更加容易升级到新版本的 RocksDB。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;架构与实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Titan 的基本架构如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;882&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;882&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 1：Titan 在 Flush 和 Compaction 的时候将 value 分离出 &lt;code&gt;LSM-tree&lt;/code&gt;，这样做的好处是写入流程可以和 RockDB 保持一致，减少对 &lt;code&gt;RocksDB&lt;/code&gt; 的侵入性改动。&lt;/blockquote&gt;&lt;p&gt;Titan 的核心组件主要包括：&lt;code&gt;BlobFile&lt;/code&gt;、&lt;code&gt;TitanTableBuilder&lt;/code&gt;、&lt;code&gt;Version&lt;/code&gt; 和 &lt;code&gt;GC&lt;/code&gt;，下面将逐一进行介绍。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;&lt;code&gt;BlobFile&lt;/code&gt; &lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;BlobFile&lt;/code&gt; 是用来存放从 &lt;code&gt;LSM-tree&lt;/code&gt; 中分离出来的 value 的文件，其格式如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;1327&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;1327&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 2：&lt;code&gt;BlobFile&lt;/code&gt; 主要由 blob record 、meta block、meta index block 和 footer 组成。其中每个 blob record 用于存放一个 key-value 对；meta block 支持可扩展性，可以用来存放和 &lt;code&gt;BlobFile&lt;/code&gt; 相关的一些属性等；meta index block 用于检索 meta block。&lt;/blockquote&gt;&lt;p&gt;&lt;code&gt;BlobFile&lt;/code&gt; 有几点值得关注的地方：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;BlobFile&lt;/code&gt; 中的 key-value 是有序存放的，目的是在实现 &lt;code&gt;Iterator&lt;/code&gt; 的时候可以通过 prefetch 的方式提高顺序读取的性能。&lt;/li&gt;&lt;li&gt;每个 blob record 都保留了 value 对应的 user key 的拷贝，这样做的目的是在进行 GC 的时候，可以通过查询 user key 是否更新来确定对应 value 是否已经过期，但同时也带来了一定的写放大。&lt;/li&gt;&lt;li&gt;&lt;code&gt;BlobFile&lt;/code&gt; 支持 blob record 粒度的 compression，并且支持多种 compression algorithm，包括 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/google/snappy&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Snappy&lt;/a&gt;&lt;/code&gt;、&lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lz4/lz4&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;LZ4&lt;/a&gt;&lt;/code&gt;和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/facebook/zstd&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Zstd&lt;/a&gt;&lt;/code&gt; 等，目前 Titan 默认使用的 compression algorithm 是 &lt;code&gt;LZ4&lt;/code&gt; 。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;TitanTableBuilder&lt;/b&gt;&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;TitanTableBuilder&lt;/code&gt; 是实现分离 key-value 的关键。我们知道 RocksDB 支持使用用户自定义 table builder 创建 &lt;code&gt;SST&lt;/code&gt;，这使得我们可以不对 build table 流程做侵入性的改动就可以将 value 从 &lt;code&gt;SST&lt;/code&gt; 中分离出来。下面将介绍 &lt;code&gt;TitanTableBuilder&lt;/code&gt; 的主要工作流程：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;777&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;777&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 3：&lt;code&gt;TitanTableBuilder&lt;/code&gt; 通过判断 value size 的大小来决定是否将 value 分离到 &lt;code&gt;BlobFile&lt;/code&gt; 中去。如果 value size 大于等于 &lt;code&gt;min_blob_size&lt;/code&gt; 则将 value 分离到 &lt;code&gt;BlobFile&lt;/code&gt; ，并生成 index 写入 &lt;code&gt;SST&lt;/code&gt;；如果 value size 小于 &lt;code&gt;min_blob_size&lt;/code&gt; 则将 value 直接写入 &lt;code&gt;SST&lt;/code&gt;。&lt;/blockquote&gt;&lt;p&gt;Titan 和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/dgraph-io/badger&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Badger&lt;/a&gt;&lt;/code&gt; 的设计有很大区别。&lt;code&gt;Badger&lt;/code&gt; 直接将 &lt;code&gt;WAL&lt;/code&gt; 改造成 &lt;code&gt;VLog&lt;/code&gt;，这样做的好处是减少一次 Flush 的开销。而 Titan 不这么设计的主要原因有两个：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;假设 &lt;code&gt;LSM-tree&lt;/code&gt; 的 max level 是 5，放大因子为 10，则 &lt;code&gt;LSM-tree&lt;/code&gt; 总的写放大大概为 1 + 1 + 10 + 10 + 10 + 10，其中 Flush 的写放大是 1，其比值是 42 : 1，因此 Flush 的写放大相比于整个 LSM-tree 的写放大可以忽略不计。&lt;/li&gt;&lt;li&gt;在第一点的基础上，保留 &lt;code&gt;WAL&lt;/code&gt; 可以使 Titan 极大地减少对 RocksDB 的侵入性改动，而这也正是我们的设计目标之一。 &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;Version&lt;/b&gt;&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Titan 使用 &lt;code&gt;Version&lt;/code&gt; 来代表某个时间点所有有效的 &lt;code&gt;BlobFile&lt;/code&gt;，这是从 &lt;code&gt;LevelDB&lt;/code&gt; 中借鉴过来的管理数据文件的方法，其核心思想便是 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Multiversion_concurrency_control&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MVCC&lt;/a&gt;&lt;/code&gt;，好处是在新增或删除文件的同时，可以做到并发读取数据而不需要加锁。每次新增文件或者删除文件的时候，&lt;code&gt;Titan&lt;/code&gt; 都会生成一个新的 &lt;code&gt;Version&lt;/code&gt; ，并且每次读取数据之前都要获取一个最新的 &lt;code&gt;Version&lt;/code&gt;。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;380&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;380&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 4：新旧 &lt;code&gt;Version&lt;/code&gt; 按顺序首尾相连组成一个双向链表，&lt;code&gt;VersionSet&lt;/code&gt; 用来管理所有的 &lt;code&gt;Version&lt;/code&gt;，它持有一个 &lt;code&gt;current&lt;/code&gt; 指针用来指向当前最新的 &lt;code&gt;Version&lt;/code&gt;。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Garbage Collection&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Garbage Collection (GC) 的目的是回收空间，一个高效的 GC 算法应该在权衡写放大和空间放大的同时，用最少的周期来回收最多的空间。在设计 GC 的时候有两个主要的问题需要考虑：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;何时进行 GC&lt;/li&gt;&lt;li&gt;挑选哪些文件进行 GC&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Titan 使用 RocksDB 提供的两个特性来解决这两个问题，这两个特性分别是 &lt;code&gt;TablePropertiesCollector&lt;/code&gt; 和&lt;code&gt;EventListener&lt;/code&gt; 。下面将讲解我们是如何通过这两个特性来辅助 GC 工作的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;code&gt;BlobFileSizeCollector&lt;/code&gt; &lt;/b&gt;&lt;/p&gt;&lt;p&gt;RocksDB 允许我们使用自定义的 &lt;code&gt;TablePropertiesCollector&lt;/code&gt; 来搜集 &lt;code&gt;SST&lt;/code&gt; 上的 properties 并写入到对应文件中去。&lt;code&gt;Titan&lt;/code&gt; 通过一个自定义的 &lt;code&gt;TablePropertiesCollector&lt;/code&gt; —— &lt;code&gt;BlobFileSizeCollector&lt;/code&gt; 来搜集每个 &lt;code&gt;SST&lt;/code&gt; 中有多少数据是存放在哪些 &lt;code&gt;BlobFile&lt;/code&gt; 上的，我们将它收集到的 properties 命名为 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt;，它的工作流程和数据格式如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;346&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;346&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 5：左边 &lt;code&gt;SST&lt;/code&gt; 中 Index 的格式为：第一列代表 &lt;code&gt;BlobFile&lt;/code&gt; 的文件 ID，第二列代表 blob record 在 &lt;code&gt;BlobFile&lt;/code&gt; 中的 offset，第三列代表 blob record 的 size。右边 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt; 中的每一行代表一个 &lt;code&gt;BlobFile&lt;/code&gt; 以及 &lt;code&gt;SST&lt;/code&gt; 中有多少数据保存在这个 &lt;code&gt;BlobFile&lt;/code&gt; 中，第一列代表 &lt;code&gt;BlobFile&lt;/code&gt; 的文件 ID，第二列代表数据大小。&lt;/blockquote&gt;&lt;p&gt;&lt;code&gt;&lt;b&gt;EventListener&lt;/b&gt;&lt;/code&gt; &lt;/p&gt;&lt;p&gt;我们知道 RocksDB 是通过 Compaction 来丢弃旧版本数据以回收空间的，因此每次 Compaction 完成后 Titan 中的某些 &lt;code&gt;BlobFile&lt;/code&gt; 中便可能有部分或全部数据过期。因此我们便可以通过监听 Compaction 事件来触发 GC，通过搜集比对 Compaction 中输入输出 &lt;code&gt;SST&lt;/code&gt; 的 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt; 来决定挑选哪些 &lt;code&gt;BlobFile&lt;/code&gt; 进行 GC。其流程大概如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;652&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;652&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 6：inputs 代表参与 Compaction 的所有 &lt;code&gt;SST&lt;/code&gt; 的 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt;，outputs 代表 Compaction 生成的所有 &lt;code&gt;SST&lt;/code&gt; 的 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt;，discardable size 是通过计算 inputs 和 outputs 得出的每个 &lt;code&gt;BlobFile&lt;/code&gt; 被丢弃的数据大小，第一列代表 &lt;code&gt;BlobFile&lt;/code&gt; 的文件 ID，第二列代表被丢弃的数据大小。&lt;/blockquote&gt;&lt;p&gt;Titan 会为每个有效的 &lt;code&gt;BlobFile&lt;/code&gt; 在内存中维护一个 discardable size 变量，每次 Compaction 结束之后都对相应的 &lt;code&gt;BlobFile&lt;/code&gt; 的 discardable size 变量进行累加。每次 GC 开始时就可以通过挑选 discardable size 最大的 &lt;code&gt;BlobFile&lt;/code&gt; 来作为作为候选的文件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Sample&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每次进行 GC 前我们都会挑选一系列&lt;code&gt;BlobFile&lt;/code&gt;作为候选文件，挑选的方法如上一节所述。为了减小写放大，我们可以容忍一定的空间放大，所以我们只有在&lt;code&gt;BlobFile&lt;/code&gt;可丢弃的数据达到一定比例之后才会对其进行 GC。我们使用 Sample 算法来获取每个候选文件中可丢弃数据的大致比例。Sample 算法的主要逻辑是随机取&lt;code&gt;BlobFile&lt;/code&gt;中的一段数据 A，计其大小为 a，然后遍历 A 中的 key，累加过期的 key 所在的 blob record 的 size 计为 d，最后计算得出 d 占 a 比值 为 r，如果 r &amp;gt;=&lt;code&gt;discardable_ratio&lt;/code&gt;则对该&lt;code&gt;BlobFile&lt;/code&gt;进行 GC，否则不对其进行 GC。上一节我们已经知道每个&lt;code&gt;BlobFile&lt;/code&gt;都会在内存中维护一个 discardable size，如果这个 discardable size 占整个&lt;code&gt;BlobFile&lt;/code&gt;数据大小的比值已经大于或等于&lt;code&gt;discardable_ratio&lt;/code&gt;则不需要对其进行 Sample。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;基准测试&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们使用&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/go-ycsb&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;go-ycsb&lt;/a&gt;测试了 TiKV 在 Txn Mode 下分别使用 RocksDB 和 Titan 的性能表现，本节我会简要说明下我们的测试方法和测试结果。由于篇幅的原因，我们只挑选两个典型的 value size 做说明，更详细的测试分析报告将会放在下一篇文章。&lt;/p&gt;&lt;p&gt;&lt;b&gt;测试环境&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CPU：Intel® Xeon® CPU E5-2630 v4 @ 2.20GHz（40个核心）&lt;/li&gt;&lt;li&gt;Memory：128GB（我们通过 Cgroup 限制 TiKV 进程使用内存不超过 32GB）&lt;/li&gt;&lt;li&gt;Disk：SATA SSD 1.5TB（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//linux.die.net/man/1/fio&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;fio&lt;/a&gt; 测试：4KB block size 混合随机读写情况下读写 IOPS 分别为 43.8K 和 18.7K）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;测试计划&lt;/b&gt;&lt;/p&gt;&lt;p&gt;数据集选定的基本原则是原始数据大小（不算上写放大因素）要比可用内存小，这样可以防止所有数据被缓存到内存中，减少 Cache 所带来的影响。这里我们选用的数据集大小是 64GB，进程的内存使用限制是 32GB。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;294&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1238&quot; data-original=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;294&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1238&quot; data-original=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;我们主要测试 5 个常用的场景：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Data Loading Performance：使用预先计算好的 key 数量和固定的 value 大小，以一定的速度并发写入。&lt;/li&gt;&lt;li&gt;Update Performance：由于 Titan 在纯写入场景下不需要 GC（&lt;code&gt;BlobFile&lt;/code&gt; 中没有可丢弃数据），因此我们还需要通过更新来测试 &lt;code&gt;GC&lt;/code&gt; 对性能的影响。&lt;/li&gt;&lt;li&gt;Output Size：这一步我们会测量更新场景完成后引擎所占用的硬盘空间大小，以此反映 GC 的空间回收效果。&lt;/li&gt;&lt;li&gt;Random Key Lookup Performance：这一步主要测试点查性能，并且点查次数要远远大于 key 的数量。&lt;/li&gt;&lt;li&gt;Sorted Range Iteration Performance：这一步主要测试范围查询的性能，每次查询 2 million 个相连的 key。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;测试结果&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 7 Data Loading Performance：Titan 在写场景中的性能要比 RocksDB 高 70% 以上，并且随着 value size 的变大，这种性能的差异会更加明显。值得注意的是，数据在写入 KV Engine 之前会先写入 Raft Log，因此 Titan 的性能提升会被摊薄，实际上裸测 RocksDB 和 Titan 的话这种性能差异会更大。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 8 Update Performance：Titan 在更新场景中的性能要比 RocksDB 高 180% 以上，这主要得益于 Titan 优秀的读性能和良好的 GC 算法。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 9 Output Size：Titan 的空间放大相比 RocksDB 略高，这种差距会随着 Key 数量的减少有略微的缩小，这主要是因为 &lt;code&gt;BlobFile&lt;/code&gt; 中需要存储 Key 而造成的写放大。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 10 Random Key Lookup： Titan 拥有比 RocksDB 更卓越的点读性能，这主要得益与将 value 分离出 &lt;code&gt;LSM-tree&lt;/code&gt;的设计使得 &lt;code&gt;LSM-tree&lt;/code&gt; 变得更小，因此 Titan 在使用同样的内存量时可以将更多的 &lt;code&gt;index&lt;/code&gt; 、&lt;code&gt;filter&lt;/code&gt; 和 &lt;code&gt;DataBlock&lt;/code&gt; 缓存到 Block Cache 中去。这使得点读操作在大多数情况下仅需要一次 IO 即可（主要是用于从 &lt;code&gt;BlobFile&lt;/code&gt; 中读取数据）。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 11 Sorted Range Iteration：Titan 的范围查询性能目前和 RocksDB 相比还是有一定的差距，这也是我们未来优化的一个重要方向。&lt;/blockquote&gt;&lt;p&gt;本次测试我们对比了两个具有代表性的 value size 在 5 种不同场景下的性能差异，更多不同粒度的 value size 的测试和更详细的性能报告我们会放在下一篇文章去说明，并且我们会从更多的角度（例如 CPU 和内存的使用率等）去分析 Titan 和 RocksDB 的差异。从本次测试我们可以大致得出结论，在大 value 的场景下，Titan 会比 RocksDB 拥有更好的写、更新和点读性能。同时，Titan 的范围查询性能和空间放大都逊于 RocksDB 。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;兼容性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一开始我们便将兼容 RocksDB 作为设计 Titan 的首要目标，因此我们保留了绝大部分 RocksDB 的 API。目前仅有两个 API 是我们明确不支持的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;Merge&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;SingleDelete&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;除了 &lt;code&gt;Open&lt;/code&gt; 接口以外，其他 API 的参数和返回值都和 RocksDB 一致。已有的项目只需要很小的改动即可以将 &lt;code&gt;RocksDB&lt;/code&gt;实例平滑地升级到 Titan。值得注意的是 Titan 并不支持回退回 RocksDB。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何使用 Titan&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;创建 DB&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;#include &amp;lt;assert&amp;gt;
#include &quot;rocksdb/utilities/titandb/db.h&quot;

// Open DB
rocksdb::titandb::TitanDB* db;
rocksdb::titandb::TitanOptions options;
options.create_if_missing = true;
rocksdb::Status status =
  rocksdb::titandb::TitanDB::Open(options, &quot;/tmp/testdb&quot;, &amp;amp;db);
assert(status.ok());
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;或&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;#include &amp;lt;assert&amp;gt;
#include &quot;rocksdb/utilities/titandb/db.h&quot;

// open DB with two column families
rocksdb::titandb::TitanDB* db;
std::vector&amp;lt;rocksdb::titandb::TitanCFDescriptor&amp;gt; column_families;
// have to open default column family
column_families.push_back(rocksdb::titandb::TitanCFDescriptor(
    kDefaultColumnFamilyName, rocksdb::titandb::TitanCFOptions()));
// open the new one, too
column_families.push_back(rocksdb::titandb::TitanCFDescriptor(
    &quot;new_cf&quot;, rocksdb::titandb::TitanCFOptions()));
std::vector&amp;lt;ColumnFamilyHandle*&amp;gt; handles;
s = rocksdb::titandb::TitanDB::Open(rocksdb::titandb::TitanDBOptions(), kDBPath,
                                    column_families, &amp;amp;handles, &amp;amp;db);
assert(s.ok());
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Status&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;和 RocksDB 一样，Titan 使用 &lt;code&gt;rocksdb::Status&lt;/code&gt; 来作为绝大多数 API 的返回值，使用者可以通过它检查执行结果是否成功，也可以通过它打印错误信息：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;rocksdb::Status s = ...;
if (!s.ok()) cerr &amp;lt;&amp;lt; s.ToString() &amp;lt;&amp;lt; endl;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;销毁 DB&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;std::string value;
rocksdb::Status s = db-&amp;gt;Get(rocksdb::ReadOptions(), key1, &amp;amp;value);
if (s.ok()) s = db-&amp;gt;Put(rocksdb::WriteOptions(), key2, value);
if (s.ok()) s = db-&amp;gt;Delete(rocksdb::WriteOptions(), key1);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;在 TiKV 中使用 Titan&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;目前 Titan 在 TiKV 中是默认关闭的，我们通过 TiKV 的配置文件来决定是否开启和设置 Titan，相关的配置项包括 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/12a1ea8d13b6478c8a4d07f0bb7411f3367dc8f9/etc/config-template.toml%23L375&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;[rocksdb.titan]&lt;/a&gt;&lt;/code&gt; 和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/12a1ea8d13b6478c8a4d07f0bb7411f3367dc8f9/etc/config-template.toml%23L531&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;[rocksdb.defaultcf.titan]&lt;/a&gt;&lt;/code&gt;， 开启 Titan 只需要进行如下配置即可：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;[rocksdb.titan]
enabled = true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意一旦开启 Titan 就不能回退回 RocksDB 了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;未来的工作&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;优化 &lt;code&gt;Iterator&lt;/code&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们通过测试发现，目前使用 Titan 做范围查询时 IO Util 很低，这也是为什么其性能会比 RocksDB 差的重要原因之一。因此我们认为 Titan 的 &lt;code&gt;Iterator&lt;/code&gt; 还存在着巨大的优化空间，最简单的方法是可以通过更加激进的 prefetch 和并行 prefetch 等手段来达到提升 &lt;code&gt;Iterator&lt;/code&gt; 性能的目的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;code&gt;GC&lt;/code&gt; 速度控制和自动调节&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通常来说，GC 的速度太慢会导致空间放大严重，过快又会对服务的 QPS 和延时带来影响。目前 Titan 支持自动 GC，虽然可以通过减小并发度和 batch size 来达到一定程度限制 GC 速度的目的，但是由于每个 &lt;code&gt;BlobFile&lt;/code&gt; 中的 blob record 数目不定，若 &lt;code&gt;BlobFile&lt;/code&gt; 中的 blob record 过于密集，将其有效的 key 更新回 &lt;code&gt;LSM-tree&lt;/code&gt; 时仍然可能堵塞业务的写请求。为了达到更加精细化的控制 GC 速度的目的，后续我们将使用 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Token_bucket&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Token Bucket&lt;/a&gt;&lt;/code&gt; 算法限制一段时间内 GC 能够更新的 key 数量，以降低 GC 对 QPS 和延时的影响，使服务更加稳定。&lt;/p&gt;&lt;p&gt;另一方面，我们也正在研究自动调节 GC 速度的算法，这样我们便可以，在服务高峰期的时候降低 GC 速度来提供更高的服务质量；在服务低峰期的时候提高 GC 速度来加快空间的回收。&lt;/p&gt;&lt;p&gt;&lt;b&gt;增加用于判断 key 是否存在的 API&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 在某些场景下仅需要判断某个 key 是否存在，而不需要读取对应的 value。通过提供一个这样的 API 可以极大地提高性能，因为我们已经看到将 value 移出 &lt;code&gt;LSM-tree&lt;/code&gt; 之后，&lt;code&gt;LSM-tree&lt;/code&gt; 本身会变的非常小，以至于我们可以将更多地 &lt;code&gt;index&lt;/code&gt;、&lt;code&gt;filter&lt;/code&gt; 和 &lt;code&gt;DataBlock&lt;/code&gt; 存放到内存当中去，这样去检索某个 key 的时候可以做到只需要少量甚至不需要 IO 。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多阅读：&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-23-55521489</guid>
<pubDate>Wed, 23 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB-Lightning Toolset &amp; TiDB-DM 正式开源，前排开“坑”、PR 走起</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-21-55397024.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/55397024&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4edef4e37af3792ed09885ebb444570e_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;在刚刚结束的 TiDB DevCon 2019 上，我们宣布将大家&lt;b&gt;期待已久&lt;/b&gt;的 TiDB-Ligthning Toolset 和 TiDB-DM 开源（惊不惊喜、意不意外？！），感兴趣的小伙伴们赶紧前排关注一波，开“坑（issues）”讨论，PR 走起！&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;TiDB-Lightning Toolset&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB-Lightning Toolset 是一套快速全量导入 SQL dump 文件到 TiDB 集群的工具集，&lt;/b&gt;自 2.1.0 版本起随 TiDB 发布，最新的测试结果显示，速度可达到传统执行 SQL 导入方式的至少 5 倍，导入 1T 数据需要 5 ～ 6 个小时，适合在上线前用作迁移现有的大型数据库到全新的 TiDB 集群。&lt;/p&gt;&lt;u&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a2bf35e4f1559fb58731a5935c6ac73e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;451&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-a2bf35e4f1559fb58731a5935c6ac73e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a2bf35e4f1559fb58731a5935c6ac73e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;451&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-a2bf35e4f1559fb58731a5935c6ac73e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-a2bf35e4f1559fb58731a5935c6ac73e_b.jpg&quot;&gt;&lt;/figure&gt;&lt;/u&gt;&lt;p&gt;&lt;b&gt;原理解读：&lt;/b&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-ecosystem-tools-2/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-Lightning Toolset 介绍&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;项目地址：&lt;/b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//github.com/pingcap/tidb-lightning&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;github.com/pingcap/tidb-lightning&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB-DM&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB-DM（Data Migration）是用于将数据从 MySQL/MariaDB 迁移到 TiDB 的工具。&lt;/b&gt;该工具既支持以全量备份文件的方式将 MySQL/MariaDB 的数据导入到 TiDB，也支持通过解析执行 MySQL/MariaDB binlog 的方式将数据增量同步到 TiDB。特别地，对于有多个 MySQL/MariaDB 实例的分库分表需要合并后同步到同一个 TiDB 集群的场景，DM 提供了良好的支持。如果你需要从 MySQL/MariaDB 迁移到 TiDB，或者需要将 TiDB 作为 MySQL/MariaDB 的从库，DM 将是一个非常好的选择。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c81491308e3a85cb04da15391950e23e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;457&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-c81491308e3a85cb04da15391950e23e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c81491308e3a85cb04da15391950e23e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;457&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-c81491308e3a85cb04da15391950e23e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-c81491308e3a85cb04da15391950e23e_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;原理解读：&lt;/b&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-DM 架构设计与实现原理&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;项目地址：&lt;/b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//github.com/pingcap/dm&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;github.com/pingcap/dm&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-21-55397024</guid>
<pubDate>Mon, 21 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 3.0 Beta Release Notes</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-21-55348308.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/55348308&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-897efb5b298c75e3f8ece404672f8d6b_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;2019 年 1 月 19 日，TiDB 发布 3.0 Beta 版，对应 master branch 的 TiDB-Ansible。相比 2.1 版本，该版本对系统稳定性、优化器、统计信息以及执行引擎做了很多改进。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;新特性&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持 View&lt;/li&gt;&lt;li&gt;支持 Window Function&lt;/li&gt;&lt;li&gt;支持 Range Partition&lt;/li&gt;&lt;li&gt;支持 Hash Partition&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;SQL 优化器&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;重新支持聚合消除的优化规则&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;NOT EXISTS&lt;/code&gt; 子查询，将其转化为 Anti Semi Join&lt;/li&gt;&lt;li&gt;添加 &lt;code&gt;tidb_enable_cascades_planner&lt;/code&gt; 变量以支持新的 Cascades 优化器。目前 Cascades 优化器尚未实现完全，默认关闭&lt;/li&gt;&lt;li&gt;支持在事务中使用 Index Join&lt;/li&gt;&lt;li&gt;优化 Outer Join 上的常量传播，使得对 Join 结果里和 Outer 表相关的过滤条件能够下推过 Outer Join 到 Outer 表上，减少 Outer Join 的无用计算量，提升执行性能&lt;/li&gt;&lt;li&gt;调整投影消除的优化规则到聚合消除之后，消除掉冗余的 &lt;code&gt;Project&lt;/code&gt; 算子&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;IFNULL&lt;/code&gt; 函数，当输入参数具有非 NULL 的属性的时候，消除该函数&lt;/li&gt;&lt;li&gt;支持对 &lt;code&gt;_tidb_rowid&lt;/code&gt; 构造查询的 Range，避免全表扫，减轻集群压力&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;IN&lt;/code&gt; 子查询为先聚合后做 Inner Join 并，添加变量 &lt;code&gt;tidb_opt_insubq_to_join_and_agg&lt;/code&gt; 以控制是否开启该优化规则并默认打开&lt;/li&gt;&lt;li&gt;支持在 &lt;code&gt;DO&lt;/code&gt; 语句中使用子查询&lt;/li&gt;&lt;li&gt;添加 Outer Join 消除的优化规则，减少不必要的扫表和 Join 操作，提升执行性能&lt;/li&gt;&lt;li&gt;修改 &lt;code&gt;TIDB_INLJ&lt;/code&gt; 优化器 Hint 的行为，优化器将使用 Hint 中指定的表当做 Index Join 的 Inner 表&lt;/li&gt;&lt;li&gt;更大范围的启用 &lt;code&gt;PointGet&lt;/code&gt;，使得当 Prepare 语句的执行计划缓存生效时也能利用上它&lt;/li&gt;&lt;li&gt;引入贪心的 Join Reorder 算法，优化多表 Join 时 Join 顺序选择的问题&lt;/li&gt;&lt;li&gt;支持 View&lt;/li&gt;&lt;li&gt;支持 Window Function&lt;/li&gt;&lt;li&gt;当 &lt;code&gt;TIDB_INLJ&lt;/code&gt; 未生效时，返回 warning 给客户端，增强易用性&lt;/li&gt;&lt;li&gt;支持根据过滤条件和表的统计信息推导过滤后数据的统计信息的功能&lt;/li&gt;&lt;li&gt;增强 Range Partition 的 Partition Pruning 优化规则&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;SQL 执行引擎&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;优化 Merge Join 算子，使其支持空的 &lt;code&gt;ON&lt;/code&gt; 条件&lt;/li&gt;&lt;li&gt;优化日志，打印执行 &lt;code&gt;EXECUTE&lt;/code&gt; 语句时使用的用户变量&lt;/li&gt;&lt;li&gt;优化日志，为 &lt;code&gt;COMMIT&lt;/code&gt; 语句打印慢查询信息&lt;/li&gt;&lt;li&gt;支持 &lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt; 功能，使得 SQL 调优过程更加简单&lt;/li&gt;&lt;li&gt;优化列很多的宽表的写入性能&lt;/li&gt;&lt;li&gt;支持 &lt;code&gt;admin show next_row_id&lt;/code&gt;&lt;/li&gt;&lt;li&gt;添加变量 &lt;code&gt;tidb_init_chunk_size&lt;/code&gt; 以控制执行引擎使用的初始 Chunk 大小&lt;/li&gt;&lt;li&gt;完善 &lt;code&gt;shard_row_id_bits&lt;/code&gt;，对自增 ID 做越界检查&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;code&gt;Prepare&lt;/code&gt;语句&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对包含子查询的 &lt;code&gt;Prepare&lt;/code&gt; 语句，禁止其添加到 &lt;code&gt;Prepare&lt;/code&gt; 语句的执行计划缓存中，确保输入不同的用户变量时执行计划的正确性&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;Prepare&lt;/code&gt; 语句的执行计划缓存，使得当语句中包含非确定性函数的时候，该语句的执行计划也能被缓存&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;Prepare&lt;/code&gt; 语句的执行计划缓存，使得 &lt;code&gt;DELETE&lt;/code&gt;/&lt;code&gt;UPDATE&lt;/code&gt;/&lt;code&gt;INSERT&lt;/code&gt; 的执行计划也能被缓存&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;Prepare&lt;/code&gt; 语句的执行计划缓存，当执行 &lt;code&gt;DEALLOCATE&lt;/code&gt; 语句时从缓存中剔除对应的执行计划&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;Prepare&lt;/code&gt; 语句的执行计划缓存，通过控制其内存使用以避免缓存过多执行计划导致 TiDB OOM 的问题&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;Prepare&lt;/code&gt; 语句，使得 &lt;code&gt;ORDER BY&lt;/code&gt;/&lt;code&gt;GROUP BY&lt;/code&gt;/&lt;code&gt;LIMIT&lt;/code&gt; 子句中可以使用 “?” 占位符&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;权限管理&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;增加对 &lt;code&gt;ANALYZE&lt;/code&gt; 语句的权限检查&lt;/li&gt;&lt;li&gt;增加对 &lt;code&gt;USE&lt;/code&gt; 语句的权限检查&lt;/li&gt;&lt;li&gt;增加对 &lt;code&gt;SET GLOBAL&lt;/code&gt; 语句的权限检查&lt;/li&gt;&lt;li&gt;增加对 &lt;code&gt;SHOW PROCESSLIST&lt;/code&gt; 语句的权限检查&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;Server&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持了对 SQL 语句的 &lt;code&gt;Trace&lt;/code&gt; 功能&lt;/li&gt;&lt;li&gt;支持了插件框架&lt;/li&gt;&lt;li&gt;支持同时使用 &lt;code&gt;unix_socket&lt;/code&gt; 和 TCP 两种方式连接数据库&lt;/li&gt;&lt;li&gt;支持了系统变量 &lt;code&gt;interactive_timeout&lt;/code&gt;&lt;/li&gt;&lt;li&gt;支持了系统变量 &lt;code&gt;wait_timeout&lt;/code&gt;&lt;/li&gt;&lt;li&gt;提供了变量 &lt;code&gt;tidb_batch_commit&lt;/code&gt;，可以按语句数将事务分解为多个事务&lt;/li&gt;&lt;li&gt;支持 &lt;code&gt;ADMIN SHOW SLOW&lt;/code&gt; 语句，方便查看慢日志&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;兼容性&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持了 &lt;code&gt;ALLOW_INVALID_DATES&lt;/code&gt; 这种 SQL mode&lt;/li&gt;&lt;li&gt;提升了 load data 对 CSV 文件的容错能力&lt;/li&gt;&lt;li&gt;支持了 MySQL 320 握手协议&lt;/li&gt;&lt;li&gt;支持将 unsigned bigint 列声明为自增列&lt;/li&gt;&lt;li&gt;支持 &lt;code&gt;SHOW CREATE DATABASE IF NOT EXISTS&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;当过滤条件中包含用户变量时不对其进行谓词下推的操作，更加兼容 MySQL 中使用用户变量模拟 Window Function 的行为&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;DDL&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持快速恢复误删除的表&lt;/li&gt;&lt;li&gt;支持动态调整 ADD INDEX 的并发数&lt;/li&gt;&lt;li&gt;支持更改表或者列的字符集到 utf8/utf8mb4&lt;/li&gt;&lt;li&gt;默认字符集从 &lt;code&gt;utf8&lt;/code&gt; 变为 &lt;code&gt;utf8mb4&lt;/code&gt;&lt;/li&gt;&lt;li&gt;支持 RANGE PARTITION&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Tools&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB-Lightning&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;大幅优化 SQL 转 KV 的处理速度&lt;/li&gt;&lt;li&gt;对单表支持 batch 导入，提高导入性能和稳定性&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;PD&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;增加 &lt;code&gt;RegionStorage&lt;/code&gt; 单独存储 Region 元信息&lt;/li&gt;&lt;li&gt;增加 shuffle hot region 调度&lt;/li&gt;&lt;li&gt;增加调度参数相关 Metrics&lt;/li&gt;&lt;li&gt;增加集群 Label 信息相关 Metrics&lt;/li&gt;&lt;li&gt;增加导入数据场景模拟&lt;/li&gt;&lt;li&gt;修复 Leader 选举相关的 Watch 问题&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiKV&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;支持了分布式 GC&lt;/li&gt;&lt;li&gt;在 Apply snapshot 之前检查 RocksDB level 0 文件，避免产生 Write stall&lt;/li&gt;&lt;li&gt;支持了逆向 &lt;code&gt;raw_scan&lt;/code&gt; 和 &lt;code&gt;raw_batch_scan&lt;/code&gt;&lt;/li&gt;&lt;li&gt;更好的夏令时支持&lt;/li&gt;&lt;li&gt;支持了使用 HTTP 方式获取监控信息&lt;/li&gt;&lt;li&gt;支持批量方式接收和发送 Raft 消息&lt;/li&gt;&lt;li&gt;引入了新的存储引擎 Titan&lt;/li&gt;&lt;li&gt;升级 gRPC 到 v1.17.2&lt;/li&gt;&lt;li&gt;支持批量方式接收客户端请求和发送回复&lt;/li&gt;&lt;li&gt;多线程 Apply&lt;/li&gt;&lt;li&gt;线程 Raftstore&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;英文版 Release Notes&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/docs/blob/master/releases/3.0beta.md&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic3.zhimg.com/v2-2e73d3d3251663decc70dfbbe5be5f6a_ipico.jpg&quot; data-image-width=&quot;283&quot; data-image-height=&quot;283&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pingcap/docs&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-21-55348308</guid>
<pubDate>Mon, 21 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在转转的业务实战</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-16-55026939.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/55026939&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eb4578b6386a0021b17d7c418e728fbe_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br&gt;陈维，转转优品技术部 RD。&lt;/blockquote&gt;&lt;p&gt;世界级的开源分布式数据库 TiDB 自 2016 年 12 月正式发布第一个版本以来，业内诸多公司逐步引入使用，并取得广泛认可。&lt;/p&gt;&lt;p&gt;对于互联网公司，数据存储的重要性不言而喻。在 NewSQL 数据库出现之前，一般采用单机数据库（比如 MySQL）作为存储，随着数据量的增加，“分库分表”是早晚面临的问题，即使有诸如 MyCat、ShardingJDBC 等优秀的中间件，“分库分表”还是给 RD 和 DBA 带来较高的成本；NewSQL 数据库出现后，由于它不仅有 NoSQL 对海量数据的管理存储能力、还支持传统关系数据库的 ACID 和 SQL，所以对业务开发来说，存储问题已经变得更加简单友好，进而可以更专注于业务本身。而 TiDB，正是 NewSQL 的一个杰出代表！&lt;/p&gt;&lt;p&gt;站在业务开发的视角，TiDB 最吸引人的几大特性是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;支持 MySQL 协议（开发接入成本低）；&lt;/li&gt;&lt;li&gt;100% 支持事务（数据一致性实现简单、可靠）；&lt;/li&gt;&lt;li&gt;无限水平拓展（不必考虑分库分表）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;基于这几大特性，TiDB 在业务开发中是值得推广和实践的，但是，它毕竟不是传统的关系型数据库，以致我们对关系型数据库的一些使用经验和积累，在 TiDB 中是存在差异的，现主要阐述“事务”和“查询”两方面的差异。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 事务和 MySQL 事务的差异&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;MySQL 事务和 TiDB 事务对比&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;400&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;400&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f0bc66dc787eead6082fb48e978407d1_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;在 TiDB 中执行的事务 b，返回影响条数是 1（认为已经修改成功），但是提交后查询，status 却不是事务 b 修改的值，而是事务 a 修改的值。&lt;/p&gt;&lt;p&gt;可见，MySQL 事务和 TiDB 事务存在这样的差异：&lt;/p&gt;&lt;p&gt;MySQL 事务中，可以通过影响条数，作为写入（或修改）是否成功的依据；而在 TiDB 中，这却是不可行的！&lt;/p&gt;&lt;p&gt;作为开发者我们需要考虑下面的问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;同步 RPC 调用中，如果需要严格依赖影响条数以确认返回值，那将如何是好？&lt;/li&gt;&lt;li&gt;多表操作中，如果需要严格依赖某个主表数据更新结果，作为是否更新（或写入）其他表的判断依据，那又将如何是好？&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;原因分析及解决方案&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于 MySQL，当更新某条记录时，会先获取该记录对应的行级锁（排他锁），获取成功则进行后续的事务操作，获取失败则阻塞等待。&lt;/p&gt;&lt;p&gt;对于 TiDB，使用 Percolator 事务模型：可以理解为乐观锁实现，事务开启、事务中都不会加锁，而是在提交时才加锁。参见 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247484286%26idx%3D2%26sn%3D45b7d9e29af3965567f1743f0c2b536c%26chksm%3Deb162414dc61ad02877378fb97d1790a946c72f4c344260c037d1ae0f9817f2e4d14d1c6233e%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这篇文章&lt;/a&gt;&lt;/u&gt;（TiDB 事务算法）。&lt;/p&gt;&lt;p&gt;其简要流程如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;733&quot; data-rawheight=&quot;618&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;733&quot; data-original=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;733&quot; data-rawheight=&quot;618&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;733&quot; data-original=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-480a8128457b1092c4e9df41caf07b69_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;在事务提交的 PreWrite 阶段，当“锁检查”失败时：如果开启冲突重试，事务提交将会进行重试；如果未开启冲突重试，将会抛出写入冲突异常。&lt;/p&gt;&lt;p&gt;可见，对于 MySQL，由于在写入操作时加上了排他锁，变相将并行事务从逻辑上串行化；而对于 TiDB，属于乐观锁模型，在事务提交时才加锁，并使用事务开启时获取的“全局时间戳”作为“锁检查”的依据。&lt;/p&gt;&lt;p&gt;所以，在业务层面避免 TiDB 事务差异的本质在于避免锁冲突，即，当前事务执行时，不产生别的事务时间戳（无其他事务并行）。处理方式为事务串行化。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;TiDB 事务串行化&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在业务层，可以借助分布式锁，实现串行化处理，如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;714&quot; data-rawheight=&quot;350&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;714&quot; data-original=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;714&quot; data-rawheight=&quot;350&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;714&quot; data-original=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-4e5f59504ec0e644ff5f2da424de4e65_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;基于 Spring 和分布式锁的事务管理器拓展&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 Spring 生态下，spring-tx 中定义了统一的事务管理器接口：&lt;code&gt;PlatformTransactionManager&lt;/code&gt;，其中有获取事务（getTransaction）、提交（commit）、回滚（rollback）三个基本方法；使用装饰器模式，事务串行化组件可做如下设计：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;800&quot; data-rawheight=&quot;647&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;800&quot; data-original=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;800&quot; data-rawheight=&quot;647&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;800&quot; data-original=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-058e6ce48a070e242e7ce42c2eb97cc5_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;其中，关键点有：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;超时时间：为避免死锁，锁必须有超时时间；为避免锁超时导致事务并行，事务必须有超时时间，而且锁超时时间必须大于事务超时时间（时间差最好在秒级）。&lt;/li&gt;&lt;li&gt;加锁时机：TiDB 中“锁检查”的依据是事务开启时获取的“全局时间戳”，所以加锁时机必须在事务开启前。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;事务模板接口设计&lt;/b&gt;&lt;/p&gt;&lt;p&gt;隐藏复杂的事务重写逻辑，暴露简单友好的 API：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;257&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;257&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-41d1ac47b69b440efa9b5af6646980cf_b.jpg&quot;&gt;&lt;/figure&gt;&lt;u&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;213&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;213&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-2332bab1107b2161a427057787b243b0_b.jpg&quot;&gt;&lt;/figure&gt;&lt;/u&gt;&lt;h2&gt;&lt;b&gt;TiDB 查询和 MySQL 的差异&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 TiDB 使用过程中，偶尔会有这样的情况，某几个字段建立了索引，但是查询过程还是很慢，甚至不经过索引检索。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;索引混淆型（举例）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;表结构：&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;CREATE TABLE `t_test` (
	  `id` bigint(20) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;主键id&#39;,
	  `a` int(11) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;a&#39;,
	  `b` int(11) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;b&#39;,
	  `c` int(11) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;c&#39;,
	  PRIMARY KEY (`id`),
	  KEY `idx_a_b` (`a`,`b`),
	  KEY `idx_c` (`c`)
	) ENGINE=InnoDB;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;查询：&lt;/b&gt;如果需要查询 (a=1 且 b=1）或 c=2 的数据，在 MySQL 中，sql 可以写为：&lt;code&gt;SELECT id from t_test where (a=1 and b=1) or (c=2);&lt;/code&gt;，MySQL 做查询优化时，会检索到 &lt;code&gt;idx_a_b&lt;/code&gt; 和&lt;code&gt;idx_c&lt;/code&gt; 两个索引；但是在 TiDB（v2.0.8-9）中，这个 sql 会成为一个慢 SQL，需要改写为：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;SELECT id from t_test where (a=1 and b=1) UNION SELECT id from t_test where (c=2);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;小结：导致该问题的原因，可以理解为 TiDB 的 sql 解析还有优化空间（官方回复已在优化计划中）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;冷热数据型（举例）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;表结构：&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;CREATE TABLE `t_job_record` (
	  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;主键id&#39;,
	  `job_code` varchar(255) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;任务code&#39;,
	  `record_id` bigint(20) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;记录id&#39;,
	  `status` tinyint(3) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;执行状态:0 待处理&#39;,
	  `execute_time` bigint(20) NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;执行时间（毫秒）&#39;,
	  PRIMARY KEY (`id`),
	  KEY `idx_status_execute_time` (`status`,`execute_time`),
	  KEY `idx_record_id` (`record_id`)
	) ENGINE=InnoDB COMMENT=&#39;异步任务job&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;数据说明：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;a. 冷数据，&lt;code&gt;status=1&lt;/code&gt; 的数据（已经处理过的数据）；&lt;/p&gt;&lt;p&gt;b. 热数据，&lt;code&gt;status=0 并且 execute_time&amp;lt;= 当前时间&lt;/code&gt; 的数据。&lt;/p&gt;&lt;p&gt;&lt;b&gt;慢查询&lt;/b&gt;：对于热数据，数据量一般不大，但是查询频度很高，假设当前（毫秒级）时间为：1546361579646，则在 MySQL 中，查询 sql 为：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;SELECT * FROM t_job_record where status=0 and execute_time&amp;lt;= 1546361579646
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个在 MySQL 中很高效的查询，在 TiDB 中虽然也可从索引检索，但其耗时却不尽人意（百万级数据量，耗时百毫秒级）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原因分析：&lt;/b&gt;在 TiDB 中，底层索引结构为 LSM-Tree，如下图：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;206&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;206&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-28159187b532d99a7ac34059f1ab04e1_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;当从内存级的 C0 层查询不到数据时，会逐层扫描硬盘中各层；且 merge 操作为异步操作，索引数据更新会存在一定的延迟，可能存在无效索引。由于逐层扫描和异步 merge，使得查询效率较低。&lt;/p&gt;&lt;p&gt;优化方式：尽可能缩小过滤范围，比如结合异步 job 获取记录频率，在保证不遗漏数据的前提下，合理设置 execute_time 筛选区间，例如 1 小时，sql 改写为：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;SELECT * FROM t_job_record  where status=0 and execute_time&amp;gt;1546357979646 and execute_time&amp;lt;= 1546361579646
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;优化效果：&lt;/b&gt;耗时 10 毫秒级别（以下）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;关于查询的启发&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在基于 TiDB 的业务开发中，先摒弃传统关系型数据库带来的对 sql 先入为主的理解或经验，谨慎设计每一个 sql，如 DBA 所提倡：设计 sql 时务必关注执行计划，必要时请教 DBA。&lt;/p&gt;&lt;p&gt;和 MySQL 相比，TiDB 的底层存储和结构决定了其特殊性和差异性；但是，TiDB 支持 MySQL 协议，它们也存在一些共同之处，比如在 TiDB 中使用“预编译”和“批处理”，同样可以获得一定的性能提升。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;服务端预编译&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 MySQL 中，可以使用 &lt;code&gt;PREPARE stmt_name FROM preparable_stm&lt;/code&gt;  对 sql 语句进行预编译，然后使用 &lt;code&gt;EXECUTE stmt_name [USING @var_name [, @var_name] ...]&lt;/code&gt; 执行预编译语句。如此，同一 sql 的多次操作，可以获得比常规 sql 更高的性能。&lt;/p&gt;&lt;p&gt;mysql-jdbc 源码中，实现了标准的 &lt;code&gt;Statement&lt;/code&gt; 和 &lt;code&gt;PreparedStatement&lt;/code&gt; 的同时，还有一个&lt;code&gt;ServerPreparedStatement&lt;/code&gt; 实现，&lt;code&gt;ServerPreparedStatement&lt;/code&gt; 属于&lt;code&gt;PreparedStatement&lt;/code&gt;的拓展，三者对比如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;320&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;320&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-7b504d262d5b9e2d941a312f8798e02a_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;容易发现，&lt;code&gt;PreparedStatement&lt;/code&gt; 和  &lt;code&gt;Statement&lt;/code&gt;的区别主要区别在于参数处理，而对于发送数据包，调用服务端的处理逻辑是一样（或类似）的；经测试，二者速度相当。其实，&lt;code&gt;PreparedStatement&lt;/code&gt; 并不是服务端预处理的；&lt;code&gt;ServerPreparedStatement&lt;/code&gt; 才是真正的服务端预处理，速度也较 &lt;code&gt;PreparedStatement&lt;/code&gt; 快；其使用场景一般是：频繁的数据库访问，sql 数量有限（有缓存淘汰策略，使用不宜会导致两次 IO）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;批处理&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;对于多条数据写入，常用 sql 为 &lt;code&gt;insert … values(…),(…)&lt;/code&gt;；而对于多条数据更新，亦可以使用&lt;code&gt;update … case … when… then… end&lt;/code&gt; 来减少 IO 次数。但它们都有一个特点，数据条数越多，sql 越加复杂，sql 解析成本也更高，耗时增长可能高于线性增长。而批处理，可以复用一条简单 sql，实现批量数据的写入或更新，为系统带来更低、更稳定的耗时。&lt;/p&gt;&lt;p&gt;对于批处理，作为客户端，&lt;code&gt;java.sql.Statement&lt;/code&gt; 主要定义了两个接口方法，&lt;code&gt;addBatch&lt;/code&gt;  和  &lt;code&gt;executeBatch&lt;/code&gt;  来支持批处理。&lt;/p&gt;&lt;p&gt;批处理的简要流程说明如下：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;864&quot; data-original=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;864&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;864&quot; data-original=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-f134ea114f7ac12f6120f484dea108ce_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;经业务中实践，使用批处理方式的写入（或更新），比常规 &lt;code&gt;insert … values (…),(…)&lt;/code&gt;（或  &lt;code&gt;update … case … when… then… end&lt;/code&gt;）性能更稳定，耗时也更低。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;本文转载自“转转技术”，原文链接：&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/Qyvy_YBIBhZJo1uYHTL93g%3Fscene%3D25%23wechat_redirect&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic1.zhimg.com/v2-4a5821a1189a52c4708ad4a65139e744_180x120.jpg&quot; data-image-width=&quot;470&quot; data-image-height=&quot;200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB业务实战&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-16-55026939</guid>
<pubDate>Wed, 16 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十四）TiDB Binlog 源码解析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-15-54940241.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/54940241&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d734c2c18de0b1415b4b39a664766a19_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：姚维&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB Binlog Overview&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这篇文章不是讲 TiDB Binlog 组件的源码，而是讲 TiDB 在执行 DML/DDL 语句过程中，如何将 Binlog 数据 发送给 TiDB Binlog 集群的 Pump 组件。目前 TiDB 在 DML 上的 Binlog 用的类似 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/refman/5.7/en/binary-log-formats.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Row-based&lt;/a&gt; 的格式。具体 Binlog 具体的架构细节可以参看这篇 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;文章&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;这里只描述 TiDB 中的代码实现。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DML Binlog&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 采用 protobuf 来编码 binlog，具体的格式可以见 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tipb/blob/master/proto/binlog/binlog.proto&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;binlog.proto&lt;/a&gt;。这里讨论 TiDB 写 Binlog 的机制，以及 Binlog 对 TiDB 写入的影响。&lt;/p&gt;&lt;p&gt;TiDB 会在 DML 语句提交，以及 DDL 语句完成的时候，向 pump 输出 Binlog。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Statement 执行阶段&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;DML 语句包括 Insert/Replace、Update、Delete，这里挑 Insert 语句来阐述，其他的语句行为都类似。首先在 Insert 语句执行完插入（未提交）之前，会把自己新增的数据记录在 &lt;code&gt;binlog.TableMutation&lt;/code&gt; 结构体中。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;// TableMutation 存储表中数据的变化
message TableMutation {
	    // 表的 id，唯一标识一个表
	    optional int64 table_id      = 1 [(gogoproto.nullable) = false]; 
	    
	    // 保存插入的每行数据
	    repeated bytes inserted_rows = 2;
	    
	    // 保存修改前和修改后的每行的数据
	    repeated bytes updated_rows  = 3;
	    
	    // 已废弃
	    repeated int64 deleted_ids   = 4;
	    
	    // 已废弃
	    repeated bytes deleted_pks   = 5;
	     
	    // 删除行的数据
	    repeated bytes deleted_rows  = 6;
	    
	    // 记录数据变更的顺序
	    repeated MutationType sequence = 7;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个结构体保存于跟每个 Session 链接相关的事务上下文结构体中 &lt;code&gt;TxnState.mutations&lt;/code&gt;。 一张表对应一个 &lt;code&gt;TableMutation&lt;/code&gt; 对象，&lt;code&gt;TableMutation&lt;/code&gt; 里面保存了这个事务对这张表的所有变更数据。Insert 会把当前语句插入的行，根据 &lt;code&gt;RowID&lt;/code&gt; + &lt;code&gt;Row-value&lt;/code&gt; 的格式编码之后，追加到 &lt;code&gt;TableMutation.InsertedRows&lt;/code&gt; 中：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;func (t *Table) addInsertBinlog(ctx context.Context, h int64, row []types.Datum, colIDs []int64) error {
	mutation := t.getMutation(ctx)
	pk, err := codec.EncodeValue(ctx.GetSessionVars().StmtCtx, nil, types.NewIntDatum(h))
	if err != nil {
		return errors.Trace(err)
	}
	value, err := tablecodec.EncodeRow(ctx.GetSessionVars().StmtCtx, row, colIDs, nil, nil)
	if err != nil {
		return errors.Trace(err)
	}
	bin := append(pk, value...)
	mutation.InsertedRows = append(mutation.InsertedRows, bin)
	mutation.Sequence = append(mutation.Sequence, binlog.MutationType_Insert)
	return nil
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;等到所有的语句都执行完之后，在 &lt;code&gt;TxnState.mutations&lt;/code&gt; 中就保存了当前事务对所有表的变更数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Commit 阶段&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于 DML 而言，TiDB 的事务采用 2-phase-commit 算法，一次事务提交会分为 Prewrite 阶段，以及 Commit 阶段。这里分两个阶段来看看 TiDB 具体的行为。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Prewrite Binlog&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 &lt;code&gt;session.doCommit&lt;/code&gt; 函数中，TiDB 会构造 &lt;code&gt;binlog.PrewriteValue&lt;/code&gt;：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;message PrewriteValue {
    optional int64         schema_version = 1 [(gogoproto.nullable) = false];
    repeated TableMutation mutations      = 2 [(gogoproto.nullable) = false];
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个 &lt;code&gt;PrewriteValue&lt;/code&gt; 中包含了跟这次变动相关的所有行数据，TiDB 会填充一个类型为 &lt;code&gt;binlog.BinlogType_Prewrite&lt;/code&gt; 的 Binlog：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;info := &amp;amp;binloginfo.BinlogInfo{
	Data: &amp;amp;binlog.Binlog{
		Tp:            binlog.BinlogType_Prewrite,
		PrewriteValue: prewriteData,
	},
	Client: s.sessionVars.BinlogClient.(binlog.PumpClient),
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;TiDB 这里用一个事务的 Option &lt;code&gt;kv.BinlogInfo&lt;/code&gt; 来把 &lt;code&gt;BinlogInfo&lt;/code&gt; 绑定到当前要提交的 transaction 对象中：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;s.txn.SetOption(kv.BinlogInfo, info)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 &lt;code&gt;twoPhaseCommitter.execute&lt;/code&gt; 中，在把数据 prewrite 到 TiKV 的同时，会调用 &lt;code&gt;twoPhaseCommitter.prewriteBinlog&lt;/code&gt;，这里会把关联的 &lt;code&gt;binloginfo.BinlogInfo&lt;/code&gt; 取出来，把 Binlog 的 &lt;code&gt;binlog.PrewriteValue&lt;/code&gt; 输出到 Pump。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;binlogChan := c.prewriteBinlog()
err := c.prewriteKeys(NewBackoffer(prewriteMaxBackoff, ctx), c.keys)
if binlogChan != nil {
	binlogErr := &amp;lt;-binlogChan // 等待 write prewrite binlog 完成
	if binlogErr != nil {
		return errors.Trace(binlogErr)
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里值得注意的是，在 prewrite 阶段，是需要等待 write prewrite binlog 完成之后，才能继续做接下去的提交的，这里是为了保证 TiDB 成功提交的事务，Pump 至少一定能收到 Prewrite Binlog。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Commit Binlog&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 &lt;code&gt;twoPhaseCommitter.execute&lt;/code&gt; 事务提交结束之后，事务可能提交成功，也可能提交失败。TiDB 需要把这个状态告知 Pump：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;err = committer.execute(ctx)
if err != nil {
	committer.writeFinishBinlog(binlog.BinlogType_Rollback, 0)
	return errors.Trace(err)
}
committer.writeFinishBinlog(binlog.BinlogType_Commit, int64(committer.commitTS))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果发生了 error，那么输出的 Binlog 类型就为 &lt;code&gt;binlog.BinlogType_Rollback&lt;/code&gt;，如果成功提交，那么输出的 Binlog 类型就为 &lt;code&gt;binlog.BinlogType_Commit&lt;/code&gt;。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;func (c *twoPhaseCommitter) writeFinishBinlog(tp binlog.BinlogType, commitTS int64) {
	if !c.shouldWriteBinlog() {
		return
	}
	binInfo := c.txn.us.GetOption(kv.BinlogInfo).(*binloginfo.BinlogInfo)
	binInfo.Data.Tp = tp
	binInfo.Data.CommitTs = commitTS
	go func() {
		err := binInfo.WriteBinlog(c.store.clusterID)
		if err != nil {
			log.Errorf(&quot;failed to write binlog: %v&quot;, err)
		}
	}()
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;值得注意的是，这里 WriteBinlog 是单独启动 goroutine 异步完成的，也就是 Commit 阶段，是不再需要等待写 binlog 完成的。这里可以节省一点 commit 的等待时间，这里不需要等待是因为 Pump 即使接收不到这个 Commit Binlog，在超过 timeout 时间后，Pump 会自行根据 Prewrite Binlog 到 TiKV 中确认当条事务的提交状态。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DDL Binlog&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一个 DDL 有如下几个状态：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;const (
	JobStateNone    		JobState = 0
	JobStateRunning 		JobState = 1
	JobStateRollingback  	JobState = 2
	JobStateRollbackDone 	JobState = 3
	JobStateDone         	JobState = 4
	JobStateSynced 			JobState = 6
	JobStateCancelling 		JobState = 7
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这些状态代表了一个 DDL 任务所处的状态：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;JobStateNone&lt;/code&gt;，代表 DDL 任务还在处理队列，TiDB 还没有开始做这个 DDL。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRunning&lt;/code&gt;，当 DDL Owner 开始处理这个任务的时候，会把状态设置为 &lt;code&gt;JobStateRunning&lt;/code&gt;，之后 DDL 会开始变更，TiDB 的 Schema 可能会涉及多个状态的变更，这中间不会改变 DDL job 的状态，只会变更 Schema 的状态。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateDone&lt;/code&gt;， 当 TiDB 完成自己所有的 Schema 状态变更之后，会把 Job 的状态改为 Done。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateSynced&lt;/code&gt;，当 TiDB 每做一次 schema 状态变更，就会需要跟集群中的其他 TiDB 做一次同步，但是当 Job 状态为 &lt;code&gt;JobStateDone&lt;/code&gt; 之后，在 TiDB 等到所有的 TiDB 节点同步之后，会将状态修改为 &lt;code&gt;JobStateSynced&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateCancelling&lt;/code&gt;，TiDB 提供语法 &lt;code&gt;ADMIN CANCEL DDL JOBS job_ids&lt;/code&gt; 用于取消某个正在执行或者还未执行的 DDL 任务，当成功执行这个命令之后，DDL 任务的状态会变为 &lt;code&gt;JobStateCancelling&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRollingback&lt;/code&gt;，当 DDL Owner 发现 Job 的状态变为 &lt;code&gt;JobStateCancelling&lt;/code&gt; 之后，它会将 job 的状态改变为 &lt;code&gt;JobStateRollingback&lt;/code&gt;，以示已经开始处理 cancel 请求。&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRollbackDone&lt;/code&gt;，在做 cancel 的过程，也会涉及 Schema 状态的变更，也需要经历 Schema 的同步，等到状态回滚已经做完了，TiDB 会将 Job 的状态设置为 &lt;code&gt;JobStateRollbackDone&lt;/code&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;对于 Binlog 而言，DDL 的 Binlog 输出机制，跟 DML 语句也是类似的，只有开始处理事务提交阶段，才会开始写 Binlog 出去。那么对于 DDL 来说，跟 DML 不一样，DML 有事务的概念，对于 DDL 来说，SQL 的事务是不影响 DDL 语句的。但是 DDL 里面，上面提到的 Job 的状态变更，是作为一个事务来提交的（保证状态一致性）。所以在每个状态变更，都会有一个事务与之对应，但是上面提到的中间状态，DDL 并不会往外写 Binlog，只有 &lt;code&gt;JobStateRollbackDone&lt;/code&gt; 以及 &lt;code&gt;JobStateDone&lt;/code&gt; 这两种状态，TiDB 会认为 DDL 语句已经完成，会对外发送 Binlog，发送之前，会把 Job 的状态从 &lt;code&gt;JobStateDone&lt;/code&gt; 修改为 &lt;code&gt;JobStateSynced&lt;/code&gt;，这次修改，也涉及一次事务提交。这块逻辑的代码如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;worker.handleDDLJobQueue():

if job.IsDone() || job.IsRollbackDone() {
		binloginfo.SetDDLBinlog(d.binlogCli, txn, job.ID, job.Query)
		if !job.IsRollbackDone() {
			job.State = model.JobStateSynced
		}
		err = w.finishDDLJob(t, job)
		return errors.Trace(err)
}

type Binlog struct {
	DdlQuery []byte
	DdlJobId         int64
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;DdlQuery&lt;/code&gt; 会设置为原始的 DDL 语句，&lt;code&gt;DdlJobId&lt;/code&gt; 会设置为 DDL 的任务 ID。&lt;/p&gt;&lt;p&gt;对于最后一次 Job 状态的提交，会有两条 Binlog 与之对应，这里有几种情况：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;如果事务提交成功，类型分别为 &lt;code&gt;binlog.BinlogType_Prewrite&lt;/code&gt; 和 &lt;code&gt;binlog.BinlogType_Commit&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;如果事务提交失败，类型分别为 &lt;code&gt;binlog.BinlogType_Prewrite&lt;/code&gt; 和 &lt;code&gt;binlog.BinlogType_Rollback&lt;/code&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;所以，Pumps 收到的 DDL Binlog，如果类型为 &lt;code&gt;binlog.BinlogType_Rollback&lt;/code&gt; 应该只认为如下状态是合法的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;JobStateDone&lt;/code&gt; （因为修改为 &lt;code&gt;JobStateSynced&lt;/code&gt; 还未成功）&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRollbackDone&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;如果类型为 &lt;code&gt;binlog.BinlogType_Commit&lt;/code&gt;，应该只认为如下状态是合法的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;JobStateSynced&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;JobStateRollbackDone&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;当 TiDB 在提交最后一个 Job 状态的时候，如果事务提交失败了，那么 TiDB Owner 会尝试继续修改这个 Job，直到成功。也就是对于同一个 &lt;code&gt;DdlJobId&lt;/code&gt;，后续还可能会有多次 Binlog，直到出现 &lt;code&gt;binlog.BinlogType_Commit&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多 TiDB 源码阅读系列文章：&lt;/b&gt;&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-15-54940241</guid>
<pubDate>Tue, 15 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 源码阅读系列文章（二十三）Prepare/Execute 请求处理</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-03-53967950.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53967950&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-44348f6cb008b2c6fef5670d96cda457_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：苏立&lt;/p&gt;&lt;blockquote&gt;在之前的一篇文章&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiDB 源码阅读系列文章（三）SQL 的一生》&lt;/a&gt;中，我们介绍了 TiDB 在收到客户端请求包时，最常见的&lt;code&gt;Command --- COM_QUERY&lt;/code&gt;的请求处理流程。本文我们将介绍另外一种大家经常使用的&lt;code&gt;Command --- Prepare/Execute&lt;/code&gt;请求在 TiDB 中的处理过程。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;Prepare/Execute Statement 简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;首先我们先简单回顾下客户端使用 Prepare 请求过程：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;客户端发起 Prepare 命令将带 “?” 参数占位符的 SQL 语句发送到数据库，成功后返回 &lt;code&gt;stmtID&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;具体执行 SQL 时，客户端使用之前返回的 &lt;code&gt;stmtID&lt;/code&gt;，并带上请求参数发起 Execute 命令来执行 SQL。&lt;/li&gt;&lt;li&gt;不再需要 Prepare 的语句时，关闭 &lt;code&gt;stmtID&lt;/code&gt; 对应的 Prepare 语句。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;相比普通请求，Prepare 带来的好处是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;减少每次执行经过 Parser 带来的负担，因为很多场景，线上运行的 SQL 多是相同的内容，仅是参数部分不同，通过 Prepare 可以通过首次准备好带占位符的 SQL，后续只需要填充参数执行就好，可以做到“一次 Parse，多次使用”。&lt;/li&gt;&lt;li&gt;在开启 PreparePlanCache 后可以达到“一次优化，多次使用”，不用进行重复的逻辑和物理优化过程。&lt;/li&gt;&lt;li&gt;更少的网络传输，因为多次执行只用传输参数部分，并且返回结果 Binary 协议。&lt;/li&gt;&lt;li&gt;因为是在执行的同时填充参数，可以防止 SQL 注入风险。&lt;/li&gt;&lt;li&gt;某些特性比如 serverSideCursor 需要是通过 Prepare statement 才能使用。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;TiDB 和 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/refman/5.7/en/sql-syntax-prepared-statements.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MySQL 协议&lt;/a&gt; 一样，对于发起 Prepare/Execute 这种使用访问模式提供两种方式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Binary 协议：即上述的使用 &lt;code&gt;COM_STMT_PREPARE&lt;/code&gt;，&lt;code&gt;COM_STMT_EXECUTE&lt;/code&gt;，&lt;code&gt;COM_STMT_CLOSE&lt;/code&gt; 命令并且通过 Binary 协议获取返回结果，这是目前各种应用开发常使用的方式。&lt;/li&gt;&lt;li&gt;文本协议：使用 &lt;code&gt;COM_QUERY&lt;/code&gt;，并且用 &lt;code&gt;PREPARE&lt;/code&gt;，&lt;code&gt;EXECUTE&lt;/code&gt;，&lt;code&gt;DEALLOCATE PREPARE&lt;/code&gt; 使用文本协议获取结果，这个效率不如上一种，多用于非程序调用场景，比如在 MySQL 客户端中手工执行。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下面我们主要以 Binary 协议来看下 TiDB 的处理过程。文本协议的处理与 Binary 协议处理过程比较类似，我们会在后面简要介绍一下它们的差异点。&lt;/p&gt;&lt;h2&gt;&lt;code&gt;&lt;b&gt;COM_STMT_PREPARE&lt;/b&gt;&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;首先，客户端发起 &lt;code&gt;COM_STMT_PREPARE&lt;/code&gt;，在 TiDB 收到后会进入 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L51&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;clientConn#handleStmtPrepare&lt;/a&gt;&lt;/code&gt;，这个函数会通过调用 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L305&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDBContext#Prepare&lt;/a&gt;&lt;/code&gt; 来进行实际 Prepare 操作并返回 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/com-stmt-prepare-response.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;结果&lt;/a&gt; 给客户端，实际的 Prepare 处理主要在 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/session/session.go%23L924&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;session#PrepareStmt&lt;/a&gt;&lt;/code&gt;和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/executor/prepared.go%23L73&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PrepareExec&lt;/a&gt;&lt;/code&gt; 中完成：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;调用 Parser 完成文本到 AST 的转换，这部分可以参考&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-5/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiDB 源码阅读系列文章（五）TiDB SQL Parser 的实现》&lt;/a&gt;。&lt;/li&gt;&lt;li&gt;使用名为 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/executor/prepared.go%23L57&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;paramMarkerExtractor&lt;/a&gt;&lt;/code&gt; 的 visitor 从 AST 中提取 “?” 表达式，并根据出现位置（offset）构建排序 Slice，后面我们会看到在 Execute 时会通过这个 Slice 值来快速定位并替换 “?” 占位符。&lt;/li&gt;&lt;li&gt;检查参数个数是否超过 Uint16 最大值（这个是 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/com-stmt-prepare-response.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;协议限制&lt;/a&gt;，对于参数只提供 2 个 Byte）。&lt;/li&gt;&lt;li&gt;进行 Preprocess， 并且创建 LogicPlan， 这部分实现可以参考之前关于 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-7/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;逻辑优化的介绍&lt;/a&gt;，这里生成 LogicPlan 主要为了获取并检查组成 Prepare 响应中需要的列信息。&lt;/li&gt;&lt;li&gt;生成 &lt;code&gt;stmtID&lt;/code&gt;，生成的方式是当前会话中的递增 int。&lt;/li&gt;&lt;li&gt;保存 &lt;code&gt;stmtID&lt;/code&gt; 到 &lt;code&gt;ast.Prepared&lt;/code&gt; (由 AST，参数类型信息，schema 版本，是否使用 &lt;code&gt;PreparedPlanCache&lt;/code&gt; 标记组成) 的映射信息到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/sessionctx/variable/session.go%23L185&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;SessionVars#PreparedStmts&lt;/a&gt;&lt;/code&gt; 中供 Execute 部分使用。&lt;/li&gt;&lt;li&gt;保存 &lt;code&gt;stmtID&lt;/code&gt; 到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L57&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDBStatement&lt;/a&gt;&lt;/code&gt; （由 &lt;code&gt;stmtID&lt;/code&gt;，参数个数，SQL 返回列类型信息，&lt;code&gt;sendLongData&lt;/code&gt; 预 &lt;code&gt;BoundParams&lt;/code&gt; 组成）的映射信息保存到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L53&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDBContext#stmts&lt;/a&gt;&lt;/code&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在处理完成之后客户端会收到并持有 &lt;code&gt;stmtID&lt;/code&gt; 和参数类型信息，返回列类型信息，后续即可通过 &lt;code&gt;stmtID&lt;/code&gt; 进行执行时，server 可以通过 6、7 步保存映射找到已经 Prepare 的信息。&lt;/p&gt;&lt;h2&gt;&lt;code&gt;&lt;b&gt;COM_STMT_EXECUTE&lt;/b&gt;&lt;/code&gt;&lt;/h2&gt;&lt;p&gt; Prepare 成功之后，客户端会通过 &lt;code&gt;COM_STMT_EXECUTE&lt;/code&gt; 命令请求执行，TiDB 会进入 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L108&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;clientConn#handleStmtExecute&lt;/a&gt;&lt;/code&gt;，首先会通过 stmtID 在上节介绍中保存的 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L53&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDBContext#stmts&lt;/a&gt;&lt;/code&gt; 中获取前面保存的 &lt;code&gt;TiDBStatement&lt;/code&gt;，并解析出是否使用 &lt;code&gt;userCursor&lt;/code&gt; 和请求参数信息，并且调用对应 &lt;code&gt;TiDBStatement&lt;/code&gt; 的 Execute 进行实际的 Execute 逻辑：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;生成 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/blob/732efe993f70da99fdc18acb380737be33f2333a/ast/misc.go%23L218&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ast.ExecuteStmt&lt;/a&gt;&lt;/code&gt; 并调用 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/optimize.go%23L28&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;planer.Optimize&lt;/a&gt;&lt;/code&gt; 生成 &lt;code&gt;plancore.Execute&lt;/code&gt;，和普通优化过程不同的是会执行 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/optimize.go%23L53&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Exeucte#OptimizePreparedPlan&lt;/a&gt;&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;使用 &lt;code&gt;stmtID&lt;/code&gt; 通过 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/sessionctx/variable/session.go%23L190&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;SessionVars#PreparedStmts&lt;/a&gt;&lt;/code&gt; 获取到到 Prepare 阶段的 &lt;code&gt;ast.Prepared&lt;/code&gt; 信息。&lt;/li&gt;&lt;li&gt;使用上一节第 2 步中准备的 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/common_plans.go%23L167&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;prepared.Params&lt;/a&gt;&lt;/code&gt; 来快速查找并填充参数值；同时会保存一份参数到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/sessionctx/variable/session.go%23L190&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;sessionVars.PreparedParams&lt;/a&gt;&lt;/code&gt; 中，这个主要用于支持 &lt;code&gt;PreparePlanCache&lt;/code&gt; 延迟获取参数。&lt;/li&gt;&lt;li&gt;判断对比判断 Prepare 和 Execute 之间 schema 是否有变化，如果有变化则重新 Preprocess。&lt;/li&gt;&lt;li&gt;之后调用 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/common_plans.go%23L188&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Execute#getPhysicalPlan&lt;/a&gt;&lt;/code&gt; 获取物理计划，实现中首先会根据是否启用 PreparedPlanCache 来查找已缓存的 Plan，本文后面我们也会专门介绍这个。&lt;/li&gt;&lt;li&gt;在没有开启 PreparedPlanCache 或者开启了但没命中 cache 时，会对 AST 进行一次正常的 Optimize。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在获取到 PhysicalPlan 后就是正常的 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/35134962&quot; class=&quot;internal&quot;&gt;Executing 执行&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;&lt;code&gt;&lt;b&gt;COM_STMT_CLOSE&lt;/b&gt;&lt;/code&gt;&lt;/h2&gt;&lt;p&gt; 在客户不再需要执行之前的 Prepared 的语句时，可以通过&lt;code&gt;COM_STMT_CLOSE&lt;/code&gt;来释放服务器资源，TiDB 收到后会进入&lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L501&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;clientConn#handleStmtClose&lt;/a&gt;&lt;/code&gt;，会通过&lt;code&gt;stmtID&lt;/code&gt;在&lt;code&gt;TiDBContext#stmts&lt;/code&gt;中找到对应的&lt;code&gt;TiDBStatement&lt;/code&gt;，并且执行&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/driver_tidb.go%23L152&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Close&lt;/a&gt;清理之前的保存的&lt;code&gt;TiDBContext#stmts&lt;/code&gt;和&lt;code&gt;SessionVars#PrepareStmts&lt;/code&gt;，不过通过代码我们看到，对于前者的确直接进行了清理，对于后者不会删除而是加入到&lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/session/session.go%23L1020&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RetryInfo#DroppedPreparedStmtIDs&lt;/a&gt;&lt;/code&gt;中，等待当前事务提交或回滚才会从&lt;code&gt;SessionVars#PrepareStmts&lt;/code&gt;中清理，之所以延迟删除是由于 TiDB 在事务提交阶段遇到冲突会根据配置决定是否重试事务，参与重试的语句可能只有 Execute 和 Deallocate，为了保证重试还能通过&lt;code&gt;stmtID&lt;/code&gt;找到 prepared 的语句 TiDB 目前使用延迟到事务执行完成后才做清理。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;其他 &lt;code&gt;COM_STMT&lt;/code&gt;&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;br&gt;除了上面介绍的 3 个 &lt;code&gt;COM_STMT&lt;/code&gt;，还有另外几个 &lt;code&gt;COM_STMT_SEND_LONG_DATA&lt;/code&gt;，&lt;code&gt;COM_STMT_FETCH&lt;/code&gt;，&lt;code&gt;COM_STMT_RESET&lt;/code&gt; 也会在 Prepare 中使用到。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;COM_STMT_SEND_LONG_DATA&lt;/b&gt;&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;某些场景我们 SQL 中的参数是 &lt;code&gt;TEXT&lt;/code&gt;，&lt;code&gt;TINYTEXT&lt;/code&gt;，&lt;code&gt;MEDIUMTEXT&lt;/code&gt;，&lt;code&gt;LONGTEXT&lt;/code&gt; and &lt;code&gt;BLOB&lt;/code&gt;，&lt;code&gt;TINYBLOB&lt;/code&gt;，&lt;code&gt;MEDIUMBLOB&lt;/code&gt;，&lt;code&gt;LONGBLOB&lt;/code&gt; 列时，客户端通常不会在一次 Execute 中带大量的参数，而是单独通过 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/com-stmt-send-long-data.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;COM_SEND_LONG_DATA&lt;/a&gt;&lt;/code&gt; 预先发到 TiDB，最后再进行 Execute。&lt;/p&gt;&lt;p&gt;TiDB 的处理在 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L514&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;client#handleStmtSendLongData&lt;/a&gt;&lt;/code&gt;，通过 &lt;code&gt;stmtID&lt;/code&gt; 在 &lt;code&gt;TiDBContext#stmts&lt;/code&gt; 中找到 &lt;code&gt;TiDBStatement&lt;/code&gt; 并提前放置 &lt;code&gt;paramID&lt;/code&gt; 对应的参数信息，进行追加参数到 &lt;code&gt;boundParams&lt;/code&gt;（所以客户端其实可以多次 send 数据并追加到一个参数上），Execute 时会通过 &lt;code&gt;stmt.BoundParams()&lt;/code&gt; 获取到提前传过来的参数并和 Execute 命令带的参数 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L176&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;一起执行&lt;/a&gt;，在每次执行完成后会重置 &lt;code&gt;boundParams&lt;/code&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;COM_STMT_FETCH&lt;/b&gt;&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;通常的 Execute 执行后，TiDB 会向客户端持续返回结果，返回速率受 &lt;code&gt;max_chunk_size&lt;/code&gt; 控制（见《&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-10/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读系列文章（十）Chunk 和执行框架简介&lt;/a&gt;》）， 但实际中返回的结果集可能非常大。客户端受限于资源（一般是内存）无法一次处理那么多数据，就希望服务端一批批返回，&lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/com-stmt-fetch.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;COM_STMT_FETCH&lt;/a&gt;&lt;/code&gt; 正好解决这个问题。&lt;/p&gt;&lt;p&gt;它的使用首先要和 &lt;code&gt;COM_STMT_EXECUTE&lt;/code&gt; 配合（也就是必须使用 Prepared 语句执行）， &lt;code&gt;handleStmtExeucte&lt;/code&gt; 请求协议 flag 中有标记要使用 cursor，execute 在完成 plan 拿到结果集后并不立即执行而是把它缓存到 &lt;code&gt;TiDBStatement&lt;/code&gt; 中，并立刻向客户端回包中带上列信息并标记 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/status-flags.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ServerStatusCursorExists&lt;/a&gt;&lt;/code&gt;，这部分逻辑可以参看 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L193&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;handleStmtExecute&lt;/a&gt;&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;客户端看到 &lt;code&gt;ServerStatusCursorExists&lt;/code&gt; 后，会用 &lt;code&gt;COM_STMT_FETCH&lt;/code&gt; 向 TiDB 拉去指定 fetchSize 大小的结果集，在 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L210&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;connClient#handleStmtFetch&lt;/a&gt;&lt;/code&gt; 中，会通过 session 找到 &lt;code&gt;TiDBStatement&lt;/code&gt; 进而找到之前缓存的结果集，开始实际调用执行器的 Next 获取满足 fetchSize 的数据并返回客户端，如果执行器一次 Next 超过了 fetchSize 会只返回 fetchSize 大小的数据并把剩下的数据留着下次再给客户端，最后对于结果集最后一次返回会标记 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/internals/en/status-flags.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ServerStatusLastRowSend&lt;/a&gt;&lt;/code&gt; 的 flag 通知客户端没有后续数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;COM_STMT_RESET&lt;/b&gt;&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;主要用于客户端主动重置 &lt;code&gt;COM_SEND_LONG_DATA&lt;/code&gt; 发来的数据，正常 &lt;code&gt;COM_STMT_EXECUTE&lt;/code&gt; 后会自动重置，主要针对客户端希望主动废弃之前数据的情况，因为 &lt;code&gt;COM_STMT_SEND_LONG_DATA&lt;/code&gt; 是一直追加的操作，客户端某些场景需要主动放弃之前预存的参数，这部分逻辑主要位于 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/server/conn_stmt.go%23L531&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;connClient#handleStmtReset&lt;/a&gt;&lt;/code&gt; 中。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Prepared Plan Cache&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;通过前面的解析过程我们看到在 Prepare 时完成了 AST 转换，在之后的 Execute 会通过 &lt;code&gt;stmtID&lt;/code&gt; 找之前的 AST 来进行 Plan 跳过每次都进行 Parse SQL 的开销。如果开启了 Prepare Plan Cache，可进一步在 Execute 处理中重用上次的 PhysicalPlan 结果，省掉查询优化过程的开销。&lt;/p&gt;&lt;p&gt;TiDB 可以通过 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/config/config.toml.example%23L167&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;修改配置文件&lt;/a&gt; 开启 Prepare Plan Cache， 开启后每个新 Session 创建时会初始化一个 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/util/kvcache/simple_lru.go%23L38&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;SimpleLRUCache&lt;/a&gt;&lt;/code&gt; 类型的 &lt;code&gt;preparedPlanCache&lt;/code&gt; 用于保存用于缓存 Plan 结果，缓存的 key 是 &lt;code&gt;pstmtPlanCacheKey&lt;/code&gt;（由当前 DB，连接 ID，&lt;code&gt;statementID&lt;/code&gt;，&lt;code&gt;schemaVersion&lt;/code&gt;， &lt;code&gt;snapshotTs&lt;/code&gt;，&lt;code&gt;sqlMode&lt;/code&gt;，&lt;code&gt;timezone&lt;/code&gt; 组成，所以要命中 plan cache 这以上元素必须都和上次缓存的一致），并根据配置的缓存大小和内存大小做 LRU。&lt;/p&gt;&lt;p&gt;在 Execute 的处理逻辑 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/executor/prepared.go%23L161&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PrepareExec&lt;/a&gt;&lt;/code&gt; 中除了检查 &lt;code&gt;PreparePlanCache&lt;/code&gt; 是否开启外，还会判断当前的语句是否能使用 &lt;code&gt;PreparePlanCache&lt;/code&gt;。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;只有 &lt;code&gt;SELECT&lt;/code&gt;，&lt;code&gt;INSERT&lt;/code&gt;，&lt;code&gt;UPDATE&lt;/code&gt;，&lt;code&gt;DELETE&lt;/code&gt; 有可能可以使用 &lt;code&gt;PreparedPlanCache&lt;/code&gt;	。&lt;/li&gt;&lt;li&gt;并进一步通过 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/cacheable_checker.go%23L43&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;cacheableChecker&lt;/a&gt;&lt;/code&gt; visitor 检查 AST 中是否有变量表达式，子查询，&quot;order by ?&quot;，&quot;limit ?，?&quot; 和 UnCacheableFunctions 的函数调用等不可以使用 PlanCache 的情况。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;如果检查都通过则在 &lt;code&gt;Execute#getPhysicalPlan&lt;/code&gt; 中会用当前环境构建 cache key 查找 &lt;code&gt;preparePlanCache&lt;/code&gt;。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;未命中 Cache&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们首先来看下没有命中 Cache 的情况。发现没有命中后会用 &lt;code&gt;stmtID&lt;/code&gt; 找到的 AST 执行 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/executor/prepared.go%23L161&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Optimize&lt;/a&gt;，但和正常执行 Optimize 不同对于 Cache 的 Plan， 我需要对 “?” 做延迟求值处理， 即将占位符转换为一个 function 做 Plan 并 Cache， 后续从 Cache 获取后 function 在执行时再从具体执行上下文中实际获取执行参数。&lt;/p&gt;&lt;p&gt;回顾下构建 LogicPlan 的过程中会通过 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/expression_rewriter.go%23L151&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;expressionRewriter&lt;/a&gt;&lt;/code&gt; 将 AST 转换为各类 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/expression/expression.go%23L42&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;expression.Expression&lt;/a&gt;&lt;/code&gt;，通常对于 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/types/parser_driver/value_expr.go%23L167&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ParamMarkerExpr&lt;/a&gt;&lt;/code&gt; 会重写为 Constant 类型的 expression，但如果该条 stmt 支持 Cache 的话会重写为 Constant 并带上一个特殊的 &lt;code&gt;DeferredExpr&lt;/code&gt; 指向一个 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/expression/builtin_other.go%23L787&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;GetParam&lt;/a&gt;&lt;/code&gt; 的函数表达式，而这个函数会在执行时实际从前面 Execute 保存到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/sessionctx/variable/session.go%23L190&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;sessionVars.PreparedParams&lt;/a&gt;&lt;/code&gt; 中获取，这样就做到了 Plan 并 Cache 一个参数无关的 Plan，然后实际执行的时填充参数。&lt;/p&gt;&lt;p&gt;新获取 Plan 后会保存到 &lt;code&gt;preparedPlanCache&lt;/code&gt; 供后续使用。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;命中 Cache&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;让我们回到 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/common_plans.go%23L188&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;getPhysicalPlan&lt;/a&gt;&lt;/code&gt;，如果 Cache 命中在获取 Plan 后我们需要重新 build plan 的 range，因为前面我们保存的 Plan 是一个带 &lt;code&gt;GetParam&lt;/code&gt; 的函数表达式，而再次获取后，当前参数值已经变化，我们需要根据当前 Execute 的参数来重新修正 range，这部分逻辑代码位于 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lysu/tidb/blob/source-read-prepare/planner/core/common_plans.go%23L214&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Execute#rebuildRange&lt;/a&gt;&lt;/code&gt; 中，之后就是正常的执行过程了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;文本协议的 Prepared&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;前面主要介绍了二进制协议的 Prepared 执行流程，还有一种执行方式是通过二进制协议来执行。&lt;/p&gt;&lt;p&gt;客户端可以通过 &lt;code&gt;COM_QUREY&lt;/code&gt; 发送：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;PREPARE stmt_name FROM prepareable_stmt;
EXECUTE stmt_name USING @var_name1, @var_name2,...
DEALLOCTE PREPARE stmt_name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;来进行 Prepared，TiDB 会走正常 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/35134962&quot; class=&quot;internal&quot;&gt;文本 Query 处理流程&lt;/a&gt;，将 SQL 转换 Prepare，Execute，Deallocate 的 Plan， 并最终转换为和二进制协议一样的 &lt;code&gt;PrepareExec&lt;/code&gt;，&lt;code&gt;ExecuteExec&lt;/code&gt;，&lt;code&gt;DealocateExec&lt;/code&gt; 的执行器进行执行。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;写在最后&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prepared 是提高程序 SQL 执行效率的有效手段之一。熟悉 TiDB 的 Prepared 实现，可以帮助各位读者在将来使用 Prepared 时更加得心应手。另外，如果有兴趣向 TiDB 贡献代码的读者，也可以通过本文更快的理解这部分的实现。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多 TiDB 源码阅读系列文章：&lt;/b&gt;&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-03-53967950</guid>
<pubDate>Thu, 03 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>写给社区的回顾和展望：TiDB 2019, Level Up !</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-03-53915150.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53915150&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1ba53738a37e83c05ad220d4100d9092_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄东旭 &lt;a class=&quot;member_mention&quot; href=&quot;http://www.zhihu.com/people/5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hash=&quot;5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hovercard=&quot;p$b$5940b1ec1c21a3538c6cfcf5711a75a6&quot;&gt;@Ed Huang&lt;/a&gt; &lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;2018 年对于 TiDB 和 PingCAP 来说是一个由少年向成年的转换的一年，如果用一个关键字来概括就是「蜕变」。&lt;/b&gt;在这一年很欣喜的看到 TiDB 和 TiKV 在越来越多的用户使用在了越来越广泛的场景中，作为一个刚刚 3 岁多的开源项目，没有背后强大的社区的话，是没有办法取得这样的进展的。&lt;br&gt;同时在技术上，2018 年我觉得也交出了一份令人满意的答卷，TiDB 的几个主要项目今年一共合并了 4380 个提交，这几天在整理 2018 年的 Change Log 时候，对比了一下年初的版本，这 4380 个 Commits 背后代表了什么，这里简单写一个文章总结一下。&lt;/blockquote&gt;&lt;p&gt;回想起来，TiDB 是最早定位为 HTAP 的通用分布式数据库之一，如果熟悉我们的老朋友一定知道，我们最早时候一直都是定位 NewSQL，当然现在也是。但是 NewSQL 这个词有个问题，到底 New 在哪，解决了哪些问题，很难一目了然，其实一开始我们就想解决一个 MySQL 分库分表的问题，但是后来慢慢随着我们的用户越来越多，使用的场景也越来越清晰，很多用户的场景已经开始超出了一个「更大的 MySQL 」的使用范围，于是我们从实验室和学术界找到了我们觉得更加清晰的定义：HTAP，希望能构建一个融合 OLTP 和 OLAP 通用型分布式数据库。但是要达成这个目标非常复杂，我们的判断是如果不是从最底层重新设计，很难达到我们的目标，&lt;b&gt;我们认为这是一条更困难但是正确的路，现在看来，这条路是走对了，而且未来会越走越快，越走越稳。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;346&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;346&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-c3d4de87e423ad3610b21841f89bb337_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;在 SQL 层这边，TiDB 选择了 MySQL 的协议兼容，一方面持续的加强语法兼容性，另一方面选择自研优化器和执行器，带来的好处就是没有任何历史负担持续优化。回顾今年最大的一个工作应该是重构了执行器框架，&lt;/b&gt;TiDB的 SQL 层还是经典的 Volcano 模型，我们引入了新的内存数据结构 Chunk 来批量处理多行数据，并对各个算子都实现了基于 Chunk 的迭代器接口，这个改进对于 OLAP 请求的改进非常明显，在 TiDB 的 TPC-H 测试集上能看出来（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/docs-cn/blob/master/benchmark/tpch.md&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/docs&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-cn/blob/master/benchmark/tpch.md&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），Chunk 的引入为我们全面的向量化执行和 CodeGen 支持打下了基础。目前在 TiKV 内部对于下推算子的执行还没有使用 Chunk 改造，不过这个已经在计划中，在 TiKV 中这个改进，预期对查询性能的提升也将非常显著。&lt;/p&gt;&lt;p&gt;&lt;b&gt;另一方面，一个数据库查询引擎最核心的组件之一：优化器，在今年也有长足的进步。&lt;/b&gt;我们在 2017 年就已经全面引入了基于代价的 SQL 优化（CBO，Cost-Based Optimization），我们在今年改进了我们的代价评估模型，加入了一些新的优化规则，同时实现了 Join Re-Order 等一系列优化，从结果上来看，目前在 TPC-H 的测试集上，对于所有 Query，TiDB 的 SQL 优化器大多已给出了最优的执行计划。CBO 的另一个关键模块是统计信息收集，在今年，我们引入了自动的统计信息收集算法，使优化器的适应性更强。另外针对 OLTP 的场景 TiDB 仍然保留了轻量的 RBO 甚至直接 Bypass 优化器，以提升 OLTP 性能。另外，感谢三星韩国研究院的几位工程师的贡献，他们给 TiDB 引入了 Query Plan Cache，对高并发场景下查询性能的提升也很明显。另外在功能上，我们引入了 Partition Table 的支持，对于一些 Partition 特性很明显的业务，TiDB 能够更加高效的调度数据的写入读取和更新。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;787&quot; data-rawheight=&quot;362&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;787&quot; data-original=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;787&quot; data-rawheight=&quot;362&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;787&quot; data-original=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-4468c215440d1dd644f7b933ce05a6d8_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;一直以来，TiDB 的 SQL 层作为纯 Go 语言实现的最完备的 MySQL 语法兼容层，很多第三方的 MySQL 工具在使用着 TiDB 的 SQL Parser，其中的优秀代表比如&lt;u&gt;&lt;b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/user-case-xiaomi/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;小米的 Soar&lt;/a&gt;&lt;/b&gt;&lt;/u&gt;（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/XiaoMi/soar&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/XiaoMi/soar&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;）。&lt;b&gt;为了方便第三方更好的复用 TiDB Parser，我们在 2018 年将 Parser 从主项目中剥离了出来，成为了一个独立的项目：pingcap/parser，希望能帮到更多的人。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;说到 TiDB 的底层存储 TiKV 今年也有很多让人眼前一亮的更新。&lt;/b&gt;在 TiKV 的基石——一致性算法 Raft 这边，大家知道 TiKV 采用的是 Multi-Raft 的架构，内部通过无数个 Raft Group 动态的分裂、合并、移动以达到动态伸缩和动态负载均衡。我们在今年仍然持续在扩展 Multi-Raft 的边界，我们今年加入了动态的 Raft Group 合并，以减轻元信息存储和心跳通信的负担；给 Raft 扩展了 Learner 角色（只同步 Log 不投票的角色） 为 OLAP Read 打下基础；给 Raft 的基础算法加入了 Pre-Vote 的阶段，让整个系统在异常网络状态下可靠性更高。&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3b605b032874f9ed2adaeae0c663e062_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;412&quot; data-rawheight=&quot;395&quot; class=&quot;content_image&quot; width=&quot;412&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3b605b032874f9ed2adaeae0c663e062_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;412&quot; data-rawheight=&quot;395&quot; class=&quot;content_image lazy&quot; width=&quot;412&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-3b605b032874f9ed2adaeae0c663e062_b.jpg&quot;&gt;&lt;figcaption&gt;Raft Group Merge&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在性能方面，我们花了很大的精力重构了我们单机上多 Raft Group 的线程模型（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/pull/3568&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/tikv/tikv/pu&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ll/3568&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）， 虽然还没有合并到 master 分支，在我们测试中，这个优化带来了两倍以上的吞吐提升，同时写入延迟降低至现在的版本的 1/2 ，预计在这两周我们会完成这个巨大的 PR 的 Code Review，各位同学可以期待一下 :) &lt;/p&gt;&lt;p&gt;第三件事情是我们开始将 TiKV 的本地存储引擎的接口彻底抽象出来，目标是能做到对 RocksDB 的弱耦合，这点的意义很大，不管是社区还是我们自己，对新的单机存储引擎支持将变得更加方便。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;510&quot; data-rawheight=&quot;464&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;510&quot; data-original=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;510&quot; data-rawheight=&quot;464&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;510&quot; data-original=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-5a502d2a8676a335c278cde6caa03710_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;在 TiKV 社区这边，今年的另外一件大事是加入了 CNCF，变成了 CNCF 的托管项目，也是 CNCF 基金会第一个非结构化数据库项目。&lt;/b&gt; 后来很多朋友问我，为什么捐赠的是 TiKV 而不是 TiDB，其实主要的原因就像我在当天的一条 Tweet 说的，TiKV 更像是的一个更加通用的组件，当你有一个可以弹性伸缩的，支持跨行 ACID 事务的 Key-Value 数据库时，你会发现构建其他很多可靠的分布式系统会容易很多，这在我们之后的 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/mKygN5EQoaaeFMDIFeAzsw&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Hackathon&lt;/a&gt;&lt;/u&gt; 中得到了很好的体现。另外社区已经开始出现基于 TiKV 构建的 Redis 协议支持，以及分布式队列系统，例如 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/2tyAtcmKUU2L1yoE_V3SsA&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;meitu/titan&lt;/a&gt;&lt;/u&gt; 项目。作为一个基金会项目，社区不仅仅可以直接使用，更能够将它作为构建其他系统的基石，我觉得更加有意义。类似的，今年我们将我们的 Raft 实现从主项目中独立了出来（pingcap/raft-rs），也是希望更多的人能从中受益。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;“……其 KV与 SQL分层的方式，刚好符合我们提供 NoSQL 存储和关系型存储的需求，另外，PingCAP 的文档齐全，社区活跃，也已经在实际应用场景有大规模的应用，公司在北京，技术交流也非常方便，事实证明，后面提到的这几个优势都是对的……”&lt;/i&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;                                                          ——美图公司 Titan 项目负责人任勇全对 TiKV 的评论&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;在 TiDB 的设计之初，我们坚定将调度和元信息从存储层剥离出来（PD），现在看来，好处正渐渐开始显示出来。&lt;/b&gt;今年在 PD 上我们花了很大精力在处理热点探测和快速热点调度，调度和存储分离的架构让我们不管是在开发，测试还是上线新的调度策略时效率很高。瞬时热点一直是分布式存储的最大敌人，如何快速发现和处理，我们也有计划尝试将机器学习引入 PD 的调度中，这是 2019 会尝试的一个事情。总体来说，这个是一个长期的课题。&lt;/p&gt;&lt;p&gt;我在几个月前的一篇文章提到过 TiDB 为什么从 Day-1 起就 All-in Kubernetes （《&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/m2_Mf0-x_KpPHbnOawyy2A&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;十问 TiDB ：关于架构设计的一些思考&lt;/a&gt;》），今年很欣喜的看到，Kubernetes 及其周边生态已经渐渐成熟，已经开始有很多公司用 Kubernetes 来运行 Mission-critical 的系统，这也佐证了我们当年的判断。2018 年下半年，我们也开源了我们的 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/9Buo-pFMHF892muIdTITPQ&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Operator&lt;/a&gt;&lt;/u&gt;（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-operator&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），这个项目并不止是一个简单的在 K8s 上自动化运维 TiDB 的工具，在我们的战略里面，是作为 Cloud TiDB 的重要基座，过去设计一个完善的多租户系统是一件非常困难的事情，同时调度对象是数据库这种带状态服务，更是难上加难，TiDB-Operator 的开源也是希望能够借助社区的力量，一起将它做好。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;543&quot; data-rawheight=&quot;347&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;543&quot; data-original=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;543&quot; data-rawheight=&quot;347&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;543&quot; data-original=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-05db611483e0ead293347c886e7d36c8_b.jpg&quot;&gt;&lt;figcaption&gt;多租户 TiDB&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;今年还做了一件很大的事情，我们成立了一个新的部门 TEP（TiDB Enterprise Platform）专注于商业化组件及相关的交付质量控制。作为一个企业级的分布式数据库，TiDB 今年完成了商业化从0到1的跨越，越来越的付费客户证明 TiDB 的核心的成熟度已经可以委以重任，成立 TEP 小组也是希望在企业级产品方向上继续发力。从 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-2/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-Lightning&lt;/a&gt;&lt;/u&gt;（MySQL 到 TiDB 高速离线数据导入工具）到 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-DM&lt;/a&gt;&lt;/u&gt;（TiDB-DataMigration，端到端的数据迁移-同步工具）能看到发力的重点在让用户无缝的从上游迁移到 TiDB 上。另一方面，&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-Binlog&lt;/a&gt;&lt;/u&gt; 虽然不是今年的新东西，但是今年这一年在无数个社区用户的场景中锻炼，越来越稳定。&lt;b&gt;做工具可能在很多人看来并不是那么「高科技」， 很多时候也确实是脏活累活，但是这些经过无数用户场景打磨的周边工具和生态才是一个成熟的基础软件的护城河和竞争壁垒，在 PingCAP 内部，负责工具和外围系统研发的团队规模几乎和内核团队是 1:1 的配比，重要性可见一斑。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在使用场景上，TiDB 的使用规模也越来越大，下面这张图是我们统计的我们已知 TiDB 的用户，包括上线和准上线的用户，&lt;b&gt;从 1.0 GA 后，几乎是以一个指数函数的曲线在增长，应用的场景也从简单的 MySQL Sharding 替代方案变成横跨 OLTP 到实时数据中台的通用数据平台组件。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;598&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1004&quot; data-original=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1004&quot; data-rawheight=&quot;598&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1004&quot; data-original=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-9c68e680e6482544d0dfaa2f4d04882a_b.jpg&quot;&gt;&lt;figcaption&gt;TiDB 的用户数统计&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;今年几个比较典型的用户案例，  &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/user-case-meituan/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;美团&lt;/a&gt;&lt;/u&gt; 的横跨 OLTP 和实时数仓的深度实践，到 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/user-case-zhuanzhuan/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;转转&lt;/a&gt;&lt;/u&gt; 的 All-in TiDB 的体验，再到 TiDB 支撑的北京银行的核心交易系统。可以看到，这些案例从互联网公司的离线线数据存储到要求极端 SLA 的传统银行核心交易系统，TiDB 在这些场景里面都发光发热，甚至有互联网公司（转转）都喊出了 All-in TiDB 的口号，我们非常珍视这份信任，一定尽全力做出漂亮的产品，高质量的服务好我们的用户和客户。另一方面，TiDB 也慢慢开始产生国际影响力的，在线视频巨头葫芦软件（&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//Hulu.com&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;Hulu.com&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;），印度最大的在线票务网站 BookMyShow，东南亚最大的电商之一 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/user-case-shopee/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Shopee&lt;/a&gt;&lt;/u&gt; 等等都在大规模的使用 TiDB，在北美和欧洲也已经不少准上线和测试中的的巨头互联网公司。&lt;/p&gt;&lt;p&gt;&lt;b&gt;简单回顾了一下过去的 2018 年，我们看看未来在哪里。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;其实从我们在 2018 年做的几个比较大的技术决策就能看到，2019 年将是上面几个方向的延续。大的方向的几个指导思想是：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Predicable.&lt;/b&gt; （靠谱，在更广泛的场景中，做到行为可预测。）&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Make it right before making it fast.&lt;/b&gt;（稳定，先做稳，再做快。）&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Ease of use.&lt;/b&gt; （好用，简单交给用户，复杂留给自己。）&lt;/p&gt;&lt;p&gt;对于真正的 HTAP 场景来说，最大的挑战的是如何很好的做不同类型的 workload 隔离和数据结构根据访问特性自适应。我们在这个问题上给出了自己的答案：通过拓展 Raft 的算法，将不同的副本存储成异构的数据结构以适应不同类型的查询。&lt;/p&gt;&lt;p&gt;这个方法有以下好处：&lt;/p&gt;&lt;p&gt;1. 本身在 Multi-Raft 的层面上修改，不会出现由数据传输组件造成的瓶颈（类似 Kafka 或者 DTS），因为 Multi-Raft 本身就是可扩展的，数据同步的单位从 binlog，变成 Raft log，这个效率会更高，进一步降低了同步的延迟。&lt;/p&gt;&lt;p&gt;2. 更好的资源隔离，通过 PD 的调度，可以真正将不同的副本调度到隔离的物理机器上，真正做到互不影响。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-186c87ef885e27a9190489ff06031740_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;314&quot; data-rawheight=&quot;493&quot; class=&quot;content_image&quot; width=&quot;314&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-186c87ef885e27a9190489ff06031740_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;314&quot; data-rawheight=&quot;493&quot; class=&quot;content_image lazy&quot; width=&quot;314&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-186c87ef885e27a9190489ff06031740_b.jpg&quot;&gt;&lt;figcaption&gt;TiDB 2019 年会变成这个样子&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;691&quot; data-rawheight=&quot;341&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;691&quot; data-original=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;691&quot; data-rawheight=&quot;341&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;691&quot; data-original=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-e2163d8d7f6f591bbc1b8aec08035849_b.jpg&quot;&gt;&lt;figcaption&gt;Learner 在 HTAP 中的应用&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在执行器方面，我们会继续推进向量化，不出意外的话，今年会完成所有算子的全路径的向量化执行。&lt;/p&gt;&lt;p&gt;这个 HTAP 方案的另一个关键是存储引擎本身。2019 年，我们会引入新的存储引擎，当然第一阶段仍然会继续在 RocksDB 上改进，改进的目标仍然是减小 LSM-Tree 本身的写放大问题。选用的模型是 WiscKey （FAST16，&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;usenix.org/system/files&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/conference/fast16/fast16-papers-lu.pdf&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），WiscKey 的核心思想是将 Value 从 LSM-Tree 中剥离出来，以减少写放大，如果关注 TiKV 的朋友，已经能注意到我们已经在前几天将一个&lt;b&gt;新存储引擎 Titan&lt;/b&gt;（PingCAP 的 Titan，很遗憾和美图那个项目重名了） 合并到了 TiKV 的主干分支，这个 Titan 是我们在 RocksDB 上的 WiscKey 模型的一个实现，除了 WiscKey 的核心本身，我们还加入了对小 KV 的 inline 等优化，Titan 在我们的内部测试中效果很好，对长度随机的 key-value 写入的吞吐基本能达到原生 RocksDB 的 2 - 3 倍，当然性能提升并不是我最关注的，这个引擎对于 TiDB 最大的意义就是，这个引擎将让 TiDB 适应性更强，做到更加稳定，更加「可预测」。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;618&quot; data-rawheight=&quot;266&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;618&quot; data-original=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;618&quot; data-rawheight=&quot;266&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;618&quot; data-original=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-309ffc3c820dbfa3f6e2a2655dbaf54d_b.jpg&quot;&gt;&lt;figcaption&gt;TiKV 新的本地存储引擎 Titan&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 Titan 走向稳定的同时，我们也在调研从头构建一个更适合 TiDB 的 OLTP workload 的存储引擎，前面说到 2018 年做了抽象 TiKV 的本地存储引擎的事情就是为了这个打基础，当然我们仍然会走 LSM-Tree 的路线。这里多提一句，其实很多人都误解了 LSM-Tree 模型的真正优势，在我看来并不是性能，而是：做到可接受的性能的同时，LSM-Tree 的实现非常简单可维护，只有简单的东西才可以依赖，这个决定和我们在 Raft 与 Paxos 之间的选择偏好也是一致的。另外 LSM-Tree 的设计从宏观上来说，更加符合「冷热分层」以适配异构存储介质的想法，这个我相信是未来在存储硬件上的大趋势。&lt;/p&gt;&lt;p&gt;&lt;b&gt;至于在 OLAP 的存储引擎这边，我们走的就是纯列式存储的路线了，但是会和传统的 Columnar 数据结构的设计不太一样，这块的进展，我们会在&lt;/b&gt; &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/community/devcon2019/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;1 月 19 号的 TiDB DevCon&lt;/a&gt; &lt;b&gt;&lt;u&gt;上首秀，这里先卖个关子。&lt;/u&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;另一个大的方向是事务模型，目前来说，TiDB 从诞生起，事务模型就没有改变过，走的是传统的 Percolator 的 2PC。这个模型的好处是简单，吞吐也不是瓶颈，但是一个比较大的问题是延迟，尤其在跨数据中心的场景中，这里的延迟主要表现在往 TSO 拿时间戳的网络 roundtrip 上，当然了，我目前仍然认为时钟（TSO）是一个必不可少组件，在不降低一致性和隔离级别的大前提下也是目前我们的最好选择，另外 Percolator 的模型也不是没有办法对延迟进行优化，我们其实在 2018 年，针对 Percolator 本身做了一些理论上的改进，减少了几次网络的 roundtrip，也在年中书写了新的 2PC 改进的完整的 TLA+ 的证明（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tla-plus/blob/master/OptimizedCommitTS/OptimizedCommitTS.tla&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tla-&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;plus/blob/master/OptimizedCommitTS/OptimizedCommitTS.tla&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），证明了这个新算法的正确性，2019 年在这块还会有比较多的改进，其实我们一直在思考，怎么样能够做得更好，选择合适的时机做合适的优化。另外一方面，在事务模型这方面，2PC 在理论和工程上还有很多可以改进的空间，但是现在的当务之急继续的优化代码结构和整体的稳定性，这部分的工作在未来一段时间还是会专注在理论和证明上。另外一点大家可以期待的是，2019 年我们会引入安全的 Follower/Learner Read，这对保持一致性的前提下读的吞吐会提升，另外在跨数据中心的场景，读的延迟会更小。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1030&quot; data-rawheight=&quot;506&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1030&quot; data-original=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1030&quot; data-rawheight=&quot;506&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1030&quot; data-original=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-1072f0cf56c13c004f38ea7f7023b96b_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;差不多就这些吧，最后放一句我特别喜欢的丘吉尔的一句名言作为结尾。&lt;/p&gt;&lt;p&gt;Success is not final, failure is not fatal: it is the courage to continue that counts.&lt;/p&gt;&lt;p&gt;成功不是终点，失败也并非终结，最重要的是继续前进的勇气。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-03-53915150</guid>
<pubDate>Thu, 03 Jan 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiPrometheus：基于 TiDB 的 TSDB | TiDB Hackathon 优秀项目</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-28-53457577.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53457577&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f0ec6cb07659b51a79c3ac8332284862_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是&lt;b&gt;菜哥和他的朋友们队&lt;/b&gt;的&lt;b&gt;于畅&lt;/b&gt;同学，他们的项目&lt;b&gt; TiPrometheus&lt;/b&gt;已经被 Prometheus adapter 合并。该项目分两个小项目，分别解决了时序数据的存储与计算问题。存储主要兼容 Prometheus 语法和数据格式，实现了精确查询、模糊查询，完全兼容现有语法。所有数据仅存在 TiKV 中。计算主要通过 TiKV 调用 Lua 实现，通过 Lua 动态扩展实现数据计算的功能。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;项目简介&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;既然你关注了 TiDB， 想必你一定是个关注 Infrastructure 的硬汉子。监控作为 Infra 不可或缺的一环，其核心便是 &lt;b&gt;TSDB（time series database）&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;TSDB 是一种以时间为主要索引的数据库。主要用来存储大量以时间为序列的指标数据，数据结构也比较简单，通常包括特征信息，指标数据和 timestamp。常见的 TSDB 包括 InfluxDB，OpenTSDB，Prometheus。&lt;/p&gt;&lt;p&gt;而 Prometheus 是一整套监控系统，时序数据库是它的存储部分，下面这张架构图来自于 Prometheus 官方，简单概括了其架构和生态的组成。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;564&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;564&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-761c4f11257f1592da39e85a1f9ecccc_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;Prometheus 还支持一个图上没有体现的功能 Remote Storage，可以进行远程的读写，对查询是透明的。这个功能主要是用来做长存储。&lt;b&gt;我们的项目就是实现了一个基于 TiKV 的TSDB 来做 Prometheus 的 Remote Storage。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;核心实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prometheus 记录的数据结构分为两部分 label, samples。label 记录了一些特征信息。samples 包含了指标数据和 timestamp。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;&quot;labels&quot;: [{
 &quot;job&quot;:        &quot;node&quot;,
 &quot;instance&quot;:   &quot;123.123.1.211:9090&quot;,
}]
&quot;samples&quot;:[{
 &quot;timestamp&quot;: 1473305798
 &quot;value&quot;: 0.9
}]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;label 和时间范围结合，可以查询到需要的 value。&lt;/p&gt;&lt;p&gt;为了查询这些记录，我们需要构建两种索引 label index 和 time index，并以特殊的 key 存储 value。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;label index&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;每对 label 为会以 index:label:&amp;lt;name&amp;gt;#&amp;lt;latency&amp;gt; 为key，labelID 为 value 存入。新的记录会追加到 value 后面。这是一种搜索中常用的倒排索引。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;time index&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;每个 sample 项会以 index:label:&amp;lt;name&amp;gt;#&amp;lt;latency&amp;gt; 为 key，timestamp 为 value。splitTime为时间切片的起始点。新的 timestamp 会追加到 value 后面。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;doc 存储&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们将每一条 samples 记录以 timeseries:doc:&amp;lt;labelID&amp;gt;:&amp;lt;timestamp&amp;gt; 为 key 存入 TiKV，其中 labelID 是 label 全文的散列值。&lt;/p&gt;&lt;p&gt;下面做一个梳理。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;893&quot; data-rawheight=&quot;679&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;893&quot; data-original=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;893&quot; data-rawheight=&quot;679&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;893&quot; data-original=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f02edf9c84cde9b4a085350a4026c971_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;写入过程&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;ol&gt;&lt;li&gt;生成 labelID&lt;/li&gt;&lt;li&gt;构建 label index，index:label:&amp;lt;name&amp;gt;#&amp;lt;latency&amp;gt; &quot;labelID,labelID&quot;&lt;/li&gt;&lt;li&gt;构建 time index，index:timeseries:&amp;lt;labelID&amp;gt;:&amp;lt;splitTime&amp;gt; &quot;ts,ts&quot;&lt;/li&gt;&lt;li&gt;写入时序数据，timeseries:doc:&amp;lt;labelID&amp;gt;:&amp;lt;timestamp&amp;gt; &quot;value&quot;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;查询过程&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;1. 根据倒排索引查出 labelID 的集合，多对 label 的查询会对 labelID 集合求交集。&lt;/p&gt;&lt;p&gt;2. 根据 labelID 和时间范围内的时间分片查询包含的 timestamp。&lt;/p&gt;&lt;p&gt;3. 根据 labelID 和 timestamp 查出所需的 value。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;扯完这些没用的我们来聊些正经的。&lt;/i&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;我们为什么要做这样一个项目&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在 2018 年下半年，PingCAP 组织的 Hackathon，当时作为萌新即将参加比赛，想着一定要文体两开花，弘扬开源文化。&lt;/p&gt;&lt;p&gt;萌生了四个想法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiKV TSDB&lt;/li&gt;&lt;li&gt;Machine Learning on TiSpark&lt;/li&gt;&lt;li&gt;魔改 TiKV + Lua 做成 mapreduce&lt;/li&gt;&lt;li&gt;geo 全文检索&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;核心想法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1. 能做出来，符合参赛要求。&lt;/p&gt;&lt;p&gt;2. 确实能解决生产问题而不是一个比赛项目。&lt;/p&gt;&lt;p&gt;摸了摸头发，觉得 ML on TiSpark 太硬核，根本做不完。&lt;/p&gt;&lt;p&gt;TiHaoop 也太硬核，也做不完。&lt;/p&gt;&lt;p&gt;geo 没在厂里的生产中遇到什么问题。&lt;/p&gt;&lt;p&gt;最后辗转反侧思考一番，拍脑袋决定双线操作，做基于 TiKV 的 TSDB 和 TiKV + Lua，完成时序检索功能的同时，增加更丰富的算子（比赛前两天才想好做什么）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;比赛过程&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;周五&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;原计划，提前看看 rust，作为 rust 萌新。&lt;/p&gt;&lt;p&gt;于是前一天和同事借了本 rust 书，准备一天速成 rust。&lt;/p&gt;&lt;p&gt;后来发现还是看电视剧更管用。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Day 1（周六）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;周六参加比赛的时候，原以为会有个很长的开场致辞，所以决定 10 点再去。&lt;/p&gt;&lt;p&gt;到了现场，发现大家已经开始撸代码了？？？&lt;/p&gt;&lt;p&gt;整体过程还算顺利，但其中也遇到了一些问题。&lt;/p&gt;&lt;p&gt;Prometheus 的依赖和 TiKV 的一些依赖不兼容，于是 fork 一份 Prometheus 依赖，野路子改两行，兼容了。&lt;/p&gt;&lt;p&gt;下午 5 点的时候，时序基本实现了，但联调发现有数据读写不一致的情况。因菜哥的一个 bug 导致，然后开始了漫长的 debug，一共历时 5 个小时（特别说明，我们组叫菜哥和他的朋友们）。&lt;/p&gt;&lt;p&gt;晚 10 点，准备回家了，不准备再 debug 了，一个 bug 查了 5 个小时。作为娱乐队，熬夜写代码是不可能。&lt;/p&gt;&lt;p&gt;各回各家，各找各妈。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Day 2（周日）&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;开始漫长的半天精通 Lua 虚拟机 + rust。&lt;/p&gt;&lt;p&gt;也遇到了一些问题，比如为什么 TiKV 编译这么慢？？？一天只有 24 次编译机会？？？&lt;/p&gt;&lt;p&gt;下午 2 点，作为第一个讲的团队，及时生成了一个 PPT ，毕竟 PPT 工程师的基础还在。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;648&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;648&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-ab7635a51549e55ae4798cbed9fe0b7d_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;一周后的周一&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;之前写的渣代码，简单写了个 README。抱着尝试的心态，给 Prometheus adapter 提了个 PR。&lt;/p&gt;&lt;p&gt;&lt;b&gt;然后，居然被合进去了！！！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;一下午写的代码居然被合进去了！！！&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;成果&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;彻底打通了 TiKV 和 Prometheus。&lt;/p&gt;&lt;p&gt;为 TiKV 的时序存储和计算提供了一个思路（之前做过 TiDB 存储时序数据）。&lt;/p&gt;&lt;p&gt;为 Prometheus 的长存储提供了一个还算好用的方案（M3 其实还可以，Thanos 是分片机制，不能算真正意义的分布式存储）。&lt;/p&gt;&lt;p&gt;已在公司生产环境试用，需要经过大数据量的测试，如果没问题计划替代现有方案。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;感悟&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;参加 Hackathon，和周末加两天班没有太大的区别。&lt;/p&gt;&lt;p&gt;最先开始来，只是想混个奖品，比如说书包。去年参加 DevCon 给的布袋用了一年，还没坏，今年准备再领一个。&lt;/p&gt;&lt;p&gt;见到了很多年龄比我们小，但技术又还不错的小伙伴，比如兰海他们组，udf 那个组。也见到了一些年龄稍长的参赛者。&lt;/p&gt;&lt;p&gt;他们的存在，让我们在充满杂事的日常工作中又有了继续奋斗的动力。&lt;/p&gt;&lt;p&gt;似乎，当时选择这个行业没有错，而不仅仅是一份工作。&lt;/p&gt;&lt;p&gt;Just for fun。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;感谢&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;感谢唐刘老师和申砾老师的指导。&lt;/p&gt;&lt;p&gt;感谢 PingCAP 举办了这场大型网友见面活动，收获颇丰。&lt;/p&gt;&lt;p&gt;&lt;b&gt;项目地址：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/bragfoo/TiPrometheus&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/bragfoo/TiPr&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ometheus&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;（代码比较渣，思路供参考）。&lt;/p&gt;&lt;blockquote&gt;&lt;i&gt;打个广告：由菜哥和他的朋友们翻译的书：《Go 语言并发之道》已登陆京东、淘宝。非常棒一本 Go 语言书籍，搜索即可购买。&lt;/i&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;参考资料：&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;1. &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//fabxc.org/tsdb/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;fabxc.org/tsdb/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;2.&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//docs.influxdata.com/influxdb/v1.7/concepts/storage_engine/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;docs.influxdata.com/inf&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;luxdb/v1.7/concepts/storage_engine/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;3. &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/prometheus/prometheus/tree/release-1.8/storage&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/prometheus/p&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;rometheus/tree/release-1.8/storage&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;br&gt;&lt;b&gt;延伸阅读：&lt;/b&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D1%26sn%3D72d9d52558e83eb97cd709c67b5a4149%26chksm%3Deb1628e0dc61a1f60bb99ffe2fe42fafe91570159094fc5e3d46039b5490bd0c391ee500b8d6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Hackathon 2018 回顾&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D2%26sn%3D7eb3d41b2b5cf2a8a440b12121796e2d%26chksm%3Deb1628e0dc61a1f6719856b0eeadd4e878c3b59e0127f8b8f65ed1fb99b2a8981739b5449ce7%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;天真贝叶斯学习机 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D2%26sn%3D5f1ee6e838c3a86556fcd556662112c5%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiQuery：All Diagnosis in SQL | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D1%26sn%3D8a8861419dd22344a021667545005769%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;让 TiDB 访问多种数据源 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D2%26sn%3D3a601b2ff9100a9797605a825e478c01%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Lab 诞生记 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/_o_4XRJ22NfAfBvUFQqueQ&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiEye：Region 信息变迁历史可视化工具 | TiDB Hackathon 2018 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/sg3RiE8Fp96aQZlo1MM0hg&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiPrometheus：基于 TiDB 的 TSDB | TiDB Hackathon 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/q3ZHJ2rxMqfpdRd2oof2xw&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TBSSQL 的那些事 | TiDB Hackathon 优秀项目分享&lt;/a&gt;&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-28-53457577</guid>
<pubDate>Fri, 28 Dec 2018 00:00:00 +0800</pubDate>
</item>
<item>
<title>TBSSQL 的那些事 | TiDB Hackathon 优秀项目分享</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2018-12-27-53455060.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/53455060&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1409789f904133511501fd27f2be0c7e_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文作者是来自 TiBoys 队的崔秋同学，他们的项目 TBSSQL 在 TiDB Hackathon 2018 中获得了一等奖。&lt;br&gt;&lt;b&gt;TiDB Batch and Streaming SQL（简称 TBSSQL）&lt;/b&gt;扩展了 TiDB 的 SQL 引擎，支持用户以类似 StreamSQL 的语法将 Kafka, Pulsar 等外部数据源以流式表的方式接入 TiDB。通过简单的 SQL 语句，用户可以实现对流式数据的过滤，流式表与普通表的 Join（比如流式事实表与多个普通维度表），甚至通过 CREATE TABLE AS SELECT 语法将处理过的流式数据写入普通表中。此外，针对流式数据的时间属性, 我们实现了基于时间窗口的聚合/排序算子, 使得我们可以对流式数据进行时间维度的聚合/排序。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;序&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;算起来这应该是第三次参加的 Hackathon 了，第一次参加的时候还是在小西天的豌豆荚，和东旭一起，做跨平台数据传输的工具，两天一夜；第二次和奇叔一起在 3W 咖啡，又是两天一夜；这次在自己家举办 Hackathon 比赛，下定决心一定要佛性一些，本着能抱大腿就不单干的心态，迅速决定拉唐长老（唐刘）下水。接下来就计划着折腾点啥，因为我们两个前端都不怎么样，所以只能硬核一些，于是拍了两个方案。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方案一&lt;/b&gt;：之前跟唐长老合作过很长一段时间，我们两个对于测试质量之类的事情也都非常关注，所以想着能不能在 Chaos 系统上做一些文章，把一些前沿的测试理论和经验方法结合到系统里面来，做一套通用的分布式系统测试框架，就像 Jepsen 那样，用这套系统去测试和验证主流的开源分布式项目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;方案二&lt;/b&gt;：越接近于业务实时性的数据处理越有价值，不管是 Kafka/KSQL，Flink/Spark Streaming 都是在向着实时流计算领域方向进行未来的探索。TiDB 虽然已经能够支持类 Real Time OLAP 的场景，但是对于更实时的流式数据处理方面还没有合适的解决方案，不过 TiDB 具有非常好的 Scale 能力，天然的能存储海量的数据库表数据，所以在 Streaming Event 和 Table 关联的场景下具有非常明显的优势。如果在 TiDB 上能够实现一个 Streaming SQL 的引擎，实现 Batch/Streaming 的计算融合，那将会是一件非常有意思的事情。&lt;/p&gt;&lt;p&gt;&lt;b&gt;因为打 Hackathon 比赛主要是希望折腾一些新的东西，所以我们两个简单讨论完了之后还是倾向于方案二，当然做不做的出来另说。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当我们正准备做前期调研和设计的时候，Hackathon 主办方把唐长老拉去做现场导师，参赛规则规定导师不能下场比赛，囧，于是就这样被被动放了鸽子。好在后来遇到了同样被霸哥（韩飞）当导师而放鸽子的川总（杜川），川总对于 Streaming SQL 非常感兴趣，于是难兄难弟一拍即合，迅速决定抱团取暖。随后，Robot 又介绍了同样还没有组队的社区小伙伴 GZY（高志远），这样算是凑齐了三个人，但是一想到没有前端肯定搞不定，于是就拜托娘家人（Dashbase）的交际小王子 WPH（王鹏翰）出马，帮助去召唤一个靠谱的前端小伙伴，后来交际未果直接把自己卖进了队伍，&lt;b&gt;这样终于凑齐了四后端，不，应该是三后端 + 一伪前端的组合。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;因为马上要准备提交项目和团队名称，大家都一致觉得方案二非常有意思，所以就选定了更加儒雅的 TBSSQL（TiDB Batch and Streaming SQL）作为项目名称，TSBSQL 遗憾落选。在团队名称方面，打酱油老男孩 / Scboy / TiStream / 养生 Hackathon / 佛系 Hackathon 都因为不够符合气质被遗憾淘汰，最后代表更有青春气息的 TiBoys 入选（跟着我左手右手一个慢动作，逃……&lt;/p&gt;&lt;h2&gt;&lt;b&gt;前期准备&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;所谓 “三军未动, 粮草先行”，既然已经报名了，还是要稍作准备，虽然已经确定了大的方向，但是具体的落地方案还没有细化，而且人员的分工也不是太明确。又经过一轮简单的讨论之后，明确了大家的职责方向，我这边主要负责项目整体设计，进度管理以及和 TiDB 核心相关的代码，川总主要负责 TiDB 核心技术攻关，GZY 负责流数据源数据的采集部分，WPH 负责前端展现以及 Hackathon 当天的 Demo 演示，分工之后大家就开始分头调研动工。&lt;/p&gt;&lt;p&gt;作为这两年来基本没怎么写过代码的退役型选手来说，心里还是非常没底的，也不知道现在 TiDB 代码结构和细节变成什么样了，不求有功，但求别太拖后腿。&lt;/p&gt;&lt;p&gt;对于项目本身的典型应用场景，大家还是比较明确的，觉得这个方向是非常有意义的。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;应用层系统：实时流事件和离线数据的关联查询，比如在线广告推荐系统，在线推荐系统，在线搜索，以及实时反欺诈系统等。&lt;/li&gt;&lt;li&gt;内部数据系统：&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;实时数据采样统计，比如内部监控系统；&lt;/li&gt;&lt;li&gt;时间窗口数据分析系统，比如实时的数据流数据分析（分析一段时间内异常的数据流量和系统指标），用于辅助做 AI Ops 相关的事情（比如根据数据流量做节点自动扩容/自动提供参数调优/异常流量和风险报告等等）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;业界 Streaming 相关的系统很多，前期我这边快速地看了下能不能站在巨人的肩膀上做事情，有没有可借鉴或者可借用的开源项目。&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Apache Beam&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本质上 Apache Beam 还是一个批处理和流处理融合的 SDK Model，用户可以在应用层使用更简单通用的函数接口实现业务的处理，如果使用 Beam 的话，还需要实现自定义的 Runner，因为 TiDB 本身主要的架构设计非常偏重于数据库方向，内部并没有特别明确的通用型计算引擎，所以现阶段基本上没有太大的可行性。当然也可以选择用 Flink 作为 Runner 连接 TiDB 数据源，但是这就变成了 Flink&amp;amp;TiDB 的事情了，和 Beam 本身关系其实就不大了。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Apache Flink / Spark Streaming&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Flink 是一个典型的流处理系统，批处理可以用流处理来模拟出来。&lt;br&gt;本身 Flink 也是支持 SQL 的，但是是一种嵌入式 SQL，也就是 SQL 和应用程序代码写在一起，这种做法的好处是可以直接和应用层进行整合，但是不好的地方在于，接口不是太清晰，有业务侵入性。阿里内部有一个增强版的 Flink 项目叫 Blink，在这个领域比较活跃。如果要实现批处理和流处理融合的话，需要内部定制和修改 Flink 的代码，把 TiDB 作为数据源对接起来，还有可能需要把一些环境信息提交给 TiDB 以便得到更好的查询结果，当然或许像 TiSpark 那样，直接 Flink 对接 TiKV 的数据源应该也是可以的。因为本身团队对于 Scala/Java 代码不是很熟悉，而且 Flink 的模式会有一定的侵入性，所以就没有在这方面进行更多的探索。同理，没有选择 Spark Streaming 也是类似的原因。当然有兴趣的小伙伴可以尝试下这个方向，也是非常有意思的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Kafka SQL&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因为 Kafka 本身只是一个 MQ，以后会向着流处理方向演进，但是目前并没有实现批处理和流处理统一的潜力，所以更多的我们只是借鉴 Kafka SQL 的语法。目前 Streaming SQL 还没有一个统一的标准 SQL，Kafka SQL 也只是一个 SQL 方言，支持的语法还比较简单，但是非常实用，而且是偏交互式的，没有业务侵入性。非常适合在 Hackathon 上做 Demo 演示，我们在项目实现中也是主要参考了 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/confluentinc/ksql/blob/0.1.x/docs/syntax-reference.md%23ksql-statements&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kafka SQL&lt;/a&gt; 的定义，当然，&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//ci.apache.org/projects/flink/flink-docs-stable/dev/table/sql.html%23specifying-a-query&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Flink&lt;/a&gt; 和 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//calcite.apache.org/docs/stream.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Calcite&lt;/a&gt; 也有自己定义的 Streaming 语法，这里就不再讨论了。&lt;/p&gt;&lt;p&gt;调研准备工作讨论到这里基本上也就差不多了，于是我们开始各自备（hua）战（shui），出差的出差，加班的加班，接客户的接客户，学 Golang 的学 Golang，在这种紧（fang）张（fei）无（zi）比（wo）的节奏中，迎来了 Hackathon 比赛的到来。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Hackathon 流水账&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;i&gt;具体的技术实现方面都是比较硬核的东西，细节也比较多，扔在最后面写，免的大家看到一半就点×了。&lt;/i&gt;&lt;br&gt;&lt;i&gt;至于参加 Hackathon 的感受，因为不像龙哥那么文豪，也不像马老师那么俏皮，而且本来读书也不多，所以也只能喊一句“黑客马拉松真是太好玩了”！&lt;/i&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Day 1&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3:30 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由于飞机晚点，川总这个点儿才辗转到酒店。睡觉之前非常担心一觉睡过头，让这趟 Hackathon 之旅还没开始就结束了，没想到躺下以后满脑子都是技术细节，怎么都睡不着。漫漫长夜，无眠。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7:45 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;川总早早来到 Hackathon 现场。由于来太早，其他选手都还没到，所以他提前刺探刺探敌情的计划也泡汤了，只好在赛场瞎晃悠一番熟悉熟悉环境，顺道跟大奖合了个影。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8b766025ff4f9fce32df9469140aa1a3_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;11:00 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;简单的开幕式之后，Hackathon 正式开始。我们首先搞定的是 Streaming SQL 的语法定义以及 Parser 相关改动。这一部分在之前就经过比较详细的在线讨论了，所以现场只需要根据碰头后统一的想法一顿敲敲敲就搞定了。快速搞定这一块以后，我们就有了 SQL 语法层面的 Streaming 实现。当然此时 Streaming 也仅限于语法层面，Streaming 在 SQL 引擎层面对应的其实还是普通的TiDB Table。&lt;/p&gt;&lt;p&gt;接下来是 DDL 部分。这一块我们已经想好了要复用 TiDB Table 的 Meta 结构 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/parser/blob/e5d56f38f4b2fdfb1d7010180cb038bd9f58c071/model/model.go%23L140&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TableInfo&lt;/a&gt; ，因此主要工作就是按照 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-17/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDL源码解析&lt;/a&gt; 依葫芦画瓢，难度也不大，以至于我们还有闲心纠结一下 SHOW TABLES 语法里到底要不要屏蔽掉 Streaming Table 的问题。&lt;/p&gt;&lt;p&gt;整体上来看上午的热身活动还是进行的比较顺利的，起码 Streaming DDL 这块没有成为太大的问题。这里面有个插曲就是我在 Hackathon 之前下载编译 TiDB，结果发现 TiDB 的 parser 已经用上时髦的 go module 了（也是好久好久没看 TiDB 代码），折腾好半天，不过好处就是 Hackathon 当天的时候改起来 parser 就比较轻车熟路了，所以赛前编译一个 TiDB 还是非常有必要的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;15:30 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;随着热身的结束，马上迎来了稳定的敲敲敲阶段。川总简单弄了一个 Mock 的 StreamReader 然后丢给了我，因为我之前写 TiDB 的时候，时代比较遥远，那时候都还在用周 sir 的 Datum，现在一看，为了提高内存效率和性能，已经换成了高大上的 Chunk，于是一个很常见的问题：如何用最正确的做法把一个传过来的 Json 数据格式化成 Table Row 数据放到 Chunk 里面，让彻底我懵逼了。&lt;/p&gt;&lt;p&gt;这里面倒不是技术的问题，主要是类型太多，如果枚举所有类型，搞起来很麻烦，按道理应该有更轻快的办法，但是翻了源代码还是没找到解决方案。这个时候果断去求助现场导师，也顺便去赛场溜（ci）达（tan）一（di）圈（qing）。随便扫了一眼，惊呆了，龙哥他们竟然已经开始写 PPT 了，之前知道龙哥他们强，但是没想到强到这个地步，还让不让大家一块欢快地玩耍了。同时，也了解到了不少非常有意思的项目，比如用机器学习方法去自动调节 TiDB 的调度参数，用 Lua 给 TiKV 添加 UDF 之类的，在 TiDB 上面实现异构数据库的关联查询（简直就是 F1 的大一统，而且听小道消息，他们都已经把 Join 推到 PG 上面去了，然而我们还没开始进入到核心开发流程），在 TiKV 上面实现时序数据库和 Memcached 协议等等，甚至东旭都按捺不住自己 Hackathon 起来了（嘻嘻，可以学学我啊 ;D ）。&lt;/p&gt;&lt;p&gt;本来还想去聊聊各个项目的具体实现方案，但是一想到自己挖了一堆坑还没填，只能默默回去膜拜 TiNiuB 项目。看起来不能太佛系了，于是乎我赶紧召开了一次内部团队 sync 的 catch up，明确下分工，川总开始死磕 TBSSQL 的核心逻辑 Streaming Aggregation 的实现，我这边继续搞不带 Aggregation 的 Streaming SQL 的其他实现，GZY 已经部署起来了 Pulsar，开始准备 Mock 数据，WPH 辅助 GZY 同时也快速理解我们的 Demo 场景，着手设计实现前端展现。&lt;/p&gt;&lt;p&gt;&lt;b&gt;18:00 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我这边和面带慈父般欣慰笑容的老师（张建）进行了一些技术方案实现上的交流后，了解到目前社区小伙伴已经在搞 CREATE TABLE AS SELECT 的重要信息（后续证明此信息值大概一千块 RMB）。&lt;/p&gt;&lt;p&gt;此时，在解决了之前的问题之后，TBSSQL 终于能跑通简单的 SELECT 语句了。我们心里稍微有点底了，于是一鼓作气，顺路也实现了带 Where 条件的 Stream Table 的 SELECT，以及 Stream Table 和 TiDB Table 的多表 Join，到这里，此时，按照分工，我这边的主体工作除了 Streaming Position 的持久化支持以外，已经写的差不多了，剩下就是去实现一些 Nice to have 的 DDL 的语法支持。川总这里首先要搞的是基于时间窗口的 Streaming Aggregation。按照我们的如意算盘，这里基本上可以复用 TiDB 现有的 Hash Aggregation 的计算逻辑，只需要加上窗口的处理就完事儿了。&lt;/p&gt;&lt;p&gt;不过实际下手的时候仔细一研究代码，发现 Aggregation 这一块代码在川总疏于研究这一段时间已经被重构了一把，加上了一个并发执行的分支，看起来还挺复杂。于是一不做二不休，川总把 Hash Aggregation 的代码拷了一份，删除了并发执行的逻辑，在比较简单的非并发分支加上窗口相关实现。不过这种方法意味着带时间窗口的 Aggregation 得单独出 Plan，Planner 上又得改一大圈。这一块弄完以后，还没来得及调试，就到吃晚饭的点儿了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;21:00 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;吃完晚饭，因为下午死磕的比较厉害，我和张建、川总出门去园区溜达了一圈。期间张建问我们搞得咋样了，我望了一眼川总，语重心长地说主要成败已经不在我了（后续证明这句语重心长至少也得值一千块 RMB），川总果断信心满满地说问题不大，一切尽在掌握之中。&lt;/p&gt;&lt;p&gt;没想到这个 Flag 刚立起来还是温的，就立马被打脸了。问题出在吃饭前搞的聚合那块（具体细节可以看下后面的坑系列），为了支持时间窗口，我们必须确保 Streaming 上的窗口列能透传到聚合算子当中，为此我们屏蔽了优化器中窗口聚合上的列裁剪规则。可是实际运行当中，我们的修改并没有生效？？？而此时，川总昨天一整晚没睡觉的副作用开始显现出来了，思路已经有点不太清醒了。于是我们把张建拖过来一起 debug。然后我这边也把用 TiDB Global Variable 控制 Streaming Position 的功能实现了，并且和 GZY 这边也实现了 Mock 数据。&lt;/p&gt;&lt;p&gt;之后，我也顺路休息休息，毕竟川总这边搞不定，我们这边搞的再好也没啥用。除了观摩川总和张建手把手，不，肩并肩结对小黑屋编程之外，我也顺便申请了部署 Kafka 联调的机器。&lt;/p&gt;&lt;p&gt;&lt;b&gt;23:00 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们这边最核心的功能还没突破，亮眼的 CREATE TABLE AS SELECT Streaming 也还没影，其实中期进度还是偏慢了（或者说之前我设计实现的功能的工作量太大了，看起来今天晚上只能死磕了，囧）。我调试 Kafka 死活调不通，端口可以 Telnet 登陆，但是写入和获取数据的时候一直报超时错误，而且我这边已经开始困上来了，有点扛不动了，后来在 Kafka 老司机 WPH 一起看了下配置参数，才发现 Advertise URL 设置成了本地地址，换成对外的 IP 就好了，当然为了简单方便，我们设置了单 Partition 的 Topic，这样 collector 的 Kafka 部分就搞的差不多了，剩下就是实现一个 http 的 restful api 来提供给 TiDB 的 StreamReader 读取，整个连通工作就差不多了。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Day 2&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;00:00 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这时候川总那边也传来了好消息，终于从 Streaming Aggregation 这个大坑里面爬出来了，后面也比较顺利地搞定了时间窗口上的聚合这块。此时时间已经到了 Hackathon 的第二天，不少其他项目的小伙伴已经收摊回家了。不过我们抱着能多做一个 Feature 是一个的心态，决定挑灯夜战。首先，川总把 Sort Executor 改了一把以支持时间窗口，可能刚刚的踩坑经历为我们攒了人品，Sort 上的改动竟然一次 AC 了。借着这股劲儿，我们又回头优化了一把 SHOW CREATE STREAM 的输出。&lt;/p&gt;&lt;p&gt;这里有个插曲就是为了近距离再回味和感受下之前的开发流程，我们特意在 TiDB 的 repo 里面开了一个 tiboys/hackathon 的分支，然后提交的时候用了标准的 Pull Request 的方式，点赞了才能 merge（后来想想打 Hackathon 不是太可取，没什么用，还挺耽误时间，不知道当时怎么想的），所以在 master 分支和 tiboys/hackathon 分支看的时候都没有任何提交记录。嘻嘻，估计龙哥也没仔细看我们的 repo，所以其实在龙哥的激励下，我们的效率还是可以的 :) 。&lt;/p&gt;&lt;p&gt;GZY 和 WPH 把今天安排的工作完成的差不多了，而且第二天还靠他们主要准备 Demo Show，就去睡觉了，川总也已经困得不行了，准备打烊睡觉。我和川总合计了一下，还差一个最重要的 Feature，抱着就试一把，不行就手工的心态，我们把社区的小伙伴王聪（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/bb7133&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;bb7133&lt;/a&gt;）提的支持 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/pull/7787&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;CREATE TABLE AS SELECT&lt;/a&gt; 语法的 PR 合到了我们的分支，冲突竟然不是太多，然后稍微改了一下来支持 Streaming，结果一运行奇迹般地发现竟然能够运行，RP 全面爆发了，于是我们就近乎免费地增加了一个 Feature。改完这个地方，川总实在坚持不住了，就回去睡了。我这边的 http restful api 也搞的差不多了，准备联调一把，StreamReader 通过 http client 从 collector 读数据，collector 通过 kafka consumer 从 kafka broker 获取数据，结果获取的 Json 数据序列化成 TiDB 自定义的 Time 类型老是出问题，于是我又花了一些时间给 Time 增加了 Marshall 和 Unmarshal 的格式化支持，到这里基本上可以 work 了，看了看时间，凌晨四点半，我也准备去睡了。期间好几次看到霸哥（韩飞）凌晨还在一直帮小（tian）伙（zi）伴（ji）查（wa）问（de）题（keng），其实霸哥认真的时候还是非常靠谱的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;7:30 AM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个时候人陆陆续续地来了，我这边也进入了打酱油的角色，年纪大了确实刚不动了，吃了早餐之后，开始准备思考接下来的分工。因为大家都是临时组队，到了 Hackathon 才碰面，基本上没有太多磨合，而且普遍第二天状态都不大好。虽然大家都很努力，但是在我之前设计的宏大项目面前，还是感觉人力不太够，所以早上 10 点我们开了第二次 sync 的 catch up，讨论接下来的安排。我去负责更新代码和 GitHub 的 Readme，川总最后再简单对代码扫尾，顺便和 GZY 去录屏（罗伯特小姐姐介绍的不翻车经验），WPH 准备画图和 PPT，因为时间有限，前端展现部分打算从卖家秀直接转到买家秀。11 点敲定代码完全封板，然后安心准备 PPT 和下午的 Demo。&lt;/p&gt;&lt;p&gt;&lt;b&gt;14:00 PM&lt;/b&gt;&lt;/p&gt;&lt;p&gt;因为抽签抽的比较靠后，主要事情在 WPH 这边，我和川总基本上也没什么大事了，顺手搞了几幅图，然后跟马老师还有其他项目的小伙伴们开始八卦聊天。因为正好周末，家里妹子买东西顺便过来慰问了下。下午主要听了各个 Team 的介绍，欣赏到了极尽浮夸的 LOGO 动画，Get 到了有困难找 Big Brother 的新技能，学习和了解了很有意思的 Idea，真心觉得这届 Hackathon 做的非常值得回忆。&lt;/p&gt;&lt;p&gt;从最后的现场展示情况来看，因为 TBSSQL 内容比较多，真的展示下来，感觉 6 分钟时间还是太赶，好在 WPH Demo 的还是非常顺利的，把我们做的事情都展示出来了。因为砍掉了一些前端展现的部分(这块我们也确实不怎么擅长)，其实对于 Hackathon 项目是非常吃亏的，不过有一点比较欣慰，就像某光头大佬说的，评委们都是懂技术的。因为实现完整性方面能做的也都搞差不多了，打的虽然很累但是也很开心，对于结果也就不怎么纠结了。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-7f8ecec65da4db0c3dba7b6077b862ce_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;因为川总晚上的飞机，小伙伴们简单沟通了几句，一致同意去园区找个地吃个晚饭，于是大家拉上霸哥去了“头一号”，也是第一次吃了大油条，中间小伙伴们各种黑谁谁谁写的 bug 巴拉巴拉的，后来看手机群里有人 @ 我说拿奖了。&lt;/p&gt;&lt;p&gt;其实很多项目各方面综合实力都不错，可以说是各有特色，很难说的上哪个项目有绝对的优势。我们之前有讨论过，TBSSQL 有获奖的赢面，毕竟从完整性，实用性和生态方面都是有潜质的，但是能获得大家最高的认可还是小意外的，特别感谢各位技术大佬们，也特别感谢帮助我们领奖的满分罗伯特小姐姐。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-fc9844f0fa969c27bf09680c526fe4e0_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;最后大家补了一张合照，算是为这次 Hackathon 画下一个句号。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-78eaa3b14c7f5f61931e49373d0b5e7e_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;至此，基本上 Hackathon 的流水账就记录完了，整个项目地址在&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/qiuyesuifeng&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/tidb&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/b&gt; &lt;b&gt;欢迎大家关注和讨论。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;选读：技术实现&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;TLDR: 文章很长，挑感兴趣的部分看看就可以了。&lt;/blockquote&gt;&lt;p&gt;在前期分析和准备之后，基本上就只有在 TiDB 上做 SQL Streaming 引擎一条路可选了，细化了下要实现的功能以及简单的系统架构，感觉工作量还是非常大的。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;903&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;903&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-beb2a80ab85d2df999dc0fcdbe787eb1_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;下面简单介绍下系统架构和各个模块的功能：&lt;/p&gt;&lt;p&gt;在数据源采集部分（collector），我们计划选取几种典型的数据源作为适配支持。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Kafka&lt;/b&gt;&lt;br&gt;最流行的开源 MQ 系统，很多 Streaming 系统对接的都是 Kafka。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Pulsar&lt;/b&gt;&lt;br&gt;流行的开源 MQ 系统，目前比较火爆，有赶超 Kafka 的势头。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Binlog&lt;/b&gt;&lt;br&gt;支持 MySQL/TiDB Binlog 处理，相当于是 MySQL Trigger 功能的升级加强版了。我们对之前的 MySQL -&amp;gt; TiDB 的数据同步工具 Syncer 也比较熟悉，所以这块工作量应该也不大。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Log&lt;/b&gt;&lt;br&gt;常见的 Log 日志，这个就没什么好解释的了。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;为了方便 Demo 和协作，collector 除了适配不同的数据源，还会提供一个 restful api 的接口，这样 TBSSQL 就可以通过 pull 的方式一直获取 streaming 的数据。因为 collector 主要是具体的工程实现，所以就不在这里细节展开了，感兴趣的话，可以参考下 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/collector&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;相关代码&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;要在 TiDB 中实现 Streaming 的功能即 TBSSQL，就需要在 TiDB 内部深入定制和修改 TiDB 的核心代码。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Streaming 有两个比较本质的特征：&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Streaming 具有流式特性，也就是说，其数据可以是一直增长，无穷无尽的。而在 Batch 系统(暂时把 MySQL/TIDB 这种数据在一定时间内相对稳定的系统简称 Batch 系统，下面都会沿用这种说法)当中，每个 SQL 的输入数据集是固定，静态的。&lt;/li&gt;&lt;li&gt;Streaming 具有时序特性。每一条数据都有其内在的时间属性（比如说事件发生时间等），数据之间有先后顺序关系。而在 Batch 系统当中，一个表中的数据在时间维度上是无序的。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;因此，要在 TiDB SQL 引擎上支持 Streaming SQL，所涉及到的算子都需要根据 Streaming 的这两个特点做修改。以聚合函数（Aggregation）为例，按照 SQL 语义，聚合算子的实现应该分成两步：首先是 Grouping, 即对输入按照聚合列进行分组；然后是 Execute, 即在各个分组上应用聚合函数进行计算，如下图所示。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;731&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;731&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-77d57d6c91a3c1c760ca4c21bda9659d_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;对于 Streaming，因为其输入可以是无尽的，Grouping 这个阶段永远不可能结束，所以按照老套路，聚合计算就没法做了。这时，就要根据 Streaming 的时序特性对 Streaming 数据进行分组。每一个分组被称为一个 Time Window（时间窗口）。就拿最简单的 Tumbling Window 来说，可以按照固定的时间间隔把 Streaming 输入切分成一个个相互无交集的窗口，然后在每一个窗口上就可以按照之前的方式进行聚合了。&lt;/p&gt;&lt;p&gt;聚合算子只是一个比较简单的例子，因为其只涉及一路输入。如果要修改多路输入的算子（比如说 Join 多个 Streaming），改动更复杂。此外，时间窗口的类型也是多种多样，刚刚例子中的 Tumbling Window 只是基础款，还有复杂一点的 Hopping Window 以及更复杂的 Sliding Window。在 Hackathon 的有限时间内，我们既要考虑实现难度，又要突出 Batch / Streaming 融合处理的特点，因此在技术上我们做出如下抉择：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;时间窗口只做最基本的 Tumbling Window。&lt;/li&gt;&lt;li&gt;实现基于时间窗口的 Aggregation 和 Sort 作为经典流式算子的代表。&lt;/li&gt;&lt;li&gt;实现单 Streaming Join 多 Batch Table 作为 Batch / Streaming 融合的示例, 多个 Streaming Join 太复杂，因为时间有限就先不做了。&lt;/li&gt;&lt;li&gt;支持 Streaming 处理结果写入 Batch Table（TiDB Table）这种常见但是非常实用的功能。也就是说要支持 &lt;code&gt;CREATE TABLE AS SELECT xxx FROM streaming&lt;/code&gt; 的类似语法。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外，既然是要支持 Streaming SQL，选择合适的 SQL 语法也是必要的，需要在 Parser 和 DDL 部分做相应的修改。单整理下，我们的 Feature List 如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;287&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;287&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-2af4303a4e8e9606e508746a3be1b78b_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;下面具体聊聊我们实现方案中的一些关键选择。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Streaming SQL 语法&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Streaming SQL 语法的核心是时间窗口的定义，Time Window 和一般 SQL 中的 Window Function 其实语义上是有区别的。在 Streaming SQL 中，Time Window 主要作用是为后续的 SQL 算子限定输入的范围，而在一般的 SQL 中，Window Funtion 本身就是一个 SQL 算子，里面的 Window 其实起到一个 Partition 的作用。&lt;br&gt;在纯 Streaming 系统当中，这种语义的差别影响不大，反而还会因为语法的一致性降低用户的学习成本，但是在 TBSSQL 这种 Batch / Streaming 混合场景下，同一套语法支持两种语义，会对用户的使用造成一定困扰，特别是在 TiDB 已经被众多用户应用到生产环境这种背景下，这种语义上的差别一定要体现在语法的差异上。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Sreaming DDL&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;DDL 这一块实现难度不大，只要照着 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-source-code-reading-17/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDL源码解析&lt;/a&gt; 依葫芦画瓢就行。这里值得一提的是在 Meta 层，我们直接（偷懒）复用了 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/parser/blob/e5d56f38f4b2fdfb1d7010180cb038bd9f58c071/model/model.go%23L140&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TableInfo&lt;/a&gt; 结构（加了判断是否为 Streaming 的 Flag 和一些表示 Streaming 属性的字段）来表示 Streaming Table。这个选择主要是从实现难度上考虑的，毕竟复用现有的结构是最快最安全的。但是从设计思想上看，这个决定其实也暗示了在 TBSSQL 当中，Streaming 是 Table 的一种特殊形式，而不是一个独立的概念。理解这一点很重要，因为这是一些其他设计的依据。比如按照以上设定，那么从语义上讲，在同一个 DB 下 Streaming 和普通 Table 就不能重名，反之的话这种重名就是可以接受的。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;StreamReader&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这一块主要有两个部分，一个是适配不同的数据源（collector），另一个是将 Streaming 数据源引入 TiDB 计算引擎（StreamReader）。collector 这部分上面已经介绍过了，这里就不再过多介绍了。StreamReader 这一块，主要要修改由 LogicalPlan 生成 PhysicalPlan（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/planner/core/find_best_task.go%23L206&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体代码&lt;/a&gt;），以及由 PhysicalPlan 生成 Executor Operator Tree 的过程（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/executor/builder.go%23L171&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体代码&lt;/a&gt;）。&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/master/executor/stream_reader.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;StreamReader&lt;/a&gt; 的 Open 方法中，会利用 Meta 中的各种元信息来初始化与 collector 之间的连接，然后在 Next 方法中通过 Pull 的方式不断拉取数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;对时间窗口的处理&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;前面我们提到，时间窗口是 Streaming 系统中的核心概念。那么这里就有一个重要的问题，Time Window 中的 Time 如何界定？如何判断什么时候应该切换 Window？最容易想到，也是最简单粗暴的方式，就是按照系统的当前时间来进行切割。这种方式问题很大，因为：&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;数据从生成到被 TBSSQL 系统接收到，肯定会有一定的延迟，而且这个延迟时间是没有办法精确预估的。因此在用户实际场景中，除非是要测量收发延迟，这个系统时间对用户没有太大意义。&lt;/li&gt;&lt;li&gt;考虑到算子并发执行的可能性（虽然还没有实现），不同机器的系统时间可能会有些许偏差，这个偏差对于 Window 操作来说可能导致致命的误差，也会导致结果的不精确（因为 Streaming 源的数据 Shuffle 到不同的处理节点上，系统时间的误差可能不太一样,可能会导致 Window 划分的不一样）。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;因此，比较合理的方式是以 Streaming 中的某一 Timestamp 类型的列来切分窗口，这个值由用户在应用层来指定。当然 Streaming 的 Schema 中可能有多个 Timestamp 列，这里可以要求用户指定一个作为 Window 列。在实现 Demo 的时候，为了省事，我们直接限定了用户 Schema 中只能有一个时间列，并且以该列作为 Window 列（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/ddl/table.go%23L58&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体代码&lt;/a&gt;）。当然这里带来一个问题，就是 Streaming 的 Schema 中必须有 Timestamp 列，不然这里就没法玩了。为此，我们在创建 Streaming 的 DDL 中加了 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/ddl/ddl_api.go%23L149&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;检查逻辑&lt;/a&gt;，强制 Streaming 的 Schema 必须有 Timestamp 列（其实我们也没想明白当初 Hackathon 为啥要写的这么细，这些细节为后来通宵埋下了浓重的伏笔，只能理解为程序猿的本能，希望这些代码大家看的时候吐槽少一些）。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Streaming DML&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;620&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;620&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-995b8625e084bf004ba66929d4bbefb4_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;这里简单 DML 指的就是不依赖时间窗口的 DML，比如说只带 Selection 和 Projection 的SELECT 语句，或者单个 Streaming Join 多个 Table。因为不依赖时间窗口，支持这类 DML 实际上不需要对计算层做任何改动，只要接入 Streaming 数据源就可以了。&lt;br&gt;对于 Streaming Join Table(如上图表示的是 Stream Join User&amp;amp;Ads 表的示意图) 可以多说一点，如果不带 Time Window，其实这里需要修改一下Planner。因为 Streaming 的流式特性，这里可能没法获取其完整输入集，因此就没法对 Streaming 的整个输入进行排序，所以 Merge Join 算法这里就没法使用了。同理，也无法基于 Streaming 的整个输入建 Hash 表，因此在 Hash Join 算法当中也只能某个普通表 Build Hash Table。不过，在我们的 Demo 阶段，输入其实也是还是有限的，所以这里其实没有做，倒也影响不大。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;基于时间窗口的 Aggregation 和 Sort&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 TBSSQL 当中，我们实现了基于固定时间窗的 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/master/executor/aggregate.go%23L934&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hash Aggregation Operator&lt;/a&gt; 和 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/commit/d36b70bdb2d54b8c34216746ff7a716cba8f4d3c&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Sort Operator&lt;/a&gt;。这里比较正规的打法其实应该是实现一个独立的 TimeWindow，各种基于时间窗口的 Operator 可以切换时间窗的逻辑，然后比如 Aggregation 和 Sort 这类算子只关心自己的计算逻辑。 但是这样一来要对 Planner 做比较大的改动，想想看难度太大了，所以我们再一次采取了直（tou）接（lan）的方法，将时间窗口直接实现分别实现在 Aggregation 和 Sort 内部，这样 Planner 这块不用做伤筋动骨的改动，只要在各个分支逻辑上修修补补就可以了。&lt;br&gt;对于 Aggregation，我们还做了一些额外的修改。Aggregation 的输出 Schema 语义上来说只包括聚合列和聚合算子的输出列。但是在引入时间窗口的情况下，为了区分不同的窗口的聚合输出，我们为聚合结果显式加上了两个 Timestamp 列 &lt;code&gt;window_start&lt;/code&gt; 和 &lt;code&gt;window_end&lt;/code&gt;, 来表示窗口的开始时间和结束时间。为了这次这个小特性，我们踩到一个大坑，费了不少劲，这个后面再仔细聊聊。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;191&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;191&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-0d954aa933dd78131240ed06fb6917b1_b.jpg&quot;&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;支持 Streaming 处理结果写入 Batch Table&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因为 TiDB 本身目前还暂时不支持 CREATE TABLE AS SELECT … 语法，而从头开始搞的话工作量又太大，因此我们一度打算放弃这个 Feature。后面经过老司机提醒，我们发现社区的小伙伴王聪（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/bb7133&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;bb7133&lt;/a&gt;）已经提了一个 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/pull/7787&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PR&lt;/a&gt; 在做这个事情了。本着试一把的想法我们把这个 PR 合到我们的分支上一跑，结果竟然没多少冲突，还真能 Work……稍微有点问题的是如果 SELECT 子句中有带时间窗口的聚合，输出的结果不太对。仔细研究了一下发现，CREATE TABLE AS SELECT 语句中做 LogicalPlan 的路径和直接执行 SELECT 时做 LogicalPlan 的入口不太一致，以至于对于前者，我们做 LogicalPlan 的时候遗漏了一些 Streaming 相关信息。这里稍作修改以后，也能够正常运行了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;遇到的困难和坑&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;本着前人采坑，后人尽量少踩的心态聊聊遇到的一些问题，主要的技术方案上面已经介绍的比较多了。限于篇幅，只描述遇到的最大的坑——消失的窗口列的故事。在做基于时间窗口的 Aggregation 的时候，我们要按照用户指定的窗口列来切窗口。但是根据 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/master/planner/core/rule_column_pruning.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;列裁剪&lt;/a&gt; 规则，如果这个窗口列没有被用作聚合列或者在聚合函数中被使用，那么这一列基本上会被优化器裁掉。这里的修改很简单（我们以为），只需要在聚合的列裁剪逻辑中，如果发现聚合带时间窗口，那么直接不做裁剪就完事儿了（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/planner/core/rule_column_pruning.go%23L96&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;代码&lt;/a&gt;）。三下五除二修改完代码，编译完后一运行，结果……瞬间 Panic 了……Debug 一看，发现刚刚的修改没有生效，Streaming 的窗口列还是被裁剪掉了，随后我们又把 Planner 的主要流程看了一遍，还是没有在其他地方发现有类似的裁剪逻辑。&lt;/p&gt;&lt;p&gt;这时我们意识到事情没有这么简单了，赶忙从导师团搬来老司机（还是上面那位）。我们一起用简单粗暴的二分大法和 Print 大法，在生成 LogicalPlan，PhysicalPlan 和 Executor 前后将各个算子的 Schema 打印出来。结果发现，在 PhysicalPlan 完成后，窗口列还是存在的，也就是说我们的修改是生效了的，但是在生成 Executor 以后，这一列却神秘消失了。所以一开始我们定位的思路就错了，问题出在生成 Executor 的过程，但是我们一直在 Planner 中定位，当然找不到问题。&lt;/p&gt;&lt;p&gt;明确了方向以后，我们很快就发现了元凶。在 Build HashAggregation 的时候，有一个不起眼的函数调用 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/qiuyesuifeng/tidb/blob/656971da00a3b1f81f5085aaa277159868fca223/executor/builder.go%23L1111&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;buildProjBelowAgg&lt;/a&gt;，这个函数悄悄地在 Aggregation 算子下面加塞了一个 Projection 算子，顺道又做了一把列裁剪，最为头疼的是，因为这个 Projection 算子是在生成 Executor 阶段才塞进去的，而 EXPLAIN 语句是走不到这里来的，所以这个 Projection 算子在做 Explain 的时候是看不见的，想当于是一个隐形的算子，所以我们就这样华丽丽地被坑了，于是就有了罗伯特小姐姐听到的那句 “xxx，出来挨打” 的桥段。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;今后的计划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从立项之初，我们就期望 TBSSQL 能够作为一个正式的 Feature 投入生产环境。为此，在设计和实现过程中，如果能用比较优雅的解决方案，我们都尽量不 Hack。但是由于时间紧迫和能力有限，目前 TBSSQL 还是处于 Demo 的阶段，离实现这个目标还有很长的路要走。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Streaming 数据源&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在对接 Streaming 数据源这块，目前 TBSSQL 有两个问题。首先，TBSSQL 默认输入数据是按照窗口时间戳严格有序的。这一点在生产环境中并不一定成立（比如因为网络原因，某一段数据出现了乱序）。为此，我们需要引入类似 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//ai.google/research/pubs/pub41378&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Google MillWheel&lt;/a&gt; 系统中 Low Watermark 的机制来保证数据的有序性。其次，为了保证有序，目前 StreamReader 只能单线程运行。在实际生产环境当中，这里很可能因为数据消费速度赶不上上游数据生产速度，导致上游数据源的堆积，这又会反过来导致产生计算结果的时间和数据生产时间之间的延迟越来越大。为了解决这个问题，我们需要将 StreamReader 并行化，而这又要求基于时间窗口的计算算子能够对多路数据进行归并排序。另外，目前采用 TiDB Global Variable 来模拟 Streaming 的位置信息，其实更好地方案是设计用一个 TiDB Table 来记录每个不同 StreamReader 读取到的数据位置，这种做法更标准。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Planner&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 Planner 这块，从前面的方案介绍可以看出，Streaming 的流式特性和时序特性决定了 Streaming SQL 的优化方式和一般 SQL 有所不同。目前 TBSSQL 的实现方式是在现有 Planner 的执行路径上加上一系列针对 Streaming SQL 的特殊分支。这种做法很不优雅，既难以理解，也难以扩展。目前，TiDB 正在基于 &lt;a href=&quot;http://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.98.9460%26rep%3Drep1%26type%3Dpdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Cascade&lt;/a&gt; 重构 Planner 架构，我们希望今后 Streaming SQL 的相关优化也基于新的 Planner 框架来完成。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 时间窗口&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前，TBSSQL 只实现了最简单的固定窗口。在固定窗口上，Aggregation、Sort 等算子很大程度能复用现有逻辑。但是在滑动窗口上，Aggregation、Sort 的计算方式和在 Batch Table 上的计算方式会完全不一样。今后，我们希望 TBSSQL 能够支持完善对各种时间窗口类型的支持。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. 多 Streaming 处理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前 TBSSQL 只能处理单路 Streaming 输入，比如单个 Streaming 的聚合，排序，以及单个Streaming 和多个 Table 之间的 Join。多个 Streaming 之间的 Join 因为涉及多个 Streaming 窗口的对齐，目前 TBSSQL 暂不支持，所以 TBSSQL 目前并不是一个完整的 Streaming SQL 引擎。我们计划今后对这一块加以完善。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TBSSQL 是一个复杂的工程，要实现 Batch/Streaming 的融合，除了以上提到这四点，TBSSQL 还有很有很多工作要做，这里就不一一详述了。或许，下次 Hackathon 可以再继续搞一把 TBSSQL 2.0 玩玩:) 有点遗憾的是作为选手出场，没有和所有优秀的参赛的小伙伴们畅谈交流，希望有机会可以补上。属于大家的青春不散场，TiDB Hackathon 2019，不见不散～～&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB Hackathon 2018 共评选出六个优秀项目，本系列文章将由这六个项目成员主笔，分享他们的参赛经验和成果。我们非常希望本届 Hackathon 诞生的优秀项目能够在社区中延续下去，感兴趣的小伙伴们可以加入进来哦。&lt;br&gt;&lt;b&gt;延伸阅读：&lt;/b&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D1%26sn%3D72d9d52558e83eb97cd709c67b5a4149%26chksm%3Deb1628e0dc61a1f60bb99ffe2fe42fafe91570159094fc5e3d46039b5490bd0c391ee500b8d6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Hackathon 2018 回顾&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D2%26sn%3D7eb3d41b2b5cf2a8a440b12121796e2d%26chksm%3Deb1628e0dc61a1f6719856b0eeadd4e878c3b59e0127f8b8f65ed1fb99b2a8981739b5449ce7%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;天真贝叶斯学习机 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D2%26sn%3D5f1ee6e838c3a86556fcd556662112c5%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiQuery：All Diagnosis in SQL | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D1%26sn%3D8a8861419dd22344a021667545005769%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;让 TiDB 访问多种数据源 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D2%26sn%3D3a601b2ff9100a9797605a825e478c01%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Lab 诞生记 | 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/_o_4XRJ22NfAfBvUFQqueQ&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiEye：Region 信息变迁历史可视化工具 | TiDB Hackathon 2018 优秀项目分享&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/sg3RiE8Fp96aQZlo1MM0hg&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiPrometheus：基于 TiDB 的 TSDB | TiDB Hackathon 优秀项目分享&lt;/a&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2018-12-27-53455060</guid>
<pubDate>Thu, 27 Dec 2018 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
