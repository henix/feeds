<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>深度 | 变分自编码器VAE面临的挑战与发展方向</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1522504540&amp;src=3&amp;ver=1&amp;signature=ssOiy3io7mPgGP3Gz0YG7n255le*VUX5zHD3lIHUNgzye11oGSjNdzjHgO27ciqtav1nH8GBrHQXjFBHmINzhQxPQBMKRNmR3avlAAdUT9iP7FsoOOrcplY4uubwHGVhiFX4GJjyevVW-ZqgeAgg2EDCMm6zWvtbpNSl14dB3NY=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    深度 | 变分自编码器VAE面临的挑战与发展方向                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-22</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自akosiorek</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;font-size: 16px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">参与：刘天赐、李泽南</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p><br></p><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">变分自编码器（VAE）与生成对抗网络（GAN）一样，是无监督学习最具前景的方法之一。本文中，牛津大学统计系在读博士 Adam Kosiorek 从原理上向我们介绍了 VAE 目前面临的挑战。同时，文中也提出了对于该方法的几种改进方向。</span></p></blockquote><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">隐变量模型</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">假设你希望通过一个定义在 x∈RD 上的概率分布来对整个世界建模，其中 p（x）表示 x 可能处于的状态。这个世界可能非常复杂，我们无法知道 p（x）的具体形式。为了解决这个问题，我们引入另一个变量 z∈Rd 来描述 x 的背景信息。例如 x 是一个图像，那么 z 就可以记录关于图像中可见物体的出现、数量、类型，以及画面的背景和光影条件的信息。这个新的变量使得我们可以将 p（x）表示为一个无限混合模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.13565891472868216" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6Ze1Er2BDDbJVz4kuDA6OGsoqHfTwsYRibdruDCNrlIHPVexACnYvNoQ/640?wx_fmt=png" data-type="png" data-w="1548"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这是一个混合模型，因为对于 z 的任意可能取值，都引入另一个条件分布，并通过 z 的概率进行加权，最终得到 p（x）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在这样的设定下，「给定 x 的观测值，隐变量 z 是什么」就成了一个非常有趣的问题。也就是说，我们希望知道后验分布 p(z∣x)。但是，z 和 x 之间可以呈现高度的非线性关系（比如，由一个多层神经网络实现），而且，D——我们观测值的维度，和 d——隐变量的维度，也可能非常大。由于边缘分布和后验分布都需要对（1）式积分求值，我们认为它们都是无法计算的。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们可以通过蒙特卡罗抽样，根据<img class="" data-ratio="0.1111111111111111" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6Tr7IDicsmicT9sJdbMQevgshOu3IiaDcJ7TpmIHvSQibO1kjIM1FEzlJIQ/640?wx_fmt=png" data-type="png" data-w="450" style="width: 222px;height: 25px;">来估计（1）式，但由于 z 的空间可能非常大，我们可能需要上百万个 z 的样本，来得到一个可靠的估计。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在训练一个概率模型的时候，我们可以使用参数分布 - 它的参数由一个参数为θ∈Θ的神经网络来确定。现在，我们就可以使用极大似然估计来学习得到这些参数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><img class="" data-ratio="0.087248322147651" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6aYMqzcoEcianohSd9eSG7nY4FhUgl2miaOufybd5yY0guwlicIh2ONktw/640?wx_fmt=png" data-type="png" data-w="1490"></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这里的问题是，我们无法最大化（1）式，因为我们无法估计它。为了解决这个问题，我们可以求助于重要抽样（importance sampling）。当我们需要对原始（名义分布）概率密度分布（pdf）估算一个期望值时，IS 使得我们可以从另一个不同的概率分布（建议分布）中抽样，然后将这些样本对名义分布求期望。用 qϕ(z∣x) 表示我们的建议分布 - 其中的参数由参数为 ϕ∈Φ的神经网络确定。我们可以得到：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.2012987012987013" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6wFD4Xx4OJ6QTrlIXU6EWBdUqNyRmpvz8lB1FxyeOmJAb8AantO1Lkg/640?wx_fmt=png" data-type="png" data-w="1540"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">根据重要性抽样的文献可知，最优的建议分布，与名义分布乘以某个函数成比例，其中这个函数的期望是我们希望估计的。在我们的设定下，「某个函数」就是 p（x|z）。根据贝叶斯定理，p(z∣x)=p(x∣z)p(z)/p(x)，我们可以看到，最优建议分布与后验分布成比例，显然，后验分布无法求解。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">变分自编码器的诞生</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">幸运的是，事实上我们可以一箭双雕：通过一个学习到的建议分布来近似估计后验分布，我们可以有效的得到边缘分布 pθ(x) 的估计。在这里，我们无意间得到了一个自编码的设定。为了学习我们的模型，我们需要：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">pθ(x,z) - 生成模型，其中包含：</span></p></li><ul class=" list-paddingleft-2" style="list-style-type: square;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">pθ(x∣z) - 一个概率形式的解码器，以及</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">p(z) - 一个定义在隐变量上的先验分布</span></p></li></ul><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">qϕ(z∣x) - 一个概率形式的编码器</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了近似估计后验分布，我们可以利用建议分布和后验分布之间的 KL 散度（可以理解为两个概率分布之间的距离），而且我们可以最小化这个结果。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.1349934469200524" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6tvyXaJiaDlOTpjECxibCdXqtibMO2ibT6b9ib8zg4OGMqYCPbBB57aFGvfg/640?wx_fmt=png" data-type="png" data-w="1526"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这个时候，我们面临的新问题就是：为了计算 KL 散度，我们需要知道后验分布。并非没有可能，只要利用一点点代数运算，我们就能得到可以计算的目标函数。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.2326797385620915" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH65e4NTG5jzDvuwQVNvIX2aLYXP0mRYHPffd6aIM15yaQ5NXEHBwrhEA/640?wx_fmt=png" data-type="png" data-w="1530"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我在第二行展开了对数项，在第三行使用了贝叶斯定理以及 pθ(x) 和 z 是独立的事实。最后一行中的 L(x;θ,ϕ) 是对数概率分布 pθ(x) 的下界 - 即通常所说的证据下界（ELBO）。我们通过整理可以得到：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.2" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH68D37m9o5B2R6gUSWBYCed3smvgeA4Ze7H9hICh2PiaL91AmHtyP2rgA/640?wx_fmt=png" data-type="png" data-w="1540"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">只需要一个从建议分布中抽得的样本，我们就可以得到近似估计：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.12195121951219512" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6u0DfwCoqTMRzEoFxiaH9D313mYs4Engv0BgzktMIVA4ocMTfcngHGnA/640?wx_fmt=png" data-type="png" data-w="1558"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们通过寻找最大化 ELBO 的ϕ和θ（通常使用随机梯度下降）来训练模型：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.1111111111111111" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6CUQZXZbaEQUWQym2uGPtD4J4g8eNLFGjRVjcdcCx7Pz6JtdzW9auFg/640?wx_fmt=png" data-type="png" data-w="1548"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">通过最大化 ELBO，我们或（1）最大化边缘分布，或（2）最小化 KL 散度，或同时完成。需要注意，ELBO 的近似估计是 f(x)=1、重要性权重为 w(x)=pθ(x,z)qϕ(z∣x) 的重要性抽样的期望的对数形式。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">这个估计量有什么问题？</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果你足够仔细的看重要性抽样，就能发现，对建议分布的支撑应该比对名义分布的支撑更广泛——应该同时避免估计量方差无限大和数值的不稳定性。在这里，最好是来优化 KL(p∣∣q) 的倒数——因为它有模式平均性质，而不是优化 KL（q∣∣p），来试图通过模式 q 去匹配找到一个最好的模式 p。这意味着我们需要从真实的后验分布中进行抽样，而这是很困难的。作为替代，我们可以使用 ELBO 的 IS 估计，作为重要性加权自编码器（IWAE）。这里的想法很简单：我们从建议分布中抽取 k 个样本，并从中计算出平均概率比，这里的每一个样本也叫「粒子」。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.12041884816753927" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6zcshGZqk9vCzU1s6DqkXaLGgXQSx2O0ricrTZo8FibZGcNjxpwar4dhQ/640?wx_fmt=png" data-type="png" data-w="1528"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">已经证明，这个估计量是在优化修正后的 KL 散度 KL(qIS∣∣pIS)，其中 qIS 和 pIS 的定义分别是：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.2138200782268579" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6s6jjib7uichQ2coBU9JnaMlVo1GftY8vj3ia7KFAVJvpVLtGURiaFEvDbA/640?wx_fmt=png" data-type="png" data-w="1534"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽管和原始分布看似接近，但 qIS 和 pIS 允许 q 和 p 中存在预想以外的小的变动。原始论文中证明，优化这个下界可以得到更好的生成模型。同时它也给出了一个近似后验分布 q 的熵更大的估计（更宽，更离散），并成功的超越了原始 KL 散度的模式匹配方法。还有一个有趣的结果，如果我们令粒子 K 的数量趋于无穷，我们就可以不需要推断模型 q。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.49282920469361147" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH62cNx89eUnIgicuyQoiagxSogddmetmcv5jbjrQpH7lRibXe7ss4yVwSFA/640?wx_fmt=png" data-type="png" data-w="1534"></p><p style="text-align: justify;line-height: 1.75em;"><em style="color: rgb(136, 136, 136);"><span style="font-size: 12px;">IWAE（第一行）和 VAE（第二行）中 z 的后验分布。图像从 IWAE 论文中复现得到。</span></em><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">IWAE 有什么问题？</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">重要性加权 ELBO，或 IWAE，推广了原始的 ELBO：对于 K=1，我们有 LK=L1=L。同时有 logp(x)≥Ln+1≥Ln≥L1。换言之，我们用来估计 LK 的粒子越多，它的结果就会越接近数据真实对数概率分布——即「界限越紧」。这意味着和原始 ELBO 的梯度相比，通过对 IWAE 求微分得到的梯度估计量可以帮助我们找到一个更好的梯度方向。除此之外，随着 K 的增加，梯度估计量的方差会相应收缩。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">对于生成模型这些点非常好，但面对建议分布的时候，就会出现问题。随着 K 的增大，建议分布中参数的梯度的大小会趋于 0，而且比方差收敛得还要快。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">令Δ(ϕ) 表示我们优化的目标函数（即 ELBO）在ϕ上的梯度的小批量估计。如果定义参数更新的信号-噪声比（SNR）如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.16558018252933507" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6iaFHagFNyYYeAuQPxYQTHbxWUsmRETqjiaEhWjllHxBLicP20SwQ5pI9g/640?wx_fmt=png" data-type="png" data-w="1534"></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">其中 E 和 V 分别表示期望和方差。可以看出对于 pθ，SNR 随着 K 增加而增加，但对于 qϕ，SNR 随着 K 增加而减小。这里的结论很简单：我们使用的粒子越多，我们的推断模型效果就会越差。如果我们关心的是表示学习，我们就会遇到问题了。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">更好的估计量</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">正如我们在最近的论文《Tighter Variational Bounds are Not Necessarily Better》中证明的，我们可以得到比 IWAE 更好的结果。思路是在推断和生成模型中使用不同的目标，通过这种方法，我们可以保证两个目标中都得到小方差非零梯度，最终得到更好的模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.6583850931677019" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6vDujNgvEAtXm4Z0CGQd1eeAZL9Yp3qfryiay0jNcdwfWbz8O5AibdkVw/640?wx_fmt=png" data-type="png" data-w="1610"></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">不同的训练目标在训练时期中信号-噪声比</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在上图中，我们比较了建议分布 qϕ的参数ϕz 在更新中的 SNR。SNR 最高的 VAE 通过最优化 L1 来训练。SNR 最低的 IWAE 则通过最优化 L64。中间的三条曲线使用的是不同的组合：生成模型中使用的 L64，推断模型中使用的则是 L8 或 L1。在当前指标下，它们效果虽然没有 VAE 好，但训练出的建议分布和生成模型都比使用 VAE 或 IWAE 得到的好。</span><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这里有一个令人惊讶的副作用：使用我们新的估计量训练的模型比使用 IWAE 本身训练的模型达到了更高的 L64 界限。为什么会这样？通过研究有效样本量（ESS）和数据的边缘概率分布的对数，似乎是最优化 L1，导致了性质最好的建议分布但是性质最差的生成模型。如果我们将一个好的建议分布和一个可以得出好的生成模型的目标结合在一起，我们应该可以得到这个目标的一个方差更小的估计，甚至因此可以得到更好的模型。请在这里查看我们论文的详情。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">论文：Tighter Variational Bounds are Not Necessarily Better</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><img class="" data-ratio="0.2506234413965087" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6hTib9HfpOU4Hia7v09KibORtzUh0mqBzAyYiaTorATumAaKBtK0Z8LFTWw/640?wx_fmt=png" data-type="png" data-w="1604"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/abs/1802.04537</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">摘要：我们同时在理论和经验上证明，使用更紧的信息下界（ELBO）可能并不利于通过减少梯度估计量的信号-噪声比来学习推断网络的过程。我们的结果对目前广为应用的暗含假设：「更紧的 ELBO 是联立模型学习和推断摊销模式中更合适的变分目标」提出了质疑。根据我们的研究，我们提出了三个新的算法：偏重要性加权自编码器（PIWAE）、多层重要性加权自编码器（MIWAE）以及联合重要性加权自编码器（CIWAE）；在这三个算法中，标准的重要性自编码器（IWAE）都可以作为一个特殊情况。我们证明了这三个自编码器都可以在 IWAE 的基础上取得效果提升——即使我们使用的是 IWAE 中的目标来测试效果。进一步来说，和 IWAE 相比，PIWAE 可以同时提升推断网络和生成网络的效果。</span><img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="width: 44px;height: 20px;"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>原文链接：http://akosiorek.github.io/ml/2018/03/14/what_is_wrong_with_vaes.html</em></span></p><p><br></p><p style="margin-bottom: 20px;white-space: normal;max-width: 100%;min-height: 1em;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="892221902" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
