<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Python王牌加速库：奇异期权定价利器</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/134445362">原文</a></p>
<div class="title-image"><img src="https://pic3.zhimg.com/v2-5cbfbac9c287b538a189c5316d4586b3_b.jpg" alt=""></div><p>作者：Yi Dong    编译：1+1=6</p><h2>​1、<b>前言</b></h2><p>在金融领域，计算效率有时可以直接转化为交易利润。<b>量化分析师面临着在研究效率和计算效率之间进行权衡的挑战</b>。使用Python可以生成简洁的研究代码，从而提高了研究效率。但是，一般的Python代码速度很慢，不适合用于生产环境。在这篇文章中，我们将探索如何使用<b>Python的GPU库来高性能实现奇异期权定价领域遇到的问题</b>。</p><h2><b>2、定价计算概述</b></h2><p>Black-Scholes模型可以有效地用欧洲行权规则为plain vanilla定价。像障碍（Barrier）期权和篮子（Basket ）期权这样的期权具有复杂的结构。蒙特卡罗模拟是一种有效的定价方法。为了得到一个精确的价格和一个小的变动，你需要许多模拟路径，计算十分密集。</p><p>幸运的是，每个模拟路径都是独立的，<b>大家可以利用多核NVIDIA GPU在一个节点内加速计算，甚至在必要时将其扩展到多个服务器</b>。由于独立路径的并行化，使用GPU可以将计算速度提高几个数量级。</p><p>传统上，对GPU的蒙特卡罗仿真是在CUDA C/ C++代码中实现的。大家必须明确地管理内存并编写大量样板代码，这对代码维护和生产效率提出了挑战。</p><p>最近，Deep Learning Derivatives（Ryan et al，2018）的论文被引入到使用深度神经网络来近似期权定价模型。该方法利用计算时间与推理时间进行定价训练，与GPU上的蒙特卡罗模拟相比，它实现了额外的数量级加速，这使得在生产环境中的实时奇异期权定价成为一个现实目标。</p><p>在这篇文章中介绍的方法对奇异期权类型没有任何限制。它适用于任何可以用蒙特卡罗方法模拟的期权定价模型。</p><p>在不失一般性的情况下，大家可以使用<b>亚式障碍期权</b>作为一个示例。亚式障碍期权是亚式期权和障碍期权的混合。衍生品价格取决于标的资产价格S、执行价格K和障碍价格B的平均值。以上下看涨期权离散化亚洲障碍期权为例。</p><ul><li>如果标的资产的平均价格低于这一水平，则该期权无效。</li><li>资产现货价格S通常在建模中被认为是属于几何布朗运动，它有三个参数：现货价格、波动率和漂移率。</li><li>期权的价格是到期时的预期利润相对于当前价值的折现。</li><li>期权的路径依赖性使得对期权价格的解析解成为不可能。</li></ul><p>这是使用蒙特卡罗模拟定价的一个很好的示例。你需要一个至少16GB的GPU来复现这个结果。</p><h2><b>3、第1部分：使用GPU Python库进行蒙特卡洛定价</b></h2><p>NVIDIA GPU被设计用来使用大量线程进行并行计算。蒙特卡罗仿真是在GPU中可以很好加速的算法之一。在下面的小节中，大家将看到在传统的CUDA代码中使用蒙特卡罗模拟，然后在Python中使用不同的库实现相同的算法。</p><p><b>CUDA方法</b></p><p>传统上，蒙特卡罗期权定价是在CUDA C/ C++中实现的。下面的CUDA C/ C++代码示例使用蒙特卡罗方法计算期权价格：</p><div class="highlight"><pre><code class="language-c"><span class="cp">#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
</span><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span><span class="cp">#include</span> <span class="cpf">&lt;iostream&gt;</span><span class="cp">
</span><span class="cp">#include</span> <span class="cpf">&lt;chrono&gt;</span><span class="cp">
</span><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
</span><span class="cp">#include</span> <span class="cpf">&lt;helper_cuda.h&gt;</span><span class="cp">
</span><span class="cp">#include</span> <span class="cpf">&lt;curand.h&gt;</span><span class="cp">
</span><span class="cp"></span>
<span class="cp">#define CHECKCURAND(expression)                         \
</span><span class="cp"></span>  <span class="p">{</span>                                                     \
    <span class="n">curandStatus_t</span> <span class="n">status</span> <span class="o">=</span> <span class="p">(</span><span class="n">expression</span><span class="p">);</span>                         \
    <span class="k">if</span> <span class="p">(</span><span class="n">status</span> <span class="o">!=</span> <span class="n">CURAND_STATUS_SUCCESS</span><span class="p">)</span> <span class="p">{</span>                        \
      <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Curand Error on line &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">__LINE__</span><span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>     \
      <span class="n">std</span><span class="o">::</span><span class="n">exit</span><span class="p">(</span><span class="n">EXIT_FAILURE</span><span class="p">);</span>                                          \
    <span class="p">}</span>                                                                   \
  <span class="p">}</span>

<span class="c1">// atomicAdd is introduced for compute capability &gt;=6.0
</span><span class="c1"></span><span class="cp">#if !defined(__CUDA_ARCH__) || __CUDA_ARCH__ &gt;= 600
</span><span class="cp">#else
</span><span class="cp"></span><span class="n">__device__</span> <span class="kt">double</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="kt">double</span><span class="o">*</span> <span class="n">address</span><span class="p">,</span> <span class="kt">double</span> <span class="n">val</span><span class="p">)</span>
<span class="p">{</span>
      <span class="n">printf</span><span class="p">(</span><span class="s">&#34;device arch &lt;=600</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">);</span>
        <span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span> <span class="kt">int</span><span class="o">*</span> <span class="n">address_as_ull</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span> <span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">address</span><span class="p">;</span>
          <span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span> <span class="kt">int</span> <span class="n">old</span> <span class="o">=</span> <span class="o">*</span><span class="n">address_as_ull</span><span class="p">,</span> <span class="n">assumed</span><span class="p">;</span>
            <span class="k">do</span> <span class="p">{</span>
                    <span class="n">assumed</span> <span class="o">=</span> <span class="n">old</span><span class="p">;</span>
                        <span class="n">old</span> <span class="o">=</span> <span class="n">atomicCAS</span><span class="p">(</span><span class="n">address_as_ull</span><span class="p">,</span> <span class="n">assumed</span><span class="p">,</span>
                                                    <span class="n">__double_as_longlong</span><span class="p">(</span><span class="n">val</span> <span class="o">+</span> <span class="n">__longlong_as_double</span><span class="p">(</span><span class="n">assumed</span><span class="p">)));</span>
                          <span class="p">}</span> <span class="k">while</span> <span class="p">(</span><span class="n">assumed</span> <span class="o">!=</span> <span class="n">old</span><span class="p">);</span>
              <span class="k">return</span> <span class="nf">__longlong_as_double</span><span class="p">(</span><span class="n">old</span><span class="p">);</span>
<span class="p">}</span>
<span class="cp">#endif
</span><span class="cp"></span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="n">sumPayoffKernel</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">d_s</span><span class="p">,</span> <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">N_PATHS</span><span class="p">,</span> <span class="kt">double</span> <span class="o">*</span><span class="n">mysum</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">unsigned</span> <span class="n">idx</span> <span class="o">=</span>  <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">unsigned</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">unsigned</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="k">extern</span> <span class="n">__shared__</span> <span class="kt">double</span> <span class="n">smdata</span><span class="p">[];</span>
  <span class="n">smdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span> <span class="o">=</span> <span class="n">idx</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">N_PATHS</span><span class="p">;</span> <span class="n">i</span><span class="o">+=</span><span class="n">stride</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="n">smdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="kt">double</span><span class="p">)</span> <span class="n">d_s</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
  <span class="p">}</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">s</span><span class="o">=</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span> <span class="n">s</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">;</span> <span class="n">s</span><span class="o">&gt;&gt;=</span><span class="mi">1</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="n">__syncthreads</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">s</span><span class="p">)</span> <span class="n">smdata</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">smdata</span><span class="p">[</span><span class="n">tid</span> <span class="o">+</span> <span class="n">s</span><span class="p">];</span>
  <span class="p">}</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="n">atomicAdd</span><span class="p">(</span><span class="n">mysum</span><span class="p">,</span> <span class="n">smdata</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">__global__</span> <span class="kt">void</span> <span class="n">barrier_option</span><span class="p">(</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">d_s</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">T</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">K</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">B</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">S0</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">sigma</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">mu</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">r</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span> <span class="n">d_normals</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">long</span> <span class="n">N_STEPS</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">long</span> <span class="n">N_PATHS</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">unsigned</span> <span class="n">idx</span> <span class="o">=</span>  <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">unsigned</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">float</span> <span class="n">tmp1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">float</span> <span class="n">tmp2</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="o">*</span><span class="n">T</span><span class="p">);</span>
  <span class="k">const</span> <span class="kt">float</span> <span class="n">tmp3</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span><span class="p">);</span>
  <span class="kt">double</span> <span class="n">running_average</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span> <span class="o">=</span> <span class="n">idx</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">N_PATHS</span><span class="p">;</span> <span class="n">i</span><span class="o">+=</span><span class="n">stride</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="kt">float</span> <span class="n">s_curr</span> <span class="o">=</span> <span class="n">S0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">unsigned</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N_STEPS</span><span class="p">;</span> <span class="n">n</span><span class="o">++</span><span class="p">){</span>
       <span class="n">s_curr</span> <span class="o">+=</span> <span class="n">tmp1</span> <span class="o">*</span> <span class="n">s_curr</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">s_curr</span><span class="o">*</span><span class="n">tmp3</span><span class="o">*</span><span class="n">d_normals</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">N_PATHS</span><span class="p">];</span>
       <span class="n">running_average</span> <span class="o">+=</span> <span class="p">(</span><span class="n">s_curr</span> <span class="o">-</span> <span class="n">running_average</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="p">;</span>
       <span class="k">if</span> <span class="p">(</span><span class="n">running_average</span> <span class="o">&lt;=</span> <span class="n">B</span><span class="p">){</span>
           <span class="k">break</span><span class="p">;</span>
       <span class="p">}</span>
    <span class="p">}</span>

    <span class="kt">float</span> <span class="n">payoff</span> <span class="o">=</span> <span class="p">(</span><span class="n">running_average</span><span class="o">&gt;</span><span class="n">K</span> <span class="o">?</span> <span class="n">running_average</span><span class="o">-</span><span class="nl">K</span> <span class="p">:</span> <span class="mf">0.f</span><span class="p">);</span>
    <span class="n">d_s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp2</span> <span class="o">*</span> <span class="n">payoff</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="n">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span> <span class="p">{</span>
  <span class="n">try</span> <span class="p">{</span>
    <span class="c1">// declare variables and constants
</span><span class="c1"></span>    <span class="n">size_t</span> <span class="n">N_PATHS</span> <span class="o">=</span> <span class="mi">8192000</span><span class="p">;</span>
    <span class="n">size_t</span> <span class="n">N_STEPS</span> <span class="o">=</span> <span class="mi">365</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">)</span>  <span class="n">N_PATHS</span> <span class="o">=</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">)</span>  <span class="n">N_STEPS</span> <span class="o">=</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">2</span><span class="p">]);</span>

    <span class="k">const</span> <span class="kt">float</span> <span class="n">T</span> <span class="o">=</span> <span class="mf">1.0f</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">K</span> <span class="o">=</span> <span class="mf">110.0f</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">B</span> <span class="o">=</span> <span class="mf">100.0f</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">S0</span> <span class="o">=</span> <span class="mf">120.0f</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.35f</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">mu</span> <span class="o">=</span> <span class="mf">0.1f</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">r</span> <span class="o">=</span> <span class="mf">0.05f</span><span class="p">;</span>


    <span class="kt">double</span> <span class="n">gpu_sum</span><span class="p">{</span><span class="mf">0.0</span><span class="p">};</span>

    <span class="kt">int</span> <span class="n">devID</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="n">cudaDeviceProp</span> <span class="n">deviceProps</span><span class="p">;</span>

    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaGetDeviceProperties</span><span class="p">(</span><span class="o">&amp;</span><span class="n">deviceProps</span><span class="p">,</span> <span class="n">devID</span><span class="p">));</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&#34;CUDA device [%s]</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">,</span> <span class="n">deviceProps</span><span class="p">.</span><span class="n">name</span><span class="p">);</span>
     <span class="n">printf</span><span class="p">(</span><span class="s">&#34;GPU Device %d: </span><span class="se">\&#34;</span><span class="s">%s</span><span class="se">\&#34;</span><span class="s"> with compute capability %d.%d</span><span class="se">\n\n</span><span class="s">&#34;</span><span class="p">,</span>  <span class="n">devID</span><span class="p">,</span> <span class="n">deviceProps</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">deviceProps</span><span class="p">.</span><span class="n">major</span><span class="p">,</span> <span class="n">deviceProps</span><span class="p">.</span><span class="n">minor</span><span class="p">);</span>
    <span class="c1">// Generate random numbers on the device
</span><span class="c1"></span>    <span class="n">curandGenerator_t</span> <span class="n">curandGenerator</span><span class="p">;</span>
    <span class="n">CHECKCURAND</span><span class="p">(</span><span class="n">curandCreateGenerator</span><span class="p">(</span><span class="o">&amp;</span><span class="n">curandGenerator</span><span class="p">,</span> <span class="n">CURAND_RNG_PSEUDO_MTGP32</span><span class="p">));</span>
    <span class="n">CHECKCURAND</span><span class="p">(</span><span class="n">curandSetPseudoRandomGeneratorSeed</span><span class="p">(</span><span class="n">curandGenerator</span><span class="p">,</span> <span class="mi">1234ULL</span><span class="p">))</span> <span class="p">;</span>

    <span class="k">const</span> <span class="n">size_t</span> <span class="n">N_NORMALS</span> <span class="o">=</span> <span class="p">(</span><span class="n">size_t</span><span class="p">)</span><span class="n">N_STEPS</span> <span class="o">*</span> <span class="n">N_PATHS</span><span class="p">;</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">d_normals</span><span class="p">;</span>
    <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_normals</span><span class="p">,</span> <span class="n">N_NORMALS</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)));</span>
    <span class="n">CHECKCURAND</span><span class="p">(</span><span class="n">curandGenerateNormal</span><span class="p">(</span><span class="n">curandGenerator</span><span class="p">,</span> <span class="n">d_normals</span><span class="p">,</span> <span class="n">N_NORMALS</span><span class="p">,</span> <span class="mf">0.0f</span><span class="p">,</span> <span class="mf">1.0f</span><span class="p">));</span>
    <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>

      <span class="c1">// before kernel launch, check the max potential blockSize
</span><span class="c1"></span>      <span class="kt">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">GRID_SIZE</span><span class="p">;</span>
      <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaOccupancyMaxPotentialBlockSize</span><span class="p">(</span><span class="o">&amp;</span><span class="n">GRID_SIZE</span><span class="p">,</span>
                                                         <span class="o">&amp;</span><span class="n">BLOCK_SIZE</span><span class="p">,</span>
                                                         <span class="n">barrier_option</span><span class="p">,</span>
                                                         <span class="mi">0</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="p">));</span>

      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;suggested block size &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">BLOCK_SIZE</span>
                <span class="o">&lt;&lt;</span> <span class="s">&#34; </span><span class="se">\n</span><span class="s">suggested grid size &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">GRID_SIZE</span>
                <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;Used grid size &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">GRID_SIZE</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

      <span class="c1">// Kernel launch
</span><span class="c1"></span>      <span class="k">auto</span> <span class="n">t1</span><span class="o">=</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

      <span class="kt">float</span> <span class="o">*</span><span class="n">d_s</span><span class="p">;</span>
      <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_s</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)));</span>

      <span class="k">auto</span> <span class="n">t3</span><span class="o">=</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>
      <span class="n">barrier_option</span><span class="o">&lt;&lt;&lt;</span><span class="n">GRID_SIZE</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_s</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">S0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d_normals</span><span class="p">,</span> <span class="n">N_STEPS</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="p">);</span>
      <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
      <span class="k">auto</span> <span class="n">t4</span><span class="o">=</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

      <span class="kt">double</span><span class="o">*</span> <span class="n">mySum</span><span class="p">;</span>
      <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mySum</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">)));</span>
      <span class="n">sumPayoffKernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">GRID_SIZE</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">)</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_s</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="p">,</span> <span class="n">mySum</span><span class="p">);</span>
      <span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
      <span class="k">auto</span> <span class="n">t5</span><span class="o">=</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;sumPayoffKernel takes &#34;</span>
                <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">duration_cast</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">microseconds</span><span class="o">&gt;</span><span class="p">(</span><span class="n">t5</span><span class="o">-</span><span class="n">t4</span><span class="p">).</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1000.f</span>
                <span class="o">&lt;&lt;</span> <span class="s">&#34; ms</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>

      <span class="n">gpu_sum</span> <span class="o">=</span> <span class="n">mySum</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">N_PATHS</span><span class="p">;</span>

      <span class="k">auto</span> <span class="n">t2</span><span class="o">=</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">high_resolution_clock</span><span class="o">::</span><span class="n">now</span><span class="p">();</span>

      <span class="c1">// clean up
</span><span class="c1"></span>      <span class="n">CHECKCURAND</span><span class="p">(</span><span class="n">curandDestroyGenerator</span><span class="p">(</span> <span class="n">curandGenerator</span> <span class="p">))</span> <span class="p">;</span>
      <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_s</span><span class="p">));</span>
      <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaFree</span><span class="p">(</span><span class="n">d_normals</span><span class="p">));</span>
      <span class="n">checkCudaErrors</span><span class="p">(</span><span class="n">cudaFree</span><span class="p">(</span><span class="n">mySum</span><span class="p">));</span>

      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;price &#34;</span>
              <span class="o">&lt;&lt;</span> <span class="n">gpu_sum</span>
              <span class="o">&lt;&lt;</span> <span class="s">&#34; time &#34;</span>
                <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">duration_cast</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">chrono</span><span class="o">::</span><span class="n">microseconds</span><span class="o">&gt;</span><span class="p">(</span><span class="n">t5</span><span class="o">-</span><span class="n">t1</span><span class="p">).</span><span class="n">count</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1000.f</span>
                <span class="o">&lt;&lt;</span> <span class="s">&#34; ms</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">catch</span><span class="p">(</span><span class="n">std</span><span class="o">::</span>
        <span class="n">exception</span><span class="o">&amp;</span> <span class="n">e</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span> <span class="s">&#34;exception: &#34;</span> <span class="o">&lt;&lt;</span> <span class="n">e</span><span class="p">.</span><span class="n">what</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="s">&#34;</span><span class="se">\n</span><span class="s">&#34;</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span> </code></pre></div><p>这段CUDA代码很长。一般来说，它主要执行以下一系列任务：</p><p>1、分配GPU内存来存储随机数和模拟路径结果。</p><p>2、调用cuRand库生成随机数。</p><p>3、启动障碍期权内核来执行并行模拟。</p><p>4、启动sum内核来聚合最终基础资产价格。</p><p>5、释放内存。</p><p>大家必须显式地执行每个步骤。在这个代码示例中，它计算下表中指定的亚式障碍期权的价格。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-fbd743fbd8c3482dd8448ea8aef88a8f_b.jpg" data-caption="" data-size="normal" data-rawwidth="449" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="449" data-original="https://pic4.zhimg.com/v2-fbd743fbd8c3482dd8448ea8aef88a8f_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-fbd743fbd8c3482dd8448ea8aef88a8f_b.jpg" data-caption="" data-size="normal" data-rawwidth="449" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="449" data-original="https://pic4.zhimg.com/v2-fbd743fbd8c3482dd8448ea8aef88a8f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-fbd743fbd8c3482dd8448ea8aef88a8f_b.jpg"/></figure><p>亚式障碍期权的参数。K是执行价格，B是障碍价格，S0是现货价格，sigma是波动率百分比，mu是漂移百分比，r是利率。</p><p>本研究中，该期权的期限为一年。在V100 GPU上编译和运行这个CUDA代码，可以<b>在26.6 ms内生成正确的期权价格$18.70，8192万条路径，365个步骤</b>。使用这些数字作为以后比较的参考基准。在实际投资中，量化分析师通常使用更少的路径来进行蒙特卡罗模拟。</p><p>可以使用许多技巧来减少模拟所需的路径数，例如<b>重要性采样方法。</b></p><p>在这五个步骤中，关键的部分是步骤3，大家需要在其中描述详细的蒙特卡罗模拟。理想情况下，大家的努力应该集中在这一步上。幸运的是，在迁移到Python GPU库之后，其他步骤可以自动处理，而不会牺牲其性能。例如：</p><p>步骤1：可以通过CuPy数组自动分配和初始化GPU内存。路径结果数组可以通过以下代码示例定义：</p><div class="highlight"><pre><code class="language-python"><span class="n">output</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N_PATHS</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cupy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> </code></pre></div><p>步骤2：CuPy随机函数引擎下的cuRAND库。分配和随机数生成可以通过以下代码示例定义:</p><div class="highlight"><pre><code class="language-python"><span class="n">randoms_gpu</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N_PATHS</span> <span class="o">*</span> <span class="n">N_STEPS</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cupy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> </code></pre></div><p>步骤4：GPU的平均值计算是CuPy库中的一个内置函数。</p><div class="highlight"><pre><code class="language-python"><span class="n">v</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> </code></pre></div><p>步骤5：通过 Python 内存管理自动释放 GPU 内存。</p><p>在这篇文章的其余部分，我们会将重点介绍第3步，使用Python对亚式障碍期权进行蒙特卡罗模拟。</p><p><b>Numba库方法-单核CPU</b></p><p>下面的代码示例是一个实现蒙特卡罗模拟优化运行在一个单核CPU：</p><div class="highlight"><pre><code class="language-python"><span class="nd">@njit</span><span class="p">(</span><span class="n">fastmath</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cpu_barrier_option</span><span class="p">(</span><span class="n">d_s</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">S0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d_normals</span><span class="p">,</span> <span class="n">N_STEPS</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="p">):</span>
    <span class="n">tmp1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span>
    <span class="n">tmp2</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
    <span class="n">tmp3</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span><span class="p">)</span>
    <span class="n">running_average</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_PATHS</span><span class="p">):</span>
        <span class="n">s_curr</span> <span class="o">=</span> <span class="n">S0</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_STEPS</span><span class="p">):</span>
            <span class="n">s_curr</span> <span class="o">+=</span> <span class="n">tmp1</span> <span class="o">*</span> <span class="n">s_curr</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">s_curr</span><span class="o">*</span><span class="n">tmp3</span><span class="o">*</span><span class="n">d_normals</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">N_PATHS</span><span class="p">]</span>
            <span class="n">running_average</span> <span class="o">=</span> <span class="n">running_average</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">s_curr</span> <span class="o">-</span> <span class="n">running_average</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">running_average</span> <span class="o">&lt;=</span> <span class="n">B</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">payoff</span> <span class="o">=</span> <span class="n">running_average</span> <span class="o">-</span> <span class="n">K</span> <span class="k">if</span> <span class="n">running_average</span><span class="o">&gt;</span><span class="n">K</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">d_s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp2</span> <span class="o">*</span> <span class="n">payoff</span> </code></pre></div><p>蒙特卡罗仿真有两个嵌套的for-loop。外部循环遍历独立路径。在内部循环中，标的资产价格逐步更新，最终价格设置为结果数组。</p><p>我们启用了fastmath编译器优化来加快计算速度。对于相同数量的仿真路径和步骤，需要<b>41.6s</b>才能产生相同的定价数。</p><p><b>Numba库方法-多核CPU</b></p><p>为了实现跨多个CPU核的计算，你可以通过<b>将range改为prange来并行化外层for循环：</b></p><div class="highlight"><pre><code class="language-python"><span class="nd">@njit</span><span class="p">(</span><span class="n">fastmath</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">parallel</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cpu_multiplecore_barrier_option</span><span class="p">(</span><span class="n">d_s</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">S0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d_normals</span><span class="p">,</span> <span class="n">N_STEPS</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="p">):</span>
    <span class="n">tmp1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span>
    <span class="n">tmp2</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
    <span class="n">tmp3</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">prange</span><span class="p">(</span><span class="n">N_PATHS</span><span class="p">):</span>
        <span class="n">s_curr</span> <span class="o">=</span> <span class="n">S0</span>
        <span class="n">running_average</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_STEPS</span><span class="p">):</span>
            <span class="n">s_curr</span> <span class="o">+=</span> <span class="n">tmp1</span> <span class="o">*</span> <span class="n">s_curr</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">s_curr</span><span class="o">*</span><span class="n">tmp3</span><span class="o">*</span><span class="n">d_normals</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">N_PATHS</span><span class="p">]</span>
            <span class="n">running_average</span> <span class="o">=</span> <span class="n">running_average</span> <span class="o">+</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">s_curr</span> <span class="o">-</span> <span class="n">running_average</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">running_average</span> <span class="o">&lt;=</span> <span class="n">B</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="n">payoff</span> <span class="o">=</span> <span class="n">running_average</span> <span class="o">-</span> <span class="n">K</span> <span class="k">if</span> <span class="n">running_average</span><span class="o">&gt;</span><span class="n">K</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">d_s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp2</span> <span class="o">*</span> <span class="n">payoff</span> </code></pre></div><p>这段代码产生了相同的定价结果，现在需要<b>2.34s</b>才能在32核、超线程化DGX-1 Intel CPU中计算出来。</p><p><b>Numba库方法-单核GPU</b></p><p>使用Numba可以很容易地从CPU代码转移到GPU代码。在函数装饰中将 njit 改为 cuda.jit。并使用 GPU 线程并行进行外部for-loop计算。</p><div class="highlight"><pre><code class="language-python"><span class="nd">@cuda.jit</span>
<span class="k">def</span> <span class="nf">numba_gpu_barrier_option</span><span class="p">(</span><span class="n">d_s</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">S0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">d_normals</span><span class="p">,</span> <span class="n">N_STEPS</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="p">):</span>
    <span class="c1"># ii - overall thread index</span>
    <span class="n">ii</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">threadIdx</span><span class="o">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockIdx</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">gridDim</span><span class="o">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cuda</span><span class="o">.</span><span class="n">blockDim</span><span class="o">.</span><span class="n">x</span>
    <span class="n">tmp1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span>
    <span class="n">tmp2</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
    <span class="n">tmp3</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span><span class="p">)</span>
    <span class="n">running_average</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ii</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="n">s_curr</span> <span class="o">=</span> <span class="n">S0</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N_STEPS</span><span class="p">):</span>
            <span class="n">s_curr</span> <span class="o">+=</span> <span class="n">tmp1</span> <span class="o">*</span> <span class="n">s_curr</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">s_curr</span><span class="o">*</span><span class="n">tmp3</span><span class="o">*</span><span class="n">d_normals</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">N_PATHS</span><span class="p">]</span>
            <span class="n">running_average</span> <span class="o">+=</span> <span class="p">(</span><span class="n">s_curr</span> <span class="o">-</span> <span class="n">running_average</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">running_average</span> <span class="o">&lt;=</span> <span class="n">B</span><span class="p">:</span>
                <span class="k">break</span>
        <span class="n">payoff</span> <span class="o">=</span> <span class="n">running_average</span> <span class="o">-</span> <span class="n">K</span> <span class="k">if</span> <span class="n">running_average</span><span class="o">&gt;</span><span class="n">K</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">d_s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp2</span> <span class="o">*</span> <span class="n">payoff</span> </code></pre></div><p>通过在 V100 GPU 上加速这种运算，计算时间可以减少到<b>65ms</b>，并产生相同的结果。</p><p><b>CuPy库方法-单核GPU</b></p><p>CuPy提供了一种从原始CUDA源定义GPU内核的简单方法。RawKernel对象允许大家使用CUDA的cuLaunchKernel接口调用内核。下面的代码示例将障碍期权的计算代码封装在RawKernel对象中：</p><div class="highlight"><pre><code class="language-c"><span class="n">cupy_barrier_option</span> <span class="o">=</span> <span class="n">cupy</span><span class="p">.</span><span class="n">RawKernel</span><span class="p">(</span><span class="n">r</span><span class="err">&#39;&#39;&#39;</span>
<span class="k">extern</span> <span class="s">&#34;C&#34;</span> <span class="n">__global__</span> <span class="kt">void</span> <span class="n">barrier_option</span><span class="p">(</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">d_s</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">T</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">K</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">B</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">S0</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">sigma</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">mu</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="n">r</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span> <span class="n">d_normals</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">long</span> <span class="n">N_STEPS</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">long</span> <span class="n">N_PATHS</span><span class="p">)</span>
<span class="p">{</span>
  <span class="kt">unsigned</span> <span class="n">idx</span> <span class="o">=</span>  <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">unsigned</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">unsigned</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

  <span class="k">const</span> <span class="kt">float</span> <span class="n">tmp1</span> <span class="o">=</span> <span class="n">mu</span><span class="o">*</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span><span class="p">;</span>
  <span class="k">const</span> <span class="kt">float</span> <span class="n">tmp2</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="o">*</span><span class="n">T</span><span class="p">);</span>
  <span class="k">const</span> <span class="kt">float</span> <span class="n">tmp3</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">T</span><span class="o">/</span><span class="n">N_STEPS</span><span class="p">);</span>
  <span class="kt">double</span> <span class="n">running_average</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span> <span class="o">=</span> <span class="n">idx</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">N_PATHS</span><span class="p">;</span> <span class="n">i</span><span class="o">+=</span><span class="n">stride</span><span class="p">)</span>
  <span class="p">{</span>
    <span class="kt">float</span> <span class="n">s_curr</span> <span class="o">=</span> <span class="n">S0</span><span class="p">;</span>
    <span class="kt">unsigned</span> <span class="n">n</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">unsigned</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">N_STEPS</span><span class="p">;</span> <span class="n">n</span><span class="o">++</span><span class="p">){</span>
       <span class="n">s_curr</span> <span class="o">+=</span> <span class="n">tmp1</span> <span class="o">*</span> <span class="n">s_curr</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">s_curr</span><span class="o">*</span><span class="n">tmp3</span><span class="o">*</span><span class="n">d_normals</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">n</span> <span class="o">*</span> <span class="n">N_PATHS</span><span class="p">];</span>
       <span class="n">running_average</span> <span class="o">+=</span> <span class="p">(</span><span class="n">s_curr</span> <span class="o">-</span> <span class="n">running_average</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">)</span> <span class="p">;</span>
       <span class="k">if</span> <span class="p">(</span><span class="n">running_average</span> <span class="o">&lt;=</span> <span class="n">B</span><span class="p">){</span>
           <span class="k">break</span><span class="p">;</span>
       <span class="p">}</span>
    <span class="p">}</span>

    <span class="kt">float</span> <span class="n">payoff</span> <span class="o">=</span> <span class="p">(</span><span class="n">running_average</span><span class="o">&gt;</span><span class="n">K</span> <span class="o">?</span> <span class="n">running_average</span><span class="o">-</span><span class="nl">K</span> <span class="p">:</span> <span class="mf">0.f</span><span class="p">);</span>
    <span class="n">d_s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp2</span> <span class="o">*</span> <span class="n">payoff</span><span class="p">;</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="err">&#39;&#39;&#39;</span><span class="p">,</span> <span class="err">&#39;</span><span class="n">barrier_option</span><span class="err">&#39;</span><span class="p">)</span> </code></pre></div><p>在Python中启动这个GPU内核并运行蒙特卡罗模拟需要<b>29ms</b>，这与本地CUDA代码的基准测试（26ms）非常接近。</p><p><b>Dask-多核GPU</b></p><p>为了获得更准确的期权价格估计，需要更多的蒙特卡罗模拟路径。之前使用的NVIDIA V100 GPU只有16GB的内存，几乎达到了运行8M模拟的内存极限。</p><p><b>DASK是RAPIDS在GPU上进行分布式计算的集成组件。大家可以利用它将蒙特卡罗模拟计算分布到跨多个节点的多个GPU。</b></p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-9d722f28b219565ce56d620dc94f4b1d_b.jpg" data-size="normal" data-rawwidth="821" data-rawheight="389" class="origin_image zh-lightbox-thumb" width="821" data-original="https://pic2.zhimg.com/v2-9d722f28b219565ce56d620dc94f4b1d_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-9d722f28b219565ce56d620dc94f4b1d_b.jpg" data-size="normal" data-rawwidth="821" data-rawheight="389" class="origin_image zh-lightbox-thumb lazy" width="821" data-original="https://pic2.zhimg.com/v2-9d722f28b219565ce56d620dc94f4b1d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-9d722f28b219565ce56d620dc94f4b1d_b.jpg"/><figcaption>https://dask.org/</figcaption></figure><p>首先，将所有计算封装在一个函数中，以允许在函数调用结束时释放分配给GPU的内存。该函数为随机数种子值添加一个额外的参数，这样每个函数调用都有一个独立的随机数序列。</p><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">get_option_price</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">S0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">N_PATHS</span> <span class="o">=</span> <span class="mi">8192000</span><span class="p">,</span> <span class="n">N_STEPS</span> <span class="o">=</span> <span class="mi">365</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">number_of_threads</span> <span class="o">=</span> <span class="mi">256</span>
    <span class="n">number_of_blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_PATHS</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">number_of_threads</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">cupy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">randoms_gpu</span> <span class="o">=</span> <span class="n">cupy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N_PATHS</span> <span class="o">*</span> <span class="n">N_STEPS</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cupy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span>  <span class="n">cupy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N_PATHS</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">cupy</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">cupy_barrier_option</span><span class="p">((</span><span class="n">number_of_blocks</span><span class="p">,),</span> <span class="p">(</span><span class="n">number_of_threads</span><span class="p">,),</span>
                   <span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">K</span><span class="p">),</span> 
                    <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">S0</span><span class="p">),</span> 
                    <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">sigma</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">mu</span><span class="p">),</span> 
                    <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">r</span><span class="p">),</span>  <span class="n">randoms_gpu</span><span class="p">,</span> <span class="n">N_STEPS</span><span class="p">,</span> <span class="n">N_PATHS</span><span class="p">))</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">out_df</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">out_df</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cudf</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">v</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
    <span class="k">return</span> <span class="n">out_df</span> </code></pre></div><p>这个函数将模拟结果返回到一个cudf GPU数据模型中，以便在以后将其聚合到一个dask cuda分布式数据模型中。使用Dask在DGX-中运行1600800万次模拟，代码示例如下：</p><div class="highlight"><pre><code class="language-python"><span class="n">x</span> <span class="o">=</span> <span class="n">dask_cudf</span><span class="o">.</span><span class="n">from_delayed</span><span class="p">([</span><span class="n">delayed</span><span class="p">(</span><span class="n">get_option_price</span><span class="p">)(</span><span class="n">T</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">K</span><span class="o">=</span><span class="mf">110.0</span><span class="p">,</span>  <span class="n">B</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">S0</span><span class="o">=</span><span class="mf">120.0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">3000</span><span class="o">+</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>  <span class="nb">range</span><span class="p">(</span><span class="mi">1600</span><span class="p">)])</span>
<span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
<span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span> </code></pre></div><p>这种额外的计算能力产生了一个更精确的定价结果18.71。调用std函数计算有800万条路径的定价的标准偏差为0.0073。</p><h2><b>4、第2部分：基于深度衍生工具的期权定价</b></h2><p>在这篇文章的第1部分中，Python被用来实现蒙特卡罗模拟，从而在GPU中有效地为奇异的期权定价。<b>在量化金融中，低延迟期权定价在生产环境中对管理投资组合风险非常重要。蒙特卡罗模拟，即使在GPU中加速，有时也不够有效。</b></p><p>本文提出了一种利用深度神经网络逼近期权定价的模型，并利用蒙特卡罗模拟生成的数据对其进行训练。结果表明，深度神经网络能够生成准确的定价数据，推理时间数量级更快。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-ba5eb023095c8b0f44e381a2e30f42f7_b.jpg" data-size="normal" data-rawwidth="782" data-rawheight="489" class="origin_image zh-lightbox-thumb" width="782" data-original="https://pic4.zhimg.com/v2-ba5eb023095c8b0f44e381a2e30f42f7_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-ba5eb023095c8b0f44e381a2e30f42f7_b.jpg" data-size="normal" data-rawwidth="782" data-rawheight="489" class="origin_image zh-lightbox-thumb lazy" width="782" data-original="https://pic4.zhimg.com/v2-ba5eb023095c8b0f44e381a2e30f42f7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ba5eb023095c8b0f44e381a2e30f42f7_b.jpg"/><figcaption>https://arxiv.org/pdf/1809.02233.pdf</figcaption></figure><p>受这篇文章的启发，我们在今天的推文中使用了类似的方法来建立一个近似的定价模型，并加快了推理延迟。利用一个高阶可微激活函数，证明了该模型可以有效地通过网络反向传递计算期权Greeks。通过使用TensorRT（<a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/TensorRT" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://</span><span class="visible">github.com/NVIDIA/Tenso</span><span class="invisible">rRT</span><span class="ellipsis"></span></a>）对模型进行转换，以提供最快的奇异期权定价速度，进一步提高了推理时间。</p><p><b>神经网络逼近</b></p><p>深度神经网络是一种很好的函数逼近器，在图像处理和自然语言处理中取得了很大的成功。深度神经网络通常具有良好的泛化能力，当神经网络训练了大量的数据时，泛化能力对不可见的数据集非常有效。由于蒙特卡罗模拟可以用来发现期权的准确价格，因此你可以使用它来生成尽可能多的数据点，给定计算预值。</p><p>一个有趣的发现来自《Noise2Noise: Learning Image Restoration without Clean Data》这篇论文。其实，讲到因为蒙特卡罗模拟中的噪声是无偏的，在随机梯度训练中可以消除。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-8b02c32389b25405c65d1bf395eb89d4_b.jpg" data-size="normal" data-rawwidth="1012" data-rawheight="579" class="origin_image zh-lightbox-thumb" width="1012" data-original="https://pic1.zhimg.com/v2-8b02c32389b25405c65d1bf395eb89d4_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-8b02c32389b25405c65d1bf395eb89d4_b.jpg" data-size="normal" data-rawwidth="1012" data-rawheight="579" class="origin_image zh-lightbox-thumb lazy" width="1012" data-original="https://pic1.zhimg.com/v2-8b02c32389b25405c65d1bf395eb89d4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8b02c32389b25405c65d1bf395eb89d4_b.jpg"/><figcaption>https://arxiv.org/pdf/1803.04189.pdf</figcaption></figure><p>这一点在《Deeply Learning Derivatives》这篇论文中也得到了证明：<b>在相同路径数的情况下，模型的预测结果要优于蒙特卡罗模拟的结果。</b></p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-02331a12b305e430b8146570a3af4385_b.jpg" data-size="normal" data-rawwidth="733" data-rawheight="300" class="origin_image zh-lightbox-thumb" width="733" data-original="https://pic2.zhimg.com/v2-02331a12b305e430b8146570a3af4385_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-02331a12b305e430b8146570a3af4385_b.jpg" data-size="normal" data-rawwidth="733" data-rawheight="300" class="origin_image zh-lightbox-thumb lazy" width="733" data-original="https://pic2.zhimg.com/v2-02331a12b305e430b8146570a3af4385_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-02331a12b305e430b8146570a3af4385_b.jpg"/><figcaption>预测模型体系结构图</figcaption></figure><p>上图解释：你生成随机期权参数（X个自变量），将它们输入到GPU的蒙特卡罗模拟中，然后计算出ground truth期权价格（Y个因变量）。然后使用这个生成的大数据集来训练一个深度神经网络，将期权定价作为一个非线性回归问题来学习。</p><p><b>数据生成</b></p><p>在第1部分中我们使用Dask可以轻松地进行分布式计算。在这里，你可以使用Dask以分布式的方式生成一个大数据集：</p><div class="highlight"><pre><code class="language-python"><span class="n">futures</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">future</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">gen_data</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span> </code></pre></div><p>gen_data函数在单个GPU中运行，生成一组数据点并将它们保存在本地存储中。你可以使用第1部分中描述的任何Python GPU蒙特卡罗模拟方法。此示例代码使用不同的种子数运行gen_data100次，并将计算分配到多GPU环境中。</p><p>将6个期权参数统一采样到下表中指定的范围内：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-a2e6dbe13042be1cd56c0876dde040e8_b.jpg" data-caption="" data-size="normal" data-rawwidth="608" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="608" data-original="https://pic1.zhimg.com/v2-a2e6dbe13042be1cd56c0876dde040e8_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-a2e6dbe13042be1cd56c0876dde040e8_b.jpg" data-caption="" data-size="normal" data-rawwidth="608" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="608" data-original="https://pic1.zhimg.com/v2-a2e6dbe13042be1cd56c0876dde040e8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-a2e6dbe13042be1cd56c0876dde040e8_b.jpg"/></figure><p>总的来说，1000万个训练数据点和500万个验证数据点是通过在分布中运行蒙特卡罗模拟产生的。对于每个蒙特卡罗模拟，大家使用819.2万条路径来计算期权价格。如第1部分所示，819.2万条路径在该特定期权参数设置的价格中的标准差为0.0073。</p><p><b>神经网络模型</b></p><p>由于我们没有关于这六个期权参数的结构信息，请选择通用的多层感知器神经网络作为定价模型。如下图：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-490517c34f04c08b882e8b7bfe3b1438_b.jpg" data-caption="" data-size="normal" data-rawwidth="267" data-rawheight="536" class="content_image" width="267"/></noscript><img src="https://pic1.zhimg.com/v2-490517c34f04c08b882e8b7bfe3b1438_b.jpg" data-caption="" data-size="normal" data-rawwidth="267" data-rawheight="536" class="content_image lazy" width="267" data-actualsrc="https://pic1.zhimg.com/v2-490517c34f04c08b882e8b7bfe3b1438_b.jpg"/></figure><p>与《Deeply Learning Derivatives》论文的不同之处在于使用<b>Elu</b>作为激活函数，计算参数的高阶微分。如果你在原始的论文中使用ReLu，二阶微分总是0。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-4301d672a9fe23e12207184d810f7d32_b.jpg" data-size="normal" data-rawwidth="993" data-rawheight="896" class="origin_image zh-lightbox-thumb" width="993" data-original="https://pic3.zhimg.com/v2-4301d672a9fe23e12207184d810f7d32_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-4301d672a9fe23e12207184d810f7d32_b.jpg" data-size="normal" data-rawwidth="993" data-rawheight="896" class="origin_image zh-lightbox-thumb lazy" width="993" data-original="https://pic3.zhimg.com/v2-4301d672a9fe23e12207184d810f7d32_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-4301d672a9fe23e12207184d810f7d32_b.jpg"/><figcaption>https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu</figcaption></figure><p>生成的随机期权参数的范围，输入参数首先通过除以[200.0,198.0,200.0,0.4,0.2,0.2]缩小到(0-1)范围。然后它们被投射到1024的隐藏维度上5次。最后一层是线性层，它将隐藏维度映射到预测的期权价格。下面的代码示例是<b>PyTorch</b>中详细的模型实现：</p><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="mi">1024</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc5</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc6</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span>
                             <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">200.0</span><span class="p">,</span>
                                           <span class="mf">198.0</span><span class="p">,</span>
                                           <span class="mf">200.0</span><span class="p">,</span>
                                           <span class="mf">0.4</span><span class="p">,</span>
                                           <span class="mf">0.2</span><span class="p">,</span>
                                           <span class="mf">0.2</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># normalize the parameter to range [0-1] </span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc5</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc6</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> </code></pre></div><p><b>神经网络训练</b></p><p>我们提供了两种方法来训练神经网络，一种是使用Ignite，另一种是使用神经模块（NeMo）。这两个都是高级DL库，可以简化训练模型。实验结果表明，采用混合精度训练和多GPU训练可以有效地提高训练速度。使用MSELoss作为损失函数，Adam作为优化器，CosineAnnealingScheduler作为学习率调度器。</p><p><b>Greeks和隐含波动率计算</b></p><p>训练收敛后，性能最好的模型保存在本地存储器中。现在你可以加载模型参数，并使用它来运行推断：</p><div class="highlight"><pre><code class="language-python"><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;check_points/512/model_best.pth.tar&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;state_dict&#39;</span><span class="p">])</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">110.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]])</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;result </span><span class="si">%.4f</span><span class="s1"> inference time </span><span class="si">%.6f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">end</span><span class="o">-</span> <span class="n">start</span><span class="p">))</span> </code></pre></div><p>当你输入与第1部分中相同的期权参数（在训练数据集中没有使用）时，该模型将生成正确的期权价格$18.714。最重要的是，与CUDA的蒙特卡罗法26ms的计算时间相比，它只需要<b>0.8ms</b>，<b>32倍的加速</b>。</p><p>近似的期权定价模型是完全可微的，这意味着你可以根据输入参数计算任意阶的微分。在金融领域，这被用来计算期权中的<b>Greeks</b>。</p><p>由于价格评估中存在噪声，用蒙特卡罗模拟法计算Greeks是一项具有挑战性的工作。数值差分法可能存在噪声。然而，在你有了神经网络近似模型之后，利用PyTorch中的自动梯度特性来计算微分。<b>由于梯度是通过网络的后向传递计算出来的，因此该算法具有较高的计算效率。</b></p><p>下面的代码示例展示了一个计算参数K、B、S0、sigma、mu、r’的一阶微分的示例：</p><div class="highlight"><pre><code class="language-python"><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">110.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">inputs</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">first_order_gradient</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">grad</span> 
</code></pre></div><p>对于高阶微分，多次使用PyTorch autograd.grad方法：</p><div class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">110.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">inputs</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">loss_grads</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">drv</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss_grads</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>
<span class="n">drv</span></code></pre></div><p>你可以生成的<b>delta</b>和<b>gamma</b>的Greek图形作为一个函数的基础价格：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-151f47ab39e317bf193659187a02c1ff_b.jpg" data-caption="" data-size="normal" data-rawwidth="1061" data-rawheight="668" class="origin_image zh-lightbox-thumb" width="1061" data-original="https://pic4.zhimg.com/v2-151f47ab39e317bf193659187a02c1ff_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-151f47ab39e317bf193659187a02c1ff_b.jpg" data-caption="" data-size="normal" data-rawwidth="1061" data-rawheight="668" class="origin_image zh-lightbox-thumb lazy" width="1061" data-original="https://pic4.zhimg.com/v2-151f47ab39e317bf193659187a02c1ff_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-151f47ab39e317bf193659187a02c1ff_b.jpg"/></figure><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-034e711dfc68cfdcd6475f0c83d49760_b.jpg" data-caption="" data-size="normal" data-rawwidth="744" data-rawheight="478" class="origin_image zh-lightbox-thumb" width="744" data-original="https://pic1.zhimg.com/v2-034e711dfc68cfdcd6475f0c83d49760_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-034e711dfc68cfdcd6475f0c83d49760_b.jpg" data-caption="" data-size="normal" data-rawwidth="744" data-rawheight="478" class="origin_image zh-lightbox-thumb lazy" width="744" data-original="https://pic1.zhimg.com/v2-034e711dfc68cfdcd6475f0c83d49760_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-034e711dfc68cfdcd6475f0c83d49760_b.jpg"/></figure><p>隐含波动率是基于期权报价对标的资产的预测波动率。模型给出的是价格与期权参数的反向映射，用蒙特卡罗模拟法很难做到这一点。但如果你有一个深度学习定价模型，这是一个简单的任务。给定价格P，隐含波动率是函数compute_price的根，如下面的代码所示：</p><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">compute_price</span><span class="p">(</span><span class="n">sigma</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">110.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">,</span> <span class="mf">120.0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]])</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> </code></pre></div><p>任何数值求根的方法都可以使用，例如<b>Brent</b>算法是计算根的有效方法。</p><p><b>TensorRT推理加速</b></p><p>在对深度学习网络进行训练之后，下一步通常是将模型部署到生产环境中。最直接的方法是将PyTorch模型置于推理模式。推理从输入到输出运行一个正向传递。如前所述，它运行得很快，可以在0.8 ms内获得准确的结果。然而，你可以做得更好。</p><p>NVIDIA提供了一个强大的推理模型优化工具TensorRT，其中包括一个深度学习推理优化器和runtime，它为深度学习推理应用程序提供低延迟和高吞吐量。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-54ff9b59db2a0d094e6dace7b800328b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="344" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-54ff9b59db2a0d094e6dace7b800328b_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-54ff9b59db2a0d094e6dace7b800328b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="344" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic4.zhimg.com/v2-54ff9b59db2a0d094e6dace7b800328b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-54ff9b59db2a0d094e6dace7b800328b_b.jpg"/></figure><p>在这篇文章中，TensorRT帮助在T4 GPU上将BERT自然语言理解推理加速到2.2 ms。受此启发，大家可以将训练有素的亚式障碍期权模型转换为TensorRT推理引擎，以获得显著的加速。</p><p>准备好TensorRT引擎文件后，可以使用它进行推理工作。</p><p>1、加载序列化的引擎文件。</p><p>2、分配CUDA设备阵列。</p><p>3、异步地将输入从主机复制到设备。</p><p>4、启动TensorRT引擎来计算结果。</p><p>5、异步地将输出从设备复制到主机。</p><p>下面的代码示例使用TensorRT引擎运行推理：</p><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">tensorrt</span> <span class="kn">as</span> <span class="nn">trt</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pycuda</span>
<span class="kn">import</span> <span class="nn">pycuda.driver</span> <span class="kn">as</span> <span class="nn">cuda</span>
<span class="kn">import</span> <span class="nn">pycuda.autoinit</span>

<span class="n">TRT_LOGGER</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="p">(</span><span class="n">trt</span><span class="o">.</span><span class="n">Logger</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;opt.engine&#34;</span><span class="p">,</span> <span class="s2">&#34;rb&#34;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">,</span> <span class="n">trt</span><span class="o">.</span><span class="n">Runtime</span><span class="p">(</span><span class="n">TRT_LOGGER</span><span class="p">)</span> <span class="k">as</span> <span class="n">runtime</span><span class="p">:</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">.</span><span class="n">deserialize_cuda_engine</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>

<span class="n">h_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">pagelocked_empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">h_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">110.0</span>
<span class="n">h_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">100.0</span>
<span class="n">h_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">120.0</span>
<span class="n">h_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.35</span>
<span class="n">h_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">h_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">h_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">pagelocked_empty</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">d_input</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">mem_alloc</span><span class="p">(</span><span class="n">h_input</span><span class="o">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">d_output</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">mem_alloc</span><span class="p">(</span><span class="n">h_output</span><span class="o">.</span><span class="n">nbytes</span><span class="p">)</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">()</span>
<span class="k">with</span> <span class="n">engine</span><span class="o">.</span><span class="n">create_execution_context</span><span class="p">()</span> <span class="k">as</span> <span class="n">context</span><span class="p">:</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">memcpy_htod_async</span><span class="p">(</span><span class="n">d_input</span><span class="p">,</span> <span class="n">h_input</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">context</span><span class="o">.</span><span class="n">set_binding_shape</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>
    <span class="n">context</span><span class="o">.</span><span class="n">execute_async</span><span class="p">(</span><span class="n">bindings</span><span class="o">=</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">d_input</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">d_output</span><span class="p">)],</span> <span class="n">stream_handle</span><span class="o">=</span><span class="n">stream</span><span class="o">.</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">cuda</span><span class="o">.</span><span class="n">memcpy_dtoh_async</span><span class="p">(</span><span class="n">h_output</span><span class="p">,</span> <span class="n">d_output</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>
    <span class="n">stream</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;result </span><span class="si">%.4f</span><span class="s1"> inference time </span><span class="si">%.6f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">h_output</span><span class="p">,</span><span class="n">end</span><span class="o">-</span> <span class="n">start</span><span class="p">))</span> 
</code></pre></div><p>与non-TensorRT方法相比，它可以在四分之一（<b>0.2 ms</b>）的推断时间内生成准确的结果。</p><h2><b>5、总结</b></h2><p>在第1部分中，我们向大家展示了在CUDA C/ C++中实现蒙特卡罗期权定价的传统方法，但有点复杂，但它具有最佳的绝对性能。使用Python的GPU库，可以用简洁的Python代码行实现完全相同的蒙特卡罗模拟，而不会带来显著的性能损失。</p><p>此外，在将模拟代码迁移到Python之后，大家可以使用其他有用的Python库来改进结果。通过使用RAPIDS/Dask，大规模的蒙特卡罗仿真可以很容易地分布在多个节点和多个GPU上，从而获得更高的精度。</p><p>在第2部分中，我们再现了论文的结果。展示了使用神经网络逼近奇异期权价格模型的几个好处。它可以将期权价格的计算速度提高35倍，且结果准确。可微神经网络使得期权Greeks的计算变得容易。大家还可以使用TensorRT进一步改进网络推断时间，并实现最优的性能。</p><p>量化投资与机器学习微信公众号，是业内垂直于<b>Quant、MFE、Fintech、AI、ML</b>等领域的<b>量化类主流自媒体。</b>公众号拥有来自<b>公募、私募、券商、期货、银行、保险资管、海外</b>等众多圈内<b>18W+</b>关注者。每日发布行业前沿研究成果和最新量化资讯。</p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
