<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Fri, 09 Aug 2019 12:44:35 +0800</lastBuildDate>
<item>
<title>分布式数据库在 ARM 平台探索之路（一） TiDB 集群在 arm 平台编译安装与部署</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-08-09-77315020.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/77315020&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ddb25062bbdfd62a9fc8581afeb07798_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;本文转自公众号 TCTP，作者 TCTP。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/voEr3WId1LeOr-o4sFptPA%3Fscene%3D25%23wechat_redirect&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;mp.weixin.qq.com/s/voEr&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;3WId1LeOr-o4sFptPA?scene=25#wechat_redirect&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;我行在 2018 年开始了基于 ARM 服务器平台的尝试，当前 TDSQL 的冷备数据全部保存在基于 ARM 服务器搭建的 CEPH 存储集群上，运行稳定。在今年贸易战的大背景下，我们数据库团队也尝试将各个数据库产品放到 ARM 平台上去编译并运行起来，为我行在基础架构层面的进一步国产化打下基础。&lt;/p&gt;&lt;p&gt;我们这次首先针对我行引入的 NewSQL 数据库 TiDB，在我行实验室的 ARM 平台上进行了编译和测试，预计会将整个测试流程和相关测试结论，整理为三篇技术文章分享出来，分别是:&lt;/p&gt;&lt;p&gt;&lt;b&gt;（一）《TiDB 集群 在 arm 平台编译、安装与部署》&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;（二）《sysbench 测试下 arm 平台 cpu ／内存／磁盘的能力》&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;（三）《 TiDB 在 arm 与 x86 平台的性能测试对比》&lt;/b&gt;&lt;/p&gt;&lt;p&gt;此次是系列文章的第一篇。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一、环境准备&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;PingCAP 提供了 TiDB-Ansible 部署方案，可以使用 Ansible 快速方便地部署一个完整的 TiDB 集群，而 TiDB-Ansible release-3.0 版本依赖 Ansible 2.4.2 及以上版本（Ansible&amp;gt;=2.4.2，最好是 2.7.11 版本），另外依赖 Python 模块：jinja2 &amp;gt;= 2.9.6 和 jmespath &amp;gt;= 0.9.0，而且内部的数据库服务器与外网一般是隔离的，所以只能选择离线安装：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、机器准备&lt;/b&gt;可以连接外网的 ARM 服务器一台&lt;/p&gt;&lt;ul&gt;&lt;li&gt;该机器需开放外网访问&lt;/li&gt;&lt;li&gt;用于下载 TiDB-Ansible、TiDB 及相关软件安装包&lt;/li&gt;&lt;li&gt;用于编译 TiDB ARM 版本&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;部署目标机器三台、部署中控机一台&lt;/p&gt;&lt;ul&gt;&lt;li&gt;无法访问外网&lt;/li&gt;&lt;li&gt;部署目标机器为 ARM 服务器&lt;/li&gt;&lt;li&gt;部署中控机和部署目标机器共用&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2、依赖包下载&lt;/b&gt;以下是主要的依赖安装包（如果在安装过程中发现还缺少其他依赖包，可以按需下载）。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2e0e1fc71d129c2c98bf03ce4264a65a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1372&quot; data-rawheight=&quot;2103&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1372&quot; data-original=&quot;https://pic3.zhimg.com/v2-2e0e1fc71d129c2c98bf03ce4264a65a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2e0e1fc71d129c2c98bf03ce4264a65a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1372&quot; data-rawheight=&quot;2103&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1372&quot; data-original=&quot;https://pic3.zhimg.com/v2-2e0e1fc71d129c2c98bf03ce4264a65a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-2e0e1fc71d129c2c98bf03ce4264a65a_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;另外上图所示的依赖包只保证当前 ARM 环境可正常，但因为不同的服务器依赖可能不完全一样，所以在安装过程发现还缺少其他依赖包，若想安装其他依赖包，可自行网上寻找相关 RPM 包按需下载，实际我安装上述 RPM 包时也存在依赖性问题，但使用 RPM 强制安装已成功安装。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3、安装依赖包&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[root@ip-localhost ansible]# cd ansible_pkg/
[root@ip-localhost ansible_pkg]# rpm -Uvh *.rpm --nodeps --force
warning: libyaml-0.1.4-11.el7_0.aarch64.rpm: Header V3 RSA/SHA256 Signature, key ID fd431d51: NOKEY
warning: python2-babel-2.7.0-1.fc31.noarch.rpm: Header V3 RSA/SHA256 Signature, key ID 3c3359c4: NOKEY
warning: python2-markupsafe-1.0-1.fc29.aarch64.rpm: Header V3 RSA/SHA256 Signature, key ID 429476b4: NOKEY
Preparing...                          ################################# [100%]
Updating / installing...
  1:python2-pyasn1-0.1.9-7.el7       ################################# [ 5%]
  2:sshpass-1.06-1.el7               ################################# [ 10%]
  3:python-ply-3.4-11.el7            ################################# [ 14%]
  4:python-pycparser-2.14-1.el7      ################################# [ 19%]
  5:python-cffi-1.6.0-5.el7          ################################# [ 24%]
  6:python-idna-2.4-1.el7            ################################# [ 29%]
  7:python-httplib2-0.9.2-0.2.el7    ################################# [ 33%]
  8:python-enum34-1.0.4-1.el7        ################################# [ 38%]
  9:python2-cryptography-1.7.2-2.el7 ################################# [ 43%]
 10:python-paramiko-2.1.1-9.el7      ################################# [ 48%]
 11:python2-pytz-2018.9-1.fc31       ################################# [ 52%]
 12:python2-babel-2.7.0-1.fc31       ################################# [ 57%]
 13:python2-markupsafe-1.0-1.fc29    ################################# [ 62%]
 14:python2-jinja2-2.10-2.el7        ################################# [ 67%]
 15:python2-jmespath-0.9.0-1.el7     ################################# [ 71%]
 16:libyaml-0.1.4-11.el7_0           ################################# [ 76%]
 17:PyYAML-3.10-11.el7               ################################# [ 81%]
 18:ansible-2.8.2-1.el7              ################################# [ 86%]
 19:python2-pip-8.1.2-8.el7          ################################# [ 90%]
 20:mariadb-1:5.5.60-1.el7_5         ################################# [ 95%]
 21:epel-release-7-11                ################################# [100%]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;4、确认 ansible 是否安装成功&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[root@ip-localhost ansible]# ansible --version
ansible 2.8.2
config file = /etc/ansible/ansible.cfg
configured module search path = [u&amp;#39;/root/.ansible/plugins/modules&amp;#39;, u&amp;#39;/usr/share/ansible/plugins/modules&amp;#39;]
ansible python module location = /usr/lib/python2.7/site-packages/ansible
executable location = /bin/ansible
python version = 2.7.5 (default, Oct 31 2018, 18:48:32) [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;5、确认 jinja2 是否安装成功&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[root@ip-localhost ansible]# pip show jinja2
Metadata-Version: 1.1
Name: Jinja2
Version: 2.10
Summary: A small but fast and easy to use stand-alone template engine written in pure python.
Home-page: http://jinja.pocoo.org/
Author: Armin Ronacher
Author-email: armin.ronacher@active-4.com
License: BSD
Location: /usr/lib/python2.7/site-packages
Requires: MarkupSafe
Classifiers:
Development Status :: 5 - Production/Stable
Environment :: Web Environment
Intended Audience :: Developers
License :: OSI Approved :: BSD License
Operating System :: OS Independent
Programming Language :: Python
Programming Language :: Python :: 2
Programming Language :: Python :: 2.6
Programming Language :: Python :: 2.7
Programming Language :: Python :: 3
Programming Language :: Python :: 3.3
Programming Language :: Python :: 3.4
Programming Language :: Python :: 3.5
Programming Language :: Python :: 3.6
Topic :: Internet :: WWW/HTTP :: Dynamic Content
Topic :: Software Development :: Libraries :: Python Modules
Topic :: Text Processing :: Markup :: HTML
Entry-points:
[babel.extractors]
jinja2 = jinja2.ext:babel_extract[i18n]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;6、确认 jmespath 是否安装成功&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[root@ip-localhost ansible]# pip show jmespath
Metadata-Version: 1.1
Name: jmespath
Version: 0.9.0
Summary: JSON Matching Expressions
Home-page: https://github.com/jmespath/jmespath.py
Author: James Saryerwinnie
Author-email: js@jamesls.com
License: UNKNOWN
Location: /usr/lib/python2.7/site-packages
Requires:
Classifiers:
Development Status :: 5 - Production/Stable
Intended Audience :: Developers
Natural Language :: English
License :: OSI Approved :: MIT License
Programming Language :: Python
Programming Language :: Python :: 2.6
Programming Language :: Python :: 2.7
Programming Language :: Python :: 3
Programming Language :: Python :: 3.3
Programming Language :: Python :: 3.4&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;二、编译 TiDB arm 版本&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 包括三大组件 PD、TiKV Server 和 TiDB Server，还包括其他周边组件，比如 Pump、Prometheus、Alertmanager、Node_exporter、Blackbox_exporter、Pushgateway 和 Grafana，所以需要把这些组件都统一编译成 ARM 版本，而且要和官方版本对齐。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、编译脚本示例&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;#!/bin/bash
# Soft Version
# TiDN Core
tidb_version=release-3.0
# TiDB Tools
tispark_version=master
dm_version=master
# Monitor
prometheus_version=v2.8.1
alertmanager_version=v0.17.0
node_exporter_version=v0.17.0
# blackbox_exporter_version=v0.12.0
#v0.12.0 meets some wrong
blackbox_exporter_version=master
pushgateway_version=v0.7.0
grafana_version=6.1.6

# Soft Dir
declare -A soft_srcs

soft_srcs=(
# [&amp;#34;tidb&amp;#34;]=&amp;#34;$tidb_version https://github.com/pingcap/tidb.git&amp;#34;
# [&amp;#34;pd&amp;#34;]=&amp;#34;$tidb_version https://github.com/pingcap/pd.git&amp;#34;
# [&amp;#34;tikv&amp;#34;]=&amp;#34;$tidb_version https://github.com/tikv/tikv.git&amp;#34;
#   [&amp;#34;tispark&amp;#34;]=&amp;#34;$tidb_version https://github.com/pingcap/tispark&amp;#34;
  [&amp;#34;tidb-binlog&amp;#34;]=&amp;#34;$tidb_version https://github.com/pingcap/tidb-binlog&amp;#34;
[&amp;#34;dm&amp;#34;]=&amp;#34;$dm_version https://github.com/pingcap/dm&amp;#34;
[&amp;#34;prometheus&amp;#34;]=&amp;#34;$prometheus_version https://github.com/prometheus/prometheus.git&amp;#34;
[&amp;#34;alertmanager&amp;#34;]=&amp;#34;$alertmanager_version https://github.com/prometheus/alertmanager.git&amp;#34;
[&amp;#34;node_exporter&amp;#34;]=&amp;#34;$node_exporter_version https://github.com/prometheus/node_exporter.git&amp;#34;
[&amp;#34;blackbox_version&amp;#34;]=&amp;#34;$blackbox_exporter_version https://github.com/prometheus/blackbox_exporter.git&amp;#34;
[&amp;#34;pushgateway&amp;#34;]=&amp;#34;$pushgateway_version https://github.com/prometheus/pushgateway.git&amp;#34;
# [&amp;#34;grafana&amp;#34;]=&amp;#34;$grafana_version https://github.com/grafana/grafana.git&amp;#34;
)

# Dir
ROOT=$PWD/build
target=$ROOT/bin
rm -rf $ROOT
mkdir -p $target

sudo yum install -y gcc gcc-c++ wget git zlib-devel

cd $ROOT
# Go
if which go &amp;gt;/dev/null; then
   echo &amp;#34;go installed, skip&amp;#34;
else
   wget https://dl.google.com/go/go1.12.6.linux-arm64.tar.gz
   sudo tar -C /usr/local -xzf go1.12.6.linux-arm64.tar.gz
   echo &amp;#34;export GOPATH=$ROOT/go&amp;#34; &amp;gt;&amp;gt; ~/.bashrc
   echo &amp;#39;export PATH=$PATH:/usr/local/go/bin:$GOPATH/bin&amp;#39; &amp;gt;&amp;gt; ~/.bashrc
   source ~/.bashrc
fi

# Rust
if which rustc &amp;gt;/dev/null; then
   echo &amp;#34;rust installed, skip&amp;#34;
else
   curl https://sh.rustup.rs -sSf | sh -s -- -y
   source $HOME/.cargo/env
fi

# Install cmake3
if which cmake3 &amp;gt;/dev/null; then
   echo &amp;#34;cmake3 installed, skip&amp;#34;
else
   wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm

   sudo rpm -ivh epel-release-latest-7.noarch.rpm
   sudo yum install -y epel-release
   sudo yum install -y cmake3

   sudo ln -s /usr/bin/cmake3 /usr/bin/cmake
fi

# Install Java
if which java &amp;gt;/dev/null;then
echo &amp;#34;java installed, skip&amp;#34;
else
ce $ROOT
wget --no-cookies --no-check-certificate --header &amp;#34;Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie&amp;#34; &amp;#34;http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-arm64-vfp-hflt.tar.gz&amp;#34;
sudo tar -C /usr/local -xzf jdk-8u141-linux-arm64-vfp-hflt.tar.gz
echo &amp;#39;export JAVA_HOME=/usr/local/jdk1.8.0_141&amp;#39; &amp;gt;&amp;gt; ~/.bashrc
echo &amp;#39;export JRE_HOME=/user/local/jdk1.8.0_141/jre&amp;#39; &amp;gt;&amp;gt; ~/.bashrc
echo &amp;#39;export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin&amp;#39; &amp;gt;&amp;gt; ~/.bashrc
fi


# Install maven
if which mvn &amp;gt;/dev/null;then
echo &amp;#34;maven installed, skip&amp;#34;
else
wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz
sudo tar -C /usr/local -xzf apache-maven-3.6.1-bin.tar.gz
echo &amp;#39;export PATH=$PATH:/usr/local/apache-maven-3.6.1/bin&amp;#39; &amp;gt;&amp;gt; ~/.bashrc
source ~/.bashrc
fi


# # RocksDB gflags
# git clone https://github.com/gflags/gflags.git
# cd gflags
# git checkout v2.0
# ./configure --build=aarch64-unknown-linux-gnu &amp;amp;&amp;amp; make &amp;amp;&amp;amp; sudo make install
# cd $ROOT

# Build Monitor
for soft in $(echo ${!soft_srcs[*]})
do
soft_src=${soft_srcs[$soft]}
cd $ROOT
git clone -b $soft_src
cd $soft
make build
if [ -d bin ];then
cp bin/* $target
else
cp $soft $target
fi
cd $ROOT
echo &amp;#34;`date +&amp;#39;%F %T&amp;#39;`: Build Soft $soft done .&amp;#34;
done

# Download Grafana
cd $ROOT
wget https://dl.grafana.com/oss/release/grafana-${grafana_version}.linux-arm64.tar.gz
tar -zxvf grafana-${grafana_version}.linux-arm64.tar.gz

cp grafana-${grafana_version}/bin/* bin/

# Build TiDB
cd $ROOT
git clone -b $tidb_version https://github.com/pingcap/tidb
cd tidb
make
cp bin/* $target
# Build PD
cd $ROOT
git clone -b $tidb_version https://github.com/pingcap/pd
cd pd
make
cp bin/* $target

# Build TiKV
cd $ROOT
git clone -b $tidb_version https://github.com/tikv/tikv.git
cd tikv
ROCKSDB_SYS_SSE=0 make release
cp target/release/tikv-*  $target

# Build tispark
cd $ROOT
git clone -b $tispark_version https://github.com/pingcap/tispark
cd tispark
mvn clean install -Dmaven.test.skip=true -P spark-2.3&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;2、 确认组件是否编译成功&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[root@ip-localhost bin]# ll
total 1492252
-rwxr-xr-x 1 tidb tidb  25880636 Jul 25 16:21 alertmanager
-rwxr-xr-x 1 tidb tidb  41476026 Jul 25 16:21 arbiter
-rwxr-xr-x 1 tidb tidb  23086365 Jul 25 16:21 binlogctl
-rwxr-xr-x 1 root root  16725668 Jul 25 16:48 blackbox_exporter
-rwxr-xr-x 1 tidb tidb  42190443 Jul 25 16:21 dmctl
-rwxr-xr-x 1 tidb tidb  42643818 Jul 25 16:21 dm-master
-rwxr-xr-x 1 tidb tidb  41231475 Jul 25 16:21 dm-tracer
-rwxr-xr-x 1 tidb tidb  45855210 Jul 25 16:21 dm-worker
-rwxr-xr-x 1 tidb tidb  45378703 Jul 25 16:21 drainer
-rwxr-xr-x 1 tidb tidb  20578913 Jul 25 16:21 grafana-cli
-rw-r--r-- 1 tidb tidb        33 Jul 25 16:21 grafana-cli.md5
-rwxr-xr-x 1 tidb tidb  41749049 Jul 25 16:21 grafana-server
-rw-r--r-- 1 tidb tidb        33 Jul 25 16:21 grafana-server.md5
-rwxr-xr-x 1 tidb tidb  15884939 Jul 25 16:21 node_exporter
-rwxr-xr-x 1 tidb tidb  27341094 Jul 25 16:21 pd-ctl
-rwxr-xr-x 1 tidb tidb  16345055 Jul 25 16:21 pd-recover
-rwxr-xr-x 1 tidb tidb  36866195 Jul 25 16:21 pd-server
-rwxr-xr-x 1 tidb tidb  16394398 Jul 25 16:21 pd-tso-bench
-rwxr-xr-x 1 tidb tidb  68935640 Jul 25 16:21 prometheus
-rwxr-xr-x 1 tidb tidb  32089280 Jul 25 16:21 pump
-rwxr-xr-x 1 tidb tidb  14439632 Jul 25 16:21 pushgateway
-rwxr-xr-x 1 tidb tidb  39814928 Jul 25 16:21 reparo
-rwxr-xr-x 1 tidb tidb   8280869 Jul 25 16:21 shadow
-rwxr-xr-x 1 tidb tidb  67211621 Jul 25 16:21 tidb-server
-rwxr-xr-x 1 tidb tidb 197494880 Jul 25 16:21 tikv-ctl
-rw-r--r-- 1 tidb tidb     20985 Jul 25 16:21 tikv-ctl.d
-rwxr-xr-x 1 tidb tidb 207880328 Jul 25 16:21 tikv-importer
-rw-r--r-- 1 tidb tidb     20995 Jul 25 16:21 tikv-importer.d
-rwxr-xr-x 1 tidb tidb 355234696 Jul 25 16:21 tikv-server
-rw-r--r-- 1 tidb tidb     20991 Jul 25 16:21 tikv-server.d
-rw-r--r-- 1 tidb tidb  32650300 Jul 25 16:59 tispark-SNAPSHOT-jar-with-dependencies.jar&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 TiDB-Ansible 的 bootstrap.yml 阶段需要使用 fio 进行性能压测，所以需要额外下载一个 fio(version 3.8) 文件。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、安装 TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1、下载 tidb-ansible 以及完成相关初始化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;根据 PingCAP 官网的离线 TiDB-Ansible 部署方案（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/how-to/deploy/orchestrated/offline-ansible/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/docs-cn/v3.&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;0/how-to/deploy/orchestrated/offline-ansible/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），完成以下初始化工作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在中控机上创建 tidb 用户，并生成 ssh key&lt;/li&gt;&lt;li&gt;在下载机上下载 TiDB-Ansible 及 TiDB 安装包，但下载机不需要安装 ansible，具体操作如下：&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下载 release-3.0 版本：&lt;/p&gt;&lt;p&gt;$ git clone -b v3.0.0 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-ansible.git&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-ansible.git&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;注：不需要执行 ansible-playbook local_prepare.yml，因为使用的是自己编译的 ARM 版二进制包&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在中控机上配置部署机器 ssh 互信及 sudo 规则&lt;/li&gt;&lt;li&gt;在部署目标机器上安装 NTP 服务&lt;/li&gt;&lt;li&gt;在部署目标机器上配置 CPUfreq 调节器模式&lt;/li&gt;&lt;li&gt;在部署目标机器上添加数据盘 ext4 文件系统挂载参数&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2、部署任务&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;把在下载机下载好的 tidb-ansible 传到部署中控机&lt;/li&gt;&lt;li&gt;在 tidb-ansible 目录下创建 resources/bin/ 目录，并且把编译的 ARM 版二进制文件全部放到 resources/bin/ 目录里（还包括 fio 文件）&lt;/li&gt;&lt;li&gt;编辑 inventory.ini&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;## TiDB Cluster Part
[tidb_servers]
TiDB-1 ansible_host=127.0.0.1  deploy_dir=/tidb/deploy_tidb/tidb tidb_port=5000 tidb_status_port=10089 labels=&amp;#34;host=ip-1&amp;#34;
TiDB-2 ansible_host=127.0.0.2  deploy_dir=/tidb/deploy_tidb/tidb tidb_port=5000 tidb_status_port=10089 labels=&amp;#34;host=ip-2&amp;#34;
TiDB-3 ansible_host=127.0.0.3  deploy_dir=/tidb/deploy_tidb/tidb tidb_port=5000 tidb_status_port=10089 labels=&amp;#34;host=ip-3&amp;#34;


[tikv_servers]
TiKV1-1 ansible_host=127.0.0.1 deploy_dir=/tidb/deploy_tidb/tikv1 tikv_port=20891  tikv_status_port=20181  labels=&amp;#34;host=TiKV1-1&amp;#34;

TiKV2-1 ansible_host=127.0.0.2 deploy_dir=/tidb/deploy_tidb/tikv1 tikv_port=20891  tikv_status_port=20181  labels=&amp;#34;host=TiKV2-1&amp;#34;

TiKV3-1 ansible_host=127.0.0.3 deploy_dir=/tidb/deploy_tidb/tikv1 tikv_port=20891  tikv_status_port=20181  labels=&amp;#34;host=TiKV3-1&amp;#34;


[pd_servers]
PD01 ansible_host=127.0.0.1  deploy_dir=/tidb/deploy_tidb/pd pd_client_port=2589 pd_peer_port=2590 labels=&amp;#34;host=ip-1&amp;#34;
PD02 ansible_host=127.0.0.2  deploy_dir=/tidb/deploy_tidb/pd pd_client_port=2589 pd_peer_port=2590 labels=&amp;#34;host=ip-2&amp;#34;
PD03 ansible_host=127.0.0.3  deploy_dir=/tidb/deploy_tidb/pd pd_client_port=2589 pd_peer_port=2590 labels=&amp;#34;host=ip-3&amp;#34;


[spark_master]

[spark_slaves]

[lightning_server]

[importer_server]

## Monitoring Part
# prometheus and pushgateway servers
[monitoring_servers]
#prometheus89 ansible_host=127.0.0.1 prometheus_port=7098 pushgateway_port=7099 labels=&amp;#34;host=ip-127.0.0.1&amp;#34;
127.0.0.1

[grafana_servers]
#grafanaleifu89 ansible_host=127.0.0.1 grafana_port=7002 grafana_collector_port=7088 labels=&amp;#34;host=ip-127.0.0.1&amp;#34;
127.0.0.1

# node_exporter and blackbox_exporter servers
[monitored_servers]
nodeblack1  ansible_host=127.0.0.1        node_exporter_port=7102 blackbox_exporter_port=7117 labels=&amp;#34;host=ip-1&amp;#34;
nodeblack2  ansible_host=127.0.0.2        node_exporter_port=7102 blackbox_exporter_port=7117 labels=&amp;#34;host=ip-2&amp;#34;
nodeblack3  ansible_host=127.0.0.3        node_exporter_port=7102 blackbox_exporter_port=7117 labels=&amp;#34;host=ip-3&amp;#34;


[alertmanager_servers]
127.0.0.1

[kafka_exporter_servers]

## Binlog Part
[pump_servers]
pump1 ansible_host=127.0.0.1  deploy_dir=/tidb/deploy_tidb/pump pump_port=8290
pump2 ansible_host=127.0.0.2  deploy_dir=/tidb/deploy_tidb/pump pump_port=8290
pump3 ansible_host=127.0.0.3  deploy_dir=/tidb/deploy_tidb/pump pump_port=8290

[drainer_servers]

## Group variables
[pd_servers:vars]
location_labels = [&amp;#34;host&amp;#34;]

## Global variables
[all:vars]
deploy_dir = /tidb/deploy_tidb

## Connection
# ssh via normal user
ansible_user = tidb

cluster_name = test-cluster-30-ga

tidb_version = v3.0.0

# process supervision, [systemd, supervise]
process_supervision = systemd

timezone = Asia/Shanghai

enable_firewalld = False
# check NTP service
enable_ntpd = True
set_hostname = True

## binlog trigger
enable_binlog = True

# kafka cluster address for monitoring, example:
# kafka_addrs = &amp;#34;192.168.0.11:9092,192.168.0.12:9092,192.168.0.13:9092&amp;#34;
kafka_addrs = &amp;#34;&amp;#34;

# zookeeper address of kafka cluster for monitoring, example:
# zookeeper_addrs = &amp;#34;192.168.0.11:2181,192.168.0.12:2181,192.168.0.13:2181&amp;#34;
zookeeper_addrs = &amp;#34;&amp;#34;

# enable TLS authentication in the TiDB cluster
enable_tls = False

# KV mode
deploy_without_tidb = False

# wait for region replication complete before start tidb-server.
wait_replication = True

# Optional: Set if you already have a alertmanager server.
# Format: alertmanager_host:alertmanager_port
alertmanager_target = &amp;#34;&amp;#34;

grafana_admin_user = &amp;#34;admin&amp;#34;
grafana_admin_password = &amp;#34;admin&amp;#34;


### Collect diagnosis
collect_log_recent_hours = 2

enable_bandwidth_limit = True
# default: 10Mb/s, unit: Kbit/s
collect_bandwidth_limit = 10000&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;初始化系统环境，修改内核参数&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;ansible-playbook bootstrap.yml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;部署 TiDB 集群软件&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;ansible-playbook deploy.yml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;启动 TiDB 集群&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;ansible-playbook start.yml&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;四、验证并使用&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1、连接 TiDB&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[root@ip-localhost ~]# mysql -uroot -h127.0.0.1 -P5000
Welcome to the MariaDB monitor. Commands end with ; or \g.
Your MySQL connection id is 207
Server version: 5.7.25-TiDB-v3.0.1-36-g709ee4f-dirty MySQL Community Server (Apache License 2.0)

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type &amp;#39;help;&amp;#39; or &amp;#39;\h&amp;#39; for help. Type &amp;#39;\c&amp;#39; to clear the current input statement.

MySQL [(none)]&amp;gt; select tidb_version();
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tidb_version()                                                                                                                                                                                                                                                                                                                         |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Release Version: v3.0.1-36-g709ee4f-dirty
Git Commit Hash: 709ee4f5c1cd08b43da651c32f78c1032a397c84
Git Branch: release-3.0
UTC Build Time: 2019-07-25 06:26:30
GoVersion: go version go1.12.6 linux/arm64
Race Enabled: false
TiKV Min Version: 2.1.0-alpha.1-ff3dd160846b7d1aed9079c389fc188f7f5ea13e
Check Table Before Drop: false |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.00 sec)

MySQL [(none)]&amp;gt; create database db_arm;
uQuery OK, 0 rows affected (1.02 sec)

MySQL [(none)]&amp;gt; use db_arm
Database changed
MySQL [db_arm]&amp;gt; create table tb_arm(i int);
Query OK, 0 rows affected (0.51 sec)

MySQL [db_arm]&amp;gt; insert into tb_arm values(1);
Query OK, 1 row affected (0.02 sec)

MySQL [db_arm]&amp;gt; select * from tb_arm;
+------+
| i   |
+------+
|    1 |
+------+
1 row in set (0.00 sec)

MySQL [db_arm]&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;2、查看监控是否正常&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 自带的监控展示平台 grafana:&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3f6a536ac2f627a425102522b21c41d4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;404&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;941&quot; data-original=&quot;https://pic1.zhimg.com/v2-3f6a536ac2f627a425102522b21c41d4_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3f6a536ac2f627a425102522b21c41d4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;404&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;941&quot; data-original=&quot;https://pic1.zhimg.com/v2-3f6a536ac2f627a425102522b21c41d4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-3f6a536ac2f627a425102522b21c41d4_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;TiDB 自带的告警平台 prometheus:&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e7af18ad009a201cdf77d48bd1fdde1d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;277&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;941&quot; data-original=&quot;https://pic2.zhimg.com/v2-e7af18ad009a201cdf77d48bd1fdde1d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e7af18ad009a201cdf77d48bd1fdde1d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;941&quot; data-rawheight=&quot;277&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;941&quot; data-original=&quot;https://pic2.zhimg.com/v2-e7af18ad009a201cdf77d48bd1fdde1d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-e7af18ad009a201cdf77d48bd1fdde1d_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p class=&quot;ztext-empty-paragraph&quot;&gt;&lt;br/&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;五、计划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;至此，在 ARM64 平台上迈出坚实的一步，完成分布式数据库 TiDB 集群的安装部署，建议各位按照上面步骤进行操作，否则可能遇到一些未知的坑或者异常；接下来，我们将继续探索 ARM64 与 X86 平台差异化对比测试，通过基准硬件和分布式数据库性能两个维度深入挖掘，欢迎有兴趣的朋友一块探讨。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-08-09-77315020</guid>
<pubDate>Fri, 09 Aug 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>三十分钟成为Contributor | 提升TiDB Parser对MySQL 8.0语法兼容性</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-08-08-77255479.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/77255479&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4e36d12fe36314dc4b5eb2d17b38a30a_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：谢腾进, 赵一霖&lt;/p&gt;&lt;p&gt;TiDB 的一大特性就是和 MySQL 高度兼容，目标是让用户能够无需修改代码即可从 MySQL 迁移至 TiDB。要达成这个目标，需要完成两个提升兼容性的任务，分别是「语法兼容」和「功能行为兼容」。&lt;/p&gt;&lt;p&gt;&lt;b&gt;本次活动聚焦于语法兼容，提升 TiDB SQL Parser 对 MySQL 8.0 的语法支持。对于新的贡献者而言，除了能将理论知识运用到实践上以外，还可以从中体验参与一个开源项目的整体流程与规范。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们把 TiDB Parser 整体看作一个函数，输入是 SQL 字符串，输出是用于描述 SQL 语句的抽象语法树（AST）。在这个转换的过程中涉及到的组件有两个：一个是 lexer，负责将字符流变成 Token，并赋予它们类别标识，这个过程叫「Tokenize」；另一个是 parser，负责将 Token 转为树状结构，便于将来遍历和转换，这个过程叫「Parse」；TiDB 使用了 parser 生成工具 goyacc，它能够根据 &lt;code&gt;parser.y&lt;/code&gt; 生成 &lt;code&gt;parser.go&lt;/code&gt;，其中包含了名称为 &lt;code&gt;Parse&lt;/code&gt; 的函数接口，供 TiDB 直接使用。更多关于 TiDB Parser 以及 Lex &amp;amp; Yacc 的信息请参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-source-code-reading-5/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB SQL Parser 的实现&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;参与流程&lt;/h2&gt;&lt;p&gt;参与流程分为 7 步：&lt;b&gt;领任务  -&amp;gt; 写 test case -&amp;gt; make test -&amp;gt; coding -&amp;gt; 补充 test case -&amp;gt; make test -&amp;gt; 提 PR&lt;/b&gt;。&lt;/p&gt;&lt;h3&gt;1. 从 Issue 领取任务&lt;/h3&gt;&lt;p&gt;我们会在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/issues/11486&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Improve parser compatibility&lt;/a&gt; 周期性发布不兼容的 Bad SQL Case 组，每组 Case 都会构成一个 Issue，Contributor 选择某个 Issue，在它的下方评论：&lt;b&gt;Let me fix it&lt;/b&gt;。在我们将 Contributor 的 Github ID 添加到 Index Issue 中后，即完成任务的领取。&lt;/p&gt;&lt;h3&gt;2. 编写测试用例&lt;/h3&gt;&lt;p&gt;根据 Issue 描述的情况在 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/blob/master/parser_test.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;parser_test.go&lt;/a&gt;&lt;/code&gt; 中编写测试用例，除了 Issue 中提到的 Case 以外，可以适当添加更多的 Case。保证目标 SQL Case 语句能够通过 Parser 解析，并且通过 Restore 还原为预期的 SQL。&lt;/p&gt;&lt;h3&gt;3. 执行所有测试&lt;/h3&gt;&lt;p&gt;parser 根目录下运行 &lt;code&gt;make test&lt;/code&gt;，确保第一次测试失败，并且失败的 Case 是第 2 步编写的。&lt;/p&gt;&lt;h3&gt;4. 编码&lt;/h3&gt;&lt;p&gt;Contributor 修改文法规则。对于涉及到语义层面的规则变动，需要同步修改 AST 节点的数据结构（AST 节点定义在 &lt;code&gt;parser/ast&lt;/code&gt; 中）。TiDB 通过 AST 树生成执行计划，修改 AST 节点可能会影响 TiDB 的行为，因此应尽量保持原有结构，不改变原有的属性，如果确实有修改 AST 树必要，我们会帮助 Contributor 检查 TiDB 的行为是否正常。另外，AST 节点其中的两个接口方法是 &lt;code&gt;Accept&lt;/code&gt; 和 &lt;code&gt;Restore&lt;/code&gt;，分别用于遍历子树和通过 AST 树还原 SQL 字符串。应确保它们的行为都符合预期。&lt;/p&gt;&lt;p&gt;另外，还要检查新加的规则是否存在冲突问题。「冲突」可以被理解为当 parser 读到某个 token 时，有两种或以上的方式来构造语法树，从而导致歧义。goyacc 所生成的 parser 采用了 &lt;code&gt;LALR(1)&lt;/code&gt; 方法进行解析，冲突有两类：一类是两条规则都能被用于归约，称为 &lt;code&gt;reduce-reduce&lt;/code&gt; 冲突。另一类是既能使用一条规则归约，又能按照另一条规则移进下一个 token，称为 &lt;code&gt;shift-reduce&lt;/code&gt; 冲突。可以通过指定优先级的方式消除冲突，具体可以参考 yacc 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.gnu.org/software/bison/manual/html_node/Precedence.html%23Precedence&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;%precedence 和 %prec 指示&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;编码完成后在项目根目录下运行 &lt;code&gt;make parser&lt;/code&gt;，这时会执行 goyacc 重新生成 &lt;code&gt;parser.go&lt;/code&gt; 文件。&lt;/p&gt;&lt;h3&gt;5. 补充 test case&lt;/h3&gt;&lt;p&gt;根据实际情况尽可能提升测试覆盖率。&lt;/p&gt;&lt;h3&gt;6. 执行所有测试&lt;/h3&gt;&lt;p&gt;parser 根目录下运行 &lt;code&gt;make test&lt;/code&gt;，确保测试通过。&lt;/p&gt;&lt;h3&gt;7. 提交 PR&lt;/h3&gt;&lt;p&gt;提交 PR 之前请先阅读 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/blob/master/CONTRIBUTING.md&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;contributing 指南&lt;/a&gt;。下面是 PR 的模板，逐项填写即可。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;### What problem does this PR solve?

#### [ Put the subtask title here ]

Issue: [ put the subtask issue link here ]

#### MySQL Syntax:

[ describe MySQL syntax here ]

#### Bad SQL Case:

[ give a SQL statement example that passes MySQL but fails TiDB parser ]

[ give a SQL statement example that passes MySQL but fails TiDB parser ]
    
...
    
### Check List
    
Tests

- Unit test&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;示例&lt;/h2&gt;&lt;p&gt;&lt;b&gt;下面以添加 「REMOVE PARTITIONING」 语法支持为例解释说明整个过程&lt;/b&gt;。&lt;/p&gt;&lt;h3&gt;1. 从 Issue 领取任务&lt;/h3&gt;&lt;p&gt;从 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb/issues/11486&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这里&lt;/a&gt; 找到 &lt;code&gt;REMOVE PARTITIONING&lt;/code&gt; 子任务。&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/issues/402&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;子任务 Issue&lt;/a&gt; 中有若干不兼容的 Case。&lt;/p&gt;&lt;p&gt;首先，手动测试任一 Case，发现在 MySQL 下输出：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;Error 1505: Partition management on a not partitioned table is not possible&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;而在 TiDB 下输出：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;Error 1064: You have an error in your SQL syntax; check the manual that corresponds to your TiDB version for the right syntax to use line 1 column 20 near &amp;#34;remove partitioning&amp;#34;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这意味着 parser 无法识别 remove 关键字。&lt;/p&gt;&lt;p&gt;确认了问题存在后，到 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/issues/402&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;该 Issue&lt;/a&gt; 下评论：&lt;b&gt;Let me fix it&lt;/b&gt;，完成任务领取。&lt;/p&gt;&lt;h3&gt;2. 编写测试用例&lt;/h3&gt;&lt;p&gt;在 &lt;code&gt;parser_test.go&lt;/code&gt; 下寻找合适位置添加测试用例，这里我们选择在 &lt;code&gt;func (s *testParserSuite) TestDDL(c *C)&lt;/code&gt;的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/pull/396/files%23diff-688912c3f38a8a80d6bdc16c02088d69R2172&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;末尾&lt;/a&gt; 添加：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// for remove partitioning
{&amp;#34;alter table t remove partitioning&amp;#34;, true, &amp;#34;ALTER TABLE `t` REMOVE PARTITIONING&amp;#34;},
{&amp;#34;alter table db.ident remove partitioning&amp;#34;, true, &amp;#34;ALTER TABLE `db`.`ident` REMOVE PARTITIONING&amp;#34;},
{&amp;#34;alter table t lock = default remove partitioning&amp;#34;, true, &amp;#34;ALTER TABLE `t` LOCK = DEFAULT REMOVE PARTITIONING&amp;#34;},&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里每一个 test case 分成了三部分，第一列是用于测试的 SQL 语句，第二列是「是否期望第一列的语句 parse 通过」，第三列是「从语法树 restore 后期望的 SQL 语句」。具体可以参考 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/blob/53c769c5836485c83d5f339faab97ef5d853d560/parser_test.go%23L308&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TestDDL.RunTest()&lt;/a&gt;&lt;/code&gt;。&lt;/p&gt;&lt;h3&gt;3. 执行所有测试&lt;/h3&gt;&lt;p&gt;parser 根目录下运行 &lt;code&gt;make test&lt;/code&gt;，确保第一次测试失败，并且 fail 的 case 是第 2 步编写的。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;FAIL: parser_test.go:1664: testParserSuite.TestDDL

parser_test.go:2148:
    s.RunTest(c, table)
parser_test.go:318:
    c.Assert(err, IsNil, comment)
... value *errors.withStack = line 1 column 20 near &amp;#34;remove partitioning&amp;#34;  (&amp;#34;line 1 column 20 near \&amp;#34;remove partitioning\&amp;#34; &amp;#34;)
... source alter table t remove partitioning&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;错误信息和期望的一致，则可以开始进行下一步。&lt;/p&gt;&lt;h3&gt;4. 编码&lt;/h3&gt;&lt;h3&gt;4.1 修改 &lt;code&gt;parser.y&lt;/code&gt; 文件&lt;/h3&gt;&lt;p&gt;首先从 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/mysql/mysql-server/blob/8.0/sql/sql_yacc.yy&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MySQL 文法&lt;/a&gt; 中找到 remove partitioning 的定义：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;alter_table_stmt: ALTER TABLE_SYM table_ident opt_alter_table_actions
| ALTER TABLE_SYM table_ident standalone_alter_table_action

opt_alter_table_actions: opt_alter_command_list
| opt_alter_command_list alter_table_partition_options

alter_table_partition_options: partition_clause
| REMOVE_SYM PARTITIONING_SYM&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;经过分析可得该语法只能出现在 SQL 语句的最后一个部分，并且只能出现一次。&lt;/p&gt;&lt;p&gt;在 &lt;code&gt;parser.y&lt;/code&gt; 中找到 &lt;code&gt;AlterTableStmt&lt;/code&gt;，其中一条规则是：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;#34;ALTER&amp;#34; IgnoreOptional &amp;#34;TABLE&amp;#34; TableName AlterTableSpecListOpt PartitionOpt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中最后一个符号 PartitionOpt 和 MySQL 中 &lt;code&gt;partition_clause&lt;/code&gt; 非常相似，为了支持 remove partitioning，容易想到引入一条规则：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;AlterTablePartitionOpt: PartitionOpt | &amp;#34;REMOVE&amp;#34; &amp;#34;PARTITIONING&amp;#34;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;将 &lt;code&gt;PartitionOpt&lt;/code&gt; 的语义动作移到 &lt;code&gt;AlterTablePartitionOpt&lt;/code&gt; 中，&lt;code&gt;REMOVE PARTITIONING&lt;/code&gt; 部分先返回 &lt;code&gt;nil&lt;/code&gt;，并添加 parser 警告，表示目前 parser 能够解析但 TiDB 尚未支持该功能。修改后的规则如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;AlterTablePartitionOpt:
    PartitionOpt
    {
        if $1 != nil {
            $$ = &amp;amp;ast.AlterTableSpec{
                Tp: ast.AlterTablePartition,
                Partition: $1.(*ast.PartitionOptions),
            }
        } else {
            $$ = nil
        }
    }
|   &amp;#34;REMOVE&amp;#34; &amp;#34;PARTITIONING&amp;#34;
    {
        $$ = nil
        yylex.AppendError(yylex.Errorf(&amp;#34;The REMOVE PARTITIONING clause is parsed but ignored by all storage engines.&amp;#34;))
        parser.lastErrorAsWarn()
    }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;由于 &lt;code&gt;REMOVE&lt;/code&gt; 和 &lt;code&gt;PARTITIONING&lt;/code&gt; 都是新添加的关键字，如果不做任何处理，lexer 扫描的时候只会将它们看作普通的标识符。于是需要在 &lt;code&gt;parser.y&lt;/code&gt; 的 &lt;code&gt;%token&lt;/code&gt; 字段上补充声明，其中一个目的是为该字符串产生一个 &lt;code&gt;tokenID&lt;/code&gt;（一个整数），供 lexer 标识。另外 &lt;code&gt;goyacc&lt;/code&gt; 也会对 &lt;code&gt;parser.y&lt;/code&gt; 中所有的字符串常量进行检查，如果没有相应的 &lt;code&gt;token&lt;/code&gt; 声明，会报 &lt;code&gt;Undefined symbol&lt;/code&gt; 的错误。&lt;/p&gt;&lt;p&gt;为支持这两个关键字，我们在文件开头的 &lt;code&gt;token&lt;/code&gt; 字段添加声明。由于 &lt;code&gt;REMOVE&lt;/code&gt; 和 &lt;code&gt;PARTITIONING&lt;/code&gt; 都是非保留关键字，它们应被添加在含有 &lt;code&gt;The following tokens belong to UnReservedKeyword&lt;/code&gt; &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/blob/53c769c5836485c83d5f339faab97ef5d853d560/parser.y%23L274&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;注释&lt;/a&gt; 的下方。此外，非保留字说明它们能够作为标识符 &lt;code&gt;Identifier&lt;/code&gt;，因此在 &lt;code&gt;Identifier&lt;/code&gt; 规则下的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/blob/53c769c5836485c83d5f339faab97ef5d853d560/parser.y%23L3717&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;UnRerservedKeyword&lt;/a&gt;&lt;/code&gt; 中也应加上 &lt;code&gt;REMOVE&lt;/code&gt; 和 &lt;code&gt;PARTITIONING&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;关于如何确定一个关键字是保留的还是非保留的，可以参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/refman/8.0/en/keywords.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MySQL 文档&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;4.2 增加「关键字-&lt;code&gt;tokenID&lt;/code&gt;」映射&lt;/h3&gt;&lt;p&gt;前文提到，添加声明是为了让 lexer 能够识别关键字并赋予对应的 &lt;code&gt;tokenID&lt;/code&gt;，对于 lexer 而言，它需要一个从关键字字符串到 &lt;code&gt;tokenID&lt;/code&gt; 的映射关系。在 TiDB parser 中，这个映射关系就是 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/blob/53c769c5836485c83d5f339faab97ef5d853d560/misc.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;misc.go&lt;/a&gt;&lt;/code&gt; 中的 &lt;code&gt;tokenMap&lt;/code&gt; 结构。&lt;/p&gt;&lt;p&gt;在这个例子中，我们往 &lt;code&gt;tokenMap&lt;/code&gt; 中添加 &lt;code&gt;remove&lt;/code&gt; 和 &lt;code&gt;partitioning&lt;/code&gt;（如果不添加，会使关键字一致性的检查测试失败）。&lt;/p&gt;&lt;h3&gt;4.3 修改 AST 节点&lt;/h3&gt;&lt;p&gt;到目前为止，我们已经让 &lt;code&gt;goyacc&lt;/code&gt; 生成的 parser 能够解析 remove partitioning 语法。但是，解析完成后并没有返回有效的数据结构（4.1 中我们返回了 &lt;code&gt;nil&lt;/code&gt;），这意味着 parser 不能够根据语法树重新生成原 SQL 语句。&lt;/p&gt;&lt;p&gt;所以，要修改现有的 AST 节点，使它们能够以某种形式保存 remove partitioning 信息。回顾目前规则层面的修改：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;AlterTableStmt:
&amp;#34;ALTER&amp;#34; IgnoreOptional &amp;#34;TABLE&amp;#34; TableName AlterTableSpecListOpt PartitionOpt&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;已改为：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;AlterTableStmt:
&amp;#34;ALTER&amp;#34; IgnoreOptional &amp;#34;TABLE&amp;#34; TableName AlterTableSpecListOpt AlterTablePartitionOpt
AlterTablePartitionOpt:
      PartitionOpt | &amp;#34;REMOVE&amp;#34; &amp;#34;PARTITIONING&amp;#34;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中几个非终结符对应的数据结构如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;AlterTableSpecListOpt -&amp;gt; []AlterTableSpec
PartitionOpt -&amp;gt; PartitionOptions&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;关于修改节点，有几种方案可以选择：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;增加一个新的节点 struct，表示 &lt;code&gt;AlterTablePartitionOpt&lt;/code&gt;。其中包含 &lt;code&gt;PartitionOptions&lt;/code&gt; 和一个 bool 值，表示是否为 remove partitioning。&lt;/li&gt;&lt;li&gt;将 remove partitioning 看作 &lt;code&gt;PartitionOptions&lt;/code&gt;，在内部添加一个 bool 成员 &lt;code&gt;isRemovePartitioning&lt;/code&gt; 以做区分。&lt;/li&gt;&lt;li&gt;将 &lt;code&gt;PartitionOpt&lt;/code&gt; 和 remove partitioning 都看作 &lt;code&gt;AlterTableSpec&lt;/code&gt;，为 &lt;code&gt;AlterTableSpec&lt;/code&gt; 的添加一个类型，单独表示 remove partitioning。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;经过比较和分析，我们发现第一个方案会引入新的数据结构，有较大的概率会引起现有的 TiDB 测试不过，为此可能要修改 TiDB 方面的代码，工作量大的同时提高了 parser 的复杂度，因此作为备选方案；第二个方案没有引入新的数据结构，但是修改了现有的数据结构（加了个 bool 成员）；第三个方案即没有添加也没有修改数据结构，并且能够以较少的代码完成任务，作为首选方案。&lt;/p&gt;&lt;p&gt;&lt;b&gt;在以上的选择中，我们遵循「尽量不修改 AST 节点结构」的原则。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;按照方案三，观察 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/pull/396/files%23diff-688d51c34d61e5d538b15582305c9a8dL1768&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;AlterTableSpec&lt;/a&gt;&lt;/code&gt;，其定义如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;type AlterTableSpec struct {
  node
  ...
  Tp              AlterTableType
  Name            string
  ...
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中一个成员是 &lt;code&gt;Tp&lt;/code&gt;，它所属的类型包含了 &lt;code&gt;AlterTable&lt;/code&gt; 的许多操作，例如 &lt;code&gt;AddColumn&lt;/code&gt;，&lt;code&gt;AddConstraint&lt;/code&gt;，&lt;code&gt;DropColumn&lt;/code&gt; 等。我们的任务是添加一个 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/pull/396/files%23diff-688d51c34d61e5d538b15582305c9a8dR1708&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;类似的 Type&lt;/a&gt;，让它能够表示 remove partitioning。&lt;/p&gt;&lt;p&gt;到这里，解析完 SQL 语句生成的 AST 树已经包含 remove partitioning 的信息了。接下来要处理 Restore，让它能够从 AST 树还原出 SQL 语句。&lt;code&gt;AlterTableSpec&lt;/code&gt; 的 Restore 十分简单，加一个 case 即可，这里不再赘述。&lt;/p&gt;&lt;h3&gt;4.4 完善 &lt;code&gt;parser.y&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;第一次修改 &lt;code&gt;parser.y&lt;/code&gt; 的时候我们在新加规则的语义动作中返回了 &lt;code&gt;nil&lt;/code&gt;，原因是尚未确定 AST 是否需要修改，以及如何修改。而到这一步，这些都已经确定下来了，把 remove partitioning 看作 &lt;code&gt;AlterTableSpec&lt;/code&gt; 类型：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;|       &amp;#34;REMOVE&amp;#34; &amp;#34;PARTITIONING&amp;#34;
      {
              $$ = &amp;amp;ast.AlterTableSpec{
                     Tp: ast.AlterTableRemovePartitioning,
              }
             yylex.AppendError(yylex.Errorf(&amp;#34;The REMOVE PARTITIONING clause is parsed but ignored by all storage engines.&amp;#34;))
             parser.lastErrorAsWarn()
      }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意，这里必须抛出警告，表示虽然目前 parser 能够解析该语法，但实际上 TiDB 并未支持相应的功能。&lt;/p&gt;&lt;h3&gt;5. 补充 test case&lt;/h3&gt;&lt;p&gt;这里，所有的代码修改引入的分支结构都能够被现有的测试覆盖，因此在提升覆盖率上没有需求。当然，如果想要测试更多类似的 case，可以将它们添加到前面提到的 &lt;code&gt;TestDDL&lt;/code&gt; 函数中。&lt;/p&gt;&lt;h3&gt;6. 执行所有测试（&lt;code&gt;make test&lt;/code&gt;）&lt;/h3&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;gofmt (simplify)
sh test.sh
ok      github.com/pingcap/parser       4.294s  coverage: 62.1% of statements in ./...
ok      github.com/pingcap/parser/ast   2.090s  coverage: 42.3% of statements in ./...
ok      github.com/pingcap/parser/auth  1.155s  coverage: 1.3% of statements in ./... [no tests to run]
ok      github.com/pingcap/parser/charset       1.094s  coverage: 2.0% of statements in ./...
ok      github.com/pingcap/parser/format        1.114s  coverage: 2.5% of statements in ./...
?       github.com/pingcap/parser/goyacc        [no test files]
ok      github.com/pingcap/parser/model 1.100s  coverage: 3.5% of statements in ./...
ok      github.com/pingcap/parser/mysql 1.102s  coverage: 1.3% of statements in ./... [no tests to run]
ok      github.com/pingcap/parser/opcode        1.099s  coverage: 1.4% of statements in ./...
ok      github.com/pingcap/parser/terror        1.091s  coverage: 2.3% of statements in ./...
ok      github.com/pingcap/parser/types 1.106s  coverage: 7.0% of statements in ./...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;确保所有的 test 都是 ok 的状态。&lt;/b&gt;&lt;/p&gt;&lt;h3&gt;7. 提交 PR&lt;/h3&gt;&lt;p&gt;按照 PR 模板逐项填写。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;### What problem does this PR solve?

#### Fix compatibility problem about keyword REMOVE PARTITIONING

Issue: pingcap/parser#402

#### MySQL Syntax:

alter_specification:
...
  | REMOVE PARTITIONING

### Bad SQL Case:

alter table t remove partitioning;
alter table t lock = default, remove partitioning;
alter table t drop check c, remove partitioning;

### Check List

Tests
- Unit test&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;需要特别指出的是，我们鼓励各位 Contributor 多使用&lt;/b&gt; &lt;b&gt;&lt;code&gt;make test&lt;/code&gt;。当不知道从何处入手或者失去目标时，&lt;code&gt;make test&lt;/code&gt;输出的错误信息或许能够引导大家进行思考和探索&lt;/b&gt;。&lt;/p&gt;&lt;blockquote&gt;Tips: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/parser/pull/396&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;完整的 PR 示例&lt;/a&gt;&lt;/blockquote&gt;&lt;h2&gt;FAQ&lt;/h2&gt;&lt;p&gt;以下是在增加 remove partitioning 语法支持时遇到的问题和解决方法。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q1. 为什么不在&lt;/b&gt; &lt;b&gt;&lt;code&gt;PartitionOpt&lt;/code&gt;&lt;/b&gt; &lt;b&gt;中直接添加规则？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;A1&lt;/b&gt;：&lt;code&gt;PartitionOpt&lt;/code&gt; 用于匹配含有 &lt;code&gt;partition by&lt;/code&gt; 的 SQL 语句，除了 &lt;code&gt;Alter Table&lt;/code&gt; 语句以外，它还被 &lt;code&gt;Create Table&lt;/code&gt; 使用，而 &lt;code&gt;remove partitioning&lt;/code&gt; 只存在于 &lt;code&gt;alter table&lt;/code&gt; 语句中，因此不能在 &lt;code&gt;PartitionOpt&lt;/code&gt; 中添加规则。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q2. 执行 make test 时报错：&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;parser.y:1100:1: undefined symbol &amp;#34;PARTITIONING&amp;#34;
parser.y:1100:1: undefined symbol &amp;#34;REMOVE&amp;#34;
make[1]: *** [Makefile:19: parser] Error 1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;A2&lt;/b&gt;：在 yacc 中，出现在规则中的字符串，要么是 &lt;code&gt;token&lt;/code&gt;（终结符），要么是非终结符。这里引用一段 yacc 的规范：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;Names refer to either tokens or nonterminal symbols.
Names representing tokens must be declared; this is most simply done by writing
%token   name1 name2 . . .&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;所以，修复方法是在 &lt;code&gt;parser.y&lt;/code&gt; 的 &lt;code&gt;%token&lt;/code&gt; 字段上添加 &lt;code&gt;PARTITIONING&lt;/code&gt; 和 &lt;code&gt;REMOVE&lt;/code&gt; 的声明。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q3. 执行 make test 时报错：&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;c.Assert(len(tokenMap)-len(aliases), Equals, keywordCount-len(windowFuncTokenMap))
... obtained int = 454
... expected int = 456&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;A3&lt;/b&gt;：这是关键字的一致性检查出了问题，解决方案是补充 &lt;code&gt;tokenMap&lt;/code&gt;（它是关键字到 &lt;code&gt;token ID&lt;/code&gt; 的映射，被 scanner 用来判断某个字符串是否为关键字）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q4. 执行 make test 时报错：&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;FAIL: parser_test.go:1666: testParserSuite.TestDDL
parser_test.go:2166:
    s.RunTest(c, table)
parser_test.go:351:
    c.Assert(restoreSQLs, Equals, expectSQLs, comment)
... obtained string = &amp;#34;ALTER TABLE `t`&amp;#34;
... expected string = &amp;#34;ALTER TABLE `t` REMOVE PARTITIONING&amp;#34;
... restore ALTER TABLE `t`; expect ALTER TABLE `t` REMOVE PARTITIONING&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;A4&lt;/b&gt;：这个错误说明 parser 已经解析通过，但不能从语法树中恢复原 SQL 语句的 remove partitioning 部分。此时应检查相应 AST 节点的 Restore 方法是否正确处理了 &lt;code&gt;REMOVE PARTITIONING&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;最后欢迎大家加入&lt;/i&gt;&lt;/b&gt; &lt;b&gt;&lt;i&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/community-cn/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Contributor Club&lt;/a&gt;，无门槛参与开源项目，改变世界从这里开始吧！&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;原文阅读：&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/30mins-become-contributor-of-tidb-20190808/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;三十分钟成为 Contributor | 提升 TiDB Parser 对 MySQL 8.0 语法的兼容性 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-08-08-77255479</guid>
<pubDate>Thu, 08 Aug 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Binlog 源码阅读系列文章（三）Pump client 介绍</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-08-06-76938408.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/76938408&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-548816c2ee059830d19edbb17354f7b3_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄佳豪&lt;/p&gt;&lt;p&gt;在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-binlog-source-code-reading-2/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;上篇文章&lt;/a&gt; 中，我们介绍了 Pump 的作用是存储 TiDB 产生的 binlog。本篇将介绍 Pump client，希望大家了解 TiDB 把 binlog 写到 Pump，以及输出数据的过程。&lt;/p&gt;&lt;h2&gt;gRPC API&lt;/h2&gt;&lt;p&gt;Pump client 的代码在 tidb-tools 下这个 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/tree/v3.0.0-rc.3/tidb-binlog/pump_client&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;路径&lt;/a&gt;，TiDB 会直接 import 这个路径使用 Pump client package。TiDB 跟 Pump 之间使用 gRPC 通信，相关的 proto 文件定义在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tipb/tree/87cb1e27ab4a86efc534fd4c5b62fda621e38465/proto/binlog&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这里&lt;/a&gt;。Pump server 提供以下两个接口：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// Interfaces exported by Pump.
service Pump {
    // Writes a binlog to the local file on the pump machine.
    // A response with an empty errmsg is returned if the binlog is written successfully.
    rpc WriteBinlog(WriteBinlogReq) returns (WriteBinlogResp) {}

    // Sends binlog stream from a given location.
    rpc PullBinlogs(PullBinlogReq) returns (stream PullBinlogResp) {}
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;本文我们主要介绍 RPC &lt;code&gt;WriteBinlog&lt;/code&gt; 这个接口，Pump client 会通过这个接口写 binlog 到 Pump。&lt;/p&gt;&lt;p&gt;&lt;code&gt;WriteBinlogReq&lt;/code&gt; 里面包含的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tipb/blob/87cb1e27ab4a86efc534fd4c5b62fda621e38465/proto/binlog/binlog.proto%23L57&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;binlog event&lt;/a&gt;：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// Binlog contains all the changes in a transaction, which can be used to reconstruct SQL statement, then export to
// other systems.
message Binlog {
    optional BinlogType    tp             = 1 [(gogoproto.nullable) = false];

    // start_ts is used in Prewrite, Commit and Rollback binlog Type.
    // It is used for pairing prewrite log to commit log or rollback log.
    optional int64         start_ts       = 2 [(gogoproto.nullable) = false];

    // commit_ts is used only in binlog type Commit.
    optional int64         commit_ts      = 3 [(gogoproto.nullable) = false];

    // prewrite key is used only in Prewrite binlog type.
    // It is the primary key of the transaction, is used to check that the transaction is
    // commited or not if it failed to pair to commit log or rollback log within a time window.
    optional bytes         prewrite_key   = 4;

    // prewrite_data is marshalled from PrewriteData type,
    // we do not need to unmarshal prewrite data before the binlog have been successfully paired.
    optional bytes         prewrite_value = 5;

    // ddl_query is the original ddl statement query.
    optional bytes         ddl_query      = 6;

    // ddl_job_id is used for DDL Binlog.
    // If ddl_job_id is setted, this is a DDL Binlog and ddl_query contains the DDL query.
    optional int64         ddl_job_id     = 7 [(gogoproto.nullable) = false];
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;TiDB 如何写 binlog&lt;/h2&gt;&lt;p&gt;TiDB 的事务采用 2-phase-commit 算法，一次事务提交会分为 Prewrite 和 Commit 阶段，有兴趣的可以看下相关文章&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-transaction-model/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiKV 事务模型概览，Google Spanner 开源实现》&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;大家可以先猜想一下 TiDB 是如何写 binlog 的？&lt;/p&gt;&lt;p&gt;如果只写一条 binlog 的话可行吗？可以很容易想到，如果只写一条 binlog 的话必须确保写 binlog 操作和事务提交操作是一个原子操作，那么就要基于事务模型再构建一个复杂的 2PC 模型，从复杂度方面考虑这个方案几乎是不可行的。&lt;/p&gt;&lt;p&gt;实际上，在 TiDB 的实现中，TiDB 会每个阶段分别写一条 binlog， 即：Prewrite binlog 和 Commit binlog，下面会简称 P-binlog 和 C-binlog ，具体写入流程如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a0d9bd1b443902939ed6a5fadb65deb2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1002&quot; data-rawheight=&quot;672&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1002&quot; data-original=&quot;https://pic3.zhimg.com/v2-a0d9bd1b443902939ed6a5fadb65deb2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a0d9bd1b443902939ed6a5fadb65deb2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1002&quot; data-rawheight=&quot;672&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1002&quot; data-original=&quot;https://pic3.zhimg.com/v2-a0d9bd1b443902939ed6a5fadb65deb2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-a0d9bd1b443902939ed6a5fadb65deb2_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;这里我们说的 P-binlog 和 C-binlog 都是通过 RPC &lt;code&gt;WriteBinlog&lt;/code&gt; 接口写入，对应着参数 &lt;code&gt;WriteBinlogReq&lt;/code&gt; 里面包含的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tipb/blob/87cb1e27ab4a86efc534fd4c5b62fda621e38465/proto/binlog/binlog.proto%23L57&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;binlog event&lt;/a&gt;，只是字段有些区别：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;P-binlog 对应的 &lt;code&gt;tp&lt;/code&gt; 是 &lt;code&gt;Prewrite&lt;/code&gt;，C-binlog 的 &lt;code&gt;tp&lt;/code&gt; 是 &lt;code&gt;Commit&lt;/code&gt; 或者 &lt;code&gt;Rollback&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;同个事务的 P-binlog 和 C-binlog 包含相同 &lt;code&gt;start_ts&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;只有 P-binlog 包含对应事务修改数据 &lt;code&gt;prewrite_value&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;只有 C-binlog 包含事务的 &lt;code&gt;commit_ts&lt;/code&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 Prepare 的阶段，TiDB 会把 Prewrite 的数据发到 TiKV，同时并发写一条 P-binlog 到其中一个 Pump。 两个操作全部成功后才会进行 Commit 阶段，所以我们提交事务时就可以确定 P-binlog 已经成功保存。写 C-binlog 是在 TiKV 提交事务后异步发送的，告诉  Pump 这个事务提交了还是回滚了。&lt;/p&gt;&lt;h3&gt; 写 binlog 对事务延迟的影响&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Prepare 阶段：并发写 P-binlog 到 Pump 和 Prewrite data 到 TiKV，如果请求 Pump 写 P-binlog 的速度快于写 TiKV 的速度，那么对延迟没有影响。一般而言写入 Pump 会比写入 TiKV 更快。&lt;/li&gt;&lt;li&gt;Commit 阶段：异步的去写 C-binlog，对延迟也没有影响。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;写 binlog 失败&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;写 P-binlog 失败，那么 transaction 不会 commit，不会对系统有任何影响。&lt;/li&gt;&lt;li&gt;写 C-binlog 失败，Pump 会等待最多 &lt;code&gt;max transaction timeout&lt;/code&gt; 的时间（这是一个 TiDB/Pump 的配置，默认为 10 分钟），然后向 TiKV 去查询 transaction 的提交状态来补全 C-binlog，但是此时同步延迟也等于 &lt;code&gt;max transaction timeout&lt;/code&gt; 。这种情况经常发生于 TiDB 进程重启或者挂掉的场景。&lt;/li&gt;&lt;li&gt;写 P-binlog 成功，但是 Prewrite 失败，那么也会和 2 类似。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;Pump client 源码&lt;/h2&gt;&lt;p&gt;Pump client 的代码维护在 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/tree/master/tidb-binlog/pump_client&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pump_client&lt;/a&gt;&lt;/code&gt;，提供了 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/client.go%23L125&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;NewPumpsClient&lt;/a&gt;&lt;/code&gt; 方法来创建一个 Pump client  实例。Pump client 的主要功能就是维护所有 Pump 状态（将 Pump 分为 avaliable 和 unavailable 两种状态），以此为依据将 TiDB 生成的 binlog 发送到合适的 Pump。为此 Pump client 主要实现了以下几个机制：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;watch etcd&lt;br/&gt;Pump 在运行时会将自己的状态信息上报到 PD（etcd）中，并且定时更新自己的状态。在创建 Pump client 的时候，会 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/client.go%23L227&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;首先从 PD（etcd）中获取所有的 Pump 状态信息&lt;/a&gt;，根据 Pump 状态是否为 Online 初步判断 Pump 为 avaliable 或者 unavailable。然后 Pump client 会 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/client.go%23L478&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;watch&lt;/a&gt; etcd 中的 Pump 状态变更，及时更新内存中维护的 Pump 状态。&lt;/li&gt;&lt;li&gt;binlog 重试机制&lt;br/&gt;对于每个 Pump，在 Pump client 中都维护了一个变量 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/pump.go%23L70&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ErrNum&lt;/a&gt;&lt;/code&gt; 来记录该 Pump 写 binlog 的失败次数，当 ErrNum 超过一定的阈值，则判断 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/pump.go%23L174&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;该 Pump 不可用&lt;/a&gt;，如果写 binlog 成功，则 href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/pump.go%23L162&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/pump.go#L162&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;重置 ErrNum。&lt;/li&gt;&lt;li&gt;发送探活请求&lt;br/&gt;在某些情况下，比如网络抖动，可能会导致 Pump 写 binlog 失败，因此该 Pump 被 Pump client 判断状态为 unavailable，但是当网络恢复后，该 Pump 仍然可以提供写 binlog 服务。Pump client 实现了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/client.go%23L531&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;detect&lt;/a&gt; 机制，会定期向 unavailable 状态的 Pump 发送探活请求，如果探活请求成功，则更新 Pump 状态为 avaliable。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;为了将 binlog 均匀地分发到所有 Pump，Pump client 使用 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/selector.go%23L47&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PumpSelector&lt;/a&gt;&lt;/code&gt; 为每一个 binlog 选择一个合适的 Pump，&lt;code&gt;PumpSelector&lt;/code&gt; 是一个接口，提供 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/selector.go%23L49&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;SetPumps&lt;/a&gt;&lt;/code&gt; 方法来设置可选的 Pump 列表，提供 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/selector.go%23L52&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Select&lt;/a&gt;&lt;/code&gt; 来为 binlog 选择 Pump。目前主要实现了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/selector.go%23L59&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hash&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/selector.go%23L109&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Round-Robin&lt;/a&gt; 两种策略。&lt;/p&gt;&lt;p&gt;为了提高 Pump client 的健壮性，binlog 写失败后会提供一定的重试，每个 Pump 可以重试写多次，同时也会尽量尝试所有的 Pump，这样就可以保证部分 Pump 有故障或者临时的网络抖动也不影响 TiDB 写 binlog，可以查看 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/c969908e6130dfbdb4ab80fb84f275df2a6fd877/tidb-binlog/pump_client/client.go%23L242&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;WriteBinlog&lt;/a&gt;&lt;/code&gt;了解具体实现方式。&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;本文给大家介绍了 TiDB 如何通过 Pump client 写 binlog 到 Pump，以及 binlog 的主要内容，后续我们将继续介绍 Pump server 是对应如何处理相应请求的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;阅读原文：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-binlog-source-code-reading-3/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog 源码阅读系列文章（三）Pump client 介绍 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多 TiDB Binlog 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23TiDB-Binlog-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Blog-cns | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-08-06-76938408</guid>
<pubDate>Tue, 06 Aug 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>我们是如何设计 Golang &amp; SQL 引擎课程的？ | Talent Plan 背后的故事</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-08-05-76778250.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/76778250&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-188adcf8f0a02c1c30930cfb1d7f3f45_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：谢海滨&lt;/p&gt;&lt;p&gt;在 &lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/73950816&quot; class=&quot;internal&quot;&gt;上篇文章&lt;/a&gt;&lt;/u&gt; 中我们介绍了 PingCAP Talent Plan - TiKV 方向的课程内容，本文将从课程设计的角度和大家聊一聊 TiDB 方向的课程内容，包括课程设计的逻辑，和课程学习过程中常见的问题及解答等。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 方向课程内容&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB 作为一个支持 MySQL 协议，以某种支持事务的分布式 KV 存储引擎为底层存储的 SQL 引擎，主要需要处理与 MySQL 客户端的交互，在底层存储引擎中存取数据，以及实现 SQL 功能。&lt;/b&gt;在 Talent Plan 课程设计上，我们主要关注在如何实现 SQL 功能，并将重点放在如何实现 SQL 优化器以及执行引擎上：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;优化器&lt;/b&gt;：主要负责生成并且优化查询计划，执行计划的好坏将极大影响执行效率，因此这一部分也可以说是整个 SQL 功能最核心的部分之一；&lt;/li&gt;&lt;li&gt;&lt;b&gt;执行引擎&lt;/b&gt;：主要负责执行生成的查询计划，大部分 SQL 的执行逻辑都在这里，目前 TiDB 的执行框架已经由经典的火山模型改进为了向量化模型。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当然，&lt;b&gt;Golang&lt;/b&gt; 作为 TiDB 使用的语言，在课程设计中也是非常重要的一部分。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;课程设计&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;在线上课程的内容设计上，我们主要希望大家可以循序渐进地掌握数据库实现 SQL 功能的一些重要基石，这些内容可能并不局限于数据库领域，这样即使大家将来不从事数据库相关工作或者研究，也可以有所收获。&lt;/b&gt;而在作业的设计上，一方面是需要紧密的贴合学习内容，另一方面是希望大家在完成作业的过程中可以尽可能有及时而准确的反馈，获得一些成就感～&lt;/p&gt;&lt;p&gt;在第一周的课程作业中，我们选取了归并排序来作为 Golang 的实战演练。相对来说，Golang 的语法并不复杂，这里主要需要掌握的是它的并发模型和性能调优工具，因此在题目的选择上需要尽可能的经典而且简单，这样大家的关注重心可以放在之前可能并不熟悉的 Golang 并发模型上，从这一点上来说，归并排序可以说是相当适合的选择了。&lt;/p&gt;&lt;p&gt;当然实现完成归并排序并不是我们唯一的目的，更重要的是大家可以对程序的运行代价有一个直观的认识，在这一点上 Go Profile 可以说是做的比较出色，通过性能这样容易衡量的指标，以及 Go Profile 这样的性能分析工具，相信大家在完成第一周作业的过程中可以更加得心应手。&lt;/p&gt;&lt;p&gt;在简单地完成和 Golang 的“初相识”后，第二周我们选择了 MapReduce 来帮助大家认识分布式计算。作为经典的计算模型，MapReduce 对 TiDB 来说非常重要，比如在 TiDB 的并行 Hash 聚合执行算子里可以看到 MapReduce 的影子。在这一周里，我们直接采用了 MIT 6.824 的 Lab 1。当然与第一周一样，这里另外一个比较重要的目标就是利用 Go Profile 来优化代码性能。&lt;/p&gt;&lt;p&gt;前两周的课程设置中，我们选择从两个比较经典的算法切入，来由浅入深地带大家学习优化性能，当然在实际操作过程中，面临的问题往往更复杂，也许 Profile 已经无法满足需求。这时我们就需要从更抽象的角度，来思考以及组合我们的计算模型。所以在第三周，我们选择了优化器作为切入点，带大家了解数据库是如何从更抽象的层面优化执行代价的。这一周，大家可以简单地了解逻辑算子和物理算子，学习它们优化方式的不同。&lt;b&gt;在为大家提交的作业评分时，我们发现很多同学只是从物理优化的层面去考虑问题，也就是只考虑了 Join 算子的不同实现方式，而没有从逻辑执行计划的角度考虑问题，导致错失大量分数，希望后面的同学可以多加注意。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最后一周的线上课程可以说是对前三周课程知识的总结。在实现并优化一个带有 Join 以及聚合的 SQL 语句的过程中，大家既可以从第二周学到的执行框架上去考虑，也可以从逻辑以及物理算子的角度考虑，可以说是一个相当开放且有趣的题目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;线下课程是线上课程的一个延伸和连续。在学习完线上课程后，相信大家会对数据库有一个基本的了解，在线下课程中我们将继续带领大家学习 TiDB 是如何实现优化器以及执行引擎的。当然除了 TiDB 现有的实现外，我们也希望可以带大家看看业界前沿的研究，拓宽视野。更多的细节已在 &lt;u&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247489103%26idx%3D1%26sn%3D7de7c0ce7733e6d18eb3f0e95fc5e426%26chksm%3Deb163125dc61b83341aa265ad7b908e93800fce82876a9f1475d09fc13bd2901fc383ebb66b1%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这篇文章&lt;/a&gt;&lt;/u&gt; 中披露，在这就不赘述了。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;FAQ&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;下面是我们经常被问到的几个问题，在此和大家分享一下。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q1: Merge Sort 的理想性能该是多少？应该要比 Normal Sort 快几倍？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A1: 这个主要取决于具体的 CPU 数目，性能上当然是要越快越好了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q2: MapReduce 的作业消耗空间比较大，跑不完怎么办？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A2: 可以更改测试文件里的数据量大小，调到自己可以运行的程度，并以此为基准来优化就可以了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q3: Plan Tree 该怎么画？有标准画法么？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A3: 在作业中只需要以书本上类似的形式画出来就可以了，当然如果算子的含义与书上的不同，需要在作业中标注出来。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q4: Join 算子优化前后耗时很接近，该怎么办？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A4: 题目中给出的接口会主要耗在读文件上，可以考虑测试读文件之后的处理时间。&lt;/p&gt;&lt;p&gt;以上是关于 PingCAP Talent Plan - TiDB 方向课程的介绍，欢迎大家参与课程学习，也非常欢迎大家对课程提出意见和建议，在这里也要特别感谢 @Jiaolong 同学提供 Effective Go 的学习地址，为大家学习 Go 语言提供了更加丰富的学习素材。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;PingCAP Talent Plan&lt;/b&gt;&lt;br/&gt;PingCAP Talent Plan 是 PingCAP 为 TiDB 开源社区小伙伴提供的进阶式学习计划。课程设置上分为两个方向，分别是面向 SQL 引擎的 TiDB 方向和面向大规模、一致性的分布式存储的 TiKV 方向。每个方向的课程都包含线上和线下两部分，线上课程侧重于对基础知识的讲解，对社区所有小伙伴们开放，时间上比较灵活。线下课程在夯实基础知识的基础上，注重实操能力的培养。&lt;br/&gt;完成线上课程并通过线上考核的小伙伴可以获得线上课程结业证书，表现优秀的还将有机会拿到 PingCAP 校招/实习免笔试绿色通道，而且有机会参与半年内 PingCAP 组织的任意一期线下课程；完成线下课程的小伙伴可以获得专属 PingCAP Talent Plan 结业证书，表现优秀的还将有机会拿到 PingCAP 校招/实习免面试绿色通道/Special Offer、 PingCAP/TiDB 全球 Meetup 的邀请函等。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-08-05-76778250</guid>
<pubDate>Mon, 05 Aug 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>微众银行数据库架构演进及 TiDB 实践经验</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-08-01-76239450.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/76239450&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4c42444de9685bbfce19c6f5fb81e4d5_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍：&lt;/b&gt;&lt;br/&gt;&lt;b&gt;胡盼盼&lt;/b&gt;，微众银行数据平台室室经理。硕士毕业于华中科技大学，毕业后加入腾讯，任高级工程师，从事分布式存储与云数据库相关的研发与运营工作；2014 年加入微众银行，负责微众银行的数据库平台的建设与运营。&lt;br/&gt;&lt;b&gt;黄蔚&lt;/b&gt;，微众银行数据库平台室高级 DBA。2011 年加入腾讯互动娱乐运营部，担任英雄联盟在内的多款海量用户产品的数据库运维工作。2015 年加入微众银行担任高级 DBA，负责监控优化、性能优化以及新技术预研，目前致力于行内 NewSQL 的推广与生态建设。&lt;/blockquote&gt;&lt;p&gt;本文将介绍微众银行的数据库架构演进过程，并分享 TiDB 在微众银行实践经验和部分业务案例。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一、微众银行数据库架构演进&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2014 年微众银行成立之时，就非常有前瞻性的确立了微众银行的 IT 基础架构的方向：去 IOE，走互联网模式的分布式架构。IOE 即 IBM、Oracle、EMC，代表了传统基础架构领域的服务器、商业数据库和存储产品体系，众所周知传统银行 IT 架构体系非常依赖于 IOE，每年也需要巨大的 IT 费用去维护和升级 。从数据库角度来看，当时除了 Oracle，能满足金融级银行场景的数据库产品并不多，微众银行基础架构团队经过多轮的评估和测试，最终确定使用腾讯主推的一款金融级别数据库 TDSQL。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ddef0690546f8c96824cbf7c7bf1dd6e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-ddef0690546f8c96824cbf7c7bf1dd6e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ddef0690546f8c96824cbf7c7bf1dd6e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-ddef0690546f8c96824cbf7c7bf1dd6e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-ddef0690546f8c96824cbf7c7bf1dd6e_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;TDSQL 是基于 MariaDB 内核 ，结合  mysql-proxy、ZooKeeper 等开源组件实现的数据库集群系统，并且基于 MySQL 半同步的机制，在内核层面做了大量优化，在性能和数据一致性方面都有大幅的提升，同时完全兼容 MySQL 语法，支持 Shard 模式（中间件分库分表）和 NoShard 模式（单机实例），同时还集成了管控平台，智能运维等功能模块。2014 年，TDSQL 已经支撑了腾讯内部的海量的计费业务，由于计费业务的场景和银行的场景有所类似，对数据库的可靠性和可用性要求也相近，所以我们当时选择了 TDSQL 作为微众银行的核心数据库。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 基于 DCN 的分布式架构&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5377ed50af8c862313aa514ff96c415d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5377ed50af8c862313aa514ff96c415d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5377ed50af8c862313aa514ff96c415d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5377ed50af8c862313aa514ff96c415d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5377ed50af8c862313aa514ff96c415d_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;确定了数据库的选型之后， 下一步就是架构设计。我们设计了基于 DCN（Data Center Node）的分布式架构。DCN 可以认为是一个“自包含单位”，它会包含应用层、接入层、数据库层，每个 DCN 承载规定数据量的用户，通俗的理解，每个 DCN，就相当于微众银行的一个小的分行；基于 DCN 可以实现集群规模的水平扩展。这种架构对于数据库来说，其实是比较友好的，因为每个 DCN 的用户规模是确定的，那么对数据库的容量和性能要求也是可确定的，因此我们不必再采用复杂的中间件分库分表的方式构建数据库，而只用单实例模式承载，极大简化了数据库架构，也降低了业务开发成本。如图 2 所示，为了实现 DCN 架构，这里有两个关键组件：RMB 和 GNS。RMB 负责各个模块以及各个 DCN 之间的消息通信；GNS 负责全局的 DCN 路由，即某个用户保存在哪个 DCN。另外这里有一个比较特殊的地方就是 ADM 管理区，它是一个统一的管理区，保存着无法进行 DCN 拆分的全局业务数据，和通过各 DCN 进行汇总的数据。后来 ADM 区成为了一个 TDSQL 的瓶颈，这是我们引入 TiDB 的动机之一。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. IDC 架构&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a65487e8a4c436738b1ef5005004d74_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-5a65487e8a4c436738b1ef5005004d74_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a65487e8a4c436738b1ef5005004d74_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-5a65487e8a4c436738b1ef5005004d74_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-5a65487e8a4c436738b1ef5005004d74_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;接下来看一下我们的 IDC 的架构。目前我们是两地六中心的架构，深圳的 5 个 IDC 是生产中心，而位于上海的跨城 IDC 是容灾中心。同城的任意两个 IDC 之前的距离控制在 10~50 公里以内，并通过多条专线互联，以此保证两个 IDC 之间的平均网络延迟可以控制在 2ms 左右，并保证网络的稳定性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 数据库部署架构&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-09b14d7f7ee3fd4d729beb493f1c934a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-09b14d7f7ee3fd4d729beb493f1c934a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-09b14d7f7ee3fd4d729beb493f1c934a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-09b14d7f7ee3fd4d729beb493f1c934a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-09b14d7f7ee3fd4d729beb493f1c934a_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;基于以上的 DCN 架构和 IDC 架构，我们设计了数据库的基础架构，如图 4 所示：我们采用同城 3 副本+跨城 2 副本的 3+2 部署模式，同城 3 副本为 1 主 2 备，跨 3 个同城 IDC 部署，副本之间采用 TDSQL 强同步，保证同城 3 IDC 之间的 RPO=0，RTO 秒级，跨城的 2 副本通过同城的一个 slave 进行异步复制，实现跨城的数据容灾。基于以上架构，我们在同城可以做到应用多活，即同城的业务流量，可以同时从 3 个 IDC 进来，任何一个 IDC 宕机，可以保证数据 0 丢失，同时在秒级内可以恢复数据库服务。这个架构在微众银行内部运行了四年多，当前已有 1500 多个实例在运行，数据量达到 PB 级，承载了银行数百个核心系统，整体上来说还比较稳定的。&lt;b&gt;但同时也遇到一些瓶颈。因为我们采用的是单实例的部署模式，对于有些无法通过 DCN 拆分进行扩展的业务场景，单实例的性能和容量就很容易到达瓶颈。&lt;/b&gt;当然，TDSQL 也提供了 TDSQL-Shard 模式，也就是通过中间件分库分表的方式把一个表 Shard 之后再处理，但我们当时评估之后认为该模式对应用的侵入性比较大，比如所有的表必须定义 shard-key，有些语法可能不太兼容，有些分布式事务的场景可能会有瓶颈，进而导致业务迁移的成本会比较高。所以在这个背景下，我们开始寻求其它的解决方案，大约在 2018 年，NewSQL 的概念逐渐被提了出来，同时也有一些商业和开源的 NewSQL 数据库出现。我们很惊喜的发现，NewSQL 数据库的特性，可以较好的解决我们当时面临的问题。NewSQL 比较通用的定义是：一个能兼容类似 MySQL 的传统单机数据库、可水平扩展、数据强一致性同步、支持分布式事务、存储与计算分离的关系型数据库。经过大量的调研，对比与分析，我们最终决定重点考察开源 NewSQL 数据库产品  TiDB。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、微众银行 TiDB 数据库实践&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. Why TiDB ?&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f42b1ac2595281ee0752aa895835fe49_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-f42b1ac2595281ee0752aa895835fe49_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f42b1ac2595281ee0752aa895835fe49_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-f42b1ac2595281ee0752aa895835fe49_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f42b1ac2595281ee0752aa895835fe49_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;除了 TiDB 的 NewSQL 特性之外，我们选择 TiDB 的另一个主要原因，就是 TiDB 是一个开源的项目，而且社区很活跃，版本迭代快速，我们觉得这是一个很好的模式，而且微众本身也是非常拥抱开源的。&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-922315545d859baa3bab69e22d8b8339_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;529&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-922315545d859baa3bab69e22d8b8339_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-922315545d859baa3bab69e22d8b8339_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;529&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-922315545d859baa3bab69e22d8b8339_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-922315545d859baa3bab69e22d8b8339_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这里展示了 TiDB 的基本架构模型，非常简洁优雅的架构，相信稍微了解 TiDB 的同学，对这个架构都比较熟悉了，在这里就不再赘述。当然，现在 TiDB 3.0 版本有了新的特性以及模块加入，比如 Titan 引擎， 针对 RocksDB 大 Value 写放大问题做了很大的优化和性能提升，再比如列式存储引擎 TiFlash ，为实现真正的 HTAP 奠定了基础。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 对 TiDB 的评估与测试&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1b8c36745b009984b2fed25afbc6ea0d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-1b8c36745b009984b2fed25afbc6ea0d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-1b8c36745b009984b2fed25afbc6ea0d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-1b8c36745b009984b2fed25afbc6ea0d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-1b8c36745b009984b2fed25afbc6ea0d_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们对 TiDB 做了一些评估和测试，对语法和 DDL、负载均衡、一致性、扩容等特性都做了很多测试。下面重点介绍以下 3 点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;支持在线 DDL，不锁表，对业务无影响。&lt;/b&gt;这个特性对我们来说，有很大的好处。因为在 MySQL 里面做 DDL，是风险比较大或者说比较重的一个操作，特别是一些超大的表的 DDL；但在 TiDB 里，这个操作变得轻量而安全。&lt;/li&gt;&lt;li&gt;&lt;b&gt;TiDB 采用乐观锁事务模型&lt;/b&gt;，这和 MySQL 的悲观锁模型是不太一样的，这个特性对于某些业务场景的兼容性可能会有问题。&lt;b&gt;TiDB 3.0 版本中已经试验性支持了悲观锁&lt;/b&gt;，并且在今年下半年有望成为一个正式功能，这是一个很好的消息。在金融场景里面悲观锁应用还是比较广泛的。&lt;/li&gt;&lt;li&gt;&lt;b&gt;支持同城 IDC 部署与切换，通过了真实的 IDC 隔离故障演练&lt;/b&gt;。我们将 TiDB，TiKV，PD 节点跨 3 个同城机房部署，然后把其中一个机房的网络全部隔离，来测试 TiDB 的可用性，包括 Raft Group 的 Leader 的切换等等，测试结果整体符合预期。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. TiDB 在微众的部署模型&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a67ebc8429d674fa9dc3a8821beeb3af_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-a67ebc8429d674fa9dc3a8821beeb3af_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a67ebc8429d674fa9dc3a8821beeb3af_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-a67ebc8429d674fa9dc3a8821beeb3af_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-a67ebc8429d674fa9dc3a8821beeb3af_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图 8 是 TiDB 在微众银行的部署模型，从一开始就选择了同城三机房部署，也就是位于最底层的存储层 TiKV 跨 3 个机房，3 个副本分布在 3 个机房，并且每个机房有 1 套独立的 TiDB Server  集群负责接入与计算；PD 模块也是跨 3 个机房部署。另外，针对比较重要的业务，我们会在容灾 IDC 挂一个容灾 TiDB 集群，这个容灾 TiDB 集群会通过 TiDB Binlog 工具从生产集群实时同步数据。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、微众银行 TiDB 业务实践&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB 在微众银行的应用场景包括 OLAP、OLTP 及部分混合场景，大部分场景在 TB 级别的业务数据规模。下面详细介绍贷款核心批量系统在测试 TiDB 过程中的实践和优化，以及数据存证系统 TiDB 迁移实践。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 贷款核心批量场景下的 TiDB 实践&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c054c24d41a54e7ac76fdca7411621cc_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-c054c24d41a54e7ac76fdca7411621cc_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c054c24d41a54e7ac76fdca7411621cc_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-c054c24d41a54e7ac76fdca7411621cc_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-c054c24d41a54e7ac76fdca7411621cc_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这个业务场景的特殊性在于每天晚上 0 点之后，需要通过线上数据库生成数亿级别的批量数据，并进行一系列相关计算，然后 ETL 到大数据平台去，其中涉及大量的增删查改操作，并且对总时效要求很高，必须在两个小时内跑完，不能出现任何问题。&lt;b&gt;存在的问题：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;跑批的时间过长，接近 2 小时，而且业务规模还在扩大。&lt;/li&gt;&lt;li&gt;分散于各个 DCN 跑批，然后进行数据汇总，架构比较复杂。&lt;/li&gt;&lt;li&gt;受限于 MySQL 主备复制的性能，无法再增加并发，跑批的时间没有办法再缩短，否则会影响联机系统可用性。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;所以我们尝试通过 TiDB 来承载这个批量场景，把每天的批量数据，汇总到一个大的 TiDB 集群中，再进行跑批，最后再 ETL 到大数据平台中去做处理。整个流程如图 9 右半部分所示，其中 “DM 工具”即 TiDB DM（TiDB Data Migration），是由 PingCAP 开发的一体化数据同步任务管理平台，支持从 MySQL 或 MariaDB 到 TiDB 的全量数据迁移和增量数据同步。&lt;b&gt;我们对应用侧和 TiDB 2.1 版本进行了一系列的调优，整体的优化效果达到预期，批量的耗时缩短了 45% 左右。我们同时也测试了 3.0 beta 版本，3.0 相对于 2.1 版本，整体批量耗时又缩短了 30% 左右。整体来看，TiDB 让我们的贷款核心批量场景下效率得到大幅度的提升。 &lt;/b&gt;在整个业务测试的过程中。我们在应用侧和数据库侧，都做了大量优化，也踩了不少坑，这里也分享几点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;a. 数据导入过程 Region 热点集中&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-af036823ebba8bd50089e355419fdf95_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-af036823ebba8bd50089e355419fdf95_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-af036823ebba8bd50089e355419fdf95_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-af036823ebba8bd50089e355419fdf95_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-af036823ebba8bd50089e355419fdf95_b.jpg&quot;/&gt;&lt;figcaption&gt;图 10&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;该业务对批量数据导入的时间很敏感，但我们测试时发现虽然底层有 6 个 TiKV 节点，但每次数据开始导入时有 3 个 TiKV 节点负载特别高，另外 3 个节点负载很低，同时写入也有瓶颈。通过排查发现这个问题的原因在于，对于快速的超大表的数据写入，TiKV 的热点调度并不及时，没有办法做到负载均衡，进而导致热点。我们和 PingCAP 伙伴们讨论解决方案后，增加了 Region 预打散的功能。就是在建表时，就对表进行 Region 打散操作 ，相当于一个空表就分散成多个 Region 分布在 6 个 TiKV 节点上，当数据导入的时候就直接写入各个 Region。从图 10 可以看到增加预打散功能后，6 台 TiKV 的负载非常均衡，并且耗时也变短了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;b. 无效 Region 过多导致系统变慢&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-41052d621941c1b3315aedead4153f4b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-41052d621941c1b3315aedead4153f4b_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-41052d621941c1b3315aedead4153f4b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-41052d621941c1b3315aedead4153f4b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-41052d621941c1b3315aedead4153f4b_b.jpg&quot;/&gt;&lt;figcaption&gt;图 11&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;另外一个遇到问题就是无效 Region 过多的问题。前面提到，该业务数据在每天跑批完成之后需要删掉，第二天全部数据需要重新生成，所以该场景下每天都有大量的数据表删除和重建，会累积大量无效 Region，导致 PD 元数据管理压力过大，Region 副本之间的心跳也会大量增加 grpc 调用，导致整个系统运行比较慢。所以我们后来灰度上线了 Region merge 功能，这个功能在 TiDB 2.1.8 以后的版本中（含 3.0 GA）引入，&lt;b&gt;从 图 11 可以看到上线 Region merge 功能之后，Region 数量直线下降， 这个功能让系统性能的提升提升了 30% 左右。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 数据存证系统 TiDB 迁移实践&lt;/b&gt;&lt;/p&gt;&lt;p&gt;数据存证系统是微众银行非常重要的系统，存储了具有法律效力的证据类数据，这些数据对客户来说是非常重要的。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d80a2e38376ad893e3ac8c3ae31993e1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-d80a2e38376ad893e3ac8c3ae31993e1_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d80a2e38376ad893e3ac8c3ae31993e1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-d80a2e38376ad893e3ac8c3ae31993e1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-d80a2e38376ad893e3ac8c3ae31993e1_b.jpg&quot;/&gt;&lt;figcaption&gt;图 12&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;随着越来越多的业务系统的接入，该场景的数据增长速度非常快，比如每一次转帐都需要生成一个证据，并且不是简单的一条记录，而是发生纠纷时法院认可的证据，所以也决定了这些数据不能删除。这些数据划分在 ADM 区，没办法做横向扩展，遇到了很大的瓶颈。基于这些场景特点，微众选择了 TiDB 的解决方案。我们有几个基本的迁移原则：1）数据不能错、不能丢；2）服务敏感度非常高，需要尽量无缝切换到 TiDB 架构上；3）因为是比较严肃的金融场景，如果在迁移过程中有任何困难，我们期望能够随时回切到 MySQL。  &lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4a12875fa5f9b3769c2e8b69e57c931a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;529&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-4a12875fa5f9b3769c2e8b69e57c931a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4a12875fa5f9b3769c2e8b69e57c931a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;529&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-4a12875fa5f9b3769c2e8b69e57c931a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-4a12875fa5f9b3769c2e8b69e57c931a_b.jpg&quot;/&gt;&lt;figcaption&gt;图 13&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;迁移整体方案如图 13，步骤流程比较长，但会更加安全。接下来介绍每个步骤中我们碰到的困难和解决方案。第一个步骤是前置检查。首先表必须有主键，如果是短时间海量连续写入，不建议用自增 ID，可以把自增 ID 改成由雪花算法生成，再把雪花算法的时间戳后几位提到最前面，这样可以保证主键足够随机 ，然后使用我们之前提到的 Split Region 的功能，提前把 Region 切分，并打散到所有的 TiKV 节点里，这样可以在写入的时候实现负载均衡，解决短时大量写入瓶颈问题。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-451da45c7e9aae68f536116a6cd04526_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-451da45c7e9aae68f536116a6cd04526_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-451da45c7e9aae68f536116a6cd04526_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-451da45c7e9aae68f536116a6cd04526_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-451da45c7e9aae68f536116a6cd04526_b.jpg&quot;/&gt;&lt;figcaption&gt;图 14&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;触发器、存储过程、视图、function 这些特性在我们行内是禁止使用的，所以对我们是没有影响的。整体来看，前置检查这一步我们重点关注的是性能问题，尤其是保证写的性能，该场景是大批量数据，短时间的数亿数据写入性能的瓶颈问题还是值得关注并解决的。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aa98e127f4b04a93f25de1224b25e442_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-aa98e127f4b04a93f25de1224b25e442_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aa98e127f4b04a93f25de1224b25e442_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-aa98e127f4b04a93f25de1224b25e442_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-aa98e127f4b04a93f25de1224b25e442_b.jpg&quot;/&gt;&lt;figcaption&gt;图 15&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;前置检查完成后，接下来就是将数据同步到 TiDB，PingCAP 提供了实时同步工具 TiDB DM，在简单配置之后可以“一键式”将 MySQL 中的数据导入 TiDB，让数据迁移变得非常便捷。当然，我们也遇到了几点问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;DM 不能保证高可用的问题。&lt;/b&gt;我们和 PingCAP 研发同学们讨论之后，临时解决方案是部署两个 dm-worker（冗余部署），一旦某个 dm-worker 发生问题，就启动另外一个 dm-worker，从下游记录的 pos 点继续同步数据。另外，我们非常期待未来 DM 整合到 TiDB 的 K8s 部署生态工具链中， 再配合云盘（比如 ceph）做状态信息的存档 ，这样会更加完善 DM 的高可用性，我们也深度参与了 PingCAP 研发同学们关于 DM 高可用方案的设计讨论。&lt;/li&gt;&lt;li&gt;&lt;b&gt;上游故障需要人工切换的问题。&lt;/b&gt;因为目前“一主多备”架构下，我们把 DM 挂载在其中一台备机，如果这台备机由于服务器故障原因导致宕机，就需要人工把 DM 挂载其他正常的备机，处理时效上会没那么及时；非常期待未来 DM 能够把这个切换操作做成自动化。&lt;/li&gt;&lt;li&gt;&lt;b&gt;从 DM 角度看， 表必须要有主键。&lt;/b&gt;一方面，DM 回放 binlog 时需要做并发处理，但是处理之前会做冲突检测，如果没有主键就做不了冲突检测，也就不能做并发回放，导致同步效率比较差。另一方面，幂等操作，比如 DM task 重启或者恢复，会从下游记录的 pos 点继续同步数据，但因为 pos 点不是实时记录，所以会导致重复回放 binlog，如果没有主键，比如重跑两次 insert，数据就重复写入了。因此就要求表必须有主键，DM task 重启或者恢复的时候，DM 内部做一个幂等转换，比如把 Insert 转换成 replace ，把 update 转换成 delete+replace，这样的话就算重跑很多次，它的结果是不会受影响的。&lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-505b66d0555b7cc8492ae3e3ee10a52e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-505b66d0555b7cc8492ae3e3ee10a52e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-505b66d0555b7cc8492ae3e3ee10a52e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-505b66d0555b7cc8492ae3e3ee10a52e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-505b66d0555b7cc8492ae3e3ee10a52e_b.jpg&quot;/&gt;&lt;figcaption&gt;图 16&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;作为一个金融场景，尤其是异构的数据同步，数据校验是一个非常严肃的确认过程。熟悉 MySQL 的同学应该了解过 pt-table-checksum 工具，它的原理和 PingCAP 提供的数据校验功能类似，将这个数据切片之后，对数据切片进行 checksum 计算，然后比对上下游所有切片的 checksum 值是否一样来判断数据一致性；但是它当前还做不到类似 pt-table-checksum 的在线校验，如果上游 MySQL 的数据一直在变，就没办法做校验了。另外，Chunk 切分偶尔不准、上下游排序规则不一致，这两个问题已经在新版本有了优化。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-684ec7f252bd84c4a69d9350ee58bf98_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-684ec7f252bd84c4a69d9350ee58bf98_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-684ec7f252bd84c4a69d9350ee58bf98_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-684ec7f252bd84c4a69d9350ee58bf98_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-684ec7f252bd84c4a69d9350ee58bf98_b.jpg&quot;/&gt;&lt;figcaption&gt;图 17&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;接下来是灰度切读流量的过程。基于安全性考虑，我们先把非关键的读流量灰度切换到 TiDB 中去，观察一段时间，再把关键的读流量也逐渐切换到 TiDB。当时遇到了执行计划不准的问题，导致把读流量切换到 TiDB 后发现，有些 SQL 查询变慢了，&lt;b&gt;这个问题在新版本中已经解决了，包括 TiDB 3.0 中也有执行计划绑定（Plan Management）、增量统计信息更新等优化。&lt;/b&gt;实际上，执行计划不准的问题在 MySQL 等一些数据库产品中比较普遍，因为统计信息不能 100% 实时更新。以前使用 MySQL 产品时，用户可能需要强制指定某个索引 Index，这个操作对业务侵入性很大，而基于上面两个功能，TiDB 在这点上对业务的侵入是很少的。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eca0e9ea329f6aefc2b3590b329f10e1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;601&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-eca0e9ea329f6aefc2b3590b329f10e1_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-eca0e9ea329f6aefc2b3590b329f10e1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;601&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-eca0e9ea329f6aefc2b3590b329f10e1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-eca0e9ea329f6aefc2b3590b329f10e1_b.jpg&quot;/&gt;&lt;figcaption&gt;图 18&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在读流量切换到 TiDB 没有出现什么问题之后，我们再把写流量切换到 TiDB，但也不是一次性切换，我们选择先双写 TiDB 和 MySQL：先写 MySQL 返回自增 ID，再用该 ID 加上业务数据异步写入到 TiDB；上下游的 ID 保存一致方便我们进行数据校验。在双写改造完成后，架构如图 18 所示。应用准备发版时，为了保证业务暂停的时间足够短，我们临时调大了消息队列 MQ 的长度，因为在整个应用关闭之后，消息队列仍在存储消息，可能会把消息队列存满。调大消息队列长度之后，再逐个关闭应用，等到所有应用都停掉后，在确认 DM 的数据同步已经追平后，就可以把 DM 断开，接下来就可以逐个启动新版本的应用了。&lt;b&gt;业务停止时间（最后一个应用关闭到新版本第一个应用启动的时间间隔）控制在 1 分钟以内，对应用的影响非常小。&lt;/b&gt;到这一步骤为止，其实整个服务读写都采用了 TiDB，但为了保证数据出现问题时能够及时回迁，于是我们把灰度上线的的周期拉长，使用 TiDB Binlog 把 TiDB 中的数据反向同步到 MySQL，如下图所示。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b8b4398ec6d3e46060c65334e48ffbe0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-b8b4398ec6d3e46060c65334e48ffbe0_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b8b4398ec6d3e46060c65334e48ffbe0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-b8b4398ec6d3e46060c65334e48ffbe0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-b8b4398ec6d3e46060c65334e48ffbe0_b.jpg&quot;/&gt;&lt;figcaption&gt;图 19&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们观察到 Drainer 与同城 IDC 的下游 MySQL 部署在一起，RPC 延迟会更短，性能会更好。在几个月之后，我们最终把反向同步关闭了，完全由 TiDB 提供服务。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-51282ea928c3dda598e570f20ecf8c2c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-51282ea928c3dda598e570f20ecf8c2c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-51282ea928c3dda598e570f20ecf8c2c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-51282ea928c3dda598e570f20ecf8c2c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-51282ea928c3dda598e570f20ecf8c2c_b.jpg&quot;/&gt;&lt;figcaption&gt;图 20&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如图 20 所示，我们还会做例行的数据备份的操作，包括使用 mysqldump 每周全量备份，使用 drainer pb 每 5 分钟备份增量 binlog，另外数据备份最好使用单独的 tidb-server  节点，对联机的请求影响最小。&lt;b&gt;在观察一段时间之后，观察到各方面的性能指标都非常稳定，然后决定将反向同步 MySQL 断掉，也就意味着数据存证系统这样一个非常重要的系统，完全跑在了 TiDB 上，回顾整个迁移过程，还是比较流畅且顺利的。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四、总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 是一个很优秀的分布式关系型数据库产品。对银行场景来说，灰度和上线的节奏没有互联网行业那么快，随着 TiDB 产品的日趋成熟，我们正在更多适合的场景试用 TiDB，也会有更多的经验和大家分享。&lt;/p&gt;&lt;p&gt;&lt;i&gt;本文根据胡盼盼、黄蔚在 TiDB TechDay 2019 北京站及深圳站上的演讲整理。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多精彩案例：&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;案例 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-08-01-76239450</guid>
<pubDate>Thu, 01 Aug 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>云上 TiDB 管理「利器」，TiDB Operator 1.0 GA 发布</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-31-75938523.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/75938523&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d8be185a91732ade72d5b868935fb470_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0f4fbcac2a5a4bb85c889b91034a897a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;412&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-0f4fbcac2a5a4bb85c889b91034a897a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0f4fbcac2a5a4bb85c889b91034a897a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;412&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-0f4fbcac2a5a4bb85c889b91034a897a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-0f4fbcac2a5a4bb85c889b91034a897a_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;去年八月份，我们 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-operator-introduction/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;开源了 TiDB Operator&lt;/a&gt; 项目，以实现 TiDB 在 Kubernetes 上的部署和运维。开源后到现在的近一年内，我们一方面基于用户反馈不断打磨项目的易用性，另一方面通过严苛的稳定性测试持续提升可靠性。今天，我们自豪地宣布 TiDB Operator 1.0 GA 正式发布！&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-10e41092848f59c6ac15a0733d3786b3_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-10e41092848f59c6ac15a0733d3786b3_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-10e41092848f59c6ac15a0733d3786b3_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-10e41092848f59c6ac15a0733d3786b3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-10e41092848f59c6ac15a0733d3786b3_b.jpg&quot;/&gt;&lt;figcaption&gt;TiDB Operator architecture&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;TiDB Operator 是 Kubernetes 上的 TiDB 集群自动运维系统。提供包括部署、升级、扩缩容、备份恢复、配置变更的 TiDB 全生命周期管理。借助 TiDB Operator，TiDB 可以无缝运行在公有云或私有部署的 Kubernetes 集群上。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;1.0 是 TiDB Operator 的首个 GA 版本，具备以下核心亮点。&lt;/p&gt;&lt;h2&gt;核心亮点&lt;/h2&gt;&lt;h3&gt;1. 简化 TiDB 运维管理&lt;/h3&gt;&lt;p&gt;TiDB 是一个复杂的分布式系统，它的部署和运维需要比较深入的领域知识，这带来了颇高的学习成本和负担。TiDB Operator 则通过自定义资源对象（Custom Resource）、自定义控制器（Custom controller）和调度器扩展（Scheduler extender）为 Kubernetes 注入 TiDB 的专业运维知识，允许用户以 Kubernetes 的声明式 API 风格来管理 TiDB 集群。具体来说，用户只需要描述集群规格，TiDB Operator 就会不断调整 Kubernetes 中的资源，驱动实际集群满足该描述。&lt;b&gt;在这种模式下，TiDB 集群会自动完成服务的健康检查、故障转移，而部署、升级、扩缩容等操作则能通过修改集群的规格定义“一键”完成，极大简化了 TiDB 集群的运维管理。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;更重要的是，标准化的集群管理 API 允许用户完成内部工具链或 PaaS 平台与 TiDB 集群管理的深度整合，真正赋能用户玩转 TiDB。&lt;/p&gt;&lt;h3&gt;2. 稳定可靠&lt;/h3&gt;&lt;p&gt;作为数据库，TiDB 往往处于整个系统架构中的最核心位置，对于稳定性有着严苛要求。这同样也是对 TiDB Operator 的要求。为了确保所有自动化运维操作的稳定可靠，我们为 TiDB Operator 专门设计了稳定性测试，在施加较大读写负载的同时，不断进行各类运维操作并模拟主机、容器、磁盘、网络、Kubernetes 组件和 TiDB Operator 组件的各类故障，观察在这些场景下 TiDB Operator 的行为是否符合预期。通过 7 * 24 小时不间断运行稳定性测试，我们发现并修复了诸多极端的边界情况。在 1.0 发布前，TiDB Operator 稳定性测试已经稳定运行数月。&lt;/p&gt;&lt;h3&gt;3. 多云支持&lt;/h3&gt;&lt;p&gt;&lt;b&gt;1.0 提供了面向 AWS、谷歌云和阿里云的 Terraform 部署脚本。&lt;/b&gt; 这些脚本能帮助大家在十几分钟内创建一个 Kubernetes 集群，并在该集群上部署一个或更多生产可用的 TiDB 集群。在后续的管理过程中，Terraform 脚本会在操作 TiDB 集群的同时对相关的云资源进行操作。比如，当扩容一个 TiDB 集群时，Terraform 脚本就会自动创建更多的云服务器来承载集群扩容后的资源需求。&lt;/p&gt;&lt;h2&gt;体验 TiDB Operator&lt;/h2&gt;&lt;p&gt;大家可以通过 Terraform 在 AWS（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/how-to/deploy/tidb-in-kubernetes/aws-eks/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;部署文档&lt;/a&gt;）、谷歌云（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/how-to/deploy/tidb-in-kubernetes/gcp-gke/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;部署文档&lt;/a&gt;）、阿里云（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/how-to/deploy/tidb-in-kubernetes/alibaba-cloud/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;部署文档&lt;/a&gt;）上快速部署 TiDB Operator 以及下属的 TiDB 集群，也可以参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/how-to/deploy/tidb-in-kubernetes/general-kubernetes/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;通用 Kubernetes 部署文档&lt;/a&gt; 在任何 Kubernetes 集群上部署并体验 TiDB Operator。&lt;/p&gt;&lt;p&gt;对于 Pre GA 版本的用户，请参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator/blob/master/CHANGELOG.md&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;1.0 Release Note&lt;/a&gt; 了解 1.0 的变更内容和升级指南。&lt;/p&gt;&lt;h2&gt;致谢&lt;/h2&gt;&lt;p&gt;感谢所有 TiDB Operator 的贡献者（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator/graphs/contributors&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-operator/graphs/contributors&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），1.0 能够走到 GA 离不开每一位贡献者的努力！&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后欢迎大家为 TiDB Operator&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator/issues&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;提交 issue&lt;/a&gt;&lt;/b&gt; &lt;b&gt;或参考&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator/blob/master/docs/CONTRIBUTING.md&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;贡献文档&lt;/a&gt;开始提交代码，TiDB Operator 期待大家的参与和反馈！&lt;/b&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-31-75938523</guid>
<pubDate>Wed, 31 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 源码解析系列文章（十一）Storage - 事务控制层</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-29-75708576.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/75708576&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1d12480f0214931303d33ba3ad7a94f2_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：张金鹏&lt;/p&gt;&lt;h2&gt;背景知识&lt;/h2&gt;&lt;p&gt;TiKV 是一个强一致的支持事务的分布式 KV 存储。TiKV 通过 raft 来保证多副本之间的强一致，事务这块 TiKV 参考了 Google 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//ai.google/research/pubs/pub36726&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Percolator 事务模型&lt;/a&gt;，并进行了一些优化。&lt;/p&gt;&lt;p&gt;当 TiKV 的 Service 层收到请求之后，会根据请求的类型把这些请求转发到不同的模块进行处理。对于从 TiDB 下推的读请求，比如 sum，avg 操作，会转发到 Coprocessor 模块进行处理，对于 KV 请求会直接转发到 Storage 进行处理。&lt;/p&gt;&lt;p&gt;KV 操作根据功能可以被划分为 Raw KV 操作以及 Txn KV 操作两大类。Raw KV 操作包括 raw put、raw get、raw delete、raw batch get、raw batch put、raw batch delete、raw scan 等普通 KV 操作。 Txn KV 操作是为了实现事务机制而设计的一系列操作，如 prewrite 和 commit 分别对应于 2PC 中的 prepare 和 commit 阶段的操作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文将为大家介绍 TiKV 源码中的 Storage 模块，它位于 Service 与底层 KV 存储引擎之间，主要负责事务的并发控制。TiKV 端事务相关的实现都在 Storage 模块中。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;源码解析&lt;/h2&gt;&lt;p&gt;接下来我们将从 Engine、Latches、Scheduler 和 MVCC 等几个方面来讲解 Storage 相关的源码。&lt;/p&gt;&lt;h3&gt;1. Engine trait&lt;/h3&gt;&lt;p&gt;TiKV 把底层 KV 存储引擎抽象成一个 Engine trait（trait 类似其他语言的 interface），定义见 &lt;code&gt;storage/kv/mod.rs&lt;/code&gt;。Engint trait 主要提供了读和写两个接口，分别为 &lt;code&gt;async_snapshot&lt;/code&gt; 和 &lt;code&gt;async_write&lt;/code&gt;。调用者把要写的内容交给 &lt;code&gt;async_write&lt;/code&gt;，&lt;code&gt;async_write&lt;/code&gt; 通过回调的方式告诉调用者写操作成功完成了或者遇到错误了。同样的，&lt;code&gt;async_snapshot&lt;/code&gt;通过回调的方式把数据库的快照返回给调用者，供调用者读，或者把遇到的错误返回给调用者。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub trait Engine: Send + Clone + &amp;#39;static {
    type Snap: Snapshot;
    fn async_write(&amp;amp;self, ctx: &amp;amp;Contect, batch: Vec&amp;lt;Modify&amp;gt;, callback: Callback&amp;lt;()&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt;;
    fn async_snapshot(&amp;amp;self, ctx: &amp;amp;Context, callback: Callback&amp;lt;Self::Snap&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;只要实现了以上两个接口，都可以作为 TiKV 的底层 KV 存储引擎。在 3.0 版本中，TiKV 支持了三种不同的 KV 存储引擎，包括单机 RocksDB 引擎、内存 B 树引擎和 RaftKV 引擎，分别位于 &lt;code&gt;storage/kv&lt;/code&gt; 文件夹下面的 &lt;code&gt;rocksdb_engine.rs&lt;/code&gt;、&lt;code&gt;btree_engine.rs&lt;/code&gt; 和 &lt;code&gt;raftkv.rs&lt;/code&gt;。其中单机 RocksDB 引擎和内存红黑树引擎主要用于单元测试和分层 benchmark，TiKV 真正使用的是 RaftKV 引擎。当调用 RaftKV 的 &lt;code&gt;async_write&lt;/code&gt; 进行写入操作时，如果 &lt;code&gt;async_write&lt;/code&gt; 通过回调方式成功返回了，说明写入操作已经通过 raft 复制给了大多数副本，并且在 leader 节点（调用者所在 TiKV）完成写入了，后续 leader 节点上的读就能够看到之前写入的内容。&lt;/p&gt;&lt;h3&gt;2. Raw KV 执行流程&lt;/h3&gt;&lt;p&gt;Raw KV 系列接口是绕过事务直接操纵底层数据的接口，没有事务控制，比较简单，所以在介绍更复杂的事务 KV 的执行流程前，我们先介绍 Raw KV 的执行流程。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Raw put&lt;/b&gt;&lt;/p&gt;&lt;p&gt;raw put 操作不需要 Storage 模块做额外的工作，直接把要写的内容通过 engine 的 &lt;code&gt;async_write&lt;/code&gt; 接口发送给底层的 KV 存储引擎就好了。调用堆栈为 &lt;code&gt;service/kv.rs: raw_put&lt;/code&gt; -&amp;gt; &lt;code&gt;storage/mod.rs: async_raw_put&lt;/code&gt;。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;impl&amp;lt;E: Engine&amp;gt; Storage&amp;lt;E&amp;gt; {
    pub fn async_raw_put(
        &amp;amp;self,
        ctx: Context,
        cf: String,
        key: Vec&amp;lt;u8&amp;gt;,
        value: Vec&amp;lt;u8&amp;gt;,
        callback: Callback&amp;lt;()&amp;gt;,
    ) -&amp;gt; Result&amp;lt;()&amp;gt; {
        // Omit some limit checks about key and value here...
        self.engine.async_write(
            &amp;amp;ctx,
            vec![Modify::Put(
                Self::rawkv_cf(&amp;amp;cf),
                Key::from_encoded(key),
                value,
            )],
            Box::new(|(_, res)| callback(res.map_err(Error::from))),
        )?;
        Ok(())
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;Raw get&lt;/b&gt;&lt;/p&gt;&lt;p&gt;同样的，raw get 只需要调用 engine 的 &lt;code&gt;async_snapshot&lt;/code&gt; 拿到数据库快照，然后直接读取就可以了。当然对于 RaftKV 引擎，&lt;code&gt;async_snapshot&lt;/code&gt; 在返回数据库快照之前会做一些检查工作，比如会检查当前访问的副本是否是 leader（3.0.0 版本只支持从 leader 进行读操作，follower read 目前仍然在开发中），另外也会检查请求中携带的 region 版本信息是否足够新。&lt;/p&gt;&lt;h3&gt;3. Latches&lt;/h3&gt;&lt;p&gt;在事务模式下，为了防止多个请求同时对同一个 key 进行写操作，请求在写这个 key 之前必须先获取这个 key 的内存锁。为了和事务中的锁进行区分，我们称这个内存锁为 latch，对应的是 &lt;code&gt;storage/txn/latch.rs&lt;/code&gt; 文件中的 Latch 结构体。每个 Latch 内部包含一个等待队列，没有拿到 latch 的请求按先后顺序插入到等待队列中，队首的请求被认为拿到了该 latch。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;#[derive(Clone)]
struct Latch {
    pub waiting: VecDeque&amp;lt;u64&amp;gt;,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Latches 是一个包含多个 Latch 的结构体，内部包含一个固定长度的 Vector，Vector 的每个 slot 对应一个 Latch。默认配置下 Latches 内部 Vector 的长度为 2048000。每个 TiKV 有且仅有一个 Latches 实例，位于 &lt;code&gt;Storage.Scheduler&lt;/code&gt; 中。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct Latches {
    slots: Vec&amp;lt;Mutex&amp;lt;Latch&amp;gt;&amp;gt;,
    size: usize,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Latches 的 &lt;code&gt;gen_lock&lt;/code&gt; 接口用于计算写入请求执行前所需要获取的所有 latch。&lt;code&gt;gen_lock&lt;/code&gt; 通过计算所有 key 的 hash，然后用这些 hash 对 Vector 的长度进行取模得到多个 slots，对这些 slots 经过排序去重得到该命令需要的所有 latch。这个过程中的排序是为了保证获取 latch 的顺序性防止出现死锁情况。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;impl Latches {
    pub fn gen_lock&amp;lt;H: Hash&amp;gt;(&amp;amp;self, keys: &amp;amp;[H]) -&amp;gt; Lock {
        // prevent from deadlock, so we sort and deduplicate the index.
        let mut slots: Vec&amp;lt;usize&amp;gt; = keys.iter().map(|x|
        self.calc_slot(x)).collect();
        slots.sort();
        slots.dedup();
        Lock::new(slots)
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3&gt;4. Storage 和事务调度器 Scheduler&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Storage&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Storage 定义在 &lt;code&gt;storage/mod.rs&lt;/code&gt; 文件中，下面我们介绍下 Storage 几个重要的成员：&lt;/p&gt;&lt;p&gt;&lt;code&gt;engine&lt;/code&gt;：代表的是底层的 KV 存储引擎。&lt;/p&gt;&lt;p&gt;&lt;code&gt;sched&lt;/code&gt;：事务调度器，负责并发事务请求的调度工作。&lt;/p&gt;&lt;p&gt;&lt;code&gt;read_pool&lt;/code&gt;：读取线程池，所有只读 KV 请求，包括事务的非事务的，如 raw get、txn kv get 等最终都会在这个线程池内执行。由于只读请求不需要获取 latches，所以为其分配一个独立的线程池直接执行，而不是与非只读事务共用事务调度器。&lt;/p&gt;&lt;p&gt;&lt;code&gt;gc_worker&lt;/code&gt;：从 3.0 版本开始，TiKV 支持分布式 GC，每个 TiKV 有一个 &lt;code&gt;gc_worker&lt;/code&gt; 线程负责定期从 PD 更新 safepoint，然后进行 GC 工作。&lt;/p&gt;&lt;p&gt;&lt;code&gt;pessimistic_txn_enabled&lt;/code&gt;： 另外 3.0 版本也支持悲观事务，&lt;code&gt;pessimistic_txn_enabled&lt;/code&gt; 为 true 表示 TiKV 以支持悲观事务的模式启动，关于悲观事务后续会有一篇源码阅读文章专门介绍，这里我们先跳过。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct Storage&amp;lt;E: Engine&amp;gt; {
    engine: E,
    sched: Scheduler&amp;lt;E&amp;gt;,
    read_pool: ReadPool,
    gc_worker: GCWorker&amp;lt;E&amp;gt;,
    pessimistic_txn_enabled: bool,
    // Other fields...
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;对于只读请求，包括 txn get 和 txn scan，Storage 调用 engine 的 &lt;code&gt;async_snapshot&lt;/code&gt; 获取数据库快照之后交给 &lt;code&gt;read_pool&lt;/code&gt;线程池进行处理。写入请求，包括 prewrite、commit、rollback 等，直接交给 Scheduler 进行处理。Scheduler 的定义在 &lt;code&gt;storage/txn/scheduler.rs&lt;/code&gt; 中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Scheduler&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct Scheduler&amp;lt;E: Engine&amp;gt; {
    engine: Option&amp;lt;E&amp;gt;,
    inner: Arc&amp;lt;SchedulerInner&amp;gt;,
}

struct SchedulerInner {
    id_alloc, AtomicU64,
    task_contexts: Vec&amp;lt;Mutex&amp;lt;HashMap&amp;lt;u64, TaskContext&amp;gt;&amp;gt;&amp;gt;,
    lathes: Latches,
    sched_pending_write_threshold: usize,
    worker_pool: SchedPool,
    high_priority_pool: SchedPool,
    // Some other fields...
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;接下来简单介绍下 Scheduler 几个重要的成员：&lt;/p&gt;&lt;p&gt;&lt;code&gt;id_alloc&lt;/code&gt;：到达 Scheduler 的请求都会被分配一个唯一的 command id。&lt;/p&gt;&lt;p&gt;&lt;code&gt;latches&lt;/code&gt;：写请求到达 Scheduler 之后会尝试获取所需要的 latch，如果暂时获取不到所需要的 latch，其对应的 command id 会被插入到 latch 的 waiting list 里，当前面的请求执行结束后会唤醒 waiting list 里的请求继续执行，这部分逻辑我们将会在下一节 prewrite 请求在 scheduler 中的执行流程中介绍。&lt;/p&gt;&lt;p&gt;&lt;code&gt;task_contexts&lt;/code&gt;：用于存储 Scheduler 中所有请求的上下文，比如暂时未能获取所需 latch 的请求都会被暂存在 &lt;code&gt;task_contexts&lt;/code&gt; 中。&lt;/p&gt;&lt;p&gt;&lt;code&gt;sched_pending_write_threshold&lt;/code&gt;：用于统计 Scheduler 内所有写入请求的写入流量，可以通过该指标对 Scheduler 的写入操作进行流控。&lt;/p&gt;&lt;p&gt;&lt;code&gt;worker_pool&lt;/code&gt;，&lt;code&gt;high_priority_pool&lt;/code&gt;：两个线程池，写请求在调用 engine 的 async_write 之前需要进行事务约束的检验工作，这些工作都是在这个两个线程池中执行的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;prewrite 请求在 Scheduler 中的执行流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面我们以 prewrite 请求为例子来讲解下写请求在 Scheduler 中是如何处理的：&lt;/p&gt;&lt;p&gt;1）Scheduler 收到 prewrite 请求的时候首先会进行流控判断，如果 Scheduler 里的请求过多，会直接返回 &lt;code&gt;SchedTooBusy&lt;/code&gt;错误，提示等一会再发送，否则进入下一步。&lt;/p&gt;&lt;p&gt;2）接着会尝试获取所需要的 latch，如果获取 latch 成功那么直接进入下一步。如果获取 latch 失败，说明有其他请求占住了 latch，这种情况说明其他请求可能也正在对相同的 key 进行操作，那么当前 prewrite 请求会被暂时挂起来，请求的上下文会暂存在 Scheduler 的 &lt;code&gt;task_contexts&lt;/code&gt; 里面。当前面的请求执行结束之后会将该 prewrite 请求重新唤醒继续执行。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;impl&amp;lt;E: Engine&amp;gt; Scheduler&amp;lt;E&amp;gt; {
    fn try_to_wake_up(&amp;amp;self, cid: u64) {
        if self.inner.acquire_lock(cid) {
            self.get_snapshot(cid);
        }
    }
    fn release_lock(&amp;amp;self, lock: &amp;amp;Lock, cid: u64) {
        let wakeup_list = self.inner.latches.release(lock, cid);
        for wcid in wakeup_list {
            self.try_to_wake_up(wcid);
        }
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;3）获取 latch 成功之后会调用 Scheduler 的 &lt;code&gt;get_snapshot&lt;/code&gt; 接口从 engine 获取数据库的快照。&lt;code&gt;get_snapshot&lt;/code&gt; 内部实际上就是调用 engine 的 &lt;code&gt;async_snapshot&lt;/code&gt; 接口。然后把 prewrite 请求以及刚刚获取到的数据库快照交给 &lt;code&gt;worker_pool&lt;/code&gt; 进行处理。如果该 prewrite 请求优先级字段是 &lt;code&gt;high&lt;/code&gt; 就会被分发到 &lt;code&gt;high_priority_pool&lt;/code&gt; 进行处理。&lt;code&gt;high_priority_pool&lt;/code&gt; 是为了那些高优先级请求而设计的，比如 TiDB 系统内部的一些请求要求 TiKV 快速返回，不能由于 &lt;code&gt;worker_pool&lt;/code&gt; 繁忙而被卡住。需要注意的是，目前 &lt;code&gt;high_priority_pool&lt;/code&gt; 与 &lt;code&gt;worker_pool&lt;/code&gt; 仅仅是语义上不同的两个线程池，它们内部具有相同的操作系统调度优先级。&lt;/p&gt;&lt;p&gt;4）&lt;code&gt;worker_pool&lt;/code&gt; 收到 prewrite 请求之后，主要工作是从拿到的数据库快照里确认当前 prewrite 请求是否能够执行，比如是否已经有更大 ts 的事务已经对数据进行了修改，具体的细节可以参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//ai.google/research/pubs/pub36726&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Percolator 论文&lt;/a&gt;，或者参考我们的官方博客 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-transaction-model/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiKV 事务模型概览》&lt;/a&gt;。当判断 prewrite 是可以执行的，会调用 engine 的 &lt;code&gt;async_write&lt;/code&gt; 接口执行真正的写入操作。这部分的具体的代码见 &lt;code&gt;storage/txn/process.rs&lt;/code&gt; 中的 &lt;code&gt;process_write_impl&lt;/code&gt; 函数。&lt;/p&gt;&lt;p&gt;5）当 &lt;code&gt;async_write&lt;/code&gt; 执行成功或失败之后，会调用 Scheduler 的 &lt;code&gt;release_lock&lt;/code&gt; 函数来释放 latch 并且唤醒等待在这些 latch 上的请求继续执行。&lt;/p&gt;&lt;h3&gt;5. MVCC&lt;/h3&gt;&lt;p&gt;TiKV MVCC 相关的代码位于 &lt;code&gt;storage/mvcc&lt;/code&gt; 文件夹下，强烈建议大家在阅读这部分代码之前先阅读 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//ai.google/research/pubs/pub36726&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Percolator 论文&lt;/a&gt;，或者我们的官方博客 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-transaction-model/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiKV 事务模型概览》&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;MVCC 下面有两个比较关键的结构体，分别为 &lt;code&gt;MvccReader&lt;/code&gt; 和 &lt;code&gt;MvccTxn&lt;/code&gt;。&lt;code&gt;MvccReader&lt;/code&gt; 位于 &lt;code&gt;storage/mvcc/reader/reader.rs&lt;/code&gt; 文件中，它主要提供读功能，将多版本的处理细节隐藏在内部。比如 &lt;code&gt;MvccReader&lt;/code&gt; 的 &lt;code&gt;get&lt;/code&gt; 接口，传入需要读的 key 以及 ts，返回这个 ts 可以看到的版本或者返回 &lt;code&gt;key is lock&lt;/code&gt; 错误等。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;impl&amp;lt;S: Snapshot&amp;gt; MvccReader&amp;lt;S&amp;gt; {
    pub fn get(&amp;amp;mut self, key: &amp;amp;Key, mut ts: u64) -&amp;gt; Result&amp;lt;Option&amp;lt;Value&amp;gt;&amp;gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;MvccTxn&lt;/code&gt; 位于 &lt;code&gt;storage/mvcc/txn.rs&lt;/code&gt; 文件中，它主要提供写之前的事务约束检验功能，上一节 prewrite 请求的处理流程中第四步就是通过调用 &lt;code&gt;MvccTxn&lt;/code&gt; 的 prewrite 接口来进行的事务约束检验。&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;TiKV 端事务相关的实现都位于 Storage 模块中，该文带大家简单概览了下这部分几个关键的点，想了解更多细节的读者可以自行阅读这部分的源码（code talks XD）。另外从 3.0 版本开始，TiDB 和 TiKV 支持悲观事务，TiKV 端对应的代码主要位于 &lt;code&gt;storage/lock_manager&lt;/code&gt; 以及上面提到的 MVCC 模块中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读&lt;/b&gt;：&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tikv-soucre-code-reading-11/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章（十一）Storage - 事务控制层 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多 TiKV 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Blog-cns | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-29-75708576</guid>
<pubDate>Mon, 29 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>文稿分享记录：TiDB Operator 设计与实现</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-23-74897388.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/74897388&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2d703c5defe9201ab3017cfe917a6c32_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;刚刚在 DockOne 微信群做了一次文稿分享，由于临到开始前才知道分享形式是纯文稿的，所以只列了一下提纲，没有 markdown 格式的正文，就先发到专栏里（方便贴图）明天再整理到 &lt;a href=&quot;https://link.zhihu.com/?target=http%3A//aleiwu.com/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;aleiwu.com&lt;/a&gt; 上。&lt;br/&gt;全文无改动，原汁原味还原分享现场🤣（什么鬼）&lt;/blockquote&gt;&lt;h2&gt;正文&lt;/h2&gt;&lt;p&gt;大家好，我是 PingCAP 的 Cloud 工程师吴叶磊，目前在做 TiDB Operator 相关的开发工作，很高兴今天能跟大家分享一下 TiDB Operator 这个项目背后的一些东西。(自己整理自己的分享感觉好奇怪啊。。。虽然只是复制粘贴）&lt;/p&gt;&lt;p&gt;首先要说的当然是我们为什么要做 TiDB Operator，这得从 TiDB 本身的架构开始说起。下面是 TiDB 的架构图：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1198&quot; data-rawheight=&quot;742&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1198&quot; data-original=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1198&quot; data-rawheight=&quot;742&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1198&quot; data-original=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;其中，TiKV 是一套分布式的 Key-Value 存储引擎，它是整个数据库的存储层，在 TiKV 中，数据被分为一个个 Region，而一个 Region 就对应一个 Raft Group，使用 Raft 协议做 Log Replication 来保证数据的强一致性。那么自然容易想到，我们只要水平增加 TiKV 节点数，再把数据切成更多的 Region 均匀分布在这些节点上，就能实现存储层的水平扩展。&lt;/p&gt;&lt;p&gt;TiDB 则是计算执行层，负责 SQL 的解析和查询计划优化，真正执行 SQL 时则通过 TiKV 提供的 API 来访问数据。&lt;/p&gt;&lt;p&gt;最后是 PD，PD 是集群的“大脑”，它一方面是是集群的 metadata server，TiKV 中的数据分布情况都会通过心跳上报给 PD 存储起来；另一方面又承担集群数据调度的任务，我们前面说 TiKV 要把数据拆成更多的 Region 均匀分布到节点上，什么时候拆、怎么拆、拆完分配到哪些节点上这些事情就都是 PD 通过调度算法来决定的。从直观上，PD 其实有点像 Kubernetes 里的 Control Plane。&lt;/p&gt;&lt;p&gt;这么一套架构的优势是分层清晰，指责明确，每一层都可以独立地做功能扩展和规模上的水平伸缩。但是这对于运维管理来说，是一个巨大的挑战，再加上 TiDB 本身的一些比较复杂的分布式共识算法和事务算法，可以说是把整个 TiDB 的运维入门门槛拉得相当高。另一方面，传统的基于虚拟机的部署方式也不能很好地发挥 TiDB 水平伸缩和故障自动转移的潜力。所以其实我们在内部很早就在尝试使用 Kubernetes 来编排管理 TiDB 集群，甚至在这个开源的 TiDB Operator 之前，我们还有一版废弃掉的 TiDB Operator。最后的事实也确实证明我们一直以来的选择和投入是正确的，相信大家听了后面的分析，也会认同这一点。&lt;/p&gt;&lt;p&gt;接下来我们正式进入 TiDB Operator 的解读。其实 Operator 模式在 Kubernetes 社区已经不新鲜了，现在大部分流行的有状态应用都有自己的 Operator。但回顾一下 Operator 的一些概念仍然非常必要。&lt;/p&gt;&lt;p&gt;我们知道 Kubernetes 里两个很重要的概念就是声明式 API 和控制循环。所有的 API 对象都是对用户意图的记录，再由控制器去 watch 这些意图，对比实际状态，执行调谐（reconcile）操作来驱动集群达成用户意图。Kubernetes 本身有很多的内置 API 对象，比如 ReplicaSet 表达我们需要一个应用有几个实例，DaemonSet 表达我们希望在部分被选中的节点上每个节点运行且只运行一个实例。那我们该怎么向 Kubernetes 表达 “我需要一个 TiDB 集群呢“？答案就是定义一个用于描述 TiDB 集群的对象，在 Kubernetes 中，目前有两种方式可以定义一个新对象，一是 CustomResourceDefinition（CRD）、二是 Aggregation ApiServer（AA），其中 CRD 是相对简单也是目前应用比较广的方法。TiDB Operator 就用 CRD 定义了一个 ”TidbCluster” 对象。&lt;/p&gt;&lt;p&gt;有了对象还没完，这个对象现在谁都还不认识它呢。这时候就是自定义控制器出场的时候了，我们的自定义控制器叫 tidb-controller-manager，它会 watch TidbCluster 对象和其它一些相关对象，并且按照我们编写的逻辑做调谐来驱动真实的 TiDB 集群向我们定义的终态转移。&lt;/p&gt;&lt;p&gt;CRD 加上控制器就是典型的 Operator 模式了。当然这还没完，很多逻辑控制器也是无能为力的，比如 Pod 的调度逻辑。这一块为了实现 TiDB 容器的自定义调度策略，我们编写了 Scheduler Extender。还有一些验证逻辑，比如某些特殊情况下，我们要阻止集群的变更，这样的逻辑就用 Admission Webhook 来实现。而在用户侧，我们则开发了 kubectl plugin 来做 TiDB 的一些特定操作。所以大家就可以知道，TiDB Operator 其实不止于 Operator，我们的核心理念是利用 Kubernetes 大量的扩展点，为 Kubernetes 全面注入 TiDB 的领域知识，把 Kubernetes 打造成 TiDB 的一个最佳底座。&lt;/p&gt;&lt;p&gt;这样做有两大好处(划重点）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;一是在 Kubernetes 基于控制循环的自运维模式下，我们可以把 TiDB 的运维门槛降到最低，让入门用户也能轻松搞定水平伸缩和故障转移这些高级玩法；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;二是我们基于 Kubernetes 的 Restful API 提供了一套标准的集群管理 API，用户可以拿着这个 API 把 TiDB 集成到自己的工具链或 PaaS 平台中，真正赋能用户去把 TiDB 玩好玩精。&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面说了一些比较玄乎的、方法论上的东西，可能大家都觉得脚快够不着地了 。下面我们就讲一些技术干货，用一些功能场景来解析 TiDB Operator 的实现机制，更为重要的是，我们认为这里面的一些套路对于在 Kubernetes 上管理有状态应用是通用的，可能能给大家带来一些启发。&lt;/p&gt;&lt;p&gt;第一是 TiDB Operator 该怎么去构建一个 TiDB 集群。我们尝试过直接操作 Pod，最后的结论是工作量太大了，k8s 自己的控制器里处理了大量的 corner case，并且有大量的单测和 e2e 测试来保障正确性，我们要自己再去实现一遍成本很高。因此我们最后的选型是 TiDB Operator 分别为 PD、TiKV、TiDB 创建一个 StatefulSet，再去管理这些 StatefulSet 来实现优雅升级和故障转移等功能。大家也可以看到有很多社区的 Operator 都是这么做的，而且部分没有这么做的 Operator 已经开始反思了，比如 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/elastic/cloud-on-k8s/issues/1173&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/elastic/clou&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;d-on-k8s/issues/1173&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;。确实按照我们的经验，管理有状态应用的 Operator 往往做到后来发现是需要自己实现 statefulset 80%的功能的，选择直接管理裸 Pod 就有点吃力不讨好了 。🤔&lt;/p&gt;&lt;p&gt;第二个是 Local PV，大部分存储型应用对磁盘性能是相当敏感的，因此 Local PV 是一个必选项。好在现在 Kubernetes 的 Local PV 支持已经比较成熟了，也有 local-pv-provisioner 来辅助创建 Local PV。但一个很让人头大的问题是用了 Local PV 之后，Pod 就和特定节点绑死了，节点故障后要调度到其它机器必须手动删除 PVC，这其实不是编排层能解决的问题，因为本地磁盘相比于背后通常会有三副本的网络存储本身就是不可靠的，使用本地磁盘的应用必须得在应用层做数据冗余。当然，TiDB 的存储层 TiKV 本身就是多副本高可用的，这种情况下我们采取的策略是不管旧的 Pod，直接创建新 Pod 来做故障转移，利用 TiKV 本身的数据调度把数据在新 Pod 上补齐。&lt;/p&gt;&lt;p&gt;接下来就是故障转移怎么做的问题。我们知道 StatefulSet 提供的语义保证是相同名字的 Pod 集群中同时最多只有一个，也就是假如发生了节点宕机，StatefulSet 是不会帮助我们做故障转移的，因为这时候 Kubernetes 并不知道是节点宕机还是网络分区，也就是它无法确定节点上的 Pod 还在不在跑。我们假设挂掉的 Pod 叫 tikv-0，那这时候 k8s 再创建一个 tikv-0 就脑裂了。当然了，在公有云上不会有这个问题，因为公有云上 Node Controller 会通过公有云 API 检查节点是不是真的消失了，假如是的话就会移除节点，那 k8s 就知道 tikv-0 不可能再运行，可以做故障转移了。可惜一难接一难😂，故障转移之后 Pod 又会碰到找不到 Local PV 的问题而 Pending🤣……&lt;/p&gt;&lt;p&gt;我们最终的解决方案是在 Tide cluster 对象的 status 中记录当前挂掉的 Pod，这个挂掉是指一方面 k8s 认为 Pod 挂了，另一方面，TiDB 集群，也就是 PD 也认为这个实例挂了，这个非常重要，因为我们实际场景中就遇到过因为单边网络问题 apiserver 认为节点掉线而其实正常运行的，这时候 PD 就救了我们一命。我们在控制循环中专门同步这些状态，一旦两面都确认某个 Pod 以及 Pod 中的实例挂了，我们就在 status 里记录下来。而另一个扩缩容控制循环会检查这个 status，假如有挂掉的实例，就给 StatefulSet 的副本数+1，实现故障转移。示意图如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1126&quot; data-rawheight=&quot;492&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1126&quot; data-original=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1126&quot; data-rawheight=&quot;492&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1126&quot; data-original=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;大家可以看到，控制器里是结合了 k8s 的信息和 PD 的信息去更新这个 failureStore 字段。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1242&quot; data-rawheight=&quot;590&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1242&quot; data-original=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1242&quot; data-rawheight=&quot;590&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1242&quot; data-original=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;当我们确认 k8s 层面和业务层面（PD）都认为实例恢复正常后，我们就会从 failureStore 中删除对应实例，自动把实例数降下来，当然对于 PD 我们是这么做的，对于 TiKV，我们把删除 failureStore 这一步交给了用户，避免节点迁移次数过多，数据迁移太频繁影响集群性能。从这个 case 我们可以看到，在自定义控制器里糅合业务状态（来自业务，比如 TiDB 的 PD）与基础设施状态（来自 k8s）是重要且必要的。&lt;/p&gt;&lt;p&gt;第三个想说的是优雅升级，以 TiKV 为例，优雅升级就是在升级前主动逐出待升级实例上的所有 Raft Group 的 Leader，避免出现请求失败。大家可能会说这个用 preStopHook 可以做，但 preStopHook 的超时时间是一个比较难确定的东西，而且也不够灵活，我们最后选择是在 Controller 中实现，示意如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;656&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1238&quot; data-original=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;656&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1238&quot; data-original=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;大家可以看到，我们其实用 StatefulSet 的 partition 字段来控制哪些序号可以被升级到新版本，哪些要呆在旧版本。升级开始时，partition = 节点数-1&lt;b&gt;（这里分享时写错了，见后文 QA）&lt;/b&gt;，也就是所有的 Pod 都不升级，然后呢，我们会去判断下一个待升级的 Pod 上是否存在 Leader，假如存在就进行逐出，逐出之后就 return 了，因为控制循环会不断进入，所以我们就会不断检查目标 Pod 上的 leader 是否逐出完了，一旦逐出完毕，就会往下走，将 partition - 1，让 k8s 把目标 Pod 升级到新版本，这样不断循环，确保每个节点在升级前都已经清干净了 leader，做到业务完全无损。后续呢，我们希望把这个功能放到 ValidatingAdmissionWebhook 上来实现，这样呢，可以做到功能与 controller 完全正交，大大提升可维护性，具体的方案我在个人博客里也有记录 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//aleiwu.com/post/tidb-opeartor-webhook/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;aleiwu.com/post/tidb-op&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;eartor-webhook/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; (不是广告（才怪 🤪&lt;/p&gt;&lt;p&gt;我们在控制器里还有很多这样和 TiDB 本身的架构和特性深度集成的功能设计，所以大家可以看到，做一个 Operator 的前提条件是要对你要运维的系统架构做到了若指掌，甚至对源码也要有所了解。&lt;/p&gt;&lt;p&gt;说了很多的 tidb-controller-manager，最后说一下 tidb-scheduler，tidb-scheduler 其实是利用 k8s 本身的调度器扩展机制开发的，我们把 kube-scheduler 和 tidb-scheduler 打到了一个 pod 里，并且整个注册为 ”tidb-scheduler“，这样所有标记了使用该 schduler 的 pod 就能走到我们所定制的调度逻辑。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1818&quot; data-rawheight=&quot;786&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1818&quot; data-original=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1818&quot; data-rawheight=&quot;786&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1818&quot; data-original=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;这里讲一个调度策略作为例子，PD 的高可用调度，PD 里内嵌了一个 etcd，所以它是一个基于 quorum 的共识系统，需要 majority 也就是超过一半的节点存活来保证可用性，我们的调度目标就是不在一台机器上部署超过半数的 PD 节点。你可能认为用 inter-pod anti-affinity 也能实现这个需求，但其实不是这样的。&lt;/p&gt;&lt;p&gt;anti-affinity 有两种，soft 和 hard，对于 soft 的反亲和性，当无法满足反亲和时，Pod 仍会被调度到同一个节点上，而 hard 则禁止这种情况出现。我们举个一个看看反亲和性为什么不能完美满足 quorum based 的系统调度需求：&lt;/p&gt;&lt;p&gt;我们假设现在有 3 个 node，5 个 pd 实例，那么下面这样的排布是能接受的：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1056&quot; data-rawheight=&quot;470&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1056&quot; data-original=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1056&quot; data-rawheight=&quot;470&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1056&quot; data-original=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;假如我们使用 hard 的反亲和性，这个拓扑无法接受；&lt;/li&gt;&lt;li&gt;假如我们使用 soft 的 f反亲和性，假设现在其中一个节点挂了，那么 Pod 就会转移到其它节点上，这时候由于 k8s 没有 de-schedule 机制，即使我们恢复了挂掉的节点，集群拓扑也不会转移回来；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;那么 tidb-scheduler 中是怎么做的呢？策略也很简单，对于每个 Node，我们假设 Pod 调度到了目标 Node 上，再计算上面的实例数是否大于一半，假如是的话，就在 filter 阶段剔除这个 Pod。使用了这样的策略之后，大家可以推演一下，上面的拓扑是可以调度出来的，而且当节点挂掉之后，PD 实例会 Pending，不会带来一个存在风险的拓扑结构。&lt;/p&gt;&lt;p&gt;时间有限，只能分享这么多了。最后呢，是用 operator 管理有状态应用的一点点总结：&lt;/p&gt;&lt;p&gt;1.站在巨人的肩膀上，尽量复用 k8s 原生对象；&lt;/p&gt;&lt;p&gt;2.使用 local pv 必须在应用层实现数据冗余；&lt;/p&gt;&lt;p&gt;3.operator 要尽可能多地去结合业务状态，通过 apiserver 推导出的业务状态在大规模集群下未必准确；&lt;/p&gt;&lt;p&gt;4.不要只着眼于自定义控制器，k8s 的扩展点还有很多，善加利用能够大幅降低复杂度；&lt;/p&gt;&lt;p&gt;最后的最后，TiDB Operator &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-operator&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; 也即将在本月 GA 了，还有很多来不及分享的特性等着大家，欢迎大家到时候关注。&lt;/p&gt;&lt;h2&gt;QA&lt;/h2&gt;&lt;p&gt;Q1：升级开始时，partition = 节点数-1，也就是所有的 Pod 都不升级,为啥是partition = 节点数-1？&lt;/p&gt;&lt;p&gt;A：这里要纠错一下，是 pod ordinal 从 0 开始计数，大于或等于 partition 序号的 pod 会被升级 ，所以最大的序号是节点数-1，最开始的 partition 是等于节点数，分享时表达错了（我自己也记错了），抱歉😅；&lt;/p&gt;&lt;p&gt;（这里其实是我在分享时犯错了，虽然看到问题反应过来了，但还是非常尴尬，感觉自己像个沙雕）&lt;/p&gt;&lt;p&gt;Q2：还有就是驱逐leader成功了怎么防止要升级的pod重新被选为leader&lt;/p&gt;&lt;p&gt;A：我们实际上是在 PD 中提交了一个驱逐 leader 的任务，PD 会持续保证驱逐完毕后没有新 leader 进来，直到升级完毕后，由控制器移除这个任务；&lt;/p&gt;&lt;p&gt;Q3：集群规模多大？多少 pod node ?&lt;/p&gt;&lt;p&gt;A：我们在 Kubernetes 上内部测试的规模较大的集群有 100 + TiKV 节点 50+ TiDB 节点，而每位研发都会部署自己的集群进行性能测试或功能测试；&lt;/p&gt;&lt;p&gt;Q4： 请问你们实现精准下线某一个pod 的功能了嘛，因为statefulset是顺序的？如何实现的？可以分享下思路嘛？ &lt;/p&gt;&lt;p&gt;A：这个功能在 1.0 中还没有实现，我们计划在 1.1 中实现这个特性。&lt;/p&gt;&lt;p&gt;Q5：想了解下数据库容器化，推荐使用localpv吗，有没有哪些坑或最佳实践推荐？我们在考虑mysql数据库容器化以及中间件容器化，是选择localpv还是线下自建ceph集群？&lt;/p&gt;&lt;p&gt;A：Local PV 其实不是一个选项，而是一个强制因素，因为网络盘的 IOPS 是达不到在线存储应用的生产环境需求的，或者说不是说线上完全不能用，而是没法支撑对性能要求比较高的场景。MySQL 的运维我相对不是很清楚，假如 MM 能够做到双副本冗余强一致的话，那理论上就能用。大多数中间件比如 Kafka、Cassandra 都有数据冗余，这些使用 local pv 在理论上都是没问题的。&lt;/p&gt;&lt;p&gt;Q6：看你的方案感觉k8s和pd的逻辑结合在一起了，二者之间如何互通？会有代码互相侵入吗？明白了，就好像问题2驱逐问题，pd收到驱逐任务，k8s控制器不断的检查是否驱逐成功，如果成功就开始升级，对吧？&lt;/p&gt;&lt;p&gt;A：这就是自定义控制器的绝佳场景了，k8s 和 pd 本身完全没有交互，是控制循环在同步两边的状态，一方面控制循环会把 PD 记录的集群状态塞到 TidbCluster 对象的 status 里面，另一方面控制循环在将实际状态向期望状态转移时，也会生成一些 PD 的任务和操作子（Opeartor）提交到 PD 中来调谐集群状态。&lt;/p&gt;&lt;h2&gt;最后&lt;/h2&gt;&lt;p&gt;最后当然是招人啦，假如你对我们正在做的事情感兴趣，无论是 Cloud 也好数据库研发也好，都以联系 wuyelei@pingcap.com 投递简历勾搭。&lt;/p&gt;</description>
<author>吴叶磊</author>
<guid isPermaLink="false">2019-07-23-74897388</guid>
<pubDate>Tue, 23 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>DM 源码阅读系列文章（十）测试框架的实现</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-23-74873268.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/74873268&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0e7419982b6c2b2cd4a4b4f88e89c20b_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：杨非&lt;/p&gt;&lt;p&gt;本文为 DM 源码阅读系列文章的第十篇，之前的文章已经详细介绍过 DM 数据同步各组件的实现原理和代码解析，相信大家对 DM 的实现细节已经有了深入的了解。本篇文章将从质量保证的角度来介绍 DM 测试框架的设计和实现，探讨如何通过多维度的测试方法保证 DM 的正确性和稳定性。&lt;/p&gt;&lt;h2&gt;测试体系&lt;/h2&gt;&lt;p&gt;DM 完整的测试体系包括以下四个部分：&lt;/p&gt;&lt;h3&gt;1. 单元测试&lt;/h3&gt;&lt;p&gt;主要用于测试每个 go 模块和具体函数实现的正确性，测试用例编写和测试运行方式依照 go 单元测试的标准，测试代码跟随项目源代码一起发布。具体测试用例编写使用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/check&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pingcap/check&lt;/a&gt; 工具包，该工具包是在 go 原生测试工具基础上进行的扩展，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/check/blob/67f458068fc864dabf17e38d4d337f28430d13ed/run.go%23L98-L131&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;按照 suite 分组进行测试&lt;/a&gt;，提供包括更丰富的检测语法糖、并行测试、序列化测试在内的一些扩展特性。单元测试的设计出发点是白盒测试，测试用例中通过尽可能明确的测试输入得到期望的测试输出。&lt;/p&gt;&lt;h3&gt;2. 集成测试&lt;/h3&gt;&lt;p&gt;用于测试各个组件之间交互的正确性和完整数据同步流程的正确性，完整的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;测试用例集合和测试工具在项目代码的 tests 目录&lt;/a&gt; 发布。集成测试首先自定义了一些 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/_utils&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM 基础测试工具集&lt;/a&gt;，包括启动 DM 组件，生成、导入测试数据，检测同步状态、上下游数据一致性等 bash 脚本，每个测试用例是一个完整的数据同步场景，通过脚本实现数据准备、启动 DM 集群、模拟上游数据输入、特定异常和恢复、数据同步校验等测试流程。集成测试的设计出发点是确定性的模拟测试场景，为了能够确定性的模拟一些特定的同步场景，为此我们还引入了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/failpoint&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;failpoint&lt;/a&gt; 来注入测试、控制测试流程， 以及 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/tracing&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;trace&lt;/a&gt; 机制来更准确地获取程序内存状态、辅助控制测试流程，具体的实现细节会在后文详细介绍。&lt;/p&gt;&lt;h3&gt;3. 破坏性测试&lt;/h3&gt;&lt;p&gt;真实的软件运行环境中会遇到各种各样的问题，包括各类硬件故障、网络延迟和隔离、资源不足等等。DM 在数据同步过程中也同样会遇到这些问题，借助于 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//thenewstack.io/chaos-tools-and-techniques-for-testing-the-tidb-distributed-newsql-database/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PingCAP 内部的自动化混沌测试平台 schrodinger&lt;/a&gt;，我们设计了多个破坏性测试用例，包括在同步过程中随机 kill DM-worker 节点，同步过程中重启部分 DM-worker 节点，分发不兼容 DDL 语句等测试场景。这一类测试的关注点是在各类破坏性操作之后数据同步能否正常恢复以及验证在这些场景下数据一致性的保证，测试用例通常以黑盒的形式去运行，并且长期、反复地进行测试。&lt;/p&gt;&lt;h3&gt;4. 稳定性测试&lt;/h3&gt;&lt;p&gt;目前该类测试运行在 PingCAP 内部的 K8s 集群上，通常每个测试的应用规模会比较大，譬如有一些 100+ 上游实例，300+ 分库分表合并的测试场景，数据负载也会相对较高，目标在于测试大规模 DM 集群在高负载下长期运行的稳定性。该类测试也属于黑盒测试，每个测试用例内会根据任务配置启动上游的 MySQL 集群、DM 集群、下游 TiDB 集群和数据导入集群。上游数据输入工具有多种，包括 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/amyangfei/data-dam&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;随机 DML 生成工具&lt;/a&gt;，schrodinger 测试用例集等。具体的测试 case 和 K8s 部署脚本可以在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/csuzhangxc/dm-k8s&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;dm-K8s 仓库&lt;/a&gt; 找到。&lt;/p&gt;&lt;h3&gt;5. 测试方法对比&lt;/h3&gt;&lt;p&gt;我们通过以下的表格对比不同测试维度在测试体系中发挥的作用和它们之间的互补性。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1220&quot; data-rawheight=&quot;926&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1220&quot; data-original=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1220&quot; data-rawheight=&quot;926&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1220&quot; data-original=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h2&gt;测试 case 与测试工具的实现 &lt;/h2&gt;&lt;h3&gt;1. 在单元测试中进行 mock&lt;/h3&gt;&lt;p&gt;我们在单元测试运行过程中希望尽量减少外部环境或内部组件的依赖，譬如测试 relay 模块时我们并不希望从上游的 MySQL 拉取 binlog，或者测试到下游的一些数据库读写操作并不希望真正部署一个下游 TiDB，这时候我们就需要对测试 case 进行适当的 mock。在单元测试中针对不同的场景采用了多种 mock 方案。接下来我们选取几种具有代表性的方案进行介绍。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mock golang interface&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 golang 中只要调用者本身实现了接口的全部方法，就默认实现了该接口，这一特性使得使用接口方法调用的代码具有良好的扩展性，对于测试也提供了天然的 mock 方法。以 worker 内部各 subtask 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/worker/subtask_test.go%23L258&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;任务暂停、恢复的测试用例&lt;/a&gt; 为例，测试过程中会涉及到 dump unit 和 load unit 的运行、出错、暂停和恢复等操作。我们定义 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/worker/subtask_test.go%23L67-L76&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MockUnit&lt;/a&gt; 并且实现了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/unit/unit.go%23L24&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;unit interface&lt;/a&gt; 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/worker/subtask_test.go%23L86-L124&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;全部方法&lt;/a&gt;，就可以在单元测试里模拟任务中 unit 的各类操作。还可以定义 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/worker/subtask_test.go%23L126-L143&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;各类注入函数&lt;/a&gt;，实现控制某些逻辑流程中的出错测试和执行路径控制。&lt;/p&gt;&lt;p&gt;&lt;b&gt;自定义 binlog 生成工具&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在前文已经介绍过 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/dm-source-code-reading-6/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;relay 处理单元从上游读取 binlog 并写入本地文件&lt;/a&gt; 的实现细节，这一过程重度依赖于 MySQL binlog 的处理和解析。为了在单元测试中完善模拟 binlog 数据流，DM 中实现了一个 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/binlog/event&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;binlog 生成工具&lt;/a&gt;，该工具包提供了通用的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/binlog/event/generator.go%23L25&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;generator&lt;/a&gt; 用于连续生成 Event 以及相对底层的生成特定 Event 的接口，支持 MySQL 和 MariaDB 两种数据库的 binlog 协议。generator 提供的生成接口会返回一个 go-mysql 的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/siddontang/go-mysql/blob/7ed1210c02a2867a8d4570f526422af9fcd4246b/replication/event.go%23L25&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;BinlogEvent&lt;/a&gt;&lt;/code&gt; 列表和 binlog 对应的 byte 数组，同时在 generator 中自动更新 binlog 位置信息和 &lt;code&gt;GTID&lt;/code&gt; 信息。类似的，更底层的生成 Event 接口会要求提供数据类型、&lt;code&gt;serverID&lt;/code&gt;、&lt;code&gt;latestPos&lt;/code&gt;、&lt;code&gt;latestGTID&lt;/code&gt; 以及可能需要的库名、表名、SQL 语句等信息，生成的结果是一个 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/binlog/event/common.go%23L28&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDLDMLResult&lt;/a&gt;&lt;/code&gt; 对象。&lt;/p&gt;&lt;p&gt;我们通过测试中的一个 case 来了解如何使用这个工具，以 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L370&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;relay 模块读取到多个 binlog event 写入文件的正确性测试&lt;/a&gt; 这个 case 为例：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=h%3Ccode%3Ettps%3A//g%3C/code%3Ei%3Ccode%3Ethub%3C/code%3E.co%3Ccode%3Em/p%3C/code%3Eingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L371-L387&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;首先配置数据库类型，serverID，GTID 和 XID 相关信息，初始化 relay log 写入目录和文件名&lt;/a&gt;&lt;/li&gt;&lt;li&gt;ref=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L390&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L390&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;初始化 allEvents 数组，用于模拟从上游接收到的 &lt;code&gt;replication.BinlogEvent&lt;/code&gt;；ref=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L391&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L391&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;初始化 allData，&lt;code&gt;allData&lt;/code&gt; 存储 binlog binary 数据，用于后续 relay log 写入的验证；ref=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L392&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L392&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;初始化 generator&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3Ccode%3Es%3A//github.co%3C/code%3Em/ping%3Ccode%3Ecap/dm/blob/7cba6d21d78%3C/code%3Edd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L396&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;通过 generator GenFileHeader 接口生成 replication.BinlogEvent 和 binlog 数据&lt;/a&gt;（对应的 binlog 中包含 &lt;code&gt;FormatDescriptionEvent&lt;/code&gt; 和 &lt;code&gt;PreviousGTIDsEvent&lt;/code&gt;）。生成的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github%3C/code%3E.com/%3Ccode%3Epingcap/d%3C/code%3Em/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L398&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;replication.BinlogEvent 保存到 allEvents&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=http%3Ccode%3Es%3A//git%3C/code%3Ehub.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L399&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;binlog 数据保存到 allData&lt;/a&gt;。&lt;/code&gt;&lt;/li&gt;&lt;li&gt;按照 3 的操作流程分别href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//gi&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;gi&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;code&gt;thub.com/pin&lt;/code&gt;gcap/&lt;code&gt;dm/blo&lt;/code&gt;b/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L402-L421&amp;#34;&amp;gt;生成 CREATE DATABASE，CREATE TABLE 和一条 INSERT 语句对应的 event/binlog 数据并保存&lt;/li&gt;&lt;li&gt;href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;code&gt;blob/7cba6d21d78dd16e9a&lt;/code&gt;b159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L424-L430&amp;#34;&amp;gt;创建 relay.FileWriter，按照顺序读取 3, 4 步骤中保存的 replication.BinlogEvent，向配置的 relay log 文件中写入 relay log&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.%3Ccode%3Ecom/pin%3C/code%3Egcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L432&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;检查 relay log 文件写入的数据长度与 allData 存储的数据长度相同&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.%3Ccode%3Ecom/pin%3C/code%3Egcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L435-L438&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;读取 relay log 文件，检查数据内容和 allData 存储的数据内容相同&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;至此我们就结合 binlog 生成工具完成了一个 relay 模块的测试 case。目前 DM 已经在很多 case 中使用 binlog 生成工具模拟生成 binlog，仍然存在的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/syncer/syncer_test.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;少量 case&lt;/a&gt; 依赖上游数据库生成 binlog，我们已经计划借助 binlog 生成工具移除这些外部依赖。&lt;/p&gt;&lt;p&gt;&lt;b&gt;其他 mock 工具&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在验证数据库读写操作逻辑正确性的测试中，使用了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/DATA-DOG/go-sqlmock&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;go-sqlmock&lt;/a&gt; 来 mock sql driver 的行为。&lt;/li&gt;&lt;li&gt;在验证 gRPC 交互逻辑的正确性测试中，使用了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/golang/mock&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;官方提供的 mock 工具&lt;/a&gt;，针对 gRPC 接口生成 mock 文件，在此基础上测试 gRPC 接口和应用逻辑的正确性。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;2. 集成测试的方法和相关工具&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Trace 信息收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DM 内部定义了一个简单的信息 trace 收集工具，其设计目标是在 DM 运行过程中，通过增加代码内部的埋点，定期收集系统运行时的各类信息。trace 工具包含一个提供 gRPC 上报信息接口和 HTTP 控制接口的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/tracer&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tracer 服务器&lt;/a&gt; 和提供埋点以及后台收集信息上传功能的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/tracing&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tracing 包&lt;/a&gt;。tracing 模块上传到 tracer 服务器的事件数据通过 &lt;code&gt;protobuf&lt;/code&gt; 进行定义，&lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/proto/tracer_base.proto%23L11-L18&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;BaseEvent&lt;/a&gt;&lt;/code&gt; 定义了最基本的 trace 事件，包含了运行代码文件名、代码行、事件时间戳、事件 ID、事件组 ID 和事件类型，用户自定义的事件需要包含 &lt;code&gt;BaseEvent&lt;/code&gt;。tracing 模块会 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/tracing/tracer.go%23L129&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;定期向 tracer 服务器同步全局时间戳&lt;/a&gt;，通过这种方式保证多节点不同的 trace 事件会保持大致的时间顺序（注意这里并不是严格的时间序，会依赖于每分钟内本地时钟的准确性，仍然有各种出现乱序的可能）。设计 tracing 模块的主要目的有以下两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对于同一个 DM 组件（DM-master/DM-worker），希望记录一些重要内存信息的数据流历史。例如在 binlog replication 处理单元处理一条 query event 过程中会经历处理 binlog event 、生成 ddl job、执行 job 这三个阶段，我们将这三个处理逻辑抽象为三个事件，三个事件在时间上是有先后关系的，在逻辑上关联了同一个 binlog 的处理流程，在 DM 中记录这三个事件的 trace event 时使用了同一个 &lt;code&gt;traceID&lt;/code&gt;（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/syncer/syncer.go%23L1597&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;处理 binlog event 生成一个新的 traceID&lt;/a&gt;，该 &lt;code&gt;traceID&lt;/code&gt; 记录在 ddl job 中，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/syncer/syncer.go%23L688&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;分发 ddl job 时记录的 trace 事件会复用此 traceID&lt;/a&gt;；&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cb%3Ccode%3Ea6d21d7%3C/code%3E8dd16e9ab159e9c0300efcbdeb1e4a/syncer/syncer.go%23L864&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;在 executor 中最后执行 ddl job 的过程中记录的 trace 事件也会复用此 traceID&lt;/a&gt;），这样就将三个事件关联起来，因为在同一个进程内，他们的时间戳真实反映了时间维度上的顺序关系。&lt;/li&gt;&lt;li&gt;由于 DM 提供了 shard DDL 的机制，多个 DM-worker 之间的数据会存在关联，譬如在进行 shard DDL 的过程中，处于同一个 shard group 内的多个 DM-worker 的 DDL 是关联在一起的。&lt;code&gt;BaseEvent&lt;/code&gt; 定义中的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/proto/tracer_base.proto%23L16&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;groupID&lt;/a&gt;&lt;/code&gt; 字段就是用来解决多进程间 trace 事件关联性的问题，定义具有相同 &lt;code&gt;groupID&lt;/code&gt; 的事件属于同一个事件组，表示它们之间在逻辑上有一定关联性。举一个例子，在 shard DDL 这个场景下，DM-master 协调 shard DDL 时会分别 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/master/server.go%23L1423-L1432&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;向 DDL owner 分发执行 SQL 的请求&lt;/a&gt;，以及 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/master/server.go%23L1457-L1466&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;向非 owner 分发忽略 DDL 的请求&lt;/a&gt;，在这两组请求中携带了相同的 &lt;code&gt;groupID&lt;/code&gt;，binlog replication 分发 ddl job 时会获取到 &lt;code&gt;groupID&lt;/code&gt;，这样就将不同进程间 shard DDL 的执行关联了起来。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们可以利用收集的 trace 信息辅助验证数据同步的正确性。譬如在 href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/safe_mode&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/t&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/safe_mode&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;验证 safe_mode 逻辑正确性的测试 中，&lt;a href=&quot;https://link.zhihu.com/?target=http%3Ccode%3Es%3A//githu%3C/code%3Eb.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/safe_mode/run.sh%23L35&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;我们将 DM 启动阶段的 safe_mode 时间调短为 0s&lt;/a&gt;，期望验证对于上游 update 操作产生的 binlog，如果该操作发生时上下游 shard DDL 没有完全同步，那么同步该 binlog 时的 &lt;code&gt;safe_mode&lt;/code&gt; 为 true；反之如果该操作发生时上下游没有进行 shard DDL 或 shard DDL 已经同步，那么 &lt;code&gt;safe_mode&lt;/code&gt; 为 false。通过 trace 机制，可以很容易从 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/_dmctl_tools/check_safe_mode.go%23L42-L55&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tracer server 的接口获取测试过程中的所有事件信息&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/_dmctl_tools/check_safe_mode.go%23L123-L133&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;并且抽取出 update DML，DDL 等对应的 trace event 信息&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=htt%3Ccode%3Eps%3A//gith%3C/code%3Eub.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/_dmctl_tools/check_safe_mode.go%23L167-L180&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;进一步通过这些信息验证 safe_mode 在 shard DDL 同步场景下工作的正确性&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Failpoint 的使用&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在集成测试中，为了对特定的同步流程或者特定的错误中断做确定性测试，我们开发了一个名为 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/failpoint&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;failpoint&lt;/a&gt; 的项目，用来在代码中注入特定的错误。现阶段 DM 集成测试的 case 都是 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/safe_mode/run.sh%23L35-L38&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;提前设定环境变量，然后启动 DM 相关进程来控制注入点的生效与否&lt;/a&gt;。目前我们正在探索将 trace 和 failpoint 结合的方案，通过 trace 获取进程内部状态，借助 failpoint 提供的 http 接口动态调整注入点，以实现更智能、更通用的错误注入测试。&lt;/p&gt;&lt;h3&gt;3. 破坏性测试和大规模测试的原理与展望&lt;/h3&gt;&lt;p&gt;&lt;b&gt;破坏性测试中的错误注入&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前破坏性测试的测试 case 并没有对外开源，我们在这里介绍 DM 破坏性测试中所使用的部分故障注入&lt;/p&gt;&lt;ul&gt;&lt;li&gt;使用 &lt;code&gt;kill -9&lt;/code&gt; 强制终止 DM-worker 进程，或者使用 &lt;code&gt;kill&lt;/code&gt; 来优雅地终止进程，然后重新启动&lt;/li&gt;&lt;li&gt;模拟上游写入 TiDB 不兼容的 DDL，通过 &lt;code&gt;sql-skip/sql-replace&lt;/code&gt; 跳过或替换不兼容 DDL 恢复同步的场景&lt;/li&gt;&lt;li&gt;模拟上游发生主从切换时 DM 进行主从切换处理的正确性&lt;/li&gt;&lt;li&gt;模拟下游 TiDB/TiKV 故障不可写入的场景&lt;/li&gt;&lt;li&gt;模拟网络出现丢包或高延迟的场景&lt;/li&gt;&lt;li&gt;在未来 DM 提供高可用支持之后，还会增加更多的高可用相关测试场景，譬如磁盘空间写满、DM-worker 节点宕机自动恢复等&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;大规模测试&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大规模测试中的上游负载复用了很多在 TiDB 中的测试用例，譬如银行转账、大规模 DDL 操作等测试场景。该测试所有 case 均运行在 K8s 中，基于 K8s deployment yaml 部署一系列的 statefuset，通过 &lt;code&gt;configmap&lt;/code&gt; 传递拓扑信息。目前 DM 正在规划实现 DM-operator 以及运行于 K8s 之上的完整解决方案，预期在未来可以更便捷地部署在 K8s 环境上，后续的大规模测试也会基于此继续展开。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;本篇文章详细地介绍了 DM 的测试体系，测试中使用到的工具和一些 case 的实例分析，分析如何通过多维度的测试保证 DM 的正确性、稳定性。然而尽管已经有了如此多的测试，我们仍不能保证 bug free，也不能保证测试 case 对于各类场景和逻辑路径进行了百分之百的覆盖，对于测试方法和测试 case 的完善仍需要不断的探索。&lt;/p&gt;&lt;p&gt;&lt;b&gt;至此 DM 的源码阅读系列就暂时告一段落了，但是 DM 还在不断地发展演化，DM 中长期的规划中有很多激动人心的改动和优化，譬如高可用方案的落地、DM on K8s、实时数据校验、更易用的数据迁移平台等（未来对于 DM 的一些新特性可能会有番外篇）。希望感兴趣的小伙伴可以持续关注 DM 的发展，也欢迎大家提供改进的建议和提&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/pulls&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PR&lt;/a&gt;。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/dm-source-code-reading-10/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/blog-cn/dm-&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;source-code-reading-10/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多 DM 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23DM-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Blog-cns | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-23-74873268</guid>
<pubDate>Tue, 23 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>PingCAP 唐刘：如何利用混沌工程打造健壮的分布式系统？</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-22-74717464.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/74717464&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a888a9babe11aed8a2c5f839eb3c8092_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：赵钰莹&lt;/p&gt;&lt;p&gt;&lt;i&gt;本文转载于 InfoQ。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;原文链接：&lt;/i&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/article/EEKM947YbboGtD_zQuLw&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/article/EEKM94&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;7YbboGtD_zQuLw&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;作为混沌工程的重要推动者，Netflix 在混沌工程手册（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/article/AsN34J2T9QDXB0s-t9JN&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/article/AsN34J&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;2T9QDXB0s-t9JN&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）中谈到，在生产环境进行软件验证的想法通常会被嘲笑。过去，这句话基本都被翻译为“我们在发布之前不打算完善地验证这些代码”。在经典的测试链路中，寻找软件缺陷的普遍信条是离生产环境越远越好。例如，在单元测试中发现缺陷要比在集成测试中发现更好，这里的逻辑是：离生产环境越远，或者是离发布越远的时候，发现的缺陷就越容易被找到根本原因并彻底修复。&lt;br/&gt;对于混沌工程而言，整个链路刚好反过来：在离生产环境越近的地方进行实验越好，理想的实践就是直接在生产环境中执行。对于软件工程师来说，最难的莫过于，系统用户永远不会如预期那样与系统进行交互，混沌工程是解决这一问题的理想方法，可以让开发者了解除代码之外，整个系统其他方面的情况，特别是状态、输入、以及第三方系统导致的难以预见的行为。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;据了解，在 TiDB 的研发初期，PingCAP 就引入了混沌工程，以此保证 TiDB 在各种极端情况下的稳定性。在 ArchSummit 全球架构师峰会（深圳站）2019 大会期间，InfoQ 就混沌工程理念及实践这一话题采访了 PingCAP 首席架构师 &lt;/b&gt;&lt;a class=&quot;member_mention&quot; href=&quot;https://www.zhihu.com/people/df9ec6a48ca50364852daa71b20a6192&quot; data-hash=&quot;df9ec6a48ca50364852daa71b20a6192&quot; data-hovercard=&quot;p$b$df9ec6a48ca50364852daa71b20a6192&quot;&gt;@唐刘&lt;/a&gt;&lt;b&gt; ，以此了解 PingCAP 的实践历程。&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_b.jpg&quot;/&gt;&lt;figcaption&gt;唐刘，PingCAP 首席架构师，主要负责分布式 Key-Value 数据库 TiKV 的研发工作，也会折腾下 TiDB 整个产品的测试，工具开发等工作。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;混沌工程与分布式系统&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;理解是实践的前提之一，唐刘在采访中坦言，混沌工程这个名字比较容易让人困惑，包括其英文“Chaos Engineering”，初次听到这个单词时确实不太好理解。&lt;b&gt;唐刘表示：“最开始，我就是把它当成一种注入测试的方法，后来才发现这其实是一门工程学科，通过实验的方式发现问题并解决问题。”&lt;/b&gt;&lt;/p&gt;&lt;p&gt;其实，混沌工程的理念很早之前就存在，唐刘表示，过去使用的错误注入就可以理解为混沌工程的一种表现方式，只不过 Netflix 将其提炼出来变成了通用准则，只要照着相关方法就能实施混沌工程，而这一技术诞生之际就与分布式系统密切相关，在《Chaos Engineering》一书中是这样表述的：&lt;/p&gt;&lt;blockquote&gt;混沌工程是在分布式系统上进行实验的学科 , 目的是建立对系统抵御生产环境中失控条件的能力以及信心 。&lt;br/&gt;注：分布式系统就是，其中有台你根本不知道的机器故障了，有可能会让你自己的服务也故障。——Leslie Lamport&lt;/blockquote&gt;&lt;p&gt;即使可预见所有在控制范围内系统的状态，也总是会出现意外情况，比如系统依靠的某些外部服务突发宕机，这在系统搬迁上云后尤为明显，云服务也并不总是稳定可靠的。采访中，唐刘解释道，混沌工程主要是解决常规测试不能覆盖的问题。对于分布式系统来说，因为其异常的复杂性，加上错误可能在任何时候、任何地点发生，众多常规测试方法并不能保证系统正确。&lt;/p&gt;&lt;p&gt;当然，在某些场景下，直接在生产环境中进行实践是非常困难且不可用的，比如将干扰直接注入到行驶中的自动驾驶汽车的传感器上，这是比较危险的，但大部分用户应该都不是在操作这类生死攸关的系统。&lt;/p&gt;&lt;p&gt;&lt;b&gt;相比较而言，唐刘认为混沌工程比较适合对数据安全性要求较高的场景。&lt;/b&gt;此外，如果业务对故障容错有所承诺，也需要通过混沌工程验证系统是否可以支持容错。量化到具体指标来看，如果开发人员确定系统会宕机并且清楚宕机之后会造成较大损失，可以通过“支持快速终止实验”和“最小化实验造成的‘爆炸半径’”等方式实施混沌工程。&lt;/p&gt;&lt;p&gt;当执行任何混沌工程实验前，应该先有一个用来立即终止实验的“红色按钮”，更好的方法则是自动化该功能，当其监测到对稳定状态有潜在危害时立即自动终止实验。第二个策略涉及在设计实验时，考虑从实验中获得有意义结论的同时，兼顾最小化实验可能造成的潜在危害。&lt;/p&gt;&lt;p&gt;&lt;b&gt;无论是架构师、开发人员还是测试人员，唐刘都建议关注这一技术，这相当于从另一个视角审视系统，尤其是对开发者而言。唐刘补充道，开发一个优秀的系统并不只是写代码就足够了，测试不应该仅仅依靠测试人员，他一直相信：&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;优秀的开发者一定是优秀的测试人员。&lt;/b&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;PingCAP 混沌工程实践&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如上文所言，在开始研发 TiDB 时，PingCAP 就决定引入混沌工程，应该算是国内吃螃蟹的团队之一。谈到当初的引入原因，唐刘表示，起初开发分布式数据库时，整个团队很自然就想到需要保证开发的数据库能够让用户放心使用，这就需要进行各种各样的测试。当时，Netflix 开发的 Chaos Monkey 已经非常知名，得益于 Netflix 成功的部署经验，PingCAP 团队想到利用该工具解决稳定性问题。&lt;/p&gt;&lt;p&gt;回顾整个实践历程，唐刘表示大概可以分为三个阶段，第一个阶段是 2017 年之前，那时并没有自动化的概念，所有实验全部需要手动完成，包括申请机器、手动部署等。虽然比较繁琐，但也在系统上线之前发现了不少问题。&lt;/p&gt;&lt;p&gt;第二个阶段是从 2017 年到 2019 年初，PingCAP 基于 K8s 搭建了一套自动化 chaos 框架，叫做 Schrodinger，这套系统极大提升了整体生产力，只需自定义要做的实验，Schrodinger 就能搞定。&lt;/p&gt;&lt;p&gt;第三个阶段则是 2019 年初至今，PingCAP 一直在做 Schrodinger Cloud，Schrodinger 主要是为测试 TiDB，也可用来测试 TiDB 的周边工具，甚至是合作伙伴的业务。面对这些需求，PingCAP 考虑基于 K8s 做一套更加通用的 Chaos 框架，采用 Operator 的方式，任何 Chaos 在 K8s 里面都是 CRD，用户只需要定义好自己的 CRD，Schrodinger 就可以自动完成后续事宜。&lt;/p&gt;&lt;p&gt;&lt;b&gt;在开发混沌工程实验时，唐刘建议可遵循以下原则，将有助于实验设计：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;建立稳定状态的假设；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;多样化现实世界事件；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;在生产环境运行实验；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;持续自动化运行实验；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;最小化“爆炸半径”。&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;614&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;614&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;具体来说，系统稳态可以通过一些指标，比如延迟和 QPS 数据等来定义，当系统指标在测试完成后，无法快速恢复稳态要求，可以认为这个系统是不稳定的；其次，引进多样化的现实变量，比如网卡、磁盘故障等；然后，最为重要的是在生产环境中进行验证，这样做是存在风险的，因此最好提前与协作部门同步；最后，自动化可以让整个过程的效率更高，最小化“爆炸半径”可以避免不必要的损失。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;举例来说，对于一个三副本的系统而言，可以通过随机杀死 Leader 节点的方式来验证系统是否可以保持稳态。可预想的情况是系统在主节点被杀死后会出现一段抖动，随后恢复正常则证明系统是具备容错能力的，反之，则证明系统存在问题。&lt;/p&gt;&lt;p&gt;在这之中，主要有两个大方向：一是发现错误；二是注入错误。TiDB 主要通过 Metrics（Prometheus 项目）、Log 和 Tracing 三种方式分析系统状态。其中，TiDB 默认不开启 Tracing 方式，因为这会对性能产生一定影响，仅在必要时启动该方法。至于注入错误，应用、存储、网络、CPU 等存在多种故障方式，唐刘在分享中提到了如下部分，供开发者参考：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;761&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;761&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;根据过往实践经验，唐刘建议希望使用混沌工程的开发者可以参考混沌工程主页（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//principlesofchaos.org/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;principlesofchaos.org/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;） 列出的步骤和原则。但是，要想真正实践，还需要做很多工作，包括更好地对系统进行错误注入，更好地发现系统问题，这些其实业界没有通用的解决方案，因此实践起来还是比较麻烦的。采访中，唐刘推荐可以阅读 Netflix 的《Chaos Engineering》一书（中文翻译版：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/theme/13&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/theme/13&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;），GitHub 上有一个 awesome-chaos-engineering的 Repo（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/dastergon/awesome-chaos-engineering&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/dastergon/aw&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;esome-chaos-engineering&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）也可以参考。此外，如果整个开发团队本来对测试就不太重视，认为这完全是测试团队的事情，那可能也很难推动混沌工程落地。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结束语&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;三年前，我很少听到有人谈论混沌工程，现在已经蛮多了。&lt;/blockquote&gt;&lt;p&gt;采访最后， &lt;a class=&quot;member_mention&quot; href=&quot;https://www.zhihu.com/people/df9ec6a48ca50364852daa71b20a6192&quot; data-hash=&quot;df9ec6a48ca50364852daa71b20a6192&quot; data-hovercard=&quot;p$b$df9ec6a48ca50364852daa71b20a6192&quot;&gt;@唐刘&lt;/a&gt; 表示如今的混沌工程在国内已经比较出名，这个概念应该已经进入普及阶段。但据唐刘的了解，国内真正对其应用很好的，并没有特别多公司，大部分仍然处于理清概念，但不知道如何实施的阶段。&lt;b&gt;在这种背景下，企业可能最需要关注的是如何建立起自己的一整套自动化测试平台，实现对系统的自动错误注入，并自动发现系统问题。在此基础上，企业可以根据业务情况考虑深入实践混沌工程。&lt;/b&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-22-74717464</guid>
<pubDate>Mon, 22 Jul 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
