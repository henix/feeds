<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>专访 | 腾讯AI Lab西雅图实验室负责人俞栋：语音识别领域的现状与进展</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1524279532&amp;src=3&amp;ver=1&amp;signature=wkek0COu6xUzfDaXM62yjBbx0KZeZsfp4FUcMZABGLPMHLFbct1DAoJqxistM7f5HIWJtxLr0TvDPm8TCN5h8XgEDMARp53mNoMoaIbGTQsZMTkd49dhsqllAXQlA4reXrLkmSb0p0E4DzNImFMDKHW8Hj1tWYRqwlOsWRyschc=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    专访 | 腾讯AI Lab西雅图实验室负责人俞栋：语音识别领域的现状与进展                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                        <span id="copyright_logo" class="rich_media_meta meta_original_tag">原创</span>
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-13</em>

                                        <em class="rich_media_meta rich_media_meta_text">邱陆陆</em>
                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心原创</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：邱陆陆</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">去年三月，语音识别和深度学习领域的著名专家俞栋宣布加入腾讯，担任腾讯成立不久的腾讯 AI Lab 副主任及西雅图实验室负责人。加入腾讯不久后，俞栋在机器之心主办的第一届全球机器智能峰会（GMIS 2017）上，发表了主题为《语音识别领域的前沿研究》的演讲，分享了语音领域的四个前沿方向，包括：更有效的序列到序列直接转换模型，鸡尾酒会问题，持续预测与适应的模型，以及前端与后端联合优化等。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">如今一年过去了，这些问题的研究现状如何？是否得到了业界的肯定与应用？腾讯 AI Lab 的进展如何？又有哪些新的目标？日前，机器之心在腾讯 AI Lab 学术论坛上采访到了「老朋友」俞栋，以下，是他关于这些问题的答案。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: center;"><strong>语音领域：Where are we standing? </strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：语音领域整体的研究处于一个什么状态？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：去年底在 NIPS 会议上南大的周志华教授还和我开玩笑说，「语音识别已经解决了，你不要做了。」相信很多人有类似的感觉。虽然很多公司已经宣称可以在标准数据集或安静的近场环境下达到「97% 识别率」、「超过人」等等水平，但是实际上市面上的产品，在很多真实应用场景下，尤其是远场、中文夹杂英文、旁边有人说话等等情况下，效果还远远达不到期望值。还有很多待研究的问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：从标准数据集到真实场景，待解决的问题都有哪些？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：有很多。比如，现在的系统鲁棒性都不太高，而且都还依赖增加数据（包括合成的模拟数据）来提高鲁棒性。这一点对于基于深度学习的系统来说尤为明显：数据没覆盖的情况就做不好，是这类方法的一个局限性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这个世界的可能性是无限的，即使采集了几十万小时的数据，也不能覆盖所有的情况，还是会有很多新的、没见过的场景。而标准数据集的一个特点是，训练集和测试集之间是强相关的，换言之，它们之间的不匹配度（mismatch）不大。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">很多机器学习方法都要遵循一个基本假设：训练集和测试集符合同一分布。不满足这一要求的话，学出来的模型的效果是没有理论保证的。而真实场景，恰恰是不保证训练集与测试集满足同分布假设的情况。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在语音识别研究的历史上，很早就有人意识到了这一问题，并开发出了很多自适应算法，试图根据场景和环境的变化做自适应。目前来说，自适应算法起到了一定的作用，但是还不能完全解决鲁棒性问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：理论上来讲，推理的数据与训练数据不满足同分布假设的话，机器学习模型会整体失效。那么自适应算法最终能够解决鲁棒性问题吗？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：真实场景和训练集之间所谓的「不满足同分布假设」大概率来讲不是「完全不满足」，而是「近似」或者「满足一些」。因此机器学习模型的识别率虽然会下降，模型仍然可用。只不过需要采用一些方法来弥补大致满足与完全满足同分布情况下的差距。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">自适应算法只是其中一种方法，它有多个变种。比如可以用另外一个模型来判定分布变化与否，甚至判定如何变化，从而将变化后的特征或分布「恢复」成和训练时所见基本一样的情况再进行识别。举个例子，如果一个模型用我的声音做训练，然后去识别你的声音，效果就会很差。但是如果有一个模型，专门刻画人声的特点，并且在训练的时候，就把「说话人身份」（Speaker ID）作为一个重要的变量放进去，那么今后在识别其他人的时候，只要把说话人身份替换掉，就能获得识别率的上升了。问题在于，这种对于变化的估算也会引入误差，相对的性能还是会有损失。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: center;"><strong>四个前沿问题的进展</strong></p><p style="text-align: center;"><strong><br></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：端到端模型之前，语音识别模型的发展历程是什么样的？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：以前的语音识别系统基于高斯混合模型（Gaussian Mixture Model, GMM）和隐马尔可夫模型（Hidden Markov Model），合在一起，叫 GMM-HMM 模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在 90 年代初期，伯克利大学的研究人员就开始用多层感知机加上隐马尔可夫模型进行语音识别，由于模型由一个传统的生成模型 HMM 和一个比较时髦的判别式模型神经网络组成，他们称其为混合模型（Hybrid Model）。2010 年，我们用深层神经网络替换掉了浅层神经网络，用上下文相关音素（Phoneme）替换单音素作为建模单元，仍然沿用混合模型的基本架构但增加了建模单元的数量，取得了识别效果上的突破。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">最近的端到端模型则完全不再需要隐马尔可夫模型，从头到尾都是一整个神经网络。有时候需要结合外部语言模型，如果数据比较多，连外部语言模型都不需要了。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：端到端识别模型有哪些进展呢？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：在即将召开的 ICASSP 上，谷歌会发表一些相关文章，部分预印版已经放在 arXiv 上了，文章里提到当使用大量训练数据时可以在语音搜索任务的某一个测试集上做到和混合模型一样的效果。但是在真实场景下当出现没见过的尾端（tail）搜索词的情况下，效果还有差距，这表明这些模型记忆能力很强但是举一反三的能力还比较欠缺。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但无论如何，这仍然是十分可观的进展了，因为之前的端到端系统和混合模型之间的差距还很大，现在这个差距在缩小，甚至在某一场景下端到端模型可以做到超越，这都是比较大的进展。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">腾讯 AI Lab 最近几个月也做了一些类似的工作，在数据量比较少，也就是端到端系统的优势更不能得到体现的情况下，用一些新的算法和技巧大幅提高了性能。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">应该看到，在端到端系统上面，大家的投入是比较多的，也确实有一些比较有意思的进展。但是端到端系统是否能替代混合模型，仍然是未知数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">只有当新模型比旧模型好很多的时候，替换才会发生。当然，这里的好是多方面的，不单单是识别率好，也可能是在其他指标不变的情况下你的运算量小了，或者是解码速度提升了。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：端到端模型在产品中有实际应用吗？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：目前的端到端系统基本上基于两个框架，一个是 CTC（Connectionist Temporal Classification）框架，一个是基于注意力机制的 seq2seq 框架。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">刚才提到的谷歌的论文用的是基于注意力的框架，投入使用相对较少。CTC 模型用得相对较多。腾讯的产品上既有 CTC 模型，也有混合模型，性能没有太大区别。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">CTC 的好处是可以采用更大的建模单元，坏处是存在一个随机延迟的问题，即结果出来的时间不是预先可知的。随机延迟的后果是断句困难，这会给用户造成一种「你怎么比别人慢」的感觉。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">因此，做交互的系统，比如语音助手类，大部分仍然在使用混合系统。而对实时性没有要求的产品，比如 YouTube 的字幕生成器，因为可以离线，所以有延时也没有关系。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 14px;color: rgb(136, 136, 136);">机器之心：鸡尾酒会问题的现状如何？</span></em></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：大家都很重视鸡尾酒会问题，因为这是远场里必须解决的重要问题，因此也有蛮多进展。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一个当然就是我们之前提出的置换不变性训练（Permutation Invariant Training）的方法，我们在很多场景下都做了不同的尝试，也有其他的学校以及公司，在我们的工作上做了一些拓展。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">此外还有其他重要的方法被提出来，比如 MERL 的深度聚类（deep clustering）方法和哥伦比亚大学的深度吸引子网络（deep attractor network）。另外 NTT 还提出了一种跟我们不同的设定，我们的设定是两人同时说话时，要将两人分开，同时识别两个人的语音。他们的设定是在两个人同时说话时，只跟踪其中一个人的声音。在这个设定下我们也有一些有趣的进展。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">不过这些都还属于研究性工作，还没有放在产品中。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：持续预测与适应的模型情况如何？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：这类模型的研究已经在我们的实验室列项了，应该是一个很有「做头」的问题，但是目前还没有很多进展。值得一提的可能是预测双向 RNN 的反向状态使单向 RNN 的性能提升的工作。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：声学前后端从独立优化到联合优化的进展如何？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋：</span></strong><span style="font-size: 14px;">相比于独立优化，现在业界的情况是前后端联合优化已经占了大多数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">前后端联合优化把前端和后端紧紧绑在了一起，好处是如果前后端系统都是自己开发的，那么效果会很好，坏处是一旦换一个前端/后端之后，会出现整体效果变差的情况。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">因此，我们要针对场景进行不同的设计。在某些场景下，我们还是要分割开做优化。例如在声学前端，降低噪声和减小变形（distortion）就是一对要同时考虑的矛盾的目标，需要针对场景做特殊优化。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: center;"><strong>从语音的角度看先验</strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：Yann LeCun 和 Christopher Manning 之前在斯坦福进行了一场关于先验的讨论。LeCun 倾向于使用尽可能少的先验，因为作出的假设总会和一小部分数据冲突，从而限制最后能达到的最低错误率。而 Manning 倾向于认为当前的模型都还是「差劲的学习者」，学界需要对先验以及结构更有信心，尝试引入更丰富的结构，允许人们在更少的时间内、以更少的数据获得更高效的学习器。站在语音研究者的角度，您如何看待先验这一问题？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋：</span></strong><span style="font-size: 14px;">先验的作用可以说是一个哲学问题了。这两位学者给出的答案不同，更多是因为他们的优化目标和场景不同。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">LeCun 希望找到一个更通用的算法，因此当然希望假设越少越好，跟问题越不相关越好。因为如果你针对某个特定问题加入先验，那么模型虽然可以保证这个问题的效果，但肯定会在某一些场景下工作得不好。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Chris Manning 的立场则不同：他面对的问题是，给定场景、给定现有模型，在这种前提下，如何把某一任务做好。这时，把先验，尤其是网络结构类型的先验添加进去，是会提升系统性能的。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这个问题也和训练数据量有很大的关系。如果想要训练一个能区分猫和老虎的分类器，但训练数据只有 5 幅猫的图片和 5 幅老虎的图片，该如何选择模型呢？这个设定下，大部分模型都无法工作，只有一种模型可以，就是添加了非常强的先验的模型。比如说，模型设计者预先知道，老虎都在山里跑，背景是绿色的，猫都在家里，背景是灰白色的。那么把这个先验知识加进去，就可以设计一个简单的模型，测试一下背景颜色，做个分类，大部分情况下就能分对了。但这些知识不是机器学出来的，是人放入的先验知识。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果训练数据非常多，接近无穷多，这时情况更有利于 LeCun 的观点，也就是做假设的意义不大甚至可能因假设不对起反作用。如果训练数据不足，就需要人在模型设计阶段把问题的特殊结构提取出来，添加进模型作为先验。不过在现实场景下的绝大多数问题训练数据都是不够多的，所以某种先验或者偏置是必须的。当然，最好的情况是机器自己能够发现或总结这些特殊结构并加以利用。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：现阶段的语音处理方面的数据量更接近于哪一端？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：在处理语音问题时，我们加了非常多先验在里面。如果目标是做通用的语音识别器，语音的数据量还差得远，比如一个人说话声音与两个人、三个人混合的声音就差很远，所以语音的数据量还是远远不够的。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但如果我们只关心某个特殊的应用场景比如「语音搜索」，那么在有些公司比如谷歌数据量有可能达到基本满足要求的程度，但是这个专门为语音搜索定制的识别器，一旦被用于识别其他内容，效果可能就没那么好了。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">现在业界还是愿意为了产品性能去投入采集数据的，因此初始训练数据集的量不会太少。存在的问题是在产品投放出去之前，采集到的数据很可能和最终产品形态不一致，不是独立同分布，因此训练效果不够好，只能用这部分数据 bootstrap 启动一个系统。等到系统投放到市场上，采集到真实数据，再用真实数据提升模型性能，进入一个正向的循环。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">数据是一个非常重要的资源，数据量不够，很多模型没办法用，即使用上了，也达不到期望的要求。这是因为现在的机器学习有三个很重要的因素，分别是数据，计算力，和算法，这三者加起来，才是最后系统的性能。三个要素中，如果缺少任何一个，系统的性能就会差很多。如果算法比别人好一点，但是数据比别人少很多，那么算法的优势很可能弥补不了数据的缺失，反之亦然。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：卷积神经网络（CNN）和循环神经网络（RNN）均可以被看做先验的一种，卷积神经网络通常被用于空间数据而循环 神经网络多被用于时序数据，如何看待用卷积神经网络处理时序数据这样的搭配呢？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：如果卷积的层数足够多的话，理论上，它是可以和循环神经网络具有同等的能力的。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在信号处理学科中，有两种滤波器，分别叫做 IIR 和 FIR（Infinite Impulse Response Filter vs. Finite Impulse Response Filter），它们和两种神经网络相对应。IIR 就相当于 RNN 模型，FIR 就相当于 CNN 模型，在卷积了足够多层之后，它就能利用足够远的信息（类似 RNN）。就好像在很多场景下，FIR 滤波器是可以近似 IIR 滤波器的。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">因此 CNN 和 RNN 都是「可选的」，选择时就要考虑其他因素：例如 RNN 相对于 CNN（或者是 IIR 相对于 FIR）训练难度就要更大一些。但同时 RNN 更容易对变化很大的序列建模，比如依赖关系忽大忽小的情景，可能更适用 LSTM 这样的模型来实现。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: center;"><strong>腾讯西雅图实验室：现状与目标</strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：能否为我们更新下西雅图实验室的现状？主要关注哪些研究方向？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋：</span></strong><span style="font-size: 14px;">西雅图实验室成立于去年 5 月，更偏向基础研究，主攻方向是语音处理和自然语言处理。这与深圳实验室强调「研究+应用并重」侧重点稍有不同。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">过去一年，腾讯 AI Lab 西雅图实验室从我 1 个员工，发展到现在有 10 余名全职语音和自然语言处理方向的员工；从没有办公场地，到办公室装修好，还在社区内举办了开放日活动。进展还不错。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">目前语音识别研究人员的方向主要可以分为声学前端、声学模型和语言模型三类。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">其中，声学前端主要围绕在家居和车载环境所必须的麦克风阵列、降噪、去回声、去混响、唤醒等功能；声学模型部分主要关注如何将声学信号建模；最后，语言模型则对语言文字本身建模。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">语言模型技术相对稳定，不同语言之间差异也不是特别大，一般擅长声学模型的研究员也能做出不错的语言模型，但是声学前端和声学模型所需的技能是完全不同的，因此要分别找到合适的人选。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：实验室是如何选择研究方向的呢？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋</span></strong><span style="font-size: 14px;">：我们会考虑两件事，一个是今天的问题：当下公司和客户有哪些需求，如何去解决。另一个是明天的问题：两到三年乃至更久之后，市场会有什么样的需求？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">比如今天已经非常常见的远场识别，其实在几年前还是鲜为人知，或者说处于不被认可的状态。这是因为一方面大家觉得没有应用场景，另一方面远场技术还没发展到一定地步，没有太好的结果。但是，亚马逊的 Echo 问世之后，大家发现这个问题的解决虽然达不到完美，但也做得还可以了。当时 Echo 的团队进行远场研究已经有 3 、4 年了。他们就是在相关技术还不成熟的时候，更多地考虑了「明天」乃至「后天」问题，对 3 到 5 年后，我们会有何种应用、需要什么技术进行预判，然后进入这一领域，投入研究。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 14px;">机器之心：未来实验室有哪些计划呢？</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">俞栋：</span></strong><span style="font-size: 14px;">在人员方面，西雅图实验室今年的人员数预计会达到 20。当然，由于人工智能火热的现状，以及我们希望维持一个较高的选人标准，这目标会有一定挑战性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2018 年，西雅图团队会和深圳的实验室团队继续加强合作。希望能在进行前沿研究的同时，给今天的问题提供一些较好的解决方案。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">此外，去年我们花了很多时间在从头建立系统上，现在，不少组件已经准备好了，我们可以更快速地进行一些更有意思的研究，希望对学术界和我们自己的产品性能提升都有一些好的影响。<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 14px;text-align: justify;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><br></span></strong></span></strong></p><p style="max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心原创，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="1287947925" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
