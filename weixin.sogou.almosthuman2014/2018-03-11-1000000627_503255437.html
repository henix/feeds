<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>深度 | 级联MobileNet-V2实现人脸关键点检测（附训练源码）</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1521578030&amp;src=3&amp;ver=1&amp;signature=8zkN6Q7pldciyGiZY6-RVY0lqCngppvQ-VsMILKBKjpVnuC8CJf3m8SC7yFSxlGg2nfAyXi4PWTg5VldOM8wZOk4QddFsQM0ewIAfMscl3me-XbqaRuQR5ambjO5YXR8XZXcCbVPP1TIRlwgmQgg028FbG-0REJ6IshwV9Y5uoQ=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    深度 | 级联MobileNet-V2实现人脸关键点检测（附训练源码）                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-11</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="font-size: 16px;white-space: normal;max-width: 100%;line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(255, 255, 255);"><span style="background-color: rgb(117, 117, 118);">机器之心投稿</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">作者：</span></strong><span style="font-size: 12px;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">余霆嵩</span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">为了能在移动端进行实时的人脸关键点检测，本实验采用最新的轻量化模型——MobileNet-V2 作为基础模型，在 CelebA 数据上，进行两级的级联 MobileNet-V2 实现人脸关键点检测。首先，将 CelebA 数据作为第一级 MobileNet-V2 的输入，经第一级 MobileNet-V2 得到粗略的关键点位置；然后，依据第一级 MobileNet-V2 的输出，采取一定的裁剪策略，将人脸区域从原始数据上裁剪出来作为第二级 MobileNet-V2 的输入；最后，经第二级 MobileNet-V2 输出最终人脸关键点定位信息。经初步训练，最终网络单模型不到 1M，仅 956KB，单张图片 inference 耗时 6ms（采用 GTX1080 在未优化的 Caffe）。实验结果表明，MobileNet-V2 是一个性能极佳的轻量化模型，可以采用较少的参数获得较好的性能；同时，级联的操作可达到从粗到精的关键点定位。</span></p></blockquote><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: center;"><strong>一、引言</strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">人脸关键点检测也称为人脸关键点检测、定位或者人脸对齐，是指给定人脸图像，定位出人脸面部的关键区域位置，包括眉毛、眼睛、鼻子、嘴巴、脸部轮廓等和人脸检测类似，由于受到姿态和遮挡等因素的影响，人脸关键点检测是一个富有挑战性的任务。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">人脸关键点检测方法大致分为三种，分别是基 ASM(Active Shape Model)[1] 和 AAM (Active Appearnce Model)[2,3] 的传统方法；基于级联形状回归的方法 [4]；基于深度学习的方法 [5-10]。在深度学习大行其道的环境下，本实验将借鉴深度学习以及级联思想进行人脸关键点检测。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">随着手机的智能化及万物物联的兴起，在移动端上部署深度学习模型的需求日益增大。然而，为获得更佳的性能，卷积神经网络的设计，从 7 层 AlexNet[11] 到上千层的 ResNet[12] 和 DenseNet[13]，网络模型越来越大，严重阻碍了深度学习在移动端的使用。因此，一种轻量的，高效率的模型——MobileNet-V1[14] 应运而生。MobileNet-V1 最早由 Google 团队于 2017 年 4 月公布在 arXiv 上，而本实验采用的是 MobileNet-V2[15]，是在 MobileNet-V1 基础上结合当下流行的残差思想而设计的一种面向移动端的卷积神经网络模型。MobileNet-V2 不仅达到满意的性能（ImageNet2012 上 top-1:74.7%），而且运行速度以及模型大小完全可达到移动端实时的指标。因此，本实验将 MobileNet-V2 作为基础模型进行级联。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: center;"><strong>二、两级级联 MobileNet-V2</strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">2.1 整体框架及思路</span></strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本实验采用两级级联 MobileNet-V2，分别称之为 level_1 和 level_2，由于个人精力有限，level_1 和 level_2 采用完全相同的网络结构。level_1 进行粗略的关键点定位；依据 level_1 输出的关键点进行人脸区域裁剪，获得人脸区域图像作为 level_2 的输入，最终关键点定位信息由 level_2 进行输出。流程如下图所示：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.504950495049505" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOjn60DvPbrrq4It9v0zspOQTia1qNzfHaCDe6JnBoZlT79kfjFPZV0vQ/640?wx_fmt=png" data-type="png" data-w="1414"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br><span style="font-size: 14px;"></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">通常进行人脸关键点检测之前，需要进行人脸检测，即将人脸检测获得的人脸图像区域作为人脸关键点检测模型的输入。然而进行人脸检测是相当耗时的，所以，在特定场景下（即确定有且仅有一个人的图像）可以采用 level_1 代替人脸检测步骤，通过 level_1 可获得人脸区域，从而提高整个任务效率和速度。本实验正是采用 level_1 剔除非人脸区域，从而使得后一级的网络可以更为精准的进行关键点定位。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本实验仅做了两级级联，其实还可像 DCNN[5] 那样继续级联，对眼睛，鼻子，嘴巴分别进行预测，或者是学习 Face++[6] 那样，在 68 点的关键点定位中，将 68 点划分为两个区域分别预测。这些都是很好的想法，十分值得借鉴，但个人精力有限，在此仅做了两级级联作为学习，希望大家可以基于 MobileNet-V2 去改进上述两种方法。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">2.2 基础模型——MobileNet-V2</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">MobileNet-V2[15] 由 Google 团队于 2018 年 1 月公布在 arXiv 上，是一种短小精悍的模型，仅数 M 的模型就在 ImageNet 上获得 74.7% 的准确率（top-1），具体分类性能如下：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.42328042328042326" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQO2A4VnKormCLZ9qZ8nB0xMmibHhiaGR1z1exTbKPofwvibMVCu4SoqcPAA/640?wx_fmt=png" data-type="png" data-w="1134"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">原版 MobileNet-V2 网络结构如下图所示：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.7323135755258127" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOoF0v1P0A3Xu8icYibCiczGuCkaVicIjypG4UGrRd6ickXemAAS1wE8BucpA/640?wx_fmt=png" data-type="png" data-w="1046"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">其中：t 表示「扩张」倍数，c 表示输出通道数，n 表示重复次数，s 表示步长 stride。 </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">解释一下原文的有误之处： </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 第五行，也就是第四个 bottleneck，stride=2，分辨率应该从 28 降低到 14；要么就是 stride=1； </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 文中提到共计采用 19 个 bottleneck，但是这里只有 17 个。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">结合本实验任务的需求，设计了一个输入尺寸为 48*48 的 MobileNet-V2（level_1 和 level_2 均采用此网络结构），网络结构如下图所示：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.8284600389863548" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQO0Hdmg1RY0PLxMIYyj7KJQadpyLHpydFoggkq7VODH05icjrcFzwTcKA/640?wx_fmt=png" data-type="png" data-w="1026"></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: center;"><strong>三、实验及结果</strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">3.1 数据集简介</span></strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">实验数据采用 CelebFaces Attributes Dataset (CelebA) 公开数据集 (http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)，CelebA 数据集是香港中文大学的开放的数据集，此数据集包含 10177 个名人，共计 202599 张图片，每张图片含 5 个关键点标注信息以及 40 个属性标签。经过实验结果分析，此数据集在人脸关键点检测上的困难点在于：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 人脸区域面积占图片面积大小不一，有部分图片中的人脸占比相当小。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">例如：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.7176470588235294" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOiajfpaEX5nCouAB3C9epGJzz7ia2Sf7N5u2hLoZicVoHw03lv1hrQAXibg/640?wx_fmt=png" data-type="png" data-w="1190"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 人脸角度多样性强，俯仰角（pitch）、偏航角 (yaw) 和横滚角 (roll) 均有，但绝大多数数据为正向无偏角，从而导致正向无偏角检测效果较好，复杂偏角检测效果较差。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.5743243243243243" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOK0TjnaHPkIvuwQMLg92bmmKy70QWHQKC98cO3bhwfFTDslicEpdyLhQ/640?wx_fmt=png" data-type="png" data-w="1480"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3. 噪音图片，图片「不完整」，例如存在大部分黑色区域图片。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="1.0395136778115501" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQObLeXVRxL2HG6b1Mv9lvwlMI6AaZwAFxnH6rXTPUQreqUXlqOe3gqHA/640?wx_fmt=png" data-type="png" data-w="658"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">3.2 评价指标</span></strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这里采用相对误差作为评价指标，相对误差计算公式为：，其中表示输出 landmark 与真实 landmark 的欧氏距离，表示图片对角线的长度。一开始，本实验采用的是双眼距离作为归一化因子，但是 CelebA 数据集并不适合采用双眼距离作为归一化因子，因为在计算时会出现 inf！即两眼在同一位置，导致分母为零，这是 LFW,ALFW 数据集不会出现的情况。两眼距离为零的图片示意图；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.670926517571885" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOFbafAn91PBe0XKHIxZ0WXAia5ySm88PTX9pF5CGeiaa6H7ic45ZyRGdFA/640?wx_fmt=png" data-type="png" data-w="1252"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">3.3 模型训练</span></strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">level_1 与 level_2 采用相同网络结构，solver 也一致。solver 为：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.13399153737658676" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOYWibAuiagICBbPnmHDyDahGGeNM8oIMO4Rd7tXibsiaBM5MGASPPibU97aA/640?wx_fmt=png" data-type="png" data-w="1418"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">level_1 的训练 loss 曲线如图所示：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.3845070422535211" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQO8UU7sG3Y3d7a09glBBFzx01F0ajt11C3bOFsaOV3MKjkvroTJWLQww/640?wx_fmt=png" data-type="png" data-w="1420"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Train loss 大概是在 0.005~0.006，Test loss 是在 0.007~0.008（注：训练的时候 loss_weighs 设置为 100，因此上图数量级上不一致）</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">训练和测试的平均误差分别为 1.01% 和 1.17%。5 个点的平均误差如下图所示：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="1.5129411764705882" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOAa2KyxY5J0ojfGy6Y4CqIwtqlQlhSJIusGccc0RwadTl1ibD3sia0SjA/640?wx_fmt=png" data-type="png" data-w="850"></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">训练集和测试集各点误差分布如图：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.5827338129496403" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOqpXqEf9kSKn8kViciba4gibh8kALZ3eHuXkfNZ48Hm2QXGTYfneA7Mmnw/640?wx_fmt=png" data-type="png" data-w="1390"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.5701881331403763" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOfeVhfRAPWx4XBibl1lTvugf3kTtj3UTIA7tmkvWyo4icN7mIKGcSicxLQ/640?wx_fmt=png" data-type="png" data-w="1382"></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">从误差分布图可以看到，误差分布呈高斯分布状，训练与测试的分布形状基本一致，通过误差分布图可知，在训练集中，绝大部分误差小于 5%，因此在选取 level_1 的裁剪阈值时，可参照此图进行调试。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">level_1 训练完毕，需要进行图片裁剪，以供 level_2 进行训练。这里采取的裁剪策略为，以 level_1 预测到的鼻子为中心，裁剪出一个正方形，这个正方形的边长为四倍鼻子到左眼的距离。以下图为例，绿色点为 level_1 的预测关键点，红色方框即为裁剪框：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.7674418604651163" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOMcDXiaia4wcAAzpeyqNfujZIVTaKucyObyqA7gUSsJa3UcZIbOLgkHng/640?wx_fmt=png" data-type="png" data-w="688"></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在裁剪时，还需要对部分图片进行丢弃，以此确保裁剪之后的图片能包含完整的人脸。本实验将丢弃阈值设置为 10%，即误差大于 10% 的图片进行丢弃，然而实验结果表明 10% 并不是一个较好的阈值，应该设置为 5% 比较合理。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">经过裁剪操作可获得 level_2 的训练数据，接着训练 level_2，level_2 的 loss 曲线如图：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.38870967741935486" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQO9L0XcJFkvxquseqhV9YIB5uXS1y2dNh7AgNtvvnYPrrcdr9fjic1b0A/640?wx_fmt=png" data-type="png" data-w="1240"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">level_2 与 level_1 的曲线有着明显的不同，即曲线出现了很大的浮动，有尖点存在。这是为什么呢？其实是因为 level_1 剪裁出了问题，在裁剪时，将误差小于 10% 的图片保留，用以裁剪。其实这个阈值（10%）还是太大了，以至于裁剪出这一些「不合格」图片。例如：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.4722222222222222" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOOgy816Sc54F3icWFXF4CrUll4Nth8JOL4VIEKBYuLGLDkcsmDcQaBMA/640?wx_fmt=png" data-type="png" data-w="1296"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">level_2 训练完毕，即可进行级联检测，forward 耗时如图所示：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.9473684210526315" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOmjdowMsqze9QIVF0Ay4uET5A286dzDbia1RNMosMjHN1xWPHt9iaySHg/640?wx_fmt=png" data-type="png" data-w="608"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">级联检测效果如下图所示，其中，绿色点为真实 landmark，红色点为 level_1 的预测，蓝色点为 level_2 的预测。可以看到蓝色点比红色点更靠近绿色点：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.6656394453004623" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOXS1aQIhZ6fH4ic2KPCOgNOlBWjOksiauWK4mbBN53RAGj2bcZCwA0Vqg/640?wx_fmt=png" data-type="png" data-w="1298"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="white-space: normal;text-align: center;"><strong>四、总结与展望</strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本实验在 CelebA 数据集上，采用最新的轻量化网络——MobileNet-V2 作为基础模型，进行级联卷积神经网络人脸关键点检测实验，初步验证 MobileNet-V2 是短小精悍的模型，并且从模型大小以及运行速度上可知，此模型可在移动端实现实时检测。此实验只是对级联思想的一个简单验证，若要获得更高精度的人脸关键点模型，还有很多可改进的地方。在此总结此实验已知的不足之处，供大家参考并改进：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1.	level_1 的输入可为 24*24 甚至更小，只要保证能依据输出的 landmark 来裁剪处人脸区域即可；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2.	level_1 的裁剪策略相对「拙劣」，可依据具体应用场景，提出不同的裁剪策略，确保输入到 level_2 的图片包含整个人脸；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3.	模型训练不足，未完全收敛，可对 solver 进行修改，获得更好的模型；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">4.	可增加第三级模型，分别对眼睛，鼻子，嘴巴进行检测，从而获得更精确定位点；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">5.	对于具体应用场景，应依据困难样本，需要针对性的做数据增强，例如 CelebA 中，正向无偏角人脸较多，而有俯仰角、偏航角和横滚角的图片占少数，从而导致模型对含偏角的图片预测效果较差；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">6.	未进行模型压缩，若需要部署，则要进行模型压缩，从而获得更小的模型；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">7.	Caffe 对 depth-wise convolution 的支持并不友好，从而在 Caffe 下体现不出 MobileNet-V2 的高效，可尝试 TensorFlow 下进行；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">8.	欢迎大家补充</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">下一步工作：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 针对以上不足进行改进；</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 寻找有兴趣的朋友进行 68 点关键点检测实验，并进行模型压缩，获得更小更好的网络。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="font-size: 14px;">本实验所有代码可在 GitHub 上获得：https://github.com/tensor-yu/cascaded_mobilenet-v2</span></p><p style="white-space: normal;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="font-size: 14px;">详细训练步骤可参见博客：http://blog.csdn.net/u011995719/article/details/79435615</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">欢迎大家提出宝贵的意见和建议。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">参考文献</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[1] T.F. Cootes, C.J. Taylor, D.H. Cooper, et al. Active Shape Models-Their Training and Application[J]. Computer Vision and Image Understanding, 1995, 61(1):38-59.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[2] G. J. Edwards, T. F. Cootes, C. J. Taylor. Face recognition using active appearance models[J]. Computer Vision—Eccv』, 1998, 1407(6):581-595.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[3] Cootes T F, Edwards G J, Taylor C J. Active appearance models[C]// European Conference on Computer Vision. Springer Berlin Heidelberg, 1998:484-498.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[4] Dollár P, Welinder P, Perona P. Cascaded pose regression[J]. IEEE, 2010, 238(6):1078-1085.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[5] Sun Y, Wang X, Tang X. Deep Convolutional Network Cascade for Facial Point Detection[C]// Computer Vision and Pattern Recognition. IEEE, 2013:3476-3483.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[6] Zhou E, Fan H, Cao Z, et al. Extensive Facial Landmark Localization with Coarse-to-Fine Convolutional Network Cascade[C]// IEEE International Conference on Computer Vision Workshops. IEEE, 2014:386-391.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[7] Zhang Z, Luo P, Chen C L, et al. Facial Landmark Detection by Deep Multi-task Learning[C]// European Conference on Computer Vision. 2014:94-108.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[8] Wu Y, Hassner T. Facial Landmark Detection with Tweaked Convolutional Neural Networks[J]. Computer Science, 2015.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[9] Zhang K, Zhang Z, Li Z, et al. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks[J]. IEEE Signal Processing Letters, 2016, 23(10):1499-1503.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[10] Kowalski M, Naruniec J, Trzcinski T. Deep Alignment Network: A Convolutional Neural Network for Robust Face Alignment[J]. 2017:2034-2043.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[11] Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[12] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]// Computer Vision and Pattern Recognition. IEEE, 2016:770-778.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[13] Huang G, Liu Z, Maaten L V D, et al. Densely Connected Convolutional Networks[C]// CVPR. 2017.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[14] Howard A G, Zhu M, Chen B, et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications[J]. 2017.</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">[15] Sandler M, Howard A, Zhu M, et al. Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification,axXiv. </span></em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">作者简介：余霆嵩，广东工业大学研三学生，研究方向：关键点定位，模型压缩，Email：</span><span style="font-size: 14px;color: rgb(123, 12, 0);">yts3221@126.com</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心投稿，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系作者获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="488920046" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
