<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Boosting</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/26215100">原文</a></p>
<div class="title-image"><img src="https://pic1.zhimg.com/v2-b78698b31a42ab3d9eca6278ce4512f5_r.jpg" alt=""></div><b>1.Boosting</b><br><p>Boosting是一种通过组合弱学习器来产生强学习器的通用且有效的方法。本文中我们将重点讲解三种Boosting算法:AdaBoost, RankBoost, Gradient Boosting。AdaBoost是第一个成功的Boosting算法,所以我们先介绍AdaBoost</p><p><b>2.AdaBoost</b></p><p>2.1动机</p>AdaBoost的动机是基于如下观察:尽管委员会中每个成员只提供一些不成熟的判断,但整个委员会却产生较为准确的决策。AdaBoost通过组合多个弱学习器来解决学习问题。给定训练数据,弱学习算法(如决策树)可以训练产生弱学习器,这些弱学习器只需要比随机猜测的准确率好一些。用不同的训练数据训练可以得到不同的弱学习器。这些弱学习器作为委员会成员,共同决策。为了从不同的弱学习器中聚集”智慧”,<u>AdaBoost解决了如下两个问题:首先,如何选择一组有不同优缺点的弱学习器,使得它们可以相互弥补不足。其次,如何组合弱学习器的输出以获得整体的更好的决策表现。</u>为了解决第一个问题,AdaBoost让每一个新加入的弱学习器都体现出一些新的数据中的模式。为了实现这一点,AdaBoost为每一个训练样本维护一个权重分布。即对任意一个样本<equation>x_i</equation>都有一个分布<equation>D(i)</equation>与之对应,表示这个样本的重要性。当衡量弱学习器的表现时,AdaBoost会考虑每个样本的权重。权重较大的误分类样本会比权重较小的误分类样本贡献更大的训练错误率。为了获得更小的加权错误率,弱分类器必须更多的聚焦于高权重的样本,保证对它们准确的预测。通过修改样本的权重<equation>D(i)</equation>就可以引导弱学习器学习训练样本的不同部分。<p>AdaBoost分多轮训练,在训练的第<equation>t</equation>轮,我们更新样本的权重到<equation>D_t</equation>并训练一个弱学习器,使得该学习器在权重分布<equation>D_t</equation>上产生最小的加权训练误差。在第一轮中,所有样本权重相同,为 <equation>\frac{1}{n\_samples}</equation> 。在之后的每一轮中,我们提高误分类样本的权重,降低准确分类样本的权重。这样我们就使得每一轮的弱学习器都更多的聚焦于上一轮中较难被准确分类的样本。现在，我们获得了一组已训练的拥有不同优缺点的弱学习器，如何有效的组合它们，使得相互优势互补来产生更准确的整体预测效果？每一个弱学习器是用不同的权重分布训练出来的，我们可以看做给不同的弱学习器分配了不同的任务，每个弱学习器都尽力完成给定的任务。直觉上看，当我们要把每个弱学习器的判断组合到最终的预测结果中时，如果弱学习器在之前的任务中表现优异，我们会更多的相信它，相反，如果弱学习器在之前的任务中表现较差，我们就更少的相信它。换句话说,我们会加权地组合弱学习器，给每个弱学习器赋予一个表示可信程度的值<equation>a_t</equation> ，这个值取决于它在被分配的任务中的表现，表现越好<equation>a_t</equation>越大，反之越小。</p><p>在详细介绍算法之前,我们先来看一个例子(Figure 1)。我们有10个样本点被标记为+1或者-1。假设我们的弱学习器是一条水平的或者竖直的线。很容易看出没有任何一条线可以完美地分类这10个样本点。正如我们即将看到的那样，AdaBoost可以通过组合弱学习器，生成一个在样本中完美分类的分类模型。</p><p>在第一轮，每个样本点具有相同的权重，弱学习器h1通过最小化加权误差得到。有三个样本点被错误分类(圆圈中的点)。在第二轮,三个样本点被赋予更大的权重(权重用样本点的大小来表示)，而其他的点权重被降低。这样h2就更多的学习错误分类点的特征。但是h2 错误分类了具有较小权重的其他样本点。在第三轮，h3只错误分类了三个具有非常低的权重的样本点。h1, h2, h3加权投票后
得到的组合模型准确分类了所有的样本点(Figure 2)。</p><blockquote>Figure1:每轮迭代中的权重分布与弱学习器</blockquote><img src="https://pic3.zhimg.com/v2-69bbb6211006472d4c402d7953e6c3fe_r.png" data-rawwidth="1076" data-rawheight="212"><blockquote>Figure2:弱学习器集成<br></blockquote><img src="https://pic4.zhimg.com/v2-20e864b6d5511c81df933b67aaa1e8d0_r.png" data-rawwidth="776" data-rawheight="420"><p> 2.2算法</p><p>现在我们给出详细算法</p><p>给定<equation>(x_1, y_1), (x_2, y_2),...,(x_m,y_m), x_i \in\chi, y_i \in \{-1, 1\}</equation></p><blockquote>1.初始化：<equation>for\ i=1,2,3...m\ \ \ D_1(i) = 1/m</equation><br>2.<equation>for\ t=1,2...,T:</equation><br>          使用<equation>D_t
</equation>训练弱学习器，获得弱学习器<equation>h_t:\chi\rightarrow\{-1,1\}</equation><br>          训练目标：最小化加权误差<equation>\epsilon_t=Pr_{i\backsim D_t}[h_t(x_t) \neq y_i]</equation><br>          计算模型的权重：<equation>a_t = \frac{1}{2} \ln(\frac{1-\epsilon_t}{\epsilon_t})</equation><br>          更新样本分布：<equation>D_{t+1} = \frac{D_t(i)}{Z_t}\times\left\{\begin{array}{lc}
e^{-a_t} &amp; if\ h_t(x_i) = y_i\\
e^{a_t} &amp; if\ h_t(x_i) \neq y_i
\end{array}\right.</equation><br>          (<equation>Z_t</equation>是归一化系数，保证<equation>D_t</equation>为一个概率分布)<br>3.输出最终结果:<equation>H(x) = sign(\Sigma^T_{t=1}a_th_t(x))</equation></blockquote><p>2.3AdaBoost的训练误差</p><p>令<equation>\gamma = \frac{1}{2} -\epsilon_t</equation>,可以证明集成的分类器<equation>H(x)</equation>在<equation>D_1
</equation>上的训练误差满足：</p><img src="https://pic3.zhimg.com/v2-af8dacabb628e47c07eb2ee989d85dce_r.png" data-rawwidth="1090" data-rawheight="206"><br><p>证明如下</p><p><img src="https://pic1.zhimg.com/v2-6b42ff0e3697cb5d340e7500c64add3b_r.png" data-rawwidth="1106" data-rawheight="1408">因此，AdaBoost训练最终会出现两种情况：一种是训练误差降低到零。另一种是<equation>\gamma_t=0(\epsilon_t=0.5)</equation>，算法阻塞：这样的分布使得每一个后续的弱分类器都只有0.5的准确率</p><br><b>3.RankBoost</b><p>3.1排序学习<br></p><p>在这一节，我们讲述一种用于排序问题的特殊Boosting算法：RankBoost</p><p>排序问题在很多领域都有出现。比如，在网页搜索中，搜索引擎需要根据永和query和网页的相关性对网页排序。在产品推荐中，推荐系统根据用户喜欢或者购买商品的可能性对商品排序。排序学习问题的学习目标是找到一条好的排序规则。RankBoost就是一个在某些训练数据上自动的学习排序规则的有效算法。</p><p>我们首先介绍排序学习的问题设定。需要被排序的数据点全集用<equation>\chi</equation>表示。学习器被提供一部分数据点的集合<equation>V</equation>用于训练<equation>V\subset\chi</equation>。训练数据点通常带有他的用户排序序号。比如，在电影推荐问题中，我们有一些电影，每个电影被从1星到5星进行排序。与4星电影相比，用户通常更喜欢5星电影。但是两个4星电影之间没有直接的大小关系比较结果。严格的说，设<equation>V = V_1\bigcup V2\bigcup \dots\bigcup V_m</equation>为训练集，其中<equation>V_i</equation>是相互不相交的<equation>\chi</equation>的子集。对于<equation>j&lt;k</equation>,<equation>V_j</equation>中所有样本的排序都比<equation>V_k</equation>中的样本小。我们可以这种形式的的用户反馈成为层次化的反馈。令<equation>E = \{(u,v)|v\quad is\ ranked\ above\quad u\}</equation>，那么<equation>E = \bigcup_{1\leq j \leq k \leq J}V_j \times V_k</equation>。我们的学习目标是找到一个规则，能够最大的与训练数据保持一致。形式化的说，排序规则就是一个实值函数<equation>F:\chi\rightarrow R</equation>,对于样本点<equation>u, v</equation>若<equation>v</equation>排序比<equation>u</equation>高，则<equation>F(v) &gt; F(u)</equation>.因此，误排序引发的与训练数据的不一致程度可以写成：<equation>\frac{1}{|E|}\sum_{(u,v)\in E}\Large 1\{F(v) \leq F(u)\}</equation>,也可以称作经验排序损失函数。</p><br>3.2 RankBoost算法<p>为了最小化经验排序损失函数，人们开发了几个不同版本的RankBoost算法。下面要介绍的算法把排序问题简化成了二分类问题。虽然这种简化不精确，但是非常有效。对于每一个数据点<equation>x</equation>，我们为+1， -1两个标签分配不同的概率(或者称之为权重)<equation>D(x, -1), D(x,+1)</equation>。在每一轮迭代中，概率的分布会被重新计算，并且一个弱学习器会以最小化加权误差为目标进行训练。</p><p>算法过程如下：</p><p>给定：非空，不相交的<equation>\chi</equation>的子集<equation>V_1, V_2, V_3...V_J</equation>，表示偏好对<equation>E = \bigcup_{1\leq j&lt;k \leq J}V_j \times V_k</equation></p><p>1.初始化：<equation>F_0 = 0</equation></p><p>2.<equation>for\ t = 1,2,...T</equation><br></p><ul><equation>for\ j=1,2,...,J\ and\ y\in\{-1,+1\}</equation><br><equation>S_{t,j}(y) = \sum_{k=1}^{j-1}\exp{(yF_{t-1}(x))}</equation><br><equation>C_{t,j}(+1) = \sum_{k=1}^{j-1}S_{t,k}(+1)</equation><br><equation>C_{t,j}(-1) = \sum_{k=j+1}^{J}S_{t,k}(-1)</equation><br><li>使用分布<equation>D_t</equation>训练弱学习器。<equation>D_t(x,y) = \frac{1}{2Z_t}\exp{-yF_{t-1}(x)C_{t,j}(y)}</equation>其中<equation>Z_t</equation>是归一化系数，保证<equation>D_t</equation>是一个分布。</li><li>以最小化加权误差<equation>\epsilon_t = \Pr_{(x,y)\sim D}[h_t(x) \neq y]</equation>为目标，训练弱学习器<equation>h_t:\chi\rightarrow\{-1, +1\}</equation></li><li>计算弱学习器权重<equation>a_t = \frac{1}{2}\ln(\frac{1-\epsilon_t}{\epsilon_t})
</equation></li><li>更新集成后的模型：<equation>F_t = F_{t-1} + \frac{1}{2}\alpha_th_t</equation></li></ul><p>输出：<equation>F(x) = F_T(x) = \frac{1}{2}\sum_{t=1}^{T}\alpha_th_t(x)</equation></p><br><p>4 Gradient Boosting</p><p>4.1概述</p><p>Gradient Boosting是一种强大而灵活的机器学习算法。可以用于分类，回归，排序等问题。在2009年，gradient boosting赢得了Yahoo排序学习问题的比赛。正如它的名字暗示的，gradient boosting是boosting和梯度下降法的结合。</p><p>4.2前向多级加性模型拟合算法</p><p>在之前几节的介绍中，AdaBoost用向前，分多级的方式拟合了一个加性模型<equation>\sum_t\rho_th_t(x)</equation>，在每一级，AdaBoost都训练得到一个新的弱学习器，这个新的弱学习器加到集成的模型中去，可以很好的弥补之前的弱学习器的不足之处。在AdaBoost中，这种"不足"是通过更大的样本权重体现的，样本权重越大，之前的分类器分类效果越不好，新的学习器更多地学习权重较大的样本的特征，这样来弥补之前的模型的不足之处。</p><p>Gradient boosting采取了相同的思路。也用向前，分多级的方式拟合了一个加性模型<equation>\sum_t\rho_th_t(x)</equation>在每一级，Gradient Boosting都训练得到一个新的弱学习器，这个新的弱学习器加到集成的模型中去，可以很好的弥补之前的弱学习器的不足之处。这时，"不足"不再以样本权重的形式显示出来，而是以梯度的形式显示。不管是样本权重还是梯度，都可以告诉我们如何提升(Boosting)原来模型的表现。</p><p>AdaBoost是第一个成功应用的Boosting算法，后来，Breiman把AdaBoost表达成了一种在特定损失函数上的梯度下降算法。后来，Freiman又把AdaBoost推广到了Gradient Boosting算法，目的是为了适应不同的损失函数。</p><p>4.3回归问题中的Gradient Boosting</p><p>Gradient Boosting可以很自然的解决回归问题，分类和排序问题可以很好的转换成一个回归问题。所以我们先从回归问题开始讲述Gradient Boosting</p><p>4.3.1用平方损失函数回归</p><br>我们首先看一个例子，你现在有一堆训练数据<equation>(x_1, y_1),(x_2, y_2),...,(x_n,y_n)</equation>，你的任务是拟合一个模型<equation>F(x)</equation>来最小化平方损失函数。假设你有个朋友送了你有个看起来还不错的模型<equation>F</equation>，但是这个模型并不完美。这个模型在预测是犯了一些错误：<equation>y_1=0.9,\ F(x_1)=0.8,\quad y_2=1.3 \ F(x_2)=1.4\dots</equation>,现在你应该如何<b>提升</b>这个模型呢？(在这个例子中，不允许改动<equation>F</equation>，但是可以加一个新的模型<equation>h(x)</equation>到<equation>F(x)</equation>上，得到新的模型<equation>F(x) + h(x)</equation>)<blockquote><p>正确的答案应该是这样：</p><equation>F(x_1)+h(x_1)=y_1\\
F(x_2)+h(x_2)=y_2\\
F(x_3)+h(x_3)=y_3\\
\dots\\
F(x_n) +h(x_n)=y_n
</equation><br>我们希望找到<equation>h(x)</equation>满足上面的式子。等价地可以写成：<equation>h(x_1)=y_1-F(x_1)\\
h(x_2)=y_2-F(x_2)\\
h(x_3)=y_3-F(x_3)\\
\dots\\
h(x_n)=y_n-F(x_n) 
</equation><br><p>回归树模型可以很好的近似这个新模型<equation>h(x)</equation></p><p>为了得到<equation>h(x)</equation>,我们首先准备如下训练数据：<equation>(x_1,y_1-F(x_1)), (x_2,y_2-F(x_2)),(x_3,y_3-F(x_3))....(x_n,y_n-F(x_n))</equation></p><p>使用这样的训练数据拟合回归树模型，得到<equation>h(x)</equation></p></blockquote><p><equation>y_i - F(x_i)</equation>被称为残差，它是旧模型<equation>F(x)</equation>预测表现不好的部分。<equation>h(x)</equation>的作用就是学习这部分旧模型表现不好的数据，这样来弥补旧模型的不足之处。如果<equation>F+h</equation>预测表现仍然不好，我们可以在此基础上继续加模型。<br></p><p>以上就是Gradient Boosting在回归问题中应用的基本思路。我们很清楚的看到，这种方法也延续了boosting的特点：增加一个弱学习器，新加入的弱学习器能弥补已知学习器的不足之处。我们现在来看看与梯度下降的关系在哪里。</p><p>梯度下降通过向梯度反方向移动来最小化函数<equation>J(\theta)</equation>, <equation>\theta_j = \theta_j-\rho\frac{\partial J}{\partial\theta_j}</equation></p><p>在Gradient Boosting中，我们发现平方损失函数为<equation>L(y,F(x)) = (y-F(x))^2/2</equation>我们的目标是通过调整<equation>F(x_i),i=1,2,3...</equation>最小化<equation>J=\sum_iL(y_i, F(x_i))</equation>。对<equation>J</equation>求导：<equation>\frac{\partial J}{\partial F(x_i)} = F(x_i) - y_i</equation>,根据梯度下降法，<equation>F(x_i) = F(x_i) - \rho \frac{\partial J}{\partial F(x_i)} =F(x_i) + \rho(y_i - F(x_i)) = F(x_i) + \rho h(x_i), where\ \rho=1</equation><br></p><p>对于平方损失函数，残差<equation>y_i-F(x_i)</equation>就是负的梯度。所以实际上我们在使用梯度下降法更新<equation>F(x)</equation></p><p>我们来总结一下刚才推导的算法：</p><blockquote><ul><li>1.初始化一个模型<equation>F(x) = \frac{\sum_{i=1}^{n}{y_i}}{n}</equation></li><br><li>2.计算负梯度：<equation>y_i - F(x_i)</equation></li><br><li>3.训练一个回归树拟合负梯度：<equation>h(x)</equation></li><br><li>4.<equation>F=F+\rho h\ where\ \rho=1</equation></li><br><li>5.若未满足停机条件，回到2</li></ul></blockquote><br><p>4.3.2用其他损失函数回归</p><p>把算法改成使用梯度的形式有一个好处，就是让我们考虑其他损失函数，用同样的方式衍生出其他类似算法。你可能会问，为什么我们应该考虑其他损失函数呢？难道平方损失函数不好吗？实际上平方损失函数有很好的数学性质，但是对利群点很敏感。我们看下面的例子：</p><p><img src="https://pic3.zhimg.com/v2-2d0887e0f1e34a04b25e3b67e6779450_r.png" data-rawwidth="1062" data-rawheight="220">假设最后一个点(1.7, 5)的y值是错误的。我们看到最后一个点贡献了97%的总平方损失。结果就是算法把注意力都集中到了最后一个点上，尽很大努力让模型更符合最后一个点，最后降低了整体的预测表现。</p><p>有一些损失函数就能够弥补平方损失的不足。比如绝对值损失函数<equation>L(y,F) = |y-F|</equation>, Huber损失函数：<equation>L(y,F) = \left\{\begin{array}{lr}\frac{1}{2}(y-F)^2&amp;|y-F|\leq\sigma\\ \sigma(|y-F| - \sigma/2) &amp; |y-F|&gt;\sigma\end{array} \right.</equation></p><p><img src="https://pic2.zhimg.com/v2-3ad7aa96359f4258f0f96f23c5d480d4_r.png" data-rawwidth="1160" data-rawheight="350">顺着Gradient Boosting的思路，我们可以用绝对值损失函数生成一个回归算法。其中负梯度为:<equation>-g(x_i) = -\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} = sign(y_i - F(x_i))</equation></p><blockquote><ul><li>1.初始化一个模型<equation>F(x) = \frac{\sum_{i=1}^{n}{y_i}}{n}</equation></li><br><li>2.计算负梯度：<equation>sign(y_i - F(x_i))</equation><br></li><br><li>3.训练一个回归树拟合负梯度：<equation>h(x)</equation></li><br><li>4.<equation>F=F+\rho h\ where\ \rho=1</equation></li><br><li>5.若未满足停机条件，回到2</li></ul></blockquote><p>同样的，使用Huber损失函数，只需要修改负梯度的计算</p><p><img src="https://pic2.zhimg.com/v2-f6bb639da3295232030970c2327bfc10_r.png" data-rawwidth="1030" data-rawheight="320">从这里我们就可以看到，使用梯度更新比使用残差更新更容易增加算法的鲁棒性。关于梯度下降的学习率可以参考Friedman的Greedy function approximation: a gradient boosting machine</p><p>4.4分类问题中的Gradient Boosting</p><p>我们使用字符识别问题的例子来讨论如何把Gradient Boosting应用到分类问题上。我们的任务是正确识别手写的大写字母，这是一个多分类问题，我们共有26个类别，分别是A，B，C...Z.数据集可以从这里下载：<a href="http://archive.ics.uci.edu/ml/datasets/Letter+Recognition" class="" data-editable="true" data-title="Letter Recognition Data Set">Letter Recognition Data Set</a> 总共20000个训练样本，每个样本有16个特征。</p><p>在我们的Gradient Boosting算法中，我们设置26个评分函数：<equation>F_A,F_B,F_C,...,F_Z</equation>。给定一个样本x,<equation>F_A(x)</equation>表示类别A给x打的分数，<equation>F_B, F_C...</equation>类似。用这些分数可以计算类别的概率</p><p><img src="https://pic4.zhimg.com/v2-85b86d7c7638c073474e07682b3084e5_r.png" data-rawwidth="548" data-rawheight="534">预测的结果就是概率最大的类别。</p><p>Gradient Boosting通过拟合真实的条件概率<equation>p(y=k|x)</equation>来完成分类问题.。为了量化模型近似的效果，我们引入KL-散度，<equation>D_{KL} (p||q) = \sum_ip_i log \frac{p_i}{q_i}</equation>,它可以表示两个概率的不相似程度。通常我们把p作为真实的分布，q作为估计出来的分布。很显然，KL散度在这里扮演了损失函数的角色。更精确的说，我们在这里使用如下步骤计算损失函数：</p><blockquote>1.把每个样本<equation>(x_i, y_i)</equation>的标签<equation>y_i</equation>转换成分布<equation>Y(x_i)</equation>,比如，<equation>y_5=G</equation>,则<equation>Y_A(x_5) = 0, Y_B(x_5) = 0,Y_C(x_5) = 0...Y_G(x_5) = 1...Y_Z(x_5) = 0</equation><br>2.用当前的模型<equation>F_A, F_B, F_C,...,F_Z</equation>计算类别概率。<br><equation>P_A(x_5) = 0.03, P_B(x_5)=0.05...P_G(x_5)=0.3....</equation><br>3.计算散度<equation>D_{KL}(Y||P)</equation></blockquote><p><img src="https://pic1.zhimg.com/v2-8267b601a90846d2d47b5579dd396331_r.png" data-rawwidth="432" data-rawheight="635">我们可以看到，Gradient Boosting把分类问题转换成了回归问题。我们的目标是最小化总的损失函数(KL散度)。对于每一个样本点，我们希望尽可能让预测的概率和真实概率接近。我们通过修改<equation>F_A,F_B,...,F_Z</equation>来达到这个目的。</p><p>对比使用Gradient Boosting的分类和回归问题，我们发现，我们现在有更多的参数需要优化。</p><equation>F_A,F_B,F_C...F_Z\ \ vs\ \ F</equation><br><p>实际上除了需要优化的参数更多，分类问题和回归问题上的Gradient Boosting过程是相似的。</p>初始化模型：<equation>F_A,F_B,F_C,...,F_Z</equation><br>迭代直到收敛：<ul><li>为A计算负梯度：<equation>-g_A(x_i) = Y_A(x_i) - P_A(x_i)</equation><br></li><li>为B计算负梯度：<equation>-g_B(x_i) = Y_B(x_i) - P_B(x_i)</equation><br></li><li>...</li><li>为Z计算负梯度：<equation>-g_Zx_i) = Y_Z(x_i) - P_Z(x_i)</equation><br></li><li>训练回归树<equation>h_A</equation>拟合负梯度<equation>-g_A(x_i)</equation></li><li>训练回归树<equation>h_B</equation>拟合负梯度<equation>-g_B(x_i)</equation></li><li>训练回归树<equation>h_C</equation>拟合负梯度<equation>-g_C(x_i)</equation></li><li>...</li><li>训练回归树<equation>h_Z</equation>拟合负梯度<equation>-g_Z(x_i)</equation></li><equation>F_A = F_A + \rho h_A \\
F_B = F_B + \rho h_B \\
F_C = F_C + \rho h_C \\
...\\
F_Z = F_Z + \rho h_Z</equation></ul><p>下面的表显示了每一轮迭代地过程(下面的例子真实标签是字母G)</p><img src="https://pic2.zhimg.com/v2-94bea1d9bd20ba52cd4fb0c9ec8d9c68_r.png" data-rawwidth="1732" data-rawheight="1488"><img src="https://pic2.zhimg.com/v2-afef5351898af8bbdc66c00b21509eff_r.png" data-rawwidth="1580" data-rawheight="720"><img src="https://pic4.zhimg.com/v2-79e1df31560d53a7bd653a2e4f80a1de_r.png" data-rawwidth="1194" data-rawheight="820"><img src="https://pic4.zhimg.com/v2-2b121dbd413640d9c8db223f55309e5d_r.png" data-rawwidth="1200" data-rawheight="810"><img src="https://pic1.zhimg.com/v2-b0abfa42b4ba4122a5d8a926b39a8765_r.png" data-rawwidth="1224" data-rawheight="836"><img src="https://pic2.zhimg.com/v2-a231f193fc0c3dddeea29cb1fe5d8358_r.png" data-rawwidth="1314" data-rawheight="872"><img src="https://pic1.zhimg.com/v2-0941568f1a8c891b62ed5425e20f5438_r.png" data-rawwidth="1268" data-rawheight="850"><img src="https://pic4.zhimg.com/v2-b8210fba7e0629f586b9354f27b6e365_r.png" data-rawwidth="1268" data-rawheight="832"><img src="https://pic4.zhimg.com/v2-060f58bbd8626c4427deb65ac48c48b1_r.png" data-rawwidth="1258" data-rawheight="834"><img src="https://pic3.zhimg.com/v2-57b6f9661d854bec29d6cbcff4105624_r.png" data-rawwidth="1256" data-rawheight="816">
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
