<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>策略||文字数据</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/40184624">原文</a></p>
<p><b>   一直以来策略用到的即时文字数据，获取和处理都比较麻烦，这里笔者简单介绍一下获取和处理方法，关于文字数据因子的使用放到下一章节</b></p><p>1<b>数据的获取</b></p><p>依旧以对证券市场分析为例，常用到的数据有“财经新闻、上市公司公告、股吧网友讨论等”。我们希望从这些数据源中能得到有价值的信息，可能是一段时间的新闻热点、可能是网友对不同事件的正负面情绪、或者其他一些。<br><br>第一步就是获取数据，新闻相关的有新浪财经、华尔街见闻等；上市公司公告有巨潮资讯网、交易所官网；股吧有东方财富网、雪球等。<br><br><br>一般选用的方法就是爬虫了，根据各网站的不同，爬取难易程度不一 。大规模爬虫可以选用：scrapy  分布式爬取，而一般简单的爬虫可以用： lxml、BeautifulSoup、 Requests、Selenium等。具体操作过程中，有些网站有比较强的反爬虫机制，需要加ip代理池等操作。<br> <br><b>举个简单例子 —— 爬取中国证券报网站上近一周的所有公司新闻。</b><br>简单过程就是：<br>a. 找到目标网页的URL。<br>b. 在目标页面URL中找到目标内容并保存。<br>一般可以通过lxml.etree用xpath定位实现、或者用BeautifulSoup根据CSS定位实现。<br><br></p><img src="https://pic3.zhimg.com/v2-1d658dea70008150395d29266a3e2165_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="546" data-watermark="watermark" data-original-src="v2-1d658dea70008150395d29266a3e2165" data-watermark-src="v2-86cb9e775c20923511b3af6b6a5e2cc6" data-private-watermark-src=""><p><br><br></p><img src="https://pic4.zhimg.com/v2-776a5e334ca5b7e7a8045fc4f042f6ae_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="605" data-watermark="watermark" data-original-src="v2-776a5e334ca5b7e7a8045fc4f042f6ae" data-watermark-src="v2-5fbecc75e54aea4ad620e4f6084a13ea" data-private-watermark-src=""><p><br><br>至于数据的储存，各种数据库就依个人喜好了，例子中直接保存到txt里了。多说一句，例子中取的数据不牵涉到动态加载内容，如有需要最简单是selenium模拟，另外方法是Chrome F12 network，分析Ajax内容，构造请求。具体今天就先略去了。<br></p><p>2<b>文本初步处理</b></p><img src="https://pic1.zhimg.com/v2-a994af8d7f9d07b77f54a2e7c8a5d760_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="445" data-watermark="watermark" data-original-src="v2-a994af8d7f9d07b77f54a2e7c8a5d760" data-watermark-src="v2-5274c1e955905cdb9b53501b66d51e30" data-private-watermark-src=""><p><br><br>取得数据之后下一步就是简单的处理了，对中文来说，就是分词，去停用词这些，可用的工具有： Jieba 、PyNlpir等。具体选哪个还是去试一下看哪个合适，自己选吧。<br><br><br>对于要让程序到practical的程度，分词还是很重要的，因为很多专业术语，所以自定义字典userdict比较重要。上面提到的两个包都可以导入自定义字典，要达到令人满意的结果，这userdict就看个人了。去停用词就是删除一些没什么实际意义的形容词、助词等。<br><br><br><b>分词程序：</b><br><br></p><img src="https://pic1.zhimg.com/v2-6d60a082b20dd98c28966fd42e30dea6_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="662" data-watermark="watermark" data-original-src="v2-6d60a082b20dd98c28966fd42e30dea6" data-watermark-src="v2-73ea147cc1d37a984818faa73138eed3" data-private-watermark-src=""><p><br><br><b>分词结果：</b><br><br></p><b><img src="https://pic4.zhimg.com/v2-444ba97f68375ba86484d0dbbf83ee82_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="393" data-watermark="watermark" data-original-src="v2-444ba97f68375ba86484d0dbbf83ee82" data-watermark-src="v2-85793a8327f3749d4b7bb8ba021afda7" data-private-watermark-src=""></b><p>3<b>提取关键词</b><br>下一步是把每个文本提取关键词，用关键词向量代表每个文本。<br>一般用的方法是有TF－IDF，具体细节可以wiki一下。很简单，主要意思就是一个词在文档中出现频率越高，对文档而言更重要； 同时一个词要是在所有文档中都出现，比如“的”，那就重要性减弱。于是抽象出 "TF : termfrequency" 和  "IDF: inverse documentary frequency"。以“国企改革”为例，“TF”算的是“国企改革”在文章中出现的频率，“IDF”算“国企改革”在所有文档中出现频率。<br></p><p>一般采用log(...) * log(...)的形式，不过这个也可以变，没有一个规定。<br>scikit-learn中有直接封装好的TF-IDF程序，在这里我贴出一个自己写的：<br></p><p><b>关键词提取程序：</b></p><img src="https://pic3.zhimg.com/v2-c016872bd77dd93252d4c91108168646_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="425" data-watermark="watermark" data-original-src="v2-c016872bd77dd93252d4c91108168646" data-watermark-src="v2-416d3e45fc7ca2852d640bdb187192e0" data-private-watermark-src=""><p><br><br><b>关键词提取结果：</b><br><br></p><b><img src="https://pic1.zhimg.com/v2-08ee558afad11ec2b699764df6e3e808_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="400" data-watermark="watermark" data-original-src="v2-08ee558afad11ec2b699764df6e3e808" data-watermark-src="v2-424aafdfda9a32b39ff108566caa64c0" data-private-watermark-src=""></b><p><br><br>这样处理之后，一片文章就可以用几十个关键词表示，再进行下一步的聚类分析等。<br><br>常用的文本相关性分析方法有：求文档间的余弦Cosine、KMeans等。</p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
