<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Sat, 02 Mar 2019 22:21:17 +0800</lastBuildDate>
<item>
<title>这些「神秘」团队到底是做什么的？| PingCAP 招聘季</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-03-01-58058910.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/58058910&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2ab741536518ae13908e30e1899ffb01_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;过去一年在 PingCAP 全力奔跑的同时，越来越多的小伙伴开始关注我们、了解我们，我们的团队也愈加庞大，我们也期待更多对我们感兴趣的小伙伴加入我们，跟我们一起做点有意义的事情。可能有些小伙伴对我司「神秘的招聘职位」感到茫然，对我们在做的事情也没有深入的了解，&lt;b&gt;于是我们准备推出「PingCAP 招聘职位深度解读」系列文章，&lt;/b&gt;介绍 PingCAP 各个团队的小伙伴们现在在做什么、接下来的规划是什么、不同团队吸纳成员的核心需求是什么等等。&lt;br&gt;本篇将带大家速览我司各个研发团队的定位和分工，并回答一个热门问题「在 PingCAP 工作是什么样的体验？」&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;作为开源的新型分布式数据库公司，PingCAP 一直致力于探索并逐步解决分布式数据库领域的诸多问题&lt;/b&gt;，比如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如何设计和实现世界前沿的分布式 SQL 优化器，让一个复杂的 SQL 查询变的无比轻快智能；&lt;/li&gt;&lt;li&gt;如何实现一致性同步的行列格式混合的 HTAP 架构，且 AP 业务对 TP 业务几乎无干扰；&lt;/li&gt;&lt;li&gt;如何在成千上万台集群规模的情况下，实现无阻塞的表结构变更操作，而不影响任何在线的业务；&lt;/li&gt;&lt;li&gt;如何实现高效的分布式事务算法，让 ACID 事务在大规模并发的分布式存场景下依然可以高效可靠；&lt;/li&gt;&lt;li&gt;如何基于 Raft 协议实现快速稳定的数据强一致复制和自动故障恢复，确保数据安全；&lt;/li&gt;&lt;li&gt;如何设计一个高效智能的调度器，负责对上百 TB 的数据进行调度，保证系统平稳运行；&lt;/li&gt;&lt;li&gt;如何在一个 PR 提交之后，让千万级的测试 cases 在三分钟内跑完，并立即看到对数据库性能有没有显著的提升，以及混沌工程的具体实践；&lt;/li&gt;&lt;li&gt;如何在 AWS，GCP，Aliyun 等公有云上一键启动 TiDB 集群，一键伸缩上百个数据库节点，理解有状态服务在 K8s 上调度的最佳实践。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;我们研发团队的定位和分工与以上问题息息相关，或者说，是围绕着 TiDB 产品展开的。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_b.jpg&quot;&gt;&lt;figcaption&gt;TiDB 产品架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;从上图可以看到，TiDB 集群主要包括三个核心组件：TiDB Server，TiKV Server 和 PD Server，分别用于解决计算、存储、调度这三个核心问题。此外，还有用于解决用户复杂 OLAP 需求的 TiSpark / TiFlash 组件。与之对应的，我们的内核研发团队分别是：&lt;b&gt;TiDB&lt;/b&gt; 团队、 &lt;b&gt;TiKV&lt;/b&gt; 团队和 &lt;b&gt;AP&lt;/b&gt;（Analytical Processing）团队，此外还有 &lt;b&gt;Cloud&lt;/b&gt; 团队、&lt;b&gt;EE&lt;/b&gt;（Efficiency Engineering）团队和新成立的&lt;b&gt;QA&lt;/b&gt;（Quality Assurance）团队。&lt;/p&gt;&lt;p&gt;所以很多对 TiDB 不太了解的小伙伴看完我们的招聘页面，可能会觉得那些五（没）花（听）八（说）门（过）的研发类职位&lt;b&gt;是特别神秘的存在……吧……&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;713&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;713&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot;&gt;&lt;figcaption&gt;招聘页面上一小部分神秘部队&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;那么这些「神秘」团队到底是做什么的？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;下面就简单的介绍一下这些研发团队是做什么的吧。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 团队负责所有和 SQL 计算相关的工作以及和客户端（业务）之间的交互，包括协议解析、语法解析、查询优化、执行计算等等，这是一个承上启下的核心模块。除此之外还包括与其他数据库之间的数据迁移和同步组件，比如 TiDB 自身的 Binlog 模块以及读取 MySQL 之类数据源 Binlog 的组件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiKV 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 是一个支持事务的，数据强一致的分布式 Key-Value 存储引擎。 从产品架构图中可以看出：无论是 TiDB Server 还是 TiSpark 组件，都是从 TiKV 存取数据的，所以我们一定要保证 TiKV 的稳定和高效。TiKV 团队主要负责的就是分布式 Key-Value 存储引擎的设计和开发，分布式调度系统的设计与研发，构建分布式压力测试框架，稳定性测试框架等工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;AP 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个是一个比较新的团队，主要负责 OLAP 业务相关的产品，包括之前已经有的 TiSpark 和正在研发中的 AP 扩展引擎 TiFlash 产品。TiDB 是一款 HTAP 的产品，而加强和补齐 HTAP 中的 AP 环节主要就这个组的责任，这里包含了基于 Raft 的一致性同步列存引擎，MPP 计算引擎开发以及大数据相关产品的整合等工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 是一个 Cloud Native 的数据库，Cloud 团队的职责就是让 TiDB 更平滑、以更大的规模跑在云上。他们将 TiDB 的组件容器化，并借助 Kubernetes 进行编排与调度。其核心是 TiDB-Operator，实现了云上的快速部署、一键伸缩和故障自治愈。编排有状态的分布式服务是 Kubernetes 最有挑战的事情之一，也是这个团队最擅长解决的问题。Cloud 团队正在努力将 TiDB 构建成为一个云上的服务，即一个 Multi-tenant, Across-cloud, Fully-managed 的 DBaaS（Database as a Service）产品。&lt;/p&gt;&lt;p&gt;&lt;b&gt;EE 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这是一个非常 Hack 的团队，致力于解决研发、测试、交付、甚至公司运营中的各种效率问题。他们信仰自动化，摒弃重复性的人工劳动，发明各种 bot 帮助提高 DevOps 的效率；他们创造了强大的“薛定谔”测试平台，将混沌工程变成现实，不断挑战分布式数据库的极限；他们深入系统内核，改造 bcc/eBPF 这些最酷的工具，将操作系统的秘密暴露无遗；他们高效率定位线上的各种疑难杂症，还第一手玩到 Optane Memory 硬件——他们就是神秘的 EE 团队。&lt;/p&gt;&lt;p&gt;&lt;b&gt;QA 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每个发布的 TiDB 版本，都有数千万的测试用例来保障产品在客户生产环境下的完美工作。QA 团队开发测试工具和自动化测试框架，并引入混沌工程、人工智能技术来保障 TiDB 的数据一致性和稳定性。&lt;/p&gt;&lt;blockquote&gt;后续我们将每周更新 1-2 篇文章为大家详细介绍以上团队和相关职位。如果大家对文章有意见或建议，欢迎在微信后台留言或者发邮件到 hire@pingcap.com 告诉我们～&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;在 PingCAP 工作是什么样的体验？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这可能是很多小伙伴们最最关心的 Part。弹性工作制、零食水果、六险一金这些就不多说了，应该已经成为很多公司的标配，我们来说点有特色的：&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作内容&lt;/b&gt;&lt;/p&gt;&lt;p&gt;选择一份工作，工作内容是否有意义、有价值，你是否有兴趣投入其中，这两点至关重要。&lt;/p&gt;&lt;p&gt;在 PingCAP，你可以亲自参与打造一款代表未来数据库产品，接触核心的分布式关系数据库技术，你的每一个想法都会被重视，每一次提交都有可能给整个产品带来意想不到的变化。&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作伙伴&lt;/b&gt;&lt;/p&gt;&lt;p&gt;他们大多来自于国内外一线互联网公司，有非常出色的技术实力，作为聪明人的你一定也想和聪明的人一起工作。团队成员整体比较年轻，氛围相对轻松、自在。在这里，你可以保留自己的个性和兴趣爱好。无论你是爱好桌游、喜欢摇滚、热爱运动，都能找到与你志同道合的小伙伴，在从事喜欢的工作的同时也可以做你自己，是不是很 Cool？&lt;/p&gt;&lt;p&gt;&lt;b&gt;开源文化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们有着&lt;b&gt;活跃的开源社区&lt;/b&gt;。截止到 2019 年 3 月 1 日，TiDB+TiKV 项目在 GitHub 上的 Star 数已经达到了 21000+，拥有 350+ Contributor，社区的力量在不断壮大。TiDB-Operator、TiDB-DM、TiDB-Lightning 等生态工具陆续开源；24 篇 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiDB-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读系列文章&lt;/a&gt;&lt;/u&gt; 已经完结，&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章&lt;/a&gt;&lt;/u&gt; 已经启动 ；除了开放的线下 Infra Meetup，我们也将内部的 Paper Reading 活动放到了线上直播平台（Bilibili ID: TiDB_Robot）…… 想要了解 2018 年 TiDB 社区的成长足迹可以查看这篇文章——&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487903%26idx%3D1%26sn%3Dc14855dae7309753a7480558be80896d%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《2018 TiDB 社区成长足迹与小红花 | TiDB DevCon 2019》&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作地点&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前除北京总部之外，我们在&lt;b&gt;上海、杭州、广州、深圳、成都、硅谷&lt;/b&gt;都设立了 Office。你可以去体验北上广深的快节奏，感受经济、文化、思想的强烈碰撞，也可以去杭州、成都，在下班或午后享受片刻的宁静与悠闲，还可以去硅谷体验前沿的技术氛围；如果你喜欢美食，可以去魔都的人民广场吃炸鸡，也可以去广州品味一下正宗的粤式茶点，还可以去硅谷 Office 尝一尝正宗的西餐，当然还有成都的火锅、小酒馆等着你；甚至你还有机会 &lt;b&gt;Remote &lt;/b&gt;在家，事业家庭两相宜。&lt;/p&gt;&lt;p&gt;&lt;b&gt;需要特别说明的是，我们并不会按照工作地点来划分工作模块&lt;/b&gt;，每一个 Office 的小伙伴都在我们的核心研发模块中承担着重要角色，而且内部的跨团队和跨地域 Transfer 都非常透明，PingCAP 的整个项目协作也都是分布式的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;全方面的成长&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;入职之后，Mentor 会为你定制化培养方案，你对于所从事模块的认知会日渐深入，公司内部小伙伴的分享以及 Paper Reading、Meetup 等活动也能够帮助你对于其他知识领域有更加深刻的认识；&lt;/li&gt;&lt;li&gt;公司为每一位小伙伴提供了分享平台，支持并鼓励大家积极分享自己的想法和见解，在这个过程中，你的语言表达能力、逻辑思维能力也能得到一定程度的提升；&lt;/li&gt;&lt;li&gt;当然，如果你具备了作为 Mentor 的能力并有意向尝试 Mentor 的角色，在 PingCAP，都有机会实现。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们一直以来的理念是希望每个 PingCAP 的小伙伴都先得到个人成长，再反哺给团队和公司，每一个小伙伴都能参与到公司发展的过程中来。我们完全不担心「把你锻炼出来，却被其他公司高价挖走了」这类事情。且不说我们的薪酬本身就很有竞争力，更重要的是，我们相信一旦你喜欢上我们的理念和工作模式，你是不会舍得离开的～&lt;/p&gt;&lt;h2&gt;&lt;b&gt;加入我们吧！&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们认为优秀的工程师或多或少有以下共同特质：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A Quick Learner&lt;/li&gt;&lt;li&gt;An Earnest Curiosity&lt;/li&gt;&lt;li&gt;Faith in Open Source&lt;/li&gt;&lt;li&gt;Self-driven    &lt;/li&gt;&lt;li&gt;Get Things Done&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果你符合以上特质，欢迎进入招聘页面查看目前开放的工作机会：&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/recruit-cn/join/%23positions&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;虚位以待&lt;/a&gt;&lt;p&gt;&lt;b&gt;简历投递通道：hire@pingcap.com&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;实习生&lt;/b&gt;：公司的各项福利和学习资源对实习生全面开放，更重要的是实习生还未毕业就有机会接触工业级项目，而且实习期间表现优异者将有机会获得校招绿色通道特权。如果小伙伴们时间不够充裕，也可以先从社区 Contributor 做起，或许下一期 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D1%26sn%3D53b17f49e05af2cd832192fd67eaf75f%26chksm%3Deb1628b1dc61a1a704544f250cee480e7f6cf95cf5e84c2ccb7f0858398e685160122ed687b6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Talent Plan&lt;/a&gt;&lt;/u&gt; 的主角就是你！&lt;/p&gt;&lt;p&gt;&lt;b&gt;伯乐推荐&lt;/b&gt;：如果你身边有符合以上要求的小伙伴，也可以找我们聊一聊，推荐成功就有机会获得伯乐推荐奖励（iPad、iPhone、MacBook Pro 等等）。伯乐推荐邮件格式：[伯乐推荐] 候选人姓名-职位名称-推荐人姓名-推荐人手机号。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-03-01-58058910</guid>
<pubDate>Fri, 01 Mar 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>新技术到底靠不靠谱？在中国用一下就知道了</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-03-01-58054995.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/58054995&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f75076bf8b4d44532150d93aedc45120_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;本文转载自公众号「AI前线」。&lt;/p&gt;&lt;p&gt;策划编辑｜Natalie&lt;/p&gt;&lt;p&gt;作者｜Kevin Xu&lt;/p&gt;&lt;p&gt;译者｜无明&lt;/p&gt;&lt;p&gt;编辑｜Debra&lt;/p&gt;&lt;blockquote&gt;AI 前线导读：中国科技公司是典型的早期采用者——不是因为赶时髦，而是确实有必要这么做。“中国式规模”让中国的互联网经济成为了高质量软件（特别是基础设施软件）工程的成长沃土，这在开源技术上得到了充分体现。国内开发者和企业向各大开源基金会贡献了越来越多的开源项目，而我们对国外的开源项目也产生了越来越大的影响。本文来自 PingCAP 全球战略和运营总经理 Kevin Xu，AI 前线经授权翻译。&lt;/blockquote&gt;&lt;p&gt;我的 87 岁的祖母住在沈阳郊区的一所老房子里。虽然她年岁已高，但却很有技术悟性。平常她会用三个 App 进行网购：在京东上买书，在拼多多上买水果，在淘宝上买其他东西（衬衫、围巾、洗涤剂、数独板）。&lt;/p&gt;&lt;p&gt;这三个 App 刚好是由中国电商市场的三巨头开发的，其规模远远超出了千禧一代（1980 至 1994 年出生的人群）和 Z 世代（1995 至 2009 年出生的人群）的受众总和。&lt;/p&gt;&lt;p&gt;正是这种“中国式规模”让中国的互联网经济成为高质量软件（特别是基础设施软件）工程的成长沃土。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;购物节狂欢&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;电子商务是中国互联网经济增长最快的垂直领域之一，同时也带动了数字支付和物流配送的发展。基础设施技术在这一领域经受了最为残酷的考验。“双十一”是最为典型的案例，这是由阿里巴巴提出的一个网购节日，每年的 11 月 11 日，淘宝和天猫都会如期庆祝这个节日。2017 年双十一总销售额为 253 亿美元，2018 年增长到了 308 亿美元。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;668&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;668&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;中国第二大电子商务平台京东也有自己的年中购物节，即“618”，这是一个为期 18 天的促销活动，截止 6 月 18 日，这天刚好是京东的成立纪念日。2017 年，618 的总销售额为 176 亿美元，2018 年增长到了 284 亿美元。&lt;/p&gt;&lt;p&gt;美国亚马逊的年中购物节 Prime Day 在 2018 年和 2017 年分别创造了 41.9 亿美元和 24.1 亿美元的销售额。美国感恩节购物季在 2018 年和 2017 年的销售额分别为 178 亿美元和 196.2 亿美元。&lt;/p&gt;&lt;p&gt;对于工程师来说，有趣的不是令人瞠目结舌的销售数据，而是如何构建可以应对这些工作负载的基础设施。2017 年，阿里巴巴公布了双十一期间系统的高峰吞吐量：每秒 25.6 万笔交易和每秒 4200 万次查询。&lt;/p&gt;&lt;p&gt;不难想象，在这些促销活动期间，肯定会不可避免地出现大量的事务、查询、数据一致性问题、实时分析容量和其他难以想象的边缘情况。&lt;/p&gt;&lt;p&gt;除了这些公司，所有其他想要搭上这些促销活动顺风车的电子商务公司、所有为用户在线购物提供电子支付解决方案的银行，以及所有的物流中心和仓储中心——他们都需要有好的基础设施技术来应对新的工作负载和流量增长。&lt;/p&gt;&lt;p&gt;由于这种增长速度，以及由此产生的竞争压力，中国科技公司在采用新技术方面具有相当强的风险承受能力。&lt;/p&gt;&lt;p&gt;一家公司找到合适的产品市场，然后在不到两个月的时间内采用未经证实但很有前景的新技术为高速增长的流量提供服务，这种事情并非闻所未闻。京东在 2016 年初开始采用 Kubernetes，当时离谷歌开源 Kubernetes 还不到一年的时间，因为他们必须解决可伸缩性问题，而 OpenStack 没能帮他们实现这一目标。(京东现在拥有全球最大的 Kubernetes 集群，运行在 2 万台裸机上)&lt;/p&gt;&lt;h2&gt;&lt;b&gt;更大的规模，更大的责任&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;中国科技公司是典型的早期采用者——不是因为赶时髦，而是确实有必要这么做。中国拥有世界上最多的互联网用户（8 亿，并且还在增加当中），他们的规模（以及规模所带来的不可预知的行为）足够大，大到足以促使这些科技公司认真对待每一项技术。从这些公司生存下来的技术会变得更强大、更有弹性，也更值得被用在其他地方。&lt;/p&gt;&lt;p&gt;很多行为是不可能在构建模式下进行预测或测试的。&lt;/p&gt;&lt;p&gt;你该如何通过 Paxos 或 Raft 来模拟系统达到 100 倍查询峰值时的网络流量？当一件商品、一首歌或一段视频突然变得像病毒一样迅速传播，而所有用户都在试图访问它们，而更糟糕的是，有价值的广告收入取决于系统不能崩溃，在这种情况下，你该如何处理数据热点问题？当数据增长率为每天数 TB 时，应该如何扩展存储容量?&lt;/p&gt;&lt;p&gt;所有这些情况，在很多中国科技公司中时有发生。他们正在迅速地寻找新的解决方案，以迎接这些挑战——这为考验这些创新技术提供了一片沃土。&lt;/p&gt;&lt;p&gt;“中国式规模”已经催生了一些由中国原创的基础设施技术。去年，云原生计算基金会（CNCF）接受了其中的三个项目：Harbor、TiKV 和 DragonFly。它们的架构和用例都在之前的一篇文章（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//softwareengineeringdaily.com/2018/12/09/chinese-open-source-software/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;softwareengineeringdaily.com&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/2018/12/09/chinese-open-source-software/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）中做了很好的介绍。在 CNCF 生态系统之外，还有其他一些值得关注的项目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;OceanBase&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由蚂蚁金服开发的分布式关系数据库，最初用于支持支付宝。支付宝在中国已经无处不在。此后，OceanBase 逐渐成为阿里巴巴所有关键电子商务平台（如淘宝和天猫）的核心交易数据库。它也是一个独立的产品，南京银行就是它的用户之一。&lt;/p&gt;&lt;p&gt;2014 年以来，它经历了五次双十一的考验。可惜的是，它是一个闭源产品，在中国以外没有得到广泛采用，所以与其架构、设计或工程方面相关的英文信息并不多。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一个开源的、兼容 MySQL 的 NewSQL 分布式数据库，由 PingCAP 于 2015 年创建。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;570&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1024&quot; data-original=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;570&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1024&quot; data-original=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;它采用了分层架构，SQL 处理层（左边的 TiDB 集群）和可水平伸缩的存储层（中间的 TiKV 集群）被分隔开来。（注：TiKV 也由 PingCAP 发起，但现在由 CNCF 托管）。这个设计灵感来自于谷歌的 Spanner 和基于 Spanner 构建的 F1 项目。PD（Placement Driver）集群保存元数据，提供一些负载均衡支持，并提供时间戳（作为系统事务模型的一部分）。TiSpark 集群是一个可选组件，用户可以直接基于保存在 TiKV 中的数据运行 Spark 作业。&lt;/p&gt;&lt;p&gt;目前，中国已经有几百家公司在生产环境中部署了 TiDB，如摩拜、北京银行和爱奇艺。国外也有一些大型互联网公司使用了 TiDB，如 Shopee 和 BookMyShow。&lt;/p&gt;&lt;p&gt;注：PingCAP 现已提供 TiDB 的企业版和云服务，同时也在维护开源社区版本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Apache Kylin&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一个快速的 OLAP（在线分析处理）引擎，最初由 eBay 中国团队开发，在 2014 年贡献给 Apache 基金会，并在 2015 年底成为顶级项目。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;509&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1024&quot; data-original=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;509&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1024&quot; data-original=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;Kylin 主要被用在 Hadoop 生态系统中，为数百亿行数据的分析查询带来可观的速度提升。用户先定义数据模型，然后利用 Hadoop 的分布式特性并行运行多个 MapReduce 作业，用以预构建必要的多维模型（也称为“MOLAP”）。最后，Kylin 将预先计算的模型存储在 HBase 中，供用户查询。它还使用 Zookeeper 来协调和管理这个过程的不同部分。&lt;/p&gt;&lt;p&gt;作为大数据分析引擎，Kylin 集成了 Tableau、MicroStrategy、Excel 等流行的 BI 工具。它还提供了一个 RESTful API，方便与第三方应用程序连接。除了 eBay，它还在 OPPO、百度、中国太平洋保险等公司经受过实战考验，三星和摩根大通也是它的用户。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Apache Skywalking&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一个相对较新的开源应用程序性能监监控（APM）工具，用于在基于容器的环境中监控微服务。2017 年底，它成为 Apache 基金会的孵化器项目。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;507&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1024&quot; data-original=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;507&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1024&quot; data-original=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;Skywalking 通过服务网格从微服务中提取指标，并利用 Jaeger 等流行工具来跟踪信息，并可以查询和分析这些指标和信息，还可以使用团队开发的 UI 进行可视化。它还提供了一个可插拔的存储接口，借助这个接口，可以将信息保存在一些流行的数据库中，比如 Elasticsearch、MySQL 和 TiDB。&lt;/p&gt;&lt;p&gt;尽管这个项目成立还不到两年，但中国的一些大型公司已经在使用它，如华为、小米和贝壳。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;中国之外&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;除了本土技术，国外的一些技术也有了“中国式规模”的味道。京东是 Prometheus、Vitesse、Jenkins 和 GitLab 等技术的用户，百度是 CockroachDB（另一个受 Spanner 启发的开源数据库，类似于 TiDB）的用户。Alluxio，一个分布式文件系统统一层，可以以内存速度运行（源自加州大学伯克利分校 AMPLab 的一个名为 Tachyon 的研究项目），也在百度、中国联通和滴滴出行等企业中得到采用。&lt;/p&gt;&lt;p&gt;中国公司不仅在大规模采用这些技术，有时候甚至直接收购它们。开源数据流平台 Apache Flink 由柏林技术大学于 2009 年创建，作为 Stratosphere 研究项目的一部分。阿里巴巴最终收购了由 Flink 创始人创办的 dataArtisan（该公司的目的是商业化 Flink）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;有价值的权衡?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;作为工程师，我们知道没有什么技术是绝对的，它们总是存在权衡。我们总是在吞吐量和延迟、数据一致性和响应时间、新特性和系统稳定性之间做出权衡。我们很少能鱼与熊掌兼得，我们也不相信把自己标榜得太高的技术。&lt;/p&gt;&lt;p&gt;市场的选择也是如此。在中国互联网经济大环境中，有一些问题一定要考虑到，特别是信息审查方面的问题。比如，对侵犯知识产权行为的法律追索仍然不太可靠，有关企业使用个人数据的监管尚处于初级阶段。&lt;/p&gt;&lt;p&gt;但如果你是一名开发者，正在寻找一些稳定可靠的技术（已经“面面俱到”的技术），那么那些已经在中国互联网环境中经受过实战考验的技术将是安全的选择。&lt;/p&gt;&lt;p&gt;如果你的团队正在构建下一个大项目，尤其是在基础设施层面，那么把这个项目交给中国的几家科技巨头公司，将会为项目带来跨越式的发展。&lt;/p&gt;&lt;p&gt;另外，你们的努力很可能也会为我的祖母带来更快乐的生活！&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;英文原文：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//softwareengineeringdaily.com/2019/02/26/china-scale-the-new-sandbox-to-battle-test-innovative-technology/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic3.zhimg.com/v2-6025e530680d1bab1e937a0be1b9bb8e_180x120.jpg&quot; data-image-width=&quot;940&quot; data-image-height=&quot;470&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;China Scale: the New Sandbox to Battle-Test Innovative Technology&lt;/a&gt;&lt;p&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Kevin Xu 是 PingCAP 全球战略和运营总经理。他在斯坦福大学完成计算机科学与法律专业的学习。主要关注分布式系统、云原生技术、自然语言处理和开源。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-03-01-58054995</guid>
<pubDate>Fri, 01 Mar 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>优秀的数据工程师，怎么用 Spark 在 TiDB 上做 OLAP 分析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-27-57855988.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57855988&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-903c0b831be84e0240629b5b88f04e6f_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;作者：RickyHuo&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文转载自公众号「大道至简bigdata」。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/OijMYyM-7F2gbvURsfJskw&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;优秀的数据工程师，怎么用Spark在TiDB上做OLAP分析&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB 是一款定位于在线事务处理/在线分析处理的融合型数据库产品，实现了一键水平伸缩，强一致性的多副本数据安全，分布式事务，实时 OLAP 等重要特性。&lt;br&gt;TiSpark 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品。它借助 Spark 平台，同时融合 TiKV 分布式集群的优势。直接使用 TiSpark 完成 OLAP 操作需要了解 Spark，还需要一些开发工作。&lt;b&gt;那么，有没有一些开箱即用的工具能帮我们更快速地使用 TiSpark 在 TiDB 上完成 OLAP 分析呢？&lt;/b&gt;&lt;br&gt;&lt;b&gt;目前开源社区上有一款工具 Waterdrop，可以基于 Spark，在 TiSpark 的基础上快速实现 TiDB 数据读取和 OLAP 分析。项目地址：&lt;/b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/InterestingLab/waterdrop&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/InterestingL&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ab/waterdrop&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;591&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;591&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_b.jpg&quot;&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;使用 Waterdrop 操作 TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我们线上有这么一个需求，从 TiDB 中读取某一天的网站访问数据，统计每个域名以及服务返回状态码的访问次数，最后将统计结果写入 TiDB 另外一个表中。 我们来看看 Waterdrop 是如何实现这么一个功能的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Waterdrop&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Waterdrop 是一个非常易用，高性能，能够应对海量数据的实时数据处理产品，它构建在 Spark 之上。Waterdrop 拥有着非常丰富的插件，支持从 TiDB、Kafka、HDFS、Kudu 中读取数据，进行各种各样的数据处理，然后将结果写入 TiDB、ClickHouse、Elasticsearch 或者 Kafka 中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;准备工作&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. TiDB 表结构介绍&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Input（存储访问日志的表）&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;CREATE TABLE access_log (
    domain VARCHAR(255),
    datetime VARCHAR(63),
    remote_addr VARCHAR(63),
    http_ver VARCHAR(15),
    body_bytes_send INT,
    status INT,
    request_time FLOAT,
    url TEXT
)
+-----------------+--------------+------+------+---------+-------+
| Field           | Type         | Null | Key  | Default | Extra |
+-----------------+--------------+------+------+---------+-------+
| domain          | varchar(255) | YES  |      | NULL    |       |
| datetime        | varchar(63)  | YES  |      | NULL    |       |
| remote_addr     | varchar(63)  | YES  |      | NULL    |       |
| http_ver        | varchar(15)  | YES  |      | NULL    |       |
| body_bytes_send | int(11)      | YES  |      | NULL    |       |
| status          | int(11)      | YES  |      | NULL    |       |
| request_time    | float        | YES  |      | NULL    |       |
| url             | text         | YES  |      | NULL    |       |
+-----------------+--------------+------+------+---------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;Output（存储结果数据的表）&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;CREATE TABLE access_collect (
    date VARCHAR(23),
    domain VARCHAR(63),
    status INT,
    hit INT
)
+--------+-------------+------+------+---------+-------+
| Field  | Type        | Null | Key  | Default | Extra |
+--------+-------------+------+------+---------+-------+
| date   | varchar(23) | YES  |      | NULL    |       |
| domain | varchar(63) | YES  |      | NULL    |       |
| status | int(11)     | YES  |      | NULL    |       |
| hit    | int(11)     | YES  |      | NULL    |       |
+--------+-------------+------+------+---------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;2. 安装 Waterdrop&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有了 TiDB 输入和输出表之后， 我们需要安装 Waterdrop，安装十分简单，无需配置系统环境变量&lt;/p&gt;&lt;p&gt;1) 准备 Spark 环境&lt;/p&gt;&lt;p&gt;2) 安装 Waterdrop&lt;/p&gt;&lt;p&gt;3) 配置 Waterdrop&lt;/p&gt;&lt;p&gt;以下是简易步骤，具体安装可以参照 Quick Start。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;# 下载安装Spark
cd /usr/local
wget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz
tar -xvf https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz
wget
# 下载安装Waterdrop
https://github.com/InterestingLab/waterdrop/releases/download/v1.2.0/waterdrop-1.2.0.zip
unzip waterdrop-1.2.0.zip
cd waterdrop-1.2.0

vim config/waterdrop-env.sh
# 指定Spark安装路径
SPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.1.0-bin-hadoop2.7}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;实现 Waterdrop 处理流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们仅需要编写一个 Waterdrop 配置文件即可完成数据的读取、处理、写入。&lt;/p&gt;&lt;p&gt;Waterdrop 配置文件由四个部分组成，分别是 &lt;code&gt;Spark&lt;/code&gt;、&lt;code&gt;Input&lt;/code&gt;、&lt;code&gt;Filter&lt;/code&gt; 和 &lt;code&gt;Output&lt;/code&gt;。&lt;code&gt;Input&lt;/code&gt; 部分用于指定数据的输入源，&lt;code&gt;Filter&lt;/code&gt; 部分用于定义各种各样的数据处理、聚合，&lt;code&gt;Output&lt;/code&gt; 部分负责将处理之后的数据写入指定的数据库或者消息队列。&lt;/p&gt;&lt;p&gt;整个处理流程为 &lt;code&gt;Input&lt;/code&gt; -&amp;gt; &lt;code&gt;Filter&lt;/code&gt; -&amp;gt; &lt;code&gt;Output&lt;/code&gt;，整个流程组成了 Waterdrop 的处理流程（Pipeline）。&lt;/p&gt;&lt;blockquote&gt;以下是一个具体配置，此配置来源于线上实际应用，但是为了演示有所简化。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;Input (TiDB)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这里部分配置定义输入源，如下是从 TiDB 一张表中读取数据。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;input {
    tidb {
        database = &quot;nginx&quot;
        pre_sql = &quot;select * from nginx.access_log&quot;
        table_name = &quot;spark_nginx_input&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;Filter&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 Filter 部分，这里我们配置一系列的转化, 大部分数据分析的需求，都是在 Filter 完成的。Waterdrop 提供了丰富的插件，足以满足各种数据分析需求。这里我们通过 SQL 插件完成数据的聚合操作。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;filter {
    sql {
        table_name = &quot;spark_nginx_log&quot;
        sql = &quot;select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=&#39;2019-01-20&#39; group by domain, status, substring(datetime, 1, 10)&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;Output (TiDB)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后， 我们将处理后的结果写入 TiDB 另外一张表中。TiDB Output 是通过 JDBC 实现的。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;output {
    tidb {
        url = &quot;jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&amp;amp;characterEncoding=utf8&quot;
        table = &quot;access_collect&quot;
        user = &quot;username&quot;
        password = &quot;password&quot;
        save_mode = &quot;append&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;Spark&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这一部分是 Spark 的相关配置，主要配置 Spark 执行时所需的资源大小以及其他 Spark 配置。&lt;br&gt;我们的 TiDB Input 插件是基于 TiSpark 实现的，而 TiSpark 依赖于 TiKV 集群和 Placement Driver (PD)。因此我们需要指定 PD 节点信息以及 TiSpark 相关配置&lt;code&gt;spark.tispark.pd.addresses&lt;/code&gt;和&lt;code&gt;spark.sql.extensions&lt;/code&gt;。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;spark {
  spark.app.name = &quot;Waterdrop-tidb&quot;
  spark.executor.instances = 2
  spark.executor.cores = 1
  spark.executor.memory = &quot;1g&quot;
  # Set for TiSpark
  spark.tispark.pd.addresses = &quot;localhost:2379&quot;
  spark.sql.extensions = &quot;org.apache.spark.sql.TiExtensions&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;运行 Waterdrop&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们将上述四部分配置组合成我们最终的配置文件&lt;code&gt;conf/tidb.conf&lt;/code&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;spark {
    spark.app.name = &quot;Waterdrop-tidb&quot;
    spark.executor.instances = 2
    spark.executor.cores = 1
    spark.executor.memory = &quot;1g&quot;
    # Set for TiSpark
    spark.tispark.pd.addresses = &quot;localhost:2379&quot;
    spark.sql.extensions = &quot;org.apache.spark.sql.TiExtensions&quot;
}
input {
    tidb {
        database = &quot;nginx&quot;
        pre_sql = &quot;select * from nginx.access_log&quot;
        table_name = &quot;spark_table&quot;
    }
}
filter {
    sql {
        table_name = &quot;spark_nginx_log&quot;
        sql = &quot;select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=&#39;2019-01-20&#39; group by domain, status, substring(datetime, 1, 10)&quot;
    }
}
output {
    tidb {
        url = &quot;jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&amp;amp;characterEncoding=utf8&quot;
        table = &quot;access_collect&quot;
        user = &quot;username&quot;
        password = &quot;password&quot;
        save_mode = &quot;append&quot;
    }
} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;执行命令，指定配置文件，运行 Waterdrop ，即可实现我们的数据处理逻辑。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Local&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;./bin/start-waterdrop.sh --config config/tidb.conf --deploy-mode client --master &#39;local[2]&#39;&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;yarn-client&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;./bin/start-waterdrop.sh --config config/tidb.conf --deploy-mode client --master yarn&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;yarn-cluster&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;./bin/start-waterdrop.sh --config config/tidb.conf --deploy-mode cluster -master yarn&lt;/code&gt;&lt;/p&gt;&lt;p&gt;如果是本机测试验证逻辑，用本地模式（Local）就可以了，一般生产环境下，都是使用&lt;code&gt;yarn-client&lt;/code&gt;或者&lt;code&gt;yarn-cluster&lt;/code&gt;模式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;检查结果&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;mysql&amp;gt; select * from access_collect;
+------------+--------+--------+------+
| date       | domain | status | hit  |
+------------+--------+--------+------+
| 2019-01-20 | b.com  |    200 |   63 |
| 2019-01-20 | a.com  |    200 |   85 |
+------------+--------+--------+------+
2 rows in set (0.21 sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在这篇文章中，我们介绍了如何使用 Waterdrop 从 TiDB 中读取数据，做简单的数据处理之后写入 TiDB 另外一个表中。仅通过一个配置文件便可快速完成数据的导入，无需编写任何代码。&lt;/p&gt;&lt;p&gt;除了支持 TiDB 数据源之外，Waterdrop 同样支持 Elasticsearch，Kafka，Kudu， ClickHouse 等数据源。&lt;/p&gt;&lt;p&gt;&lt;b&gt;与此同时，我们正在研发一个重要功能，就是在 Waterdrop 中，利用 TiDB 的事务特性，实现从 Kafka 到 TiDB 流式数据处理，并且支持端（Kafka）到端（TiDB）的 Exactly-Once 数据一致性。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;希望了解 Waterdrop 和 TiDB，ClickHouse、Elasticsearch、Kafka 结合使用的更多功能和案例，可以直接进入项目主页：&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/InterestingLab/waterdrop&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/InterestingL&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ab/waterdrop&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; ，或者联系项目负责人： Garyelephan（微信: garyelephant）、RickyHuo （微信: chodomatte1994）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-27-57855988</guid>
<pubDate>Wed, 27 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>The Way to TiDB 3.0 and Beyond (下篇)</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-26-57749943.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57749943&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-34de81f41133749c1021207829a0a288_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文为我司 Engineering VP 申砾在 TiDB DevCon 2019 上的演讲实录。在 &lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57693856&quot; class=&quot;internal&quot;&gt;上篇&lt;/a&gt;&lt;/u&gt; 中，申砾老师重点回顾了 TiDB 2.1 的特性，并分享了我们对「如何做好一个数据库」的看法。&lt;br&gt;本篇将继续介绍 TiDB 3.0 Beta 在稳定性、易用性、功能性上的提升，以及接下来在 Storage Layer 和 SQL Layer 的规划，enjoy~&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;593&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;593&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_b.jpg&quot;&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;TiDB 3.0 Beta&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2018 年年底我们开了一次用户吐槽大会，当时我们请了三个 TiDB 的重度用户，都是在生产环境有 10 套以上 TiDB 集群的用户。那次大会规则是大家不能讲 TiDB 的优点，只能讲缺点；研发同学要直面问题，不能辩解，直接提解决方案；当然我们也保护用户的安全（开个玩笑 :D），让他们放心的来吐槽。刚刚的社区实践分享也有点像吐槽大会第二季，我们也希望用户来提问题，分享他们在使用过程遇到什么坑，&lt;b&gt;因为只有直面这些问题，才有可能改进&lt;/b&gt;。所以我们在 TiDB 3.0  Beta 中有了很多改进，当然还有一些会在后续版本中去改进。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Stability at Scale&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 3.0 版本第一个目标就是「更稳定」，特别是在大规模集群、高负载的情况下保持稳定。稳定性压倒一切，如果你不稳定，用户担惊受怕，业务时断时续，后面的功能都是没有用的。所以我们希望「先把事情做对，再做快」。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 Multi-thread RaftStore&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首先来看 TiDB 3.0 一个比较亮眼的功能——多线程 Raft。我来给大家详细解释一下，为什么要做这个事情，为什么我们以前不做这个事情。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_b.jpg&quot;&gt;&lt;figcaption&gt;图 8 TiKV 抽象架构图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这是 TiKV 一个抽象的架构（图 8）。中间标红的图形是 RaftStore 模块，所有的 Raft Group 都在一个 TiKV 实例上，所有 Raft 状态机的驱动都是由一个叫做 RaftStore 的线程来做的，这个线程会驱动 Raft 状态机，并且将 Raft Log Append 到磁盘上，剩下的包括发消息给其他 TiKV 节点以及 Apply Raft Log 到状态机里面，都是由其他线程来做的。早期的时候，可能用户的数据量没那么大，或者吞吐表现不大的时候，其实是感知不到的。但是当吞吐量或者数据量大到一定程度，就会感觉到这里其实是一个瓶颈。虽然这个线程做的事情已经足够简单，但是因为 TiKV 上所有的 Raft Peer 都会通过一个线程来驱动自己的 Raft 状态机，所以当压力足够大的时候就会成为瓶颈。用户会看到整个 TiKV 的 CPU 并没有用满，但是为什么吞吐打不上去了？&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_b.jpg&quot;&gt;&lt;figcaption&gt;图 9 TiDB 3.0 Multi-thread RaftStore&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;因此在 TiDB 3.0 中做了一个比较大的改进，就是将 RaftStore 这个线程，由一个线程变成一个线程池， TiKV 上所有 Raft Peer 的 Raft 状态机驱动都由线程池来做，这样就能够充分利用 CPU，充分利用多核，在 Region 特别多以及写入量特别大的时候，依然能线性的提升吞吐。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;426&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;426&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_b.jpg&quot;&gt;&lt;figcaption&gt;图 10 TiDB 3.0 Beta oltp_insert&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;通过上图大家可以看到，随着并发不断加大，写入是能够去线性扩展的。在早期版本中，并发到一定程度的时候，RaftStore 也会成为瓶颈，那么为什么我们之前没有做这个事情？这个优化效果这么明显，之所以之前没有做，是因为之前 Raft 这块很多时候不会成为瓶颈，而在其他地方会成为瓶颈，比如说 RocksDB 的写入或者 gRPC 可能会成为瓶颈，然后我们将 RaftStore 中的功能不断的向外拆，拆到其他线程中，或者是其他线程里面做多线程，做异步等等，随着我们的优化不断深入，用户场景下的数据量、吞吐量不断加大，我们发现 RaftStore 线程已经成为需要优化的一个点，所以我们在 3.0 中做了这个事情。而且之前保持单线程也是因为单线程简单，「先把事情做对，然后再做快」。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2 Batch Message&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第二个改进是 Batch Message。我们的组件之间通讯选择了 gRPC，首先是因为 gRPC 是 Google 出品，有人在维护他，第二是用起来很简单，也有很多功能（如流控、加密）可以用。但其实很多人吐嘈它性能比较慢，在知乎上大家也能看到各种问题，包括讨论怎么去优化他，很多人也有各种优化经验，我们也一直想怎么去优化他。以前我们用的方法是来一个 message 就通过 gRPC 发出去，虽然性能可能没有那么好，或者说性能不是他最大的亮点，但有时候调性能不能单从一个模块去考虑，应该从架构上去想，就是架构需要为性能而设计，架构上的改进往往能带来性能的质变。&lt;/p&gt;&lt;p&gt;所以我们在 TiDB 3.0 Beta 中设计了 Batch Message 。以前是一个一个消息的发，现在是按照消息的目标分队列，每个队列会有一个 Timer，当消息凑到一定个数，或者是你的 Timer 到了时间（现在应该设置的是 1ms，Batch 和这个 Timer 数量都可以调），才会将发给同一个目的地的一组消息，打成一个包，一起发过去。有了这个架构上的调整之后，我们就获得了性能上的提升。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_b.jpg&quot;&gt;&lt;figcaption&gt;图 11 TiDB 3.0 Beta - Batch Message&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当然大家会想，会不会在并发比较低的时候变慢了？因为你凑不到足够的消息，那你就要等 Timer。其实是不会的，我们也做了一些设计，就是由对端先汇报「我当前是否忙」，如果对端不忙，那么选择一条一条的发，如果对端忙，那就可以一个 Batch 一个 Batch 的发，这是一个自适应的 Batch Message 的一套系统。图 11 右半部分是一个性能对比图，有了 Batch Message 之后，在高并发情况下吞吐提升非常快，在低并发情况下性能并没有下降。相信这个改进可以给大家带来很大的好处。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3 Titan&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第三点改进就是 Titan。CEO 刘奇在 Opening Keynote 中提到了我们新一代存储引擎 Titan，我们计划用 Titan 替换掉 RocksDB，TiDB 3.0 中已经内置了 Titan，但没有默认打开，如果大家想体验的话，可以通过配置文件去把 RocksDB 改成 Titan。我们为什么想改进 RocksDB 呢？是因为它在存储大的 Key Value 的时候，有存储空间放大和写放大严重的问题。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_b.jpg&quot;&gt;&lt;figcaption&gt;图 12 TiDB 3.0 中内置的新存储引擎 Titan&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以我们尝试解决这个问题。当你写入的 Key Value 比较大的时候，我们会做一个检查，然后把大的 Value 放到一个 Blob File 里去，而不是放到 LSM-Tree。这样的分开存储会让 LSM-Tree 变得很小，避免了因为 LSM-Tree 比较高的时候，特别是数据量比较大时出现的比较严重的写放大问题。有了 Titan 之后，就可以解决「单个 TiKV  服务大量数据」的需求，因为之前建议 TiKV 一个实例不要高于 1T。我们后面计划单个 TiKV 实例能够支持  2T 甚至 4T 数据，让大家能够节省存储成本，并且能在 Key Value 比较大的时候，依然能获得比较好的性能。&lt;/p&gt;&lt;p&gt;除了解决写放大问题之外，其实还有一个好处就是我们可以加一个新的 API，比如 KeyExist，用来检查 Key 是否存在，因为这时 Key 和 Value 是分开存储的，我们只需要检查 Key 是否在，不需要把 Value Load 进去。或者做 Unique Key 检查时，可以不需要把 Key Value 取出来，只需要加个接口，看这个 Key 是否存在就好了，这样能够很好的提升性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.4 Robust Access Path Selection&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第四点是保持查询计划稳定。这个在数据库领域其实是一个非常难的问题，我们依然没有 100% 解决这个问题，希望在 2019 年第一季度，最多到第二季度，能有一个非常好的解决方案。我们不希望当数据量变化 、写入变化、负载变化，查询计划突然变错，这个问题在线上使用过程中是灾难。那么为什么会跑着跑着变错？首先来说我们现在是一个 Cost-based optimizers，我们会参考统计信息和当前的数据的分布，来选择后面的 plan。那么数据的分布是如何获得的呢？我们是通过统计信息，比如直方图、CM Sketch来获取，这里就会出现两个问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;统计信息可能是不准的。统计信息毕竟是一个采样，不是全量数据，会有一些数据压缩，也会有精度上的损失。&lt;/li&gt;&lt;li&gt;随着数据不断写入，统计信息可能会落后。因为我们很难 100% 保证统计信息和数据是 Match 的。&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_b.jpg&quot;&gt;&lt;figcaption&gt;图 13 查询计划稳定性解决方案&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;一个非常通用的思路是， 除了依赖于 Cost Model 之外，我们还要依赖更多的 Hint，依赖于更多启发式规则去做 Access Path 裁减。举个例子：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;select * from t where a = x and b = y;
idx1(a, b)
idx2(b) -- pruned
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;大家通过直观印象来看，我们一定会选择第一个索引，而不是第二个索引，那么我们就可以把第二个索引裁掉，而不是因为统计信息落后了，然后估算出第二个索引的代价比较低，然后选择第二个索引。上面就是我们最近在做的一个事情，这里只举了一个简单的例子。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Usability&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 3.0 第二个目标是可用性，是让 TiDB 简单易用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 Query Tracing&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 TiDB 2.0 中，大家看一个 Query 为什么慢了依赖的是 Explain，就是看查询计划，其实那个时候大家很多都看不懂，有时候看了也不知道哪有问题。后来我们在 TiDB 2.1 中支持了 Explain Analyze，这是从 PG  借鉴过来一个特性，就是我们真正的把它执行一边，然后再看看每个算子的耗时、处理的数据量，看看它到底干了一些什么事情，但其实可能还不够细，因为还没有细化到算子内部的各种操作的耗时。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_b.jpg&quot;&gt;&lt;figcaption&gt;图 14 TiDB 3.0 - Query Tracing&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以我们又做了一个叫 Query Tracing 的东西，其实在 TiDB 2.1 之前我们已经做了一部分，在 TiDB  3.0 Beta 中做了一个收尾，就是我们可以将 Explain 结果转成一种 Tracing 格式，再通过图形化界面，把这个 Tracing 的内容展示出来，就可以看到这个算子具体干了一些什么事，每一步的消耗到底在哪里，这样就可以知道哪里有问题了。希望大家都能在 TiDB 3.0 的版本中非常直观的定位到 Query 慢的原因。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2 Plan Management&lt;/b&gt;&lt;/p&gt;&lt;p&gt;然后第二点 Plan Management 其实也是为了 Plan 不稳定这个问题做准备的。虽然我们希望数据库能自己 100% 把 Plan 选对，但是这个是非常美好的愿望，应该还没有任何一个数据库能保证自己能 100% 的解决这个问题。那么在以前的版本中，出现问题怎么办？一种是去 Analyze 一下，很多情况下他会变好，或者说你打开自动 Analyze 这个特性，或者自动 FeedBack 这个特性，可以一定程度上变好，但是还可能过一阵统计信息又落后了，又不准了，Plan 又错了，或者由于现在 cost 模型的问题，有一些 Corner Case 处理不到，导致即使统计信息是准确的， Plan 也选不对。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_b.jpg&quot;&gt;&lt;figcaption&gt;图 15  TiDB 3.0 Beta - Plan Management&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;那么我们就需要一个兜底方案，让大家遇到这个问题时不要束手无策。一种方法是让业务去改 SQL，去加 Hint，也是可以解决的，但是跟业务去沟通可能会增加他们的使用成本或者反馈周期很长，也有可能业务本身也不愿意做这个事情。&lt;/p&gt;&lt;p&gt;另外一种是用一种在线的方式，让数据库的使用者 DBA 也能非常简单给这个 Plan 加 Hint。具体怎么做呢？我们和美团的同学一起做了一个非常好的特性叫 Plan Management，就是我们有一个 Plan 管理的模块，我们可以通过 SQL 接口给某一条 Query，某一个 Query 绑定 Plan，绑定 Hint，这时我们会对 SQL 做指纹（把 Where 条件中的一些常量变成一个通配符，然后计算出一个 SQL 的指纹），然后把这个 Hint 绑定在指纹上。一条 Query 来了之后，先解成 AST，我们再生成指纹，拿到指纹之后，Plan Hint Manager 会解析出绑定的 Plan 和 Hint，有 Plan 和 Hint 之后，我们会把 AST 中的一部分节点替换掉，接下来这个 AST 就是一个「带 Hint 的 AST」，然后扔给 Optimizer，Optimizer 就能根据 Hint 介入查询优化器以及执行计划。如果出现慢的 Query，那么可以直接通过前面的 Query Tracing 去定位，再通过 Plan Management 机制在线的给数据库手动加 Hint，来解决慢 Query 的问题。这样下来也就不需要业务人员去改 SQL。这个特性应该在 TiDB 3.0 GA 正式对外提供，现在在内部已经跑得非常好了。在这里也非常感谢美团数据库开发同学的贡献。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3 Join Reorder&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 3.0 中我们增加了 Join Reorder。以前我们有一个非常简单的 Reorder 算法，就是根据 Join 这个路径上的等值条件做了一个优先选择，现在 TiDB 3.0 Beta 已经提供了第一种 Join Reorder 算法，就是一个贪心的算法。简单来说，就是我有几个需要 Join 的表，那我先从中选择 Join 之后数据量最小的那个表（是真正根据 Join 之后的代价来选的），然后我在剩下的表中再选一个，和这个再组成一个 Join Path，这样我们就能一定程度上解决很多 Join 的问题。比如 TPC-H 上的 Q5 以前是需要手动加 Hint 才能跑出来，因为它没有选对 Join 的路径，但在 TiDB 3.0 Beta 中，已经能够自动的选择最好的 Join Path 解决这个问题了。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_b.jpg&quot;&gt;&lt;figcaption&gt;图 16 TiDB 3.0 Beta - Join Reorder&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们接下来还要再做一个基于动态规划的 Join Reorder 算法，很有可能会在 3.0 GA 中对外提供。 在 Join 表比较少的时候，我们用动态规划算法能保证找到最好的一个 Join 的路径，但是如果表非常多，比如大于十几个表，那可能会选择贪心的算法，因为 Join Reorder  还是比较耗时的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Functionality&lt;/b&gt;&lt;/p&gt;&lt;p&gt;说完稳定性和易用性之外，我们再看一下功能。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_b.jpg&quot;&gt;&lt;figcaption&gt;图 17  TiDB 3.0 Beta 新增功能&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们现在做了一个插件系统，因为我们发现数据库能做的功能太多了，只有我们来做其实不太可能，而且每个用户有不一样的需求，比如说这家想要一个能够结合他们的监控系统的一个模块，那家想要一个能够结合他们的认证系统做一个模块，所以我们希望有一个扩展的机制，让大家都有机会能够在一个通用的数据库内核上去定制自己想要的特性。这个插件是基于 Golang 的 Plugin 系统。如果大家有 TiDB Server 的 Binary 和自己插件的 .so，就能在启动 TiDB Server 时加载自己的插件，获得自己定制的功能。&lt;/p&gt;&lt;p&gt;图 17 还列举了一些我们正在做的功能，比如白名单，审计日志，Slow Query，还有一些在 TiDB Hackathon 中诞生的项目，我们也想拿到插件中看看是否能够做出来。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Performance&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_b.jpg&quot;&gt;&lt;figcaption&gt;图 18 TiDB 3.0 Beta - OLTP Benchmark&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;从图 18 中可以看到，我们对 TiDB 3.0 Beta 中做了这么多性能优化之后，在 OLTP 这块进步还是比较大的，比如在 SysBench 下，无论是纯读取还是写入，还是读加写，都有几倍的提升。在解决稳定性这个问题之后，我们在性能方面会投入更多的精力。因为很多时候不能把「性能」单纯的当作性能来看，很多时候慢了，可能业务就挂了，慢了就是错误。&lt;/p&gt;&lt;p&gt;当然 TiDB 3.0 中还有其他重要特性，这里就不详细展开了。（&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/docs-cn/releases/3.0beta/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 3.0 Beta Release Notes&lt;/a&gt;&lt;/u&gt; ）&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Next?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;刚才介绍是 3.0 Beta 一些比较核心的特性，我们还在继续做更多的特性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Storage Layer&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_b.jpg&quot;&gt;&lt;figcaption&gt;图 19 TiDB 存储引擎层未来规划&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;比如在存储引擎层，我们对 Raft 层还在改进，比如说刚才我提到了我们有 Raft Learner，我们已经能够极大的减少由于调度带来的 Raft Group 不可用的概率，但是把一个 Learner 提成 Voter 再把另一个 Voter 干掉的时间间隔虽然比较短，但时间间隔依然存在，所以也并不是一个 100% 安全的方案。因此我们做了 Raft Joint Consensus。以前成员变更只能一个一个来：先把 Learner 提成 Voter，再把另一个 Voter 干掉。但有了 Raft Joint Consensus 之后，就能在一次操作中执行多个 ConfChange，从而把因为调度导致的 Region 不可用的概率降为零。&lt;/p&gt;&lt;p&gt;另外我们还在做跨数据中心的部署。前面社区实践分享中来自北京银行的于振华老师提到过，他们是一个两地三中心五部分的方案。现在的 TiDB 已经有一些机制能比较不错地处理这种场景，但我们能够做更多更好的东西，比如说我们可以支持 Witness 这种角色，它只做投票，不同步数据，对带宽的需求比较少，即使机房之间带宽非常低，他可以参与投票。在其他节点失效的情况下，他可以参与选举，决定谁是 Leader。另外我们支持通过 Follower 去读数据，但写入还是要走 Leader，这样对跨机房有什么好处呢？ 就是可以读本地机房的副本，而不是一定要读远端机房那个 Leader，但是写入还是要走远端机房的 Leader，这就能极大的降低读的延迟。除此之外，还有支持链式复制，而不是都通过 Leader 去复制，直接通过本地机房复制数据。&lt;/p&gt;&lt;p&gt;之后我们还可以基于 Learner 做数据的 Backup。通过 learner 去拉一个镜像，存到本地，或者通过 Learner 拉取镜像之后的增量，做增量的物理备份。所以之后要做物理备份是通过 Learner 实时的把 TiKV 中数据做一个物理备份，包括全量和增量。当需要恢复的时候，再通过这个备份直接恢复就好了，不需要通过 SQL 导出再导入，能比较快提升恢复速度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. SQL Layer&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_b.jpg&quot;&gt;&lt;figcaption&gt;图 20 TiDB 存储引擎层未来规划&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 SQL 层，我们还做了很多事情，比如 Optimizer 正在朝下一代做演进，它是基于最先进的 Cascades 模型。我们希望 Optimizer 能够处理任意复杂的 Query，帮大家解决从 OLTP 到 OLAP 一整套问题，甚至更复杂的问题。比如现在 TiDB 只在 TiKV 上查数据，下一步还要接入TiFlash，TiFlash 的代价或者算子其实不一样的，我们希望能够在 TiDB 上支持多个存储引擎，比如同一个 Query，可以一部分算子推到 TiFlash 上去处理，一部分算子在 TiKV 上处理，在 TiFlash 上做全表扫描，TiKV 上就做 Index 点查，最后汇总在一起再做计算。&lt;/p&gt;&lt;p&gt;我们还计划提供一个新的工具，叫 SQL Tuning Advisor。现在用户遇到了慢 Query，或者想在上线业务之前做 SQL 审核和优化建议，很多时候是人肉来做的，之后我们希望把这个过程变成自动的。&lt;/p&gt;&lt;p&gt;除此之外我们还将支持向量化的引擎，就是把这个引擎进一步做向量化。未来我们还要继续兼容最新的 MySQL 8.0 的特性 Common Table，目前计划以 MySQL 5.7 为兼容目标，和社区用户一起把 TiDB 过渡到 MySQL 8.0 兼容。&lt;/p&gt;&lt;p&gt;&lt;b&gt;说了这么多，我个人觉得，我们做一个好的数据库，有用的数据库，最重要一点是我们有大量的老师，可以向用户，向社区学习。&lt;/b&gt;不管是分享了使用 TiDB 的经验和坑也好，还是去提 Issue 报 Bug，或者是给 TiDB 提交了代码，都是在帮助我们把 TiDB 做得更好，所以在这里表示一下衷心的感谢。最后再立一个 flag，去年我们共写了 24 篇 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiDB-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读文章&lt;/a&gt;，今年还会写 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码系列文章&lt;/a&gt;&lt;/u&gt;。我们希望把项目背后只有开发同学才能理解的这套逻辑讲出来，让大家知道 TiDB 是怎样的工作的，希望今年能把这个事情做完，感谢大家。&lt;/p&gt;&lt;p&gt;延伸阅读：&lt;/p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57693856&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic3.zhimg.com/v2-8c30e2e05d268ae22671337dbb6d4a4e_180x120.jpg&quot; data-image-width=&quot;1280&quot; data-image-height=&quot;853&quot; class=&quot;internal&quot;&gt;ZoeyZhai：The Way to TiDB 3.0 and Beyond (上篇)&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-26-57749943</guid>
<pubDate>Tue, 26 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>The Way to TiDB 3.0 and Beyond (上篇)</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-26-57693856.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57693856&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8c30e2e05d268ae22671337dbb6d4a4e_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;我司 Engineering VP 申砾在 TiDB DevCon 2019  上分享了 TiDB 产品进化过程中的思考与未来规划。本文为演讲实录&lt;b&gt;上篇&lt;/b&gt;，重点回顾了 TiDB 2.1 的特性，并分享了我们对「如何做一个好的数据库」的看法。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_b.jpg&quot;&gt;&lt;figcaption&gt;我司 Engineering VP 申砾&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;感谢这么多朋友的到场，今天我会从我们的一些思考的角度来回顾过去一段时间做了什么事情，以及未来的半年到一年时间内将会做什么事情，特别是「我们为什么要做这些事情」&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;TiDB 这个产品，我们从 2015 年年中开始做，做到现在，三年半，将近四年了，从最早期的 Beta 版的时候就开始上线，到后来 RC 版本，最后在 2017 年终于发了 1.0，开始铺了一部分用户，到 2.0 的时候，用户数量就开始涨的非常快。然后我们最近发了 2.1，在 2.1 之后，我们也和各种用户去聊，跟他们聊一些使用的体验，有什么样的问题，包括对我们进行吐嘈。我们就在这些实践经验基础之上，设计了 3.0 的一些特性，以及我们的一些工作的重点。现在我们正在朝 3.0 这个版本去演进，到今天早上已经发了 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/docs-cn/releases/3.0beta/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;3.0 Beta 版本&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 2.1&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;首先我们来讲 2.1，2.1 是一个非常重要的版本，这个版本我们吸取了很多用户的使用场景中看到的问题，以及特别多用户的建议。在这里我跟大家聊一聊它有哪些比较重要的特性。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;432&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;432&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_b.jpg&quot;&gt;&lt;figcaption&gt;图 1 TiDB 2.1 新增重要功能&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;首先我们两个核心组件：存储引擎和计算引擎，在这两方面，我们做了一些非常重要的改进，当然这些改进有可能是用户看不到的。或者说这些改进其实我们是不希望用户能看到的，一旦你看到了，注意到这些改进的话，说明你的系统遇到这些问题了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Raft&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 Learner&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大家都知道 Raft 会有 Leader 和 Follower 这两个概念，Leader 来负责读写，Follower 来作为 Backup，然后随时找机会成为新的 Leader。如果你想加一个新的节点，比如说在扩容或者故障恢复，新加了一个 Follower 进来，这个时候 Raft Group 有 4 个成员， Leader、Follower 都是 Voter，都能够在写入数据时候对日志进行投票，或者是要在成员变更的时候投票的。这时一旦发生意外情况，比如网络变更或者出现网络分区，假设 2 个被隔离掉的节点都在一个物理位置上，就会导致 4 个 Voter 中 2 个不可用，那这时这个 Raft Group 就不可用了。&lt;/p&gt;&lt;p&gt;大家可能觉得这个场景并不常见，但是如果我们正在做负载均衡调度或者扩容时，一旦出现这种情况，就很有可能影响业务。所以我们加了 Learner 这个角色，Learner 的功能也是我们贡献给 etcd 这个项目的。有了 Learner 之后，我们在扩容时不会先去加一个 Follower（也就是一个 Voter），而是增加一个 Learner 的角色，它不是 Voter，所以它只会同步数据不会投票，所以无论在做数据写入还是成员变更的时候都不会算上它。当同步完所有数据时（因为数据量大的时候同步时间会比较长），拿到所有数据之后，再把它变成一个 Voter，同时再把另一个我们想下线的 Follower 下掉就好了。这样就能极大的缩短同时存在 4 个 Voter 的时间，整个 Raft Group 的可用性就得到了提升。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_b.jpg&quot;&gt;&lt;figcaption&gt;图 2 TiDB 2.1 -  Raft Learner&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其实增加 Learner 功能不只是出于提升 Raft Group 可用性，或者说出于安全角度考虑，实际上我们也在用 Learner 来做更多的事情。比如，我们可以随便去加 Learner，然后把 Learner 变成一个只读副本，很多很重的分析任务就可以在 Learner 上去做。TiFlash 这个项目其实就是用 Learner 这个特性来增加只读副本，同时保证不会影响线上写入的延迟，因为它并不参与写入的时候投票。这样的好处是第一不影响写入延迟，第二有 Raft 实时同步数据，第三我们还能在上面快速地做很复杂的分析，同时线上 OLTP 业务有物理上的隔离。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2 PreVote&lt;/b&gt;&lt;/p&gt;&lt;p&gt;除了 Learner 之外，我们 2.1 中默认开启了 PreVote 这个功能。&lt;/p&gt;&lt;p&gt;我们考虑一种意外情况，就是在 Raft group 中出现了网络隔离，有 1 个节点和另外 2 个节点隔离掉了，然后它现在发现「我找不到 Leader 了，Leader 可能已经挂掉了」，然后就开始投票，不断投票，但是因为它和其他节点是隔离开的，所以没有办法选举成功。它每次失败，都会把自己的 term 加 1，每次失败加 1，网络隔离发生一段时间之后，它的 term 就会很高。当网络分区恢复之后，它的选举消息就能发出去了，并且这个选举消息里面的 term 是比较高的。根据 Raft 的协议，当遇到一个 term 比较高的时候，可能就会同意发起选举，当前的 Leader 就会下台来参与选举。但是因为发生网络隔离这段时间他是没有办法同步数据的，此时它的 Raft Log 一定是落后的，所以即使它的 term 很高，也不可能被选成新的 Leader。所以这个时候经过一次选举之后，它不会成为新 Leader，只有另外两个有机会成为新的 Leader。&lt;/p&gt;&lt;p&gt;大家可以看到，这个选举是对整个 Raft Group 造成了危害：首先它不可能成为新的 Leader，第二它把原有的 Leader 赶下台了，并且在这个选举过程中是没有 Leader 的，这时的 Raft Group 是不能对外提供服务的。虽然这个时间会很短，但也可能会造成比较大的抖动。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_b.jpg&quot;&gt;&lt;figcaption&gt;图 3 TiDB 2.1 - Raft PreVote &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以我们有了 PreVote 这个特性。具体是这样做的（如图 3）：在进行选举之前，先用 PreVote 这套机制来进行预选举，每个成员把自己的信息，包括 term，Raft Log  Index 放进去，发给其它成员，其它成员有这个信息之后，认为「我可以选你为 Leader」，才会发起真正的选举。&lt;/p&gt;&lt;p&gt;有了 PreVote 之后，我们就可以避免这种大规模的一个节点上很多数据、很多 Raft Group、很多 Peer 的情况下突然出现网络分区，在恢复之后造成大量的 Region 出现选举，导致整个服务有抖动。 因此 PreVote 能极大的提升稳定性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Concurrent DDL Operation&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当然除了 Raft 这几个改进之外，TiDB 2.1 中还有一个比较大的改进，就是在 DDL 模块。这是我们 2.1 中一个比较显著的特性。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_b.jpg&quot;&gt;&lt;figcaption&gt;图 4 TiDB 2.1 之前的 DDL 机制&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 2.1 之前的 DDL 整套机制是这样的（如图 4）：用户将 DDL 提交到任何一个 TiDB Server，发过来一个 DDL 语句，TiDB Server 经过一些初期的检查之后会打包成一个 DDL Job，扔到 TiKV 上一个封装好的队列中，整个集群只有一个 TiDB Server 会执行 DDL，而且只有一个线程在做这个事情。这个线程会去队列中拿到队列头的一个 Job，拿到之后就开始做，直到这个 Job 做完，即 DDL 操作执行完毕后，会再把这个 Job 扔到历史队列中，并且标记已经成功，这时 TiDB Sever 能感知到这个 DDL 操作是已经结束了，然后对外返回。前面的 Job 在执行完之前，后面的 DDL 操作是不会执行的，因而会造成一个情况： 假设前面有一个 AddIndex，比如在一个百亿行表上去 AddIndex，这个时间是非常长的，后面的 Create Table 是非常快的，但这时 Create Table 操作会被 AddIndex 阻塞，只有等到 AddIndex  执行完了，才会执行 Create Table，这个给有些用户造成了困扰，所以我们在 TiDB 2.1 中做了改进。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_b.jpg&quot;&gt;&lt;figcaption&gt;图 5 TiDB 2.1 - DDL 机制&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 TiDB 2.1 中 DDL 从执行层面分为两种（如图 5）。一种是 AddIndex 操作，即回填数据（就是把所有的数据扫出来，然后再填回去），这个操作耗时是非常长的，而且一些用户是线上场景，并发度不可能调得很高，因为在回写数据的时候，可能会对集群的写入造成压力。&lt;/p&gt;&lt;p&gt;另外一种是所有其他 DDL 操作，因为不管是 Create Table 还是加一个 Column 都是非常快的，只会修改 metadata 剩下的交给后台来做。所以我们将 AddIndex 的操作和其他有 DDL 的操作分成两个队列，每种 DDL 语句按照分类，进到不同队列中，在 DDL 的处理节点上会启用多个线程来分别处理这些队列，将比较慢的 AddIndex 的操作交给单独的一个线程来做，这样就不会出现一个  AddIndex 操作阻塞其他所有 Create Table 语句的问题了。&lt;/p&gt;&lt;p&gt;这样就提升了系统的易用性，当然我们下一步还会做进一步的并行， 比如在  AddIndex 时，可以在多个表上同时  AddIndex，或者一个表上同时 Add 多个 Index。我们也希望能够做成真正并行的一个 DDL 操作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Parallel Hash Aggregation&lt;/b&gt;&lt;/p&gt;&lt;p&gt;除了刚刚提到的稳定性和易用性的提升，我们在 TiDB 2.1 中，也对分析能力做了提升。我们在聚合的算子上做了两点改进。  第一点是对整个聚合框架做了优化，就是从一行一行处理的聚合模式，变成了一批一批数据处理的聚合模式，另外我们还在哈希聚合算子上做了并行。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_b.jpg&quot;&gt;&lt;figcaption&gt;图 6 TiDB 2.1 - Parallel Hash Aggregation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;为什么我们要优化聚合算子？因为在分析场景下，有两类算子是非常重要的，是 Join 和聚合。Join 算子我们之前已经做了并行处理，而 TiDB 2.1 中我们进一步对聚合算子做了并行处理。在哈希聚合中，我们在一个聚合算子里启用多个线程，分两个阶段进行聚合。这样就能够极大的提升聚合的速度。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_b.jpg&quot;&gt;&lt;figcaption&gt;图 7 TiDB 2.0 与 TiDB 2.1 TPC-H Benchmark 对比&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图 7 是 TiDB 2.1 发布的时候，我们做了一个 TPC-H Benchmark。实际上所有的 Query 都有提升，其中 Q17 和 Q18 提升最大。因为在 TiDB  2.0 测试时，Q17、Q18 还是一个长尾的 Query，分析之后发现瓶颈就在于聚合算子的执行。整个机器的 CPU 并不忙，但就是时间很长，我们做了 Profile 发现就是聚合的时间太长了，所以在 TiDB 2.1 中，对聚合算子做了并行，并且这个并行度可以调节。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Ecosystem Tools&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 2.1 发布的时我们还发布了两个工具，分别叫 TiDB-DM 和 TiDB-Lightning。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-DM&lt;/a&gt;&lt;/u&gt;&lt;/b&gt; 全称是 TiDB Data Migration，这个工具主要用来把我们之前的 Loader 和以及 Syncer 做了产品化改造，让大家更好用，它能够做分库分表的合并，能够只同步一些表中的数据，并且它还能够对数据做一些改写，因为分库分表合并的时候，数据合到一个表中可能会冲突，这时我们就需要一种非常方便、可配置的工具来操作，而不是让用户手动的去调各种参数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-ecosystem-tools-2/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-Lightning&lt;/a&gt;&lt;/u&gt;&lt;/b&gt; 这个工具是用来做全量的数据导入。之前的 Loader 也可以做全量数据导入，但是它是走的最标准的那套 SQL 的流程，需要做 SQL 的解析优化、 两阶段提交、Raft 复制等等一系列操作。但是我们觉得这个过程可以更快。因为很多用户想迁移到 TiDB 的数据不是几十 G 或者几百 G，而是几 T、几十 T、上百 T 的数据，通过传统导入的方式会非常慢。现在 TiDB-Lightning 可以直接将本地从 MySQL 或者其他库中导出的 SQL 文本，或者是 CSV 格式的文件，直接转成 RocksDB 底层的 SST file ，然后再注入到 TiKV 中，加载进去就导入成功了，能够极大的提升导入速度。当然我们还在不断的优化，希望这个速度能不断提升，将 1TB 数据的导入，压缩到一两个小时。这两个工具，有一部分用户已经用到了（&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487797%26idx%3D1%26sn%3Dd6de39f3d0172418c682eeb7779ea5e8%26chksm%3Deb16365fdc61bf497ee794f8b99780c2ec946bd7e1ed13bd47871bfae1355d7c76b190ee7c5a%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;并且已经正式开源&lt;/a&gt;&lt;/u&gt;）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How to build a good database?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们有相当多的用户正在使用 TiDB，我们在很多的场景中见到了各种各样的 Case，甚至包括机器坏掉甚至连续坏掉的情况。见了很多场景之后，我们就在想之后如何去改进产品，如何去避免在各种场景中遇到的「坑」，于是我们在更深入地思考一个问题：如何做一个好的数据库。因为做一个产品其实挺容易的，一个人两三个月也能搞一套数据库，不管是分库分表，还是类似于在 KV 上做一个 SQL，甚至做一个分布式数据库，都是可能在一个季度甚至半年之内做出来的。但是要真正做一个好的数据库，做一个成熟的数据库，做一个能在生产系统中大规模使用，并且能够让用户自己玩起来的数据库，其实里面有非常多工作要做。&lt;/p&gt;&lt;p&gt;&lt;b&gt;首先数据库最基本的是要「有用」，就是能解决用户问题&lt;/b&gt;。而要解决用户问题，第一点就是要知道用户有什么样的问题，我们就需要跟各类用户去聊，看用户的场景，一起来分析，一起来获得使用场景中真实存在的问题。所以最近我们有大量的同事，不管是交付的同事还是研发的同事，都与用户做了比较深入的访谈，聊聊用户在使用过程中有什么的问题，有什么样的需求，用户也提供各种各样的建议。我们希望 TiDB 能够很好的解决用户场景中存在的问题，甚至是用户自己暂时还没有察觉到的问题，进一步的满足用户的各种需求。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第二点是「易用性」。&lt;/b&gt;就好像一辆车，手动挡的大家也能开，但其实大家现在都想开自动挡。我们希望我们的数据库是一辆自动挡的车，甚至未来是一辆无人驾驶的车，让用户不需要关心这些事情，只需要专注自己的业务就好了。所以我们会不断的优化现有的解决方案，给用户更多更好的解决方案，下一步再将这些方案自动化，让用户用更低的成本使用我们的数据库。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后一点「稳定性」也非常重要&lt;/b&gt;，就是让用户不会用着用着担惊受怕，比如半夜报警之类的事情。而且我们希望 TiDB 能在大规模数据集上、在大流量上也能保持稳定。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;下篇将于明日推送，重点介绍 TiDB 3.0 Beta 在稳定性、易用性和功能性上的提升，以及 TiDB 在 Storage Layer 和 SQL Layer 方面的规划。&lt;/b&gt;&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-26-57693856</guid>
<pubDate>Tue, 26 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>第一届 RustCon Asia 来了！</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-21-57330714.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57330714&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e66905c46f38af9890c7ec84896f6b63_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//rustcon.asia/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RustCon Asia&lt;/a&gt;&lt;b&gt; 来了！由秘猿科技与 PingCAP 联合主办，亚洲第一届 Rust 大会将于 4 月 20 日在中国北京开启。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大会为期 4 天，包括 20 日全天和 21 日上午的主题演讲以及 22-23 日的多个主题 workshop 环节。其中主题演讲讲师来自于国内外资深 Rust 开发者和社区活跃贡献者；workshop 主题将覆盖到 Rust 开发入门和成熟技术栈或产品的实战操作和演示。&lt;br&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;458&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;458&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_b.jpg&quot;&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;关于 RustCon Asia&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们受欧洲 RustFest、美国东部 Rust Belt Rust、俄罗斯 RustRush、美国西部 RustConf 和拉美 Rust LatAm 的影响和激励，开启亚洲的第一场 Rust 大会，并期望 RustCon Asia 未来能够周期性持续举办，连接亚洲的 Rust 的开发者与全球的 Rust 社区，相互扶持，共同布道 Rust 开发语言。&lt;/p&gt;&lt;p&gt;在亚洲，我们已有不少 Rust 开发的优秀案例。一些 Rust 项目已经在生产环境中使用多年，包括中国的银行核心系统、信任链、分布式系统、网络和云服务基础设施等。&lt;/p&gt;&lt;p&gt;我们选择北京作为 RustCon Asia 的第一站，首先因为我们的组织者秘猿科技和 PingCAP 都来自中国；其次也因为我们对中国的开发者和开发社区文化特别熟悉。秘猿科技和 PingCAP 都非常重视开发者社区，除了产品本身的号召力之外，核心团队的开发者也在各种开发者社区特别活跃，持续贡献技术知识和组织多种开发者活动。&lt;/p&gt;&lt;p&gt;未来，我们将 RustCon Asia 推进到亚洲的其他国家，更好的促进当地社区与全球社区的合作和互助。&lt;/p&gt;&lt;p&gt;RustCon Asia 目前已开启讲师席位，欢迎关注官网信息，并通过 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//cfp.rustcon.asia/events/rustcon-asia&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;CFP&lt;/a&gt; 提交您的议题信息，支持中英文双语。会议其它细节我们还在逐步确定，请随时关注我们的动态。&lt;br&gt;此次大会期望能够满足你的以下期待：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;与国内社区的老友面基，与国际社区的开发者见面；&lt;/li&gt;&lt;li&gt;中英文主题演讲，并有双向同传支持；&lt;/li&gt;&lt;li&gt;实操 workshop（无同传）；&lt;/li&gt;&lt;li&gt;涵盖从新人友好到高级的技术内容；&lt;/li&gt;&lt;li&gt;匿名议题提交和筛选，以便将最优秀内容呈现给大家；&lt;/li&gt;&lt;li&gt;与生产环境中使用 Rust 的项目成员交流；&lt;/li&gt;&lt;li&gt;开放、温馨的氛围；&lt;/li&gt;&lt;li&gt;有机会与新、老朋友一起探索北京城！&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;讲师席位和研讨会席位还在接受报名中，请于官网 CFP 处提交（支持中英文双语）：&lt;/b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//cfp.rustcon.asia/events/rustcon-asia&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;cfp.rustcon.asia/events&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/rustcon-asia&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&lt;b&gt;大会官网&lt;/b&gt;：&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//rustcon.asia/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;rustcon.asia/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&lt;b&gt;中文直达&lt;/b&gt;：&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//www.huodongxing.com/event/6479456003900&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;huodongxing.com/event/6&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;479456003900&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&lt;b&gt;Twitter &lt;/b&gt;@RustConAsia&lt;br&gt;&lt;b&gt;合作咨询&lt;/b&gt;：aimee@cryptape.com&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;关于秘猿科技&lt;/b&gt;&lt;/p&gt;&lt;p&gt;杭州秘猿科技有限公司（Cryptape Co.,Ltd.）的使命是用技术创造信任，为加密经济提供基础设施和服务。公司成立于 2016 年 ，核心团队从 2011 年开始参与或主导各种区块链项目，实践经验丰富。秘猿科技具备深厚的区块链技术研发和工程实力，核心技术人员均有超过 10 年以上开发经验。公司完全自主研发了区块链基础平台 CITA，并于 2017 年开源，其创新的架构设计解决了区块链底层扩展性问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;关于 PingCAP&lt;/b&gt;&lt;/p&gt;&lt;p&gt;PingCAP 是一家开源的新型分布式数据库公司，秉承开源是基础软件的未来这一理念，PingCAP 持续扩大社区影响力，致力于前沿技术领域的创新实现。其研发的分布式关系型数据库 TiDB 项目，具备「分布式强一致性事务、在线弹性水平扩展、故障自恢复的高可用、跨数据中心多活」等核心特性，是大数据时代理想的数据库集群和云数据库解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-21-57330714</guid>
<pubDate>Thu, 21 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在摩拜单车的深度实践及应用</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-18-57047909.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57047909&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9f67bc657e08abc949d2ff8773e22a0b_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍：&lt;/b&gt;吕磊，摩拜单车高级 DBA&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;一、业务场景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;摩拜单车 2017 年开始将 TiDB 尝试应用到实际业务当中，根据业务的不断发展，TiDB 版本快速迭代，我们将 TiDB 在摩拜单车的使用场景逐渐分为了三个等级：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;P0 级核心业务：线上核心业务，必须单业务单集群，不允许多个业务共享集群性能，跨 AZ 部署，具有异地灾备能力。&lt;/li&gt;&lt;li&gt;P1 级在线业务：线上业务，在不影响主流程的前提下，可以允许多个业务共享一套 TiDB 集群。&lt;/li&gt;&lt;li&gt;离线业务集群：非线上业务，对实时性要求不高，可以忍受分钟级别的数据延迟。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文会选择三个场景，给大家简单介绍一下 TiDB 在摩拜单车的使用姿势、遇到的问题以及解决方案。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、订单集群（P0 级业务）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;订单业务是公司的 P0 级核心业务，以前的 Sharding 方案已经无法继续支撑摩拜快速增长的订单量，单库容量上限、数据分布不均等问题愈发明显，尤其是订单合库，单表已经是百亿级别，TiDB 作为 Sharding 方案的一个替代方案，不仅完美解决了上面的问题，还能为业务提供多维度的查询。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 订单 TiDB 集群的两地三中心部署架构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;664&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;664&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_b.jpg&quot;&gt;&lt;figcaption&gt;图 1  两地三中心部署架构图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;整个集群部署在三个机房，同城 A、同城 B、异地 C。由于异地机房的网络延迟较高，设计原则是尽量使 PD Leader 和 TiKV Region Leader 选在同城机房（Raft 协议只有 Leader 节点对外提供服务），我们的解决方案如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;PD 通过 Leader priority 将三个 PD server 优先级分别设置为 5 5 3。&lt;/li&gt;&lt;li&gt;将跨机房的 TiKV 实例通过 label 划分 AZ，保证 Region 的三副本不会落在同一个 AZ 内。&lt;/li&gt;&lt;li&gt;通过 label-property reject-leader 限制异地机房的 Region Leader，保证绝大部分情况下 Region 的 Leader 节点会选在同城机房 A、B。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2.2 订单集群的迁移过程以及业务接入拓扑&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_b.jpg&quot;&gt;&lt;figcaption&gt;图 2  订单集群的迁移过程以及业务接入拓扑图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;为了方便描述，图中 Sharding-JDBC 部分称为&lt;b&gt;老 Sharding 集群&lt;/b&gt;，DBProxy 部分称为&lt;b&gt;新 Sharding 集群。&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;新 Sharding 集群按照 order_id 取模通过 DBproxy 写入各分表，解决数据分布不均、热点等问题。&lt;/li&gt;&lt;li&gt;将老 Sharding 集群的数据通过使用 DRC（摩拜自研的开源异构数据同步工具 Gravity &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/moiot/gravity&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/moiot/gravit&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）全量+增量同步到新 Sharding 集群，并将增量数据进行打标，反向同步链路忽略带标记的流量，避免循环复制。&lt;/li&gt;&lt;li&gt;为支持上线过程中业务回滚至老 Sharding 集群，需要将新 Sharding 集群上的增量数据同步回老 Sharding 集群，由于写回老 Sharding 集群需要耦合业务逻辑，因此 DRC（Gravity）负责订阅 DBProxy-Sharding 集群的增量数放入 Kafka，由业务方开发一个消费 Kafka 的服务将数据写入到老 Sharding 集群。&lt;/li&gt;&lt;li&gt;新的 TiDB 集群作为订单合库，使用 DRC（Gravity）从新 Sharding 集群同步数据到 TiDB 中。&lt;/li&gt;&lt;li&gt;新方案中 DBProxy 集群负责 order_id 的读写流量，TiDB 合库作为 readonly 负责其他多维度的查询。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2.3 使用 TiDB 遇到的一些问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1&lt;/b&gt; &lt;b&gt;上线初期新集群流量灰度到 20% 的时候，发现 TiDB coprocessor 非常高，日志出现大量 server is busy 错误。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;问题分析：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;订单数据单表超过 100 亿行，每次查询涉及的数据分散在 1000+ 个 Region 上，根据 index 构造的 handle 去读表数据的时候需要往这些 Region 上发送很多 distsql 请求，进而导致 coprocessor 上 gRPC 的 QPS 上升。&lt;/li&gt;&lt;li&gt;TiDB 的执行引擎是以 Volcano 模型运行，所有的物理 Executor 构成一个树状结构，每一层通过调用下一层的 &lt;code&gt;Next/NextChunk()&lt;/code&gt; 方法获取结果。Chunk 是内存中存储内部数据的一种数据结构，用于减小内存分配开销、降低内存占用以及实现内存使用量统计/控制，TiDB 2.0 中使用的执行框架会不断调用 Child 的 &lt;code&gt;NextChunk&lt;/code&gt; 函数，获取一个 Chunk 的数据。每次函数调用返回一批数据，数据量由一个叫 &lt;code&gt;tidb_max_chunk_size&lt;/code&gt; 的 session 变量来控制，默认是 1024 行。订单表的特性，由于数据分散，实际上单个 Region 上需要访问的数据并不多。所以这个场景 Chunk size 直接按照默认配置（1024）显然是不合适的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;解决方案：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;升级到 2.1 GA 版本以后，这个参数变成了一个全局可调的参数，并且默认值改成了 32，这样内存使用更加高效、合理，该问题得到解决。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2.3.2&lt;/b&gt; &lt;b&gt;数据全量导入 TiDB 时，由于 TiDB 会默认使用一个隐式的自增 rowid，大量 INSERT 时把数据集中写入单个 Region，造成写入热点。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;解决方案&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通过设置 &lt;code&gt;SHARD_ROW_ID_BITS&lt;/code&gt; (&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/docs/blob/master/sql/tidb-specific.md%23shard_row_id_bits&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/docs&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/blob/master/sql/tidb-specific.md#shard_row_id_bits&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;)，可以把 rowid 打散写入多个不同的 Region，缓解写入热点问题：ALTER TABLE table_name SHARD_ROW_ID_BITS = 8;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2.3.3&lt;/b&gt; &lt;b&gt;异地机房由于网络延迟相对比较高，设计中赋予它的主要职责是灾备，并不提供服务。曾经出现过一次大约持续 10s 的网络抖动，TiDB 端发现大量的 no Leader 日志，Region follower 节点出现网络隔离情况，隔离节点 term 自增，重新接入集群时候会导致 Region 重新选主，较长时间的网络波动，会让上面的选主发生多次，而选主过程中无法提供正常服务，最后可能导致雪崩。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;问题分析&lt;/b&gt;：Raft 算法中一个 Follower 出现网络隔离的场景，如下图所示。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;806&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;806&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_b.jpg&quot;&gt;&lt;figcaption&gt;图 3  Raft 算法中，Follower 出现网络隔离的场景图&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Follower C 在 election timeout 没收到心跳之后，会发起选举，并转换为 Candidate 角色。&lt;/li&gt;&lt;li&gt;每次发起选举时都会把 term 加 1，由于网络隔离，选举失败的 C 节点 term 会不断增大。&lt;/li&gt;&lt;li&gt;在网络恢复后，这个节点的 term 会传播到集群的其他节点，导致重新选主，由于 C 节点的日志数据实际上不是最新的，并不会成为 Leader，整个集群的秩序被这个网络隔离过的 C 节点扰乱，这显然是不合理的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;解决方案：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 2.1 GA 版本引入了 Raft PreVote 机制，该问题得到解决。&lt;/li&gt;&lt;li&gt;在 PreVote 算法中，Candidate 首先要确认自己能赢得集群中大多数节点的投票，才会把自己的 term 增加，然后发起真正的投票，其他节点同意发起重新选举的条件更严格，必须同时满足 ：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;没有收到 Leader 的心跳，至少有一次选举超时。&lt;/li&gt;&lt;li&gt;Candidate 日志足够新。PreVote 算法的引入，网络隔离节点由于无法获得大部分节点的许可，因此无法增加 term，重新加入集群时不会导致重新选主。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;三、在线业务集群（P1 级业务）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在线业务集群，承载了用户余额变更、我的消息、用户生命周期、信用分等 P1 级业务，数据规模和访问量都在可控范围内。产出的 TiDB Binlog 可以通过 Gravity 以增量形式同步给大数据团队，通过分析模型计算出用户新的信用分定期写回 TiDB 集群。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;729&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;729&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_b.jpg&quot;&gt;&lt;figcaption&gt;图 4  在线业务集群拓扑图&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;四、数据沙盒集群（离线业务）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;数据沙盒，属于离线业务集群，是摩拜单车的一个数据聚合集群。目前运行着近百个 TiKV 实例，承载了 60 多 TB 数据，由公司自研的 Gravity 数据复制中心将线上数据库实时汇总到 TiDB 供离线查询使用，同时集群也承载了一些内部的离线业务、数据报表等应用。目前集群的总写入 TPS 平均在 1-2w/s，QPS 峰值 9w/s+，集群性能比较稳定。&lt;/b&gt;该集群的设计优势有如下几点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;可供开发人员安全的查询线上数据。&lt;/li&gt;&lt;li&gt;特殊场景下的跨库联表 SQL。&lt;/li&gt;&lt;li&gt;大数据团队的数据抽取、离线分析、BI 报表。&lt;/li&gt;&lt;li&gt;可以随时按需增加索引，满足多维度的复杂查询。&lt;/li&gt;&lt;li&gt;离线业务可以直接将流量指向沙盒集群，不会对线上数据库造成额外负担。&lt;/li&gt;&lt;li&gt;分库分表的数据聚合。&lt;/li&gt;&lt;li&gt;数据归档、灾备。&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;614&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;614&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_b.jpg&quot;&gt;&lt;figcaption&gt;图 5  数据沙盒集群拓扑图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;4.1 遇到过的一些问题和解决方案&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1.1&lt;/b&gt; &lt;b&gt;TiDB server oom 重启&lt;/b&gt;&lt;/p&gt;&lt;p&gt;很多使用过 TiDB 的朋友可能都遇到过这一问题，当 TiDB 在遇到超大请求时会一直申请内存导致 oom, 偶尔因为一条简单的查询语句导致整个内存被撑爆，影响集群的总体稳定性。虽然 TiDB 本身有 oom action 这个参数，但是我们实际配置过并没有效果。&lt;/p&gt;&lt;p&gt;于是我们选择了一个折中的方案，也是目前 TiDB 比较推荐的方案：单台物理机部署多个 TiDB 实例，通过端口进行区分，给不稳定查询的端口设置内存限制（如图 5 中间部分的 TiDBcluster1 和 TiDBcluster2）。例：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;[tidb_servers]
tidb-01-A ansible_host=$ip_address deploy_dir=/$deploydir1 tidb_port=$tidb_port1 tidb_status_port=$status_port1
tidb-01-B ansible_host=$ip_address deploy_dir=/$deploydir2 tidb_port=$tidb_port2 tidb_status_port=$status_port2  MemoryLimit=20G 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;实际上 &lt;code&gt;tidb-01-A&lt;/code&gt;、&lt;code&gt;tidb-01-B&lt;/code&gt; 部署在同一台物理机，&lt;code&gt;tidb-01-B&lt;/code&gt; 内存超过阈值会被系统自动重启，不影响 &lt;code&gt;tidb-01-A&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;TiDB 在 2.1 版本后引入新的参数 &lt;code&gt;tidb_mem_quota_query&lt;/code&gt;，可以设置查询语句的内存使用阈值，目前 TiDB 已经可以部分解决上述问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1.2&lt;/b&gt; &lt;b&gt;TiDB-Binlog 组件的效率问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大家平时关注比较多的是如何从 MySQL 迁移到 TiDB，但当业务真正迁移到 TiDB 上以后，TiDB 的 Binlog 就开始变得重要起来。TiDB-Binlog 模块，包含 Pump&amp;amp;Drainer 两个组件。TiDB 开启 Binlog 后，将产生的 Binlog 通过 Pump 组件实时写入本地磁盘，再异步发送到 Kafka，Drainer 将 Kafka 中的 Binlog 进行归并排序，再转换成固定格式输出到下游。&lt;/p&gt;&lt;p&gt;使用过程中我们碰到了几个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Pump 发送到 Kafka 的速度跟不上 Binlog 产生的速度。&lt;/li&gt;&lt;li&gt;Drainer 处理 Kafka 数据的速度太慢，导致延时过高。&lt;/li&gt;&lt;li&gt;单机部署多 TiDB 实例，不支持多 Pump。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其实前两个问题都是读写 Kafka 时产生的，Pump&amp;amp;Drainer 按照顺序、单 partition 分别进行读&amp;amp;写，速度瓶颈非常明显，后期增大了 Pump 发送的 batch size，加快了写 Kafka 的速度。但同时又遇到一些新的问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当源端 Binlog 消息积压太多，一次往 Kafka 发送过大消息，导致 Kafka oom。&lt;/li&gt;&lt;li&gt;当 Pump 高速大批写入 Kafka 的时候，发现 Drainer 不工作，无法读取 Kafka 数据。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;和 PingCAP 工程师一起排查，最终发现这是属于 sarama 本身的一个 bug，sarama 对数据写入没有阈值限制，但是读取却设置了阈值：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/Shopify/sarama/blob/master/real_decoder.go%23L88&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/Shopify/sara&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ma/blob/master/real_decoder.go#L88&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;最后的解决方案是给 Pump 和 Drainer 增加参数 Kafka-max-message 来限制消息大小。单机部署多 TiDB 实例，不支持多 Pump，也通过更新 ansible 脚本得到了解决，将 Pump.service 以及和 TiDB 的对应关系改成 Pump-8250.service，以端口区分。&lt;/p&gt;&lt;p&gt;针对以上问题，PingCAP 公司对 TiDB-Binlog 进行了重构，新版本的 TiDB-Binlog 不再使用 Kafka 存储 binlog。Pump 以及 Drainer 的功能也有所调整，Pump 形成一个集群，可以水平扩容来均匀承担业务压力。另外，原 Drainer 的 binlog 排序逻辑移到 Pump 来做，以此来提高整体的同步性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1.3&lt;/b&gt; &lt;b&gt;监控问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当前的 TiDB 监控架构中，TiKV 依赖 Pushgateway 拉取监控数据到 Prometheus，当 TiKV 实例数量越来越多，达到 Pushgateway 的内存限制 2GB 进程会进入假死状态，Grafana 监控就会变成下图的断点样子：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;568&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;568&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_b.jpg&quot;&gt;&lt;figcaption&gt;图 6  监控拓扑图&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;260&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;260&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_b.jpg&quot;&gt;&lt;figcaption&gt;图 7  监控展示图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;目前临时处理方案是部署多套 Pushgateway，将 TiKV 的监控信息指向不同的 Pushgateway 节点来分担流量。这个问题的最终还是要用 TiDB 的新版本（2.1.3 以上的版本已经支持），Prometheus 能够直接拉取 TiKV 的监控信息，取消对 Pushgateway 的依赖。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.2 数据复制中心 Gravity (DRC)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面简单介绍一下摩拜单车自研的数据复制组件 Gravity（DRC）。&lt;/p&gt;&lt;p&gt;Gravity 是摩拜单车数据库团队自研的一套数据复制组件，目前已经稳定支撑了公司数百条同步通道，TPS 50000/s，80 线延迟小于 50ms，具有如下特点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;多数据源（MySQL, MongoDB, TiDB, PostgreSQL）。&lt;/li&gt;&lt;li&gt;支持异构（不同的库、表、字段之间同步），支持分库分表到合表的同步。&lt;/li&gt;&lt;li&gt;支持双活&amp;amp;多活，复制过程将流量打标，避免循环复制。&lt;/li&gt;&lt;li&gt;管理节点高可用，故障恢复不会丢失数据。&lt;/li&gt;&lt;li&gt;支持 filter plugin（语句过滤，类型过滤，column 过滤等多维度的过滤）。&lt;/li&gt;&lt;li&gt;支持传输过程进行数据转换。&lt;/li&gt;&lt;li&gt;一键全量 + 增量迁移数据。&lt;/li&gt;&lt;li&gt;轻量级，稳定高效，容易部署。&lt;/li&gt;&lt;li&gt;支持基于 Kubernetes 的 PaaS 平台，简化运维任务。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;使用场景：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;大数据总线：发送 MySQL Binlog，Mongo Oplog，TiDB Binlog 的增量数据到 Kafka 供下游消费。&lt;/li&gt;&lt;li&gt;单向数据同步：MySQL → MySQL&amp;amp;TiDB 的全量、增量同步。&lt;/li&gt;&lt;li&gt;双向数据同步：MySQL ↔ MySQL 的双向增量同步，同步过程中可以防止循环复制。&lt;/li&gt;&lt;li&gt;分库分表到合库的同步：MySQL 分库分表 → 合库的同步，可以指定源表和目标表的对应关系。&lt;/li&gt;&lt;li&gt;数据清洗：同步过程中，可通过 filter plugin 将数据自定义转换。&lt;/li&gt;&lt;li&gt;数据归档：MySQL→ 归档库，同步链路中过滤掉 delete 语句。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Gravity 的设计初衷是要将多种数据源联合到一起，互相打通，让业务设计上更灵活，数据复制、数据转换变的更容易，能够帮助大家更容易的将业务平滑迁移到 TiDB 上面。该项目已经在 GitHub 开源，欢迎大家交流使用&lt;/b&gt;：&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/moiot/gravity&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/moiot/gravit&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;五、总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的出现，不仅弥补了 MySQL 单机容量上限、传统 Sharding 方案查询维度单一等缺点，而且其计算存储分离的架构设计让集群水平扩展变得更容易。业务可以更专注于研发而不必担心复杂的维护成本。未来，摩拜单车还会继续尝试将更多的核心业务迁移到 TiDB 上，让 TiDB 发挥更大价值，也祝愿 TiDB 发展的越来越好。&lt;/p&gt;&lt;p&gt;更多案例阅读：&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-18-57047909</guid>
<pubDate>Mon, 18 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 源码解析系列文章（二）raft-rs proposal 示例情景分析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-15-56820135.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/56820135&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8a7fcd6586c081fcc255b95b014946b0_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;作者：屈鹏&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文为 TiKV 源码解析系列的第二篇，按照计划首先将为大家介绍 TiKV 依赖的周边库 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/raft-rs&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;raft-rs&lt;/a&gt; 。raft-rs 是 Raft 算法的 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.rust-lang.org/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Rust&lt;/a&gt;语言实现。Raft 是分布式领域中应用非常广泛的一种共识算法，相比于此类算法的鼻祖 Paxos，具有更简单、更容易理解和实现的特点。&lt;/p&gt;&lt;p&gt;分布式系统的共识算法会将数据的写入复制到多个副本，从而在网络隔离或节点失败的时候仍然提供可用性。具体到 Raft 算法中，发起一个读写请求称为一次 proposal。本文将以 raft-rs 的公共 API 作为切入点，介绍一般 proposal 过程的实现原理，让用户可以深刻理解并掌握 raft-rs API 的使用， 以便用户开发自己的分布式应用，或者优化、定制 TiKV。&lt;/p&gt;&lt;p&gt;文中引用的代码片段的完整实现可以参见 raft-rs 仓库中的 source-code 分支。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Public API 简述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;仓库中的 &lt;code&gt;examples/five_mem_node/main.rs&lt;/code&gt; 文件是一个包含了主要 API 用法的简单示例。它创建了一个 5 节点的 Raft 系统，并进行了 100 个 proposal 的请求和提交。经过进一步精简之后，主要的类型封装和运行逻辑如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;struct Node {
    // 持有一个 RawNode 实例
    raft_group: Option&amp;lt;RawNode&amp;lt;MemStorage&amp;gt;&amp;gt;,
    // 接收其他节点发来的 Raft 消息
    my_mailbox: Receiver&amp;lt;Message&amp;gt;,
    // 发送 Raft 消息给其他节点
    mailboxes: HashMap&amp;lt;u64, Sender&amp;lt;Message&amp;gt;&amp;gt;,
}
let mut t = Instant::now();
// 在 Node 实例上运行一个循环，周期性地处理 Raft 消息、tick 和 Ready。
loop {
    thread::sleep(Duration::from_millis(10));
    while let Ok(msg) = node.my_mailbox.try_recv() {
        // 处理收到的 Raft 消息
        node.step(msg); 
    }
    let raft_group = match node.raft_group.as_mut().unwrap();
    if t.elapsed() &amp;gt;= Duration::from_millis(100) {
        raft_group.tick();
        t = Instant::now();
    }
    // 处理 Raft 产生的 Ready，并将处理进度更新回 Raft 中
    let mut ready = raft_group.ready();
    persist(ready.entries());  // 处理刚刚收到的 Raft Log
    send_all(ready.messages);  // 将 Raft 产生的消息发送给其他节点
    handle_committed_entries(ready.committed_entries.take());
    raft_group.advance(ready);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这段代码中值得注意的地方是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;RawNode 是 raft-rs 库与应用交互的主要界面。要在自己的应用中使用 raft-rs，首先就需要持有一个 RawNode 实例，正如 Node 结构体所做的那样。&lt;/li&gt;&lt;li&gt;RawNode 的范型参数是一个满足 Storage 约束的类型，可以认为是一个存储了 Raft Log 的存储引擎，示例中使用的是 MemStorage。&lt;/li&gt;&lt;li&gt;在收到 Raft 消息之后，调用 &lt;code&gt;RawNode::step&lt;/code&gt; 方法来处理这条消息。&lt;/li&gt;&lt;li&gt;每隔一段时间（称为一个 tick），调用 &lt;code&gt;RawNode::tick&lt;/code&gt; 方法使 Raft 的逻辑时钟前进一步。&lt;/li&gt;&lt;li&gt;使用 &lt;code&gt;RawNode::ready&lt;/code&gt; 接口从 Raft 中获取收到的最新日志（&lt;code&gt;Ready::entries&lt;/code&gt;），已经提交的日志（&lt;code&gt;Ready::committed_entries&lt;/code&gt;），以及需要发送给其他节点的消息等内容。&lt;/li&gt;&lt;li&gt;在确保一个 Ready 中的所有进度被正确处理完成之后，调用 &lt;code&gt;RawNode::advance&lt;/code&gt; 接口。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;接下来的几节将展开详细描述。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Storage trait&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Raft 算法中的日志复制部分抽象了一个可以不断追加写入新日志的持久化数组，这一数组在 raft-rs 中即对应 Storage。使用一个表格可以直观地展示这个 trait 的各个方法分别可以从这个持久化数组中获取哪些信息：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1228&quot; data-rawheight=&quot;830&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1228&quot; data-original=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1228&quot; data-rawheight=&quot;830&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1228&quot; data-original=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;值得注意的是，这个 Storage 中并不包括持久化 Raft Log，也不会将 Raft Log 应用到应用程序自己的状态机的接口。这些内容需要应用程序自行处理。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;&lt;code&gt;RawNode::step&lt;/code&gt; 接口&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这个接口处理从该 Raft group 中其他节点收到的消息。比如，当 Follower 收到 Leader 发来的日志时，需要把日志存储起来并回复相应的 ACK；或者当节点收到 term 更高的选举消息时，应该进入选举状态并回复自己的投票。这个接口和它调用的子函数的详细逻辑几乎涵盖了 Raft 协议的全部内容，代码较多，因此这里仅阐述在 Leader 上发生的日志复制过程。&lt;/p&gt;&lt;p&gt;当应用程序希望向 Raft 系统提交一个写入时，需要在 Leader 上调用 &lt;code&gt;RawNode::propose&lt;/code&gt; 方法，后者就会调用 &lt;code&gt;RawNode::step&lt;/code&gt;，而参数是一个类型为 &lt;code&gt;MessageType::MsgPropose&lt;/code&gt; 的消息；应用程序要写入的内容被封装到了这个消息中。对于这一消息类型，后续会调用 &lt;code&gt;Raft::step_leader&lt;/code&gt; 函数，将这个消息作为一个 Raft Log 暂存起来，同时广播到 Follower 的信箱中。到这一步，propose 的过程就可以返回了，注意，此时这个 Raft Log 并没有持久化，同时广播给 Follower 的 MsgAppend 消息也并未真正发出去。应用程序需要设法将这个写入挂起，等到从 Raft 中获知这个写入已经被集群中的过半成员确认之后，再向这个写入的发起者返回写入成功的响应。那么， 如何能够让 Raft 把消息真正发出去，并接收 Follower 的确认呢？&lt;/p&gt;&lt;p&gt;&lt;code&gt;RawNode::ready&lt;/code&gt; 和 &lt;code&gt;RawNode::advance&lt;/code&gt; 接口&lt;/p&gt;&lt;p&gt;这个接口返回一个 Ready 结构体：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;pub struct Ready {
    pub committed_entries: Option&amp;lt;Vec&amp;lt;Entry&amp;gt;&amp;gt;,
    pub messages: Vec&amp;lt;Message&amp;gt;,
    // some other fields...
}
impl Ready {
    pub fn entries(&amp;amp;self) -&amp;gt; &amp;amp;[Entry] {
        &amp;amp;self.entries
    }
    // some other methods...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;一些暂时无关的字段和方法已经略去，在 propose 过程中主要用到的方法和字段分别是：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1224&quot; data-rawheight=&quot;450&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1224&quot; data-original=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1224&quot; data-rawheight=&quot;450&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1224&quot; data-original=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;对照 &lt;code&gt;examples/five_mem_node/main.rs&lt;/code&gt; 中的示例，可以知道应用程序在 propose 一个消息之后，应该调用 &lt;code&gt;RawNode::ready&lt;/code&gt; 并在返回的 Ready 上继续进行处理：包括持久化 Raft Log，将 Raft 消息发送到网络上等。&lt;/p&gt;&lt;p&gt;而在 Follower 上，也不断运行着示例代码中与 Leader 相同的循环：接收 Raft 消息，从 Ready 中收集回复并发回给 Leader……对于 propose 过程而言，当 Leader 收到了足够的确认这一 Raft Log 的回复，便能够认为这一 Raft Log 已经被确认了，这一逻辑体现在 &lt;code&gt;Raft::handle_append_response&lt;/code&gt; 之后的 &lt;code&gt;Raft::maybe_commit&lt;/code&gt; 方法中。在下一次这个 Raft 节点调用 &lt;code&gt;RawNode::ready&lt;/code&gt; 时，便可以取出这部分被确认的消息，并应用到状态机中了。&lt;/p&gt;&lt;p&gt;在将一个 Ready 结构体中的内容处理完成之后，应用程序即可调用这个方法更新 Raft 中的一些进度，包括 last index、commit index 和 apply index 等。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;&lt;code&gt;RawNode::tick&lt;/code&gt; 接口&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这是本文最后要介绍的一个接口，它的作用是驱动 Raft 内部的逻辑时钟前进，并对超时进行处理。比如对于 Follower 而言，如果它在 tick 的时候发现 Leader 已经失联很久了，便会发起一次选举；而 Leader 为了避免自己被取代，也会在一个更短的超时之后给 Follower 发送心跳。值得注意的是，tick 也是会产生 Raft 消息的，为了使这部分 Raft 消息能够及时发送出去，在应用程序的每一轮循环中一般应该先处理 tick，然后处理 Ready，正如示例程序中所做的那样。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最后用一张图展示在 Leader 上是通过哪些 API 进行 propose 的：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1574&quot; data-rawheight=&quot;818&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1574&quot; data-original=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1574&quot; data-rawheight=&quot;818&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1574&quot; data-original=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;本期关于 raft-rs 的源码解析就到此结束了，我们非常鼓励大家在自己的分布式应用中尝试 raft-rs 这个库，同时提出宝贵的意见和建议。后续关于 raft-rs 我们还会深入介绍 Configuration Change 和 Snapshot 的实现与优化等内容，展示更深入的设计原理、更详细的优化细节，方便大家分析定位 raft-rs 和 TiKV 使用中的潜在问题。&lt;/p&gt;&lt;p&gt;更多阅读：&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-15-56820135</guid>
<pubDate>Fri, 15 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>2018 TiDB 社区成长足迹与小红花 | TiDB DevCon 2019</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-13-56624608.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/56624608&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-92a3a0e2e2bfb0c412e1c516e0d6441a_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;2018 年 TiDB 产品变得更加成熟和稳定，同时 TiDB 社区力量也在发展壮大。在 TiDB DevCon 2019 上，我司联合创始人崔秋带大家一起回顾了 2018 年 TiDB 社区成长足迹，在社区荣誉时刻环节，我们为新晋 Committer 授予了证书，并为 2018 年度最佳贡献个人/团队颁发了荣誉奖杯。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-955cdd6a3188a0733755599481ac02f8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-955cdd6a3188a0733755599481ac02f8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-955cdd6a3188a0733755599481ac02f8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-955cdd6a3188a0733755599481ac02f8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-955cdd6a3188a0733755599481ac02f8_b.jpg&quot;&gt;&lt;figcaption&gt;图 1 我司联合创始人崔秋&lt;/figcaption&gt;&lt;/figure&gt;&lt;blockquote&gt;在我们眼里运营开源社区最重要的是两点，一个是人才，一个是用户。人才方面主要面向开发者，包括 TiDB Contributor、Committer 以及 TiDB 生态周边的开发者等等。另外更重要的一方面是用户。用户对 TiDB 的认识和经验、给予的反馈是更直观、更贴近业务的，并且用户实际应用的场景与我们自身测试的场景相比，会更复杂、更丰富，他们的使用经验会让大家更有共鸣，另外当用户使用 TiDB 过程中遇到一些问题，这时社区有良好的反馈，帮助用户顺利解决问题，会让用户对 TiDB 更有信心，就会考虑扩大使用的规模和深度，同时 TiDB 社区本身也会得到成长。所以，运营一个好的开源社区，更重要的是以用户为中心。2019 年我们也会秉承这个想法， 继续把「用户至上」的观念和理念发挥到极致，与用户一起成长。                                                                                                                            ——崔秋&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;Product&lt;/b&gt;&lt;/h2&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-518fb381e34fadb0105856d1014d47b1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1058&quot; data-rawheight=&quot;417&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1058&quot; data-original=&quot;https://pic2.zhimg.com/v2-518fb381e34fadb0105856d1014d47b1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-518fb381e34fadb0105856d1014d47b1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1058&quot; data-rawheight=&quot;417&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1058&quot; data-original=&quot;https://pic2.zhimg.com/v2-518fb381e34fadb0105856d1014d47b1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-518fb381e34fadb0105856d1014d47b1_b.jpg&quot;&gt;&lt;figcaption&gt;图 2 TiDB 产品架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;产品是开源社区的基石，好的产品是吸引人才、壮大社区力量的动力，而丰富产品架构、扩充生态周边也需要社区伙伴们的共同努力。2018 年，TiDB 在社区伙伴们共同努力下发布了 2.1 GA 版本。我们也开源了 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247486525%26idx%3D1%26sn%3D342f1b43912b5de5ce22253e5380a108%26chksm%3Deb162b57dc61a241272a87892549c6886ebaa196c19e040e533c0f109745bd7e781397d1321e%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-Operator&lt;/a&gt;、&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487797%26idx%3D1%26sn%3Dd6de39f3d0172418c682eeb7779ea5e8%26chksm%3Deb16365fdc61bf497ee794f8b99780c2ec946bd7e1ed13bd47871bfae1355d7c76b190ee7c5a%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-DM、TiDB-Lightning&lt;/a&gt;&lt;/u&gt;等生态工具，大家可以一起来为 TiDB 添砖加瓦。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b3c1ffc4d62e8bc3daf548f955442831_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;695&quot; data-rawheight=&quot;520&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;695&quot; data-original=&quot;https://pic2.zhimg.com/v2-b3c1ffc4d62e8bc3daf548f955442831_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b3c1ffc4d62e8bc3daf548f955442831_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;695&quot; data-rawheight=&quot;520&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;695&quot; data-original=&quot;https://pic2.zhimg.com/v2-b3c1ffc4d62e8bc3daf548f955442831_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-b3c1ffc4d62e8bc3daf548f955442831_b.jpg&quot;&gt;&lt;figcaption&gt;图 3 TiDB 产品生态&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;本着「从开源项目中获益，同时回馈开源社区」的想法，我们持续为 RocksDB、etcd 等开源项目贡献力量。同时，我们也将 grpc-rs、raft-rs 、rust-rocksdb、parser 等项目独立出来（在 github/pingcap 组织下），方便大家了解和运用。而更加令人欣喜的是，有一些开源项目正在 TiDB 生态上衍生成长起来，进一步丰富了 TiDB 生态：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-26d3b6a7446ed8acda88f54886fc0064_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;392&quot; data-rawheight=&quot;345&quot; class=&quot;content_image&quot; width=&quot;392&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-26d3b6a7446ed8acda88f54886fc0064_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;392&quot; data-rawheight=&quot;345&quot; class=&quot;content_image lazy&quot; width=&quot;392&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-26d3b6a7446ed8acda88f54886fc0064_b.jpg&quot;&gt;&lt;figcaption&gt;图 4 基于 TiDB 生态的开源项目:Gravity/Titan/Soar&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;Events&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2018 年，TiDB 社区受到了更多国内外媒体的关注，获得了 InfoWorld |  Bossie Awards 最佳数据存储与数据分析平台奖，并入选了两个重要的&lt;b&gt;「Landscape」&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;FirstMark: Big Data &amp;amp; AI Landscape 2018&lt;/li&gt;&lt;li&gt;CNCF: Cloud Native Interactive Landscape&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-584a6b051cbd97df3eed6d4ff59c6a75_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;712&quot; data-rawheight=&quot;387&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;712&quot; data-original=&quot;https://pic2.zhimg.com/v2-584a6b051cbd97df3eed6d4ff59c6a75_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-584a6b051cbd97df3eed6d4ff59c6a75_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;712&quot; data-rawheight=&quot;387&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;712&quot; data-original=&quot;https://pic2.zhimg.com/v2-584a6b051cbd97df3eed6d4ff59c6a75_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-584a6b051cbd97df3eed6d4ff59c6a75_b.jpg&quot;&gt;&lt;figcaption&gt;图 5 TiDB 获得 InfoWorld | Bossie Awards 最佳数据存储与数据分析平台奖&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-81ae94d7f8c5cd553c97e424123f8576_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1058&quot; data-rawheight=&quot;419&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1058&quot; data-original=&quot;https://pic3.zhimg.com/v2-81ae94d7f8c5cd553c97e424123f8576_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-81ae94d7f8c5cd553c97e424123f8576_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1058&quot; data-rawheight=&quot;419&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1058&quot; data-original=&quot;https://pic3.zhimg.com/v2-81ae94d7f8c5cd553c97e424123f8576_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-81ae94d7f8c5cd553c97e424123f8576_b.jpg&quot;&gt;&lt;figcaption&gt;图 6 TiDB 入选 Big Data &amp;amp;amp; AI Landscape 2018 和 Cloud Native Interactive Landscape&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;外界激励是一方面，另一方面我们也积极为社区小伙伴们创造交流、碰撞的平台。例如，在 2018 年 12 月初，我们举办了 TiDB Hackathon。经过两天一夜的「极限脑力竞技」，诞生了一系列基于 TiDB 生态的有意思的项目，希望这些项目可以在社区力量的帮助下延续下去：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487656%26idx%3D1%26sn%3Dc4ee830b5174ac062de2404ddffe821f%26chksm%3Deb1637c2dc61bed4fc52b9c30d2751f7c1a4f68290f15d17461dbf89dc3b9ae0522b83ce0983%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TBSSQL (TiDB Batch and Streaming SQL) &lt;/a&gt;&lt;/u&gt;&lt;/li&gt;&lt;li&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D2%26sn%3D3a601b2ff9100a9797605a825e478c01%26chksm%3Deb16289ddc61a18b49051feb9faf7e00b2093e83e723417ea4bab90808464eb6278bc9f979ed%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Laboratory&lt;/a&gt;&lt;/u&gt;&lt;/li&gt;&lt;li&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487479%26idx%3D1%26sn%3D8a8861419dd22344a021667545005769%26chksm%3Deb16289ddc61a18b034360c5b37437f3cdac956d6777b6711ef14d014211c58d08af969b965c%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 支持多种外部数据源的访问&lt;/a&gt;&lt;/u&gt;&lt;/li&gt;&lt;li&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487370%26idx%3D2%26sn%3D7eb3d41b2b5cf2a8a440b12121796e2d%26chksm%3Deb1628e0dc61a1f6719856b0eeadd4e878c3b59e0127f8b8f65ed1fb99b2a8981739b5449ce7%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PD 热点调度贝叶斯模型&lt;/a&gt;&lt;/u&gt;&lt;/li&gt;&lt;li&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487555%26idx%3D2%26sn%3D53807a033fef11b8103048bbcac51b69%26chksm%3Deb163729dc61be3f6a64eafe6101054b34bd7f7266b68a3a1091cb751e1eb0d9dcf3f3af39c5%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiEye&lt;/a&gt;&lt;/u&gt;&lt;/li&gt;&lt;li&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D2%26sn%3D5f1ee6e838c3a86556fcd556662112c5%26chksm%3Deb1628b1dc61a1a7e8f4cb82e2bfaab40cbfb27e986f9705f9166d629ff31a812f7ae45b1d73%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiQuery&lt;/a&gt;&lt;/u&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Content&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;内容分享至上。我们一直希望大家能够懂得开源、分享的精神，主动传播技术知识、分享推动项目进展背后的逻辑，让每个人都成为 Blogger，让社区拥有更好的信息传递和交流的氛围。所以，我们在 2018 年输出了一系列用户实践（&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//pingcap.com/cases-cn/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pingcap.com/cases-cn/&lt;/a&gt;）、&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读&lt;/a&gt;&lt;/u&gt;、&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiDB-Ecosystem-Tools&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;产品原理介绍&lt;/a&gt;&lt;/u&gt;、&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-community-guide-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;开源社区参与指南&lt;/a&gt;&lt;/u&gt;等技术文章。图 6 中标红的 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tikv-source-code-reading-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章&lt;/a&gt;&lt;/u&gt;正在「挖坑」中，敬请期待。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b54e9a512dfcca0fa8fbcd89b96e6c33_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1058&quot; data-rawheight=&quot;419&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1058&quot; data-original=&quot;https://pic4.zhimg.com/v2-b54e9a512dfcca0fa8fbcd89b96e6c33_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b54e9a512dfcca0fa8fbcd89b96e6c33_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1058&quot; data-rawheight=&quot;419&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1058&quot; data-original=&quot;https://pic4.zhimg.com/v2-b54e9a512dfcca0fa8fbcd89b96e6c33_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-b54e9a512dfcca0fa8fbcd89b96e6c33_b.jpg&quot;&gt;&lt;figcaption&gt;图 7 2018 年技术内容输出&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;除了这些线上文字分享，我们也把内部&lt;b&gt;Paper Reading&lt;/b&gt;活动放到了线上直播平台&lt;b&gt;（Bilibili ID: TiDB_Robot）&lt;/b&gt;，开放给了社区小伙伴们。因为 TiDB 的发展已经进入新型分布式数据库领域的深水区，我们需要借助前沿学术研究，结合用户的反馈建议和自己的灵感，探索 TiDB 未来方向的细节展开和落地方案，所以非常希望通过 Paper Reading 活动可以和大家共同学习和讨论。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Community Plan&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2018 年我们启动了三个社区培训计划，面向不同的人群，设置了一系列线上/线下培训课程，帮助大家了解和使用 TiDB，甚至能够独立部署、运维、调优 TiDB。2019 年我们会深入推进这些计划，感兴趣的同学可以报名加入。&lt;/p&gt;&lt;p&gt;&lt;b&gt;PingCAP University&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6e7c95584ecbbf7edb6bc70cdb2ab69b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;453&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-6e7c95584ecbbf7edb6bc70cdb2ab69b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6e7c95584ecbbf7edb6bc70cdb2ab69b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;453&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-6e7c95584ecbbf7edb6bc70cdb2ab69b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-6e7c95584ecbbf7edb6bc70cdb2ab69b_b.jpg&quot;&gt;&lt;figcaption&gt;图 8 PingCAP University &lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;  报名：university-cn@pingcap.com&lt;/li&gt;&lt;li&gt;通过 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp%253C/u%253E.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487041%26idx%3D1%26sn%3D24a620897124f227a9e59c8100c67542%26chksm%3Deb16292bdc61a03dd85c4de3666420437cff24b84506a62fb0e231a34ca99b96c3cb6b06068e%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PingCAP University &lt;/a&gt;培训/认证，能获得什么？&lt;/u&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;深度理解 TiDB 架构、原理以及最佳实践，具备独立部署、运维和调优 TiDB 的能力。&lt;/li&gt;&lt;li&gt;理论与实践相结合，强调实际动手能力，提高前沿技术视野，培养新一代 NewSQL 数据库优秀人才。&lt;/li&gt;&lt;li&gt;获得来自 PingCAP 官方的专业技术能力认可。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;未来计划：&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;面向用户的线上课程设计实现 &lt;/li&gt;&lt;li&gt;面向开发者的课程设计实现&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;TiDB Academy &lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB for MySQL DBAs（主要面向海外用户）&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/tidb-academy/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/tidb-academ&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;y/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; &lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9ac776d0369329bf9af406dbbbf7d7ee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;622&quot; data-rawheight=&quot;416&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;622&quot; data-original=&quot;https://pic3.zhimg.com/v2-9ac776d0369329bf9af406dbbbf7d7ee_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9ac776d0369329bf9af406dbbbf7d7ee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;622&quot; data-rawheight=&quot;416&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;622&quot; data-original=&quot;https://pic3.zhimg.com/v2-9ac776d0369329bf9af406dbbbf7d7ee_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-9ac776d0369329bf9af406dbbbf7d7ee_b.jpg&quot;&gt;&lt;figcaption&gt;图 9 TiDB academy 网站页面&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;TiDB Talent Plan&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a601fdb3c6a4bb48db52d11db12f53f5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;970&quot; data-rawheight=&quot;419&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;970&quot; data-original=&quot;https://pic2.zhimg.com/v2-a601fdb3c6a4bb48db52d11db12f53f5_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a601fdb3c6a4bb48db52d11db12f53f5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;970&quot; data-rawheight=&quot;419&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;970&quot; data-original=&quot;https://pic2.zhimg.com/v2-a601fdb3c6a4bb48db52d11db12f53f5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a601fdb3c6a4bb48db52d11db12f53f5_b.jpg&quot;&gt;&lt;figcaption&gt;图 10 第一期 TiDB Talent Plan&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D1%26sn%3D53b17f49e05af2cd832192fd67eaf75f%26chksm%3Deb1628b1dc61a1a704544f250cee480e7f6cf95cf5e84c2ccb7f0858398e685160122ed687b6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;第一期 TiDB Talent Plan &lt;/a&gt;&lt;/u&gt;于 2018 年12 月 12 日落幕，六位学员顺利结业。后续我们希望把 Talent Plan 的课程从线下拓展到线上，让更多对 TiDB 社区感兴趣的小伙伴可以从中找到组织，参与学习交流和深入实践。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d67978ca43340817886f5edefc738f21_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1039&quot; data-rawheight=&quot;413&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1039&quot; data-original=&quot;https://pic2.zhimg.com/v2-d67978ca43340817886f5edefc738f21_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d67978ca43340817886f5edefc738f21_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1039&quot; data-rawheight=&quot;413&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1039&quot; data-original=&quot;https://pic2.zhimg.com/v2-d67978ca43340817886f5edefc738f21_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-d67978ca43340817886f5edefc738f21_b.jpg&quot;&gt;&lt;figcaption&gt;图 11 第一期 TiDB Talent Plan 课程设置&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;除此之外，我们计划在 2019 年以北京、上海、硅谷等 7 个城市/地区为落脚点，成立 &lt;b&gt;TiDB User Group &lt;/b&gt;，力求「让用户驱动用户」，共同打造更好、更强的 TiDB 生态。同时也让更多小伙伴有机会&lt;b&gt;深度参与&lt;/b&gt;社区培训计划的课程设计、线上线下培训、社群活动组织等等。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Moment of Glory&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;回顾了 2018 年社区发展和未来计划之后，我们为 2018 年度 TiDB 社区活跃贡献者、最佳贡献个人&amp;amp;团队颁发了荣誉奖杯，并为新晋 Committer 授予证书。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2018 TiDB Active Contributors&lt;/b&gt;&lt;/p&gt;&lt;p&gt;🌟 bb7133  (TiDB TiKV)&lt;/p&gt;&lt;p&gt;🌟 niedhui  (TiKV)&lt;/p&gt;&lt;p&gt;🌟 yangwenmai  (TiDB)&lt;/p&gt;&lt;p&gt;🌟 andrewdi (TiDB)&lt;/p&gt;&lt;p&gt;🌟 mathspanda  (TiDB Operator)&lt;/p&gt;&lt;p&gt;&lt;b&gt;2018 最佳社区贡献奖&lt;/b&gt;&lt;/p&gt;&lt;p&gt;🌟 spongedu  (Du Chuan)&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b45f63861863c5e26f75ee7a24d429ae_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-b45f63861863c5e26f75ee7a24d429ae_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b45f63861863c5e26f75ee7a24d429ae_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-b45f63861863c5e26f75ee7a24d429ae_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-b45f63861863c5e26f75ee7a24d429ae_b.jpg&quot;&gt;&lt;figcaption&gt;图 10 崔秋为 spongedu 颁发荣誉奖杯&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;32 PRs (TiDB) 10 PRs (TiKV) &lt;/li&gt;&lt;li&gt;Important Features&lt;/li&gt;&lt;ul&gt;&lt;li&gt;TiDB 2.0 SQL engine refactor&lt;/li&gt;&lt;li&gt;Add chunk support for HashAgg&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Bug Fixes&lt;/li&gt;&lt;ul&gt;&lt;li&gt;17+ bug fixes (optimizer, executor, parser, expression)&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;参加 TiDB Hackathon（TBSSQL 队）获得一等奖&amp;amp;最佳贡献奖&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;🌟 &lt;/b&gt;美团点评分布式数据库项目组&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-cd5feb9ab35933c6a1cb590425b461d8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;683&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-cd5feb9ab35933c6a1cb590425b461d8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-cd5feb9ab35933c6a1cb590425b461d8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;683&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-cd5feb9ab35933c6a1cb590425b461d8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-cd5feb9ab35933c6a1cb590425b461d8_b.jpg&quot;&gt;&lt;figcaption&gt;图 10 崔秋为美团点评分布式数据库项目组负责人颁发荣誉奖杯&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;上线 20+ 套业务集群，200+节点&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/user-case-meituandianping/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;美团点评携手 PingCAP 开启新一代数据库深度实践之旅&lt;/a&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;10+ PRs, 50+ issues&lt;/li&gt;&lt;li&gt;10+ Engineers&lt;/li&gt;&lt;ul&gt;&lt;li&gt;zhongleihe / yu34po / guozhulang / zhaoxiaojie0415 / 18610314061 / wu-xiang / andyqzb / nettedfish / iamzhoug37 / Y-Rookie / benmaoer / pengji&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Important Featues&lt;/li&gt;&lt;ul&gt;&lt;li&gt;SQL Plan Management&lt;/li&gt;&lt;li&gt;Index join optimization (WIP) &lt;/li&gt;&lt;li&gt;Rowid scan optimization (WIP)&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2018 TiDB New Committers&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;🌟 &lt;/b&gt;TiKV New Committer: sunxiaoguang（知乎）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;8 PRs&lt;/li&gt;&lt;li&gt;Add Rust client support (Raw API)&lt;/li&gt;&lt;li&gt;Add Batch Raw API support (put/get/delete/scan)&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bce496f1615625a189f752da03cadb2f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-bce496f1615625a189f752da03cadb2f_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bce496f1615625a189f752da03cadb2f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-bce496f1615625a189f752da03cadb2f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-bce496f1615625a189f752da03cadb2f_b.jpg&quot;&gt;&lt;figcaption&gt;图 11 TiDB Committer 李雨来为 sunxiaoguang 授予证书&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;🌟&lt;/b&gt; TiDB New Committer: dbjoa (Samsung)&lt;/p&gt;&lt;ul&gt;&lt;li&gt;15 PRs&lt;/li&gt;&lt;li&gt;Add prepare plan cache support (Insert / Update / Delete)&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-40499dbdc074fddf391fe2a8d2a06112_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;657&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-40499dbdc074fddf391fe2a8d2a06112_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-40499dbdc074fddf391fe2a8d2a06112_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;657&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-40499dbdc074fddf391fe2a8d2a06112_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-40499dbdc074fddf391fe2a8d2a06112_b.jpg&quot;&gt;&lt;figcaption&gt;图 12 dbjoa 由于行程原因没有到场，他录制了一段视频，为 TiDB 社区送上祝福&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;希望明年在社区荣誉时刻，也见到你的 GitHub ID 哦！&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;新的一年 PR 也要满满哒！&lt;/i&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-13-56624608</guid>
<pubDate>Wed, 13 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>Titan 的设计与实现</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-01-29-55521489.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/55521489&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-579b54e6a23d0d19a9cd2950355a72af_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：郑志铨&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/rocksdb/tree/titan-5.15&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Titan&lt;/a&gt; 是由 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PingCAP&lt;/a&gt; 研发的一个基于 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/facebook/rocksdb&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RocksDB&lt;/a&gt; 的高性能单机 key-value 存储引擎，其主要设计灵感来源于 USENIX FAST 2016 上发表的一篇论文 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;WiscKey&lt;/a&gt;。&lt;code&gt;WiscKey&lt;/code&gt; 提出了一种高度基于 SSD 优化的设计，利用 SSD 高效的随机读写性能，通过将 value 分离出 &lt;code&gt;LSM-tree&lt;/code&gt; 的方法来达到降低写放大的目的。&lt;/p&gt;&lt;p&gt;我们的基准测试结果显示，当 value 较大的时候，Titan 在写、更新和点读等场景下性能都优于 RocksDB。但是根据 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//daslab.seas.harvard.edu/rum-conjecture/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RUM Conjecture&lt;/a&gt;&lt;/code&gt;，通常某些方面的提升往往是以牺牲其他方面为代价而取得的。Titan 便是以牺牲硬盘空间和范围查询的性能为代价，来取得更高的写性能。随着 SSD 价格的降低，我们认为这种取舍的意义会越来越明显。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;设计目标&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Titan 作为 TiKV 的一个子项目，首要的设计目标便是兼容 RocksDB。因为 TiKV 使用 RocksDB 作为其底层的存储引擎，而 TiKV 作为一个成熟项目已经拥有庞大的用户群体，所以我们需要考虑已有的用户也可以将已有的基于 RocksDB 的 TiKV 平滑地升级到基于 Titan 的 TiKV。&lt;/p&gt;&lt;p&gt;因此，我们总结了四点主要的设计目标：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;支持将 value 从 &lt;code&gt;LSM-tree&lt;/code&gt; 中分离出来单独存储，以降低写放大。&lt;/li&gt;&lt;li&gt;已有 RocksDB 实例可以平滑地升级到 Titan，这意味着升级过程不需要人工干预，并且不会影响线上服务。&lt;/li&gt;&lt;li&gt;100% 兼容目前 TiKV 所使用的所有 RocksDB 的特性。&lt;/li&gt;&lt;li&gt;尽量减少对 RocksDB 的侵入性改动，保证 Titan 更加容易升级到新版本的 RocksDB。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;架构与实现&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Titan 的基本架构如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;882&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;882&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-65149fdad2e1249dda3e58fb8d9e1490_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 1：Titan 在 Flush 和 Compaction 的时候将 value 分离出 &lt;code&gt;LSM-tree&lt;/code&gt;，这样做的好处是写入流程可以和 RockDB 保持一致，减少对 &lt;code&gt;RocksDB&lt;/code&gt; 的侵入性改动。&lt;/blockquote&gt;&lt;p&gt;Titan 的核心组件主要包括：&lt;code&gt;BlobFile&lt;/code&gt;、&lt;code&gt;TitanTableBuilder&lt;/code&gt;、&lt;code&gt;Version&lt;/code&gt; 和 &lt;code&gt;GC&lt;/code&gt;，下面将逐一进行介绍。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;&lt;code&gt;BlobFile&lt;/code&gt;&lt;/b&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;BlobFile&lt;/code&gt; 是用来存放从 &lt;code&gt;LSM-tree&lt;/code&gt; 中分离出来的 value 的文件，其格式如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;1327&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;1327&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-6b4972e3d5e50a18e081a2eedef51867_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 2：&lt;code&gt;BlobFile&lt;/code&gt; 主要由 blob record 、meta block、meta index block 和 footer 组成。其中每个 blob record 用于存放一个 key-value 对；meta block 支持可扩展性，可以用来存放和 &lt;code&gt;BlobFile&lt;/code&gt; 相关的一些属性等；meta index block 用于检索 meta block。&lt;/blockquote&gt;&lt;p&gt;&lt;code&gt;BlobFile&lt;/code&gt; 有几点值得关注的地方：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;BlobFile&lt;/code&gt; 中的 key-value 是有序存放的，目的是在实现 &lt;code&gt;Iterator&lt;/code&gt; 的时候可以通过 prefetch 的方式提高顺序读取的性能。&lt;/li&gt;&lt;li&gt;每个 blob record 都保留了 value 对应的 user key 的拷贝，这样做的目的是在进行 GC 的时候，可以通过查询 user key 是否更新来确定对应 value 是否已经过期，但同时也带来了一定的写放大。&lt;/li&gt;&lt;li&gt;&lt;code&gt;BlobFile&lt;/code&gt; 支持 blob record 粒度的 compression，并且支持多种 compression algorithm，包括 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/google/snappy&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Snappy&lt;/a&gt;&lt;/code&gt;、&lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/lz4/lz4&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;LZ4&lt;/a&gt;&lt;/code&gt;和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/facebook/zstd&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Zstd&lt;/a&gt;&lt;/code&gt; 等，目前 Titan 默认使用的 compression algorithm 是 &lt;code&gt;LZ4&lt;/code&gt; 。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;TitanTableBuilder&lt;/b&gt;&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;TitanTableBuilder&lt;/code&gt; 是实现分离 key-value 的关键。我们知道 RocksDB 支持使用用户自定义 table builder 创建 &lt;code&gt;SST&lt;/code&gt;，这使得我们可以不对 build table 流程做侵入性的改动就可以将 value 从 &lt;code&gt;SST&lt;/code&gt; 中分离出来。下面将介绍 &lt;code&gt;TitanTableBuilder&lt;/code&gt; 的主要工作流程：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;777&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;777&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-d52ee86abcc2fa54f32942c787879ca6_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 3：&lt;code&gt;TitanTableBuilder&lt;/code&gt; 通过判断 value size 的大小来决定是否将 value 分离到 &lt;code&gt;BlobFile&lt;/code&gt; 中去。如果 value size 大于等于 &lt;code&gt;min_blob_size&lt;/code&gt; 则将 value 分离到 &lt;code&gt;BlobFile&lt;/code&gt; ，并生成 index 写入 &lt;code&gt;SST&lt;/code&gt;；如果 value size 小于 &lt;code&gt;min_blob_size&lt;/code&gt; 则将 value 直接写入 &lt;code&gt;SST&lt;/code&gt;。&lt;/blockquote&gt;&lt;p&gt;Titan 和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/dgraph-io/badger&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Badger&lt;/a&gt;&lt;/code&gt; 的设计有很大区别。&lt;code&gt;Badger&lt;/code&gt; 直接将 &lt;code&gt;WAL&lt;/code&gt; 改造成 &lt;code&gt;VLog&lt;/code&gt;，这样做的好处是减少一次 Flush 的开销。而 Titan 不这么设计的主要原因有两个：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;假设 &lt;code&gt;LSM-tree&lt;/code&gt; 的 max level 是 5，放大因子为 10，则 &lt;code&gt;LSM-tree&lt;/code&gt; 总的写放大大概为 1 + 1 + 10 + 10 + 10 + 10，其中 Flush 的写放大是 1，其比值是 42 : 1，因此 Flush 的写放大相比于整个 LSM-tree 的写放大可以忽略不计。&lt;/li&gt;&lt;li&gt;在第一点的基础上，保留 &lt;code&gt;WAL&lt;/code&gt; 可以使 Titan 极大地减少对 RocksDB 的侵入性改动，而这也正是我们的设计目标之一。 &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;b&gt;Version&lt;/b&gt;&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Titan 使用 &lt;code&gt;Version&lt;/code&gt; 来代表某个时间点所有有效的 &lt;code&gt;BlobFile&lt;/code&gt;，这是从 &lt;code&gt;LevelDB&lt;/code&gt; 中借鉴过来的管理数据文件的方法，其核心思想便是 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Multiversion_concurrency_control&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MVCC&lt;/a&gt;&lt;/code&gt;，好处是在新增或删除文件的同时，可以做到并发读取数据而不需要加锁。每次新增文件或者删除文件的时候，&lt;code&gt;Titan&lt;/code&gt; 都会生成一个新的 &lt;code&gt;Version&lt;/code&gt; ，并且每次读取数据之前都要获取一个最新的 &lt;code&gt;Version&lt;/code&gt;。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;380&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;380&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-b7d9e3002bfc4268a4827f6505e1b1bd_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 4：新旧 &lt;code&gt;Version&lt;/code&gt; 按顺序首尾相连组成一个双向链表，&lt;code&gt;VersionSet&lt;/code&gt; 用来管理所有的 &lt;code&gt;Version&lt;/code&gt;，它持有一个 &lt;code&gt;current&lt;/code&gt; 指针用来指向当前最新的 &lt;code&gt;Version&lt;/code&gt;。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Garbage Collection&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Garbage Collection (GC) 的目的是回收空间，一个高效的 GC 算法应该在权衡写放大和空间放大的同时，用最少的周期来回收最多的空间。在设计 GC 的时候有两个主要的问题需要考虑：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;何时进行 GC&lt;/li&gt;&lt;li&gt;挑选哪些文件进行 GC&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Titan 使用 RocksDB 提供的两个特性来解决这两个问题，这两个特性分别是 &lt;code&gt;TablePropertiesCollector&lt;/code&gt; 和&lt;code&gt;EventListener&lt;/code&gt; 。下面将讲解我们是如何通过这两个特性来辅助 GC 工作的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;code&gt;BlobFileSizeCollector&lt;/code&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;RocksDB 允许我们使用自定义的 &lt;code&gt;TablePropertiesCollector&lt;/code&gt; 来搜集 &lt;code&gt;SST&lt;/code&gt; 上的 properties 并写入到对应文件中去。&lt;code&gt;Titan&lt;/code&gt; 通过一个自定义的 &lt;code&gt;TablePropertiesCollector&lt;/code&gt; —— &lt;code&gt;BlobFileSizeCollector&lt;/code&gt; 来搜集每个 &lt;code&gt;SST&lt;/code&gt; 中有多少数据是存放在哪些 &lt;code&gt;BlobFile&lt;/code&gt; 上的，我们将它收集到的 properties 命名为 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt;，它的工作流程和数据格式如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;346&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;346&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-b6966365a68aff1249cf227580b075c3_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 5：左边 &lt;code&gt;SST&lt;/code&gt; 中 Index 的格式为：第一列代表 &lt;code&gt;BlobFile&lt;/code&gt; 的文件 ID，第二列代表 blob record 在 &lt;code&gt;BlobFile&lt;/code&gt; 中的 offset，第三列代表 blob record 的 size。右边 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt; 中的每一行代表一个 &lt;code&gt;BlobFile&lt;/code&gt; 以及 &lt;code&gt;SST&lt;/code&gt; 中有多少数据保存在这个 &lt;code&gt;BlobFile&lt;/code&gt; 中，第一列代表 &lt;code&gt;BlobFile&lt;/code&gt; 的文件 ID，第二列代表数据大小。&lt;/blockquote&gt;&lt;p&gt;&lt;code&gt;&lt;b&gt;EventListener&lt;/b&gt;&lt;/code&gt; &lt;/p&gt;&lt;p&gt;我们知道 RocksDB 是通过 Compaction 来丢弃旧版本数据以回收空间的，因此每次 Compaction 完成后 Titan 中的某些 &lt;code&gt;BlobFile&lt;/code&gt; 中便可能有部分或全部数据过期。因此我们便可以通过监听 Compaction 事件来触发 GC，通过搜集比对 Compaction 中输入输出 &lt;code&gt;SST&lt;/code&gt; 的 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt; 来决定挑选哪些 &lt;code&gt;BlobFile&lt;/code&gt; 进行 GC。其流程大概如下图所示：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;652&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;652&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-e4686b7ee8ae31e83799bef437730e43_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 6：inputs 代表参与 Compaction 的所有 &lt;code&gt;SST&lt;/code&gt; 的 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt;，outputs 代表 Compaction 生成的所有 &lt;code&gt;SST&lt;/code&gt; 的 &lt;code&gt;BlobFileSizeProperties&lt;/code&gt;，discardable size 是通过计算 inputs 和 outputs 得出的每个 &lt;code&gt;BlobFile&lt;/code&gt; 被丢弃的数据大小，第一列代表 &lt;code&gt;BlobFile&lt;/code&gt; 的文件 ID，第二列代表被丢弃的数据大小。&lt;/blockquote&gt;&lt;p&gt;Titan 会为每个有效的 &lt;code&gt;BlobFile&lt;/code&gt; 在内存中维护一个 discardable size 变量，每次 Compaction 结束之后都对相应的 &lt;code&gt;BlobFile&lt;/code&gt; 的 discardable size 变量进行累加。每次 GC 开始时就可以通过挑选 discardable size 最大的 &lt;code&gt;BlobFile&lt;/code&gt; 来作为作为候选的文件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Sample&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每次进行 GC 前我们都会挑选一系列&lt;code&gt;BlobFile&lt;/code&gt;作为候选文件，挑选的方法如上一节所述。为了减小写放大，我们可以容忍一定的空间放大，所以我们只有在&lt;code&gt;BlobFile&lt;/code&gt;可丢弃的数据达到一定比例之后才会对其进行 GC。我们使用 Sample 算法来获取每个候选文件中可丢弃数据的大致比例。Sample 算法的主要逻辑是随机取&lt;code&gt;BlobFile&lt;/code&gt;中的一段数据 A，计其大小为 a，然后遍历 A 中的 key，累加过期的 key 所在的 blob record 的 size 计为 d，最后计算得出 d 占 a 比值 为 r，如果 r &amp;gt;=&lt;code&gt;discardable_ratio&lt;/code&gt;则对该&lt;code&gt;BlobFile&lt;/code&gt;进行 GC，否则不对其进行 GC。上一节我们已经知道每个&lt;code&gt;BlobFile&lt;/code&gt;都会在内存中维护一个 discardable size，如果这个 discardable size 占整个&lt;code&gt;BlobFile&lt;/code&gt;数据大小的比值已经大于或等于&lt;code&gt;discardable_ratio&lt;/code&gt;则不需要对其进行 Sample。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;基准测试&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们使用&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/go-ycsb&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;go-ycsb&lt;/a&gt;测试了 TiKV 在 Txn Mode 下分别使用 RocksDB 和 Titan 的性能表现，本节我会简要说明下我们的测试方法和测试结果。由于篇幅的原因，我们只挑选两个典型的 value size 做说明，更详细的测试分析报告将会放在下一篇文章。&lt;/p&gt;&lt;p&gt;&lt;b&gt;测试环境&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;CPU：Intel® Xeon® CPU E5-2630 v4 @ 2.20GHz（40个核心）&lt;/li&gt;&lt;li&gt;Memory：128GB（我们通过 Cgroup 限制 TiKV 进程使用内存不超过 32GB）&lt;/li&gt;&lt;li&gt;Disk：SATA SSD 1.5TB（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//linux.die.net/man/1/fio&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;fio&lt;/a&gt; 测试：4KB block size 混合随机读写情况下读写 IOPS 分别为 43.8K 和 18.7K）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;测试计划&lt;/b&gt;&lt;/p&gt;&lt;p&gt;数据集选定的基本原则是原始数据大小（不算上写放大因素）要比可用内存大，这样可以防止所有数据被缓存到内存中，减少 Cache 所带来的影响。这里我们选用的数据集大小是 64GB，进程的内存使用限制是 32GB。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;294&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1238&quot; data-original=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;294&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1238&quot; data-original=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-654eb07d1a8074029fdff9da40502dd7_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;我们主要测试 5 个常用的场景：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Data Loading Performance：使用预先计算好的 key 数量和固定的 value 大小，以一定的速度并发写入。&lt;/li&gt;&lt;li&gt;Update Performance：由于 Titan 在纯写入场景下不需要 GC（&lt;code&gt;BlobFile&lt;/code&gt; 中没有可丢弃数据），因此我们还需要通过更新来测试 &lt;code&gt;GC&lt;/code&gt; 对性能的影响。&lt;/li&gt;&lt;li&gt;Output Size：这一步我们会测量更新场景完成后引擎所占用的硬盘空间大小，以此反映 GC 的空间回收效果。&lt;/li&gt;&lt;li&gt;Random Key Lookup Performance：这一步主要测试点查性能，并且点查次数要远远大于 key 的数量。&lt;/li&gt;&lt;li&gt;Sorted Range Iteration Performance：这一步主要测试范围查询的性能，每次查询 2 million 个相连的 key。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;测试结果&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-dbe299a64478cfd1507390cb4a920938_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 7 Data Loading Performance：Titan 在写场景中的性能要比 RocksDB 高 70% 以上，并且随着 value size 的变大，这种性能的差异会更加明显。值得注意的是，数据在写入 KV Engine 之前会先写入 Raft Log，因此 Titan 的性能提升会被摊薄，实际上裸测 RocksDB 和 Titan 的话这种性能差异会更大。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-732c5f9c0ab0b43a7e134c1ad6e74f8f_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 8 Update Performance：Titan 在更新场景中的性能要比 RocksDB 高 180% 以上，这主要得益于 Titan 优秀的读性能和良好的 GC 算法。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-fc669dcecc0cf1286ee308292ba4c48e_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 9 Output Size：Titan 的空间放大相比 RocksDB 略高，这种差距会随着 Key 数量的减少有略微的缩小，这主要是因为 &lt;code&gt;BlobFile&lt;/code&gt; 中需要存储 Key 而造成的写放大。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-2988e19ff909924ffd330a4776b0f974_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 10 Random Key Lookup： Titan 拥有比 RocksDB 更卓越的点读性能，这主要得益与将 value 分离出 &lt;code&gt;LSM-tree&lt;/code&gt;的设计使得 &lt;code&gt;LSM-tree&lt;/code&gt; 变得更小，因此 Titan 在使用同样的内存量时可以将更多的 &lt;code&gt;index&lt;/code&gt; 、&lt;code&gt;filter&lt;/code&gt; 和 &lt;code&gt;DataBlock&lt;/code&gt; 缓存到 Block Cache 中去。这使得点读操作在大多数情况下仅需要一次 IO 即可（主要是用于从 &lt;code&gt;BlobFile&lt;/code&gt; 中读取数据）。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1240&quot; data-rawheight=&quot;767&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1240&quot; data-original=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-513126ff72f4ab14270dc383dc625751_b.jpg&quot;&gt;&lt;/figure&gt;&lt;blockquote&gt;图 11 Sorted Range Iteration：Titan 的范围查询性能目前和 RocksDB 相比还是有一定的差距，这也是我们未来优化的一个重要方向。&lt;/blockquote&gt;&lt;p&gt;本次测试我们对比了两个具有代表性的 value size 在 5 种不同场景下的性能差异，更多不同粒度的 value size 的测试和更详细的性能报告我们会放在下一篇文章去说明，并且我们会从更多的角度（例如 CPU 和内存的使用率等）去分析 Titan 和 RocksDB 的差异。从本次测试我们可以大致得出结论，在大 value 的场景下，Titan 会比 RocksDB 拥有更好的写、更新和点读性能。同时，Titan 的范围查询性能和空间放大都逊于 RocksDB 。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;兼容性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;一开始我们便将兼容 RocksDB 作为设计 Titan 的首要目标，因此我们保留了绝大部分 RocksDB 的 API。目前仅有两个 API 是我们明确不支持的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;Merge&lt;/code&gt;&lt;/li&gt;&lt;li&gt;&lt;code&gt;SingleDelete&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;除了 &lt;code&gt;Open&lt;/code&gt; 接口以外，其他 API 的参数和返回值都和 RocksDB 一致。已有的项目只需要很小的改动即可以将 &lt;code&gt;RocksDB&lt;/code&gt;实例平滑地升级到 Titan。值得注意的是 Titan 并不支持回退回 RocksDB。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何使用 Titan&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;创建 DB&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;#include &amp;lt;assert&amp;gt;
#include &quot;rocksdb/utilities/titandb/db.h&quot;

// Open DB
rocksdb::titandb::TitanDB* db;
rocksdb::titandb::TitanOptions options;
options.create_if_missing = true;
rocksdb::Status status =
  rocksdb::titandb::TitanDB::Open(options, &quot;/tmp/testdb&quot;, &amp;amp;db);
assert(status.ok());
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;或&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;#include &amp;lt;assert&amp;gt;
#include &quot;rocksdb/utilities/titandb/db.h&quot;

// open DB with two column families
rocksdb::titandb::TitanDB* db;
std::vector&amp;lt;rocksdb::titandb::TitanCFDescriptor&amp;gt; column_families;
// have to open default column family
column_families.push_back(rocksdb::titandb::TitanCFDescriptor(
    kDefaultColumnFamilyName, rocksdb::titandb::TitanCFOptions()));
// open the new one, too
column_families.push_back(rocksdb::titandb::TitanCFDescriptor(
    &quot;new_cf&quot;, rocksdb::titandb::TitanCFOptions()));
std::vector&amp;lt;ColumnFamilyHandle*&amp;gt; handles;
s = rocksdb::titandb::TitanDB::Open(rocksdb::titandb::TitanDBOptions(), kDBPath,
                                    column_families, &amp;amp;handles, &amp;amp;db);
assert(s.ok());
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Status&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;和 RocksDB 一样，Titan 使用 &lt;code&gt;rocksdb::Status&lt;/code&gt; 来作为绝大多数 API 的返回值，使用者可以通过它检查执行结果是否成功，也可以通过它打印错误信息：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;rocksdb::Status s = ...;
if (!s.ok()) cerr &amp;lt;&amp;lt; s.ToString() &amp;lt;&amp;lt; endl;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;销毁 DB&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;std::string value;
rocksdb::Status s = db-&amp;gt;Get(rocksdb::ReadOptions(), key1, &amp;amp;value);
if (s.ok()) s = db-&amp;gt;Put(rocksdb::WriteOptions(), key2, value);
if (s.ok()) s = db-&amp;gt;Delete(rocksdb::WriteOptions(), key1);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;在 TiKV 中使用 Titan&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;目前 Titan 在 TiKV 中是默认关闭的，我们通过 TiKV 的配置文件来决定是否开启和设置 Titan，相关的配置项包括 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/12a1ea8d13b6478c8a4d07f0bb7411f3367dc8f9/etc/config-template.toml%23L375&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;[rocksdb.titan]&lt;/a&gt;&lt;/code&gt; 和 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/12a1ea8d13b6478c8a4d07f0bb7411f3367dc8f9/etc/config-template.toml%23L531&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;[rocksdb.defaultcf.titan]&lt;/a&gt;&lt;/code&gt;， 开启 Titan 只需要进行如下配置即可：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;[rocksdb.titan]
enabled = true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意一旦开启 Titan 就不能回退回 RocksDB 了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;未来的工作&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;优化 &lt;code&gt;Iterator&lt;/code&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们通过测试发现，目前使用 Titan 做范围查询时 IO Util 很低，这也是为什么其性能会比 RocksDB 差的重要原因之一。因此我们认为 Titan 的 &lt;code&gt;Iterator&lt;/code&gt; 还存在着巨大的优化空间，最简单的方法是可以通过更加激进的 prefetch 和并行 prefetch 等手段来达到提升 &lt;code&gt;Iterator&lt;/code&gt; 性能的目的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;code&gt;GC&lt;/code&gt; 速度控制和自动调节&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通常来说，GC 的速度太慢会导致空间放大严重，过快又会对服务的 QPS 和延时带来影响。目前 Titan 支持自动 GC，虽然可以通过减小并发度和 batch size 来达到一定程度限制 GC 速度的目的，但是由于每个 &lt;code&gt;BlobFile&lt;/code&gt; 中的 blob record 数目不定，若 &lt;code&gt;BlobFile&lt;/code&gt; 中的 blob record 过于密集，将其有效的 key 更新回 &lt;code&gt;LSM-tree&lt;/code&gt; 时仍然可能堵塞业务的写请求。为了达到更加精细化的控制 GC 速度的目的，后续我们将使用 &lt;code&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Token_bucket&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Token Bucket&lt;/a&gt;&lt;/code&gt; 算法限制一段时间内 GC 能够更新的 key 数量，以降低 GC 对 QPS 和延时的影响，使服务更加稳定。&lt;/p&gt;&lt;p&gt;另一方面，我们也正在研究自动调节 GC 速度的算法，这样我们便可以，在服务高峰期的时候降低 GC 速度来提供更高的服务质量；在服务低峰期的时候提高 GC 速度来加快空间的回收。&lt;/p&gt;&lt;p&gt;&lt;b&gt;增加用于判断 key 是否存在的 API&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 在某些场景下仅需要判断某个 key 是否存在，而不需要读取对应的 value。通过提供一个这样的 API 可以极大地提高性能，因为我们已经看到将 value 移出 &lt;code&gt;LSM-tree&lt;/code&gt; 之后，&lt;code&gt;LSM-tree&lt;/code&gt; 本身会变的非常小，以至于我们可以将更多地 &lt;code&gt;index&lt;/code&gt;、&lt;code&gt;filter&lt;/code&gt; 和 &lt;code&gt;DataBlock&lt;/code&gt; 存放到内存当中去，这样去检索某个 key 的时候可以做到只需要少量甚至不需要 IO 。&lt;/p&gt;&lt;p&gt;&lt;i&gt;&lt;b&gt;更多阅读：&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-01-29-55521489</guid>
<pubDate>Tue, 29 Jan 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
