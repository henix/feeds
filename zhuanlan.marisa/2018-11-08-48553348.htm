<div class="title-image"><img src="https://pic1.zhimg.com/v2-cfad1bac7ce7f070d2594ae717c075ac_r.jpg" alt=""></div><h2>Estimate Size of Transitive Closure</h2><p>PCP的那个坑先继续咕一段时间，今天给大家介绍一下最近学到的一个很有意思的问题。 本文的idea来自<a href="https://www.ilyaraz.org/">Ilya Razenshteyn</a>, 基于<a href="http://cohenwang.com/edith/Papers/tcest.pdf">Cohen 94</a>。</p><p>一个有向图<equation>G=(V,E)</equation>。我们把能够从点<equation>v</equation>到达的点的集合叫做点<equation>v</equation>的传递闭包，记作<equation>R_v</equation>。现在我们想知道这个图<equation>G</equation>每个点的传递闭包的大小。对于这个问题的确定性算法，最好的已知复杂度是<equation>\mathcal O(\min(|V||E|), |V|^{2.38}))</equation>。</p><p>然而如果我们允许随机和近似的话，我们能够做到近乎线性：通过<equation>\mathcal O\left(\frac{m\log^2 n}{\epsilon^2}\right)</equation>的时间复杂度，对于每个点<equation>v\in V</equation>,我们可以得到<equation>R_v</equation>的近似<equation>R_v'</equation>使得<equation>(1-\epsilon)R_v &lt;R_v' &lt; (1+\epsilon)R_v</equation>。</p><h2>算法</h2><p>首先我们对每一个点<equation>v</equation>, 我们随机从指数分布给它赋值<equation>X_v</equation>， 使得<equation>X_v\sim\text{Exp}(1)</equation>， 并且反转原图所有边的方向。然后，对于所有的<equation>X_v</equation>, 我们从小到大的取出，从点<equation>v</equation>开始进行BFS，对于被BFS到的点<equation>u</equation>,我们标记<equation>Y_u = X_v</equation>,然后删除所有访问过的点。直到所有点都被访问过恰好一次。</p><p>我们可以证明,<equation>\mathbb E[Y_v] = 1/|R_v|</equation></p><p>我们每次都是在选出最小的<equation>X_v</equation>然后标记<equation>v</equation>在反向图里面能到达的所有点，所以<equation>Y_v</equation>的定义等价于<equation>Y_v=\min_{u\in R_v} X_u</equation>.</p><h2>指数分布的最小值</h2><p>定理1: 如果我们有<equation>n</equation>个指数分布的随机变量<equation>X_1\sim \text{Exp}(\lambda_1), X_2\sim \text{Exp}(\lambda_2), \ldots,X_n\sim \text{Exp}(\lambda_n)</equation>,那么<equation>\min\{X_1,X_2,\ldots, X_n\} \sim \text{Exp}(\lambda_1+\lambda_2+\ldots+\lambda_n)</equation>。</p><p>证明： 我们考虑指数分布的cdf的补集：我们知道如果<equation>X\sim \text{Exp}(\lambda)</equation>，那么<equation>\Pr[X &gt; t] = \exp(\lambda t)</equation>我们计算最小值的cdf的补集<equation>\begin{align*} \Pr(\min\{X_1,\ldots, X_n\} &gt; t) &amp;= \Pr(X_1 &gt; t, X_2 &gt; t, \ldots, X_n &gt; t) \\ &amp;= \prod_{i=1}^n \exp(-\lambda_i t) \\ &amp;=\exp\left(\sum_{i=1}^n \lambda_i t\right) \end{align*}</equation></p><p>所以<equation>\min\{X_1,X_2,\ldots, X_n\} \sim \text{Exp}(\lambda_1+\lambda_2+\ldots+\lambda_n)</equation>。</p><p>这个告诉我们了<equation>\mathbb E[Y_v] = \mathbb E[\min_{u\in R_v} X_u] = \mathbb E[\text{Exp(|R_v|)}] = 1/|R_v|.</equation></p><p>我们知道了<equation>1/Y_v</equation>是对<equation>|R_v|</equation>一个很好的估计(Estimator)，那么我们需要多少个<equation>Y_v</equation>的样本，才能很好的估计出<equation>|R_v|</equation>呢？</p><h2>Median Trick</h2><p>事实上，我们只需要<equation>O\left(\frac{\log 1/\delta}{\epsilon^2}\right)</equation>个样本就有很高的概率(<equation>1-\delta</equation>)得到<equation>1\pm\epsilon</equation>的估计。</p><p>证明：</p><p>首先假设我们我们有<equation>k</equation>个<equation>Y_v</equation>的样本，标记为<equation>Z_1,Z_2,\ldots, Z_k</equation>，然后我们取他们的平均值<equation>\bar Z = \mu = \sum Z_i/k</equation>。很显然，此处<equation>\mathbb E[\bar Z] = 1/|R_v|</equation>。 为了使用切比雪夫不等式，我们计算它的方差：</p><p><equation>\begin{align*} \text{Var}(\bar{Z}) &amp;= \text{Var}\left(\sum_{i=1}^k \frac{Z_i}{k}\right) \\ &amp;=\frac{1}{k^2}\text{Var}(\sum_{i=1}^k Z_i) \\ &amp;=\frac{1}{k^2} \cdot k \cdot \frac{1}{{|R_v|}^2} \\ &amp;=\frac{1}{k{|R_v|}^2}\\ &amp;=\frac{\mu^2}{k} \end{align*}</equation></p><p>同时，我们注意到，当<equation>\epsilon &lt; 0.1</equation>的时候，如果得到了<equation>1/|R_v|</equation>误差在<equation>0.5\epsilon</equation>的估计，那么<equation>|R_v|</equation>的误差范围在<equation>\epsilon</equation>内。</p><p>切比学夫不等式告诉我们，</p><p><equation>\begin{align*} \Pr[|{\bar{Z} -\mu}| \leq 0.5\epsilon \mu] &amp;\geq 1-\frac{\mu^2}{0.25k \epsilon^2 \mu^2} \\ &amp;=1-\frac{1}{0.25k\epsilon^2} \end{align*}</equation></p><p>我们让<equation>k= \frac{4}{\epsilon^2}</equation>, 我们有大于0.9的概率得到一个<equation>1\pm \epsilon</equation>的估计。显然这个是不够高的 (考虑到我们后面需要使用union bound)，为了得到更高的概率，我们使用median trick来继续提高成功的概率。</p><p>假设我们把上面的过程独立地重复<equation>\ell</equation>次，把这<equation>\ell</equation>次的结果标做<equation>R_1,\ldots, R_\ell</equation>。我们观察到：如果有一半以上的结果满足条件，那么这些数的中位数也满足条件。</p><p>我们定义<equation>H_i := 𝟙[{R_i \in ((1-\epsilon){|R_v|}, (1+\epsilon){|R_v|})}].</equation></p><p>因为<equation>\Pr[H_i] \geq 0.9</equation>,所以<equation>\mathbb E[\sum_{i=1}^\ell H_i] = \ell\cdot \sum_{i=1}^\ell \mathbb E[H_i] \geq 0.9l.</equation></p><p>然后我们使用Hoeffding不等式:<equation>\Pr\left[\sum_{i=1}^\ell H_i \leq \frac{\ell}{2}\right] \leq \Pr\left[\sum_{i=1}^\ell H_i - \mathbb E\left[\sum_{i=1}^\ell H_i \right] &gt; \frac{\ell}{4}\right]\leq e^{-\ell/8}.</equation></p><p>要让<equation>e^{-\ell/8} \leq \delta</equation>, 我们只需要让<equation>\ell = \mathcal O(\log 1/\delta)</equation>。所以我们一共需要<equation>\ell \cdot k=\mathcal O\left(\frac{\log 1/\delta}{\epsilon^2}\right)</equation>个样本。</p><h2>总结</h2><p>根据布尔不等式(Union bound), 我们每次都成功的概率大于<equation>n\cdot \delta</equation>, 要让这个概率大于<equation>0.9</equation>的话，那么<equation>\delta \leq 1/n</equation>，所以我们把原图复制<equation>O(\log(1/\delta)/\epsilon^2)</equation>次，然后独立的运行最开始的算法，我们就有很高的概率得到原图的<equation>1\pm\epsilon</equation>估计。 总的运行时间是:<equation>\mathcal O\left(\frac{1}{\epsilon^2} \cdot m\log^2 n \right).</equation></p>