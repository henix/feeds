<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>没想到你是这样的“兔子”---kNN</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/22874873">原文</a></p>
<div class="title-image"><img src="https://pic2.zhimg.com/v2-d21a67cb0482505880019d89ee3ae44b_b.jpg" alt=""></div><p>导语：商业哲学家 Jim Rohn 说过一句话，“你，就是你最常接触的五个人的平均。”那么，在分析一个人时，我们不妨观察和他最亲密的几个人。同理的，在判定一个未知事物时，可以观察离它最近的几个样本，这就是 kNN（k最近邻）的方法。</p><p>简介</p><p>kNN还真是直接讲例子最好懂。大家都喜欢兔子，所以就来说一说兔子的事情吧。</p><p>有一种兔子叫作<em>悲伤（Grief）</em>，它们的平均身高是 50 厘米，平均体重 5 公斤。我们拿来一百个悲伤，分别测量它们的身高和体重，画在坐标图上，用绿色方块表示。</p><figure><noscript><img src="https://pic2.zhimg.com/v2-56186a95f8810c4b9fa4552f63b0e15d_b.png" data-rawwidth="1100" data-rawheight="745" class="origin_image zh-lightbox-thumb" width="1100" data-original="https://pic2.zhimg.com/v2-56186a95f8810c4b9fa4552f63b0e15d_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-56186a95f8810c4b9fa4552f63b0e15d_b.png" data-rawwidth="1100" data-rawheight="745" class="origin_image zh-lightbox-thumb lazy" width="1100" data-original="https://pic2.zhimg.com/v2-56186a95f8810c4b9fa4552f63b0e15d_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-56186a95f8810c4b9fa4552f63b0e15d_b.png"/></figure><p>还有一种兔子呢，叫作<em>痛苦（Agony）</em>。它们体型比较小，平均身高是 30 厘米，平均体重是 4 公斤。我们将一百个痛苦的身高和体重画在同一个坐标图上，用蓝色三角表示。</p><figure><noscript><img src="https://pic4.zhimg.com/v2-af799e6f6d8671dced73ada5f557107b_b.png" data-rawwidth="1090" data-rawheight="742" class="origin_image zh-lightbox-thumb" width="1090" data-original="https://pic4.zhimg.com/v2-af799e6f6d8671dced73ada5f557107b_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-af799e6f6d8671dced73ada5f557107b_b.png" data-rawwidth="1090" data-rawheight="742" class="origin_image zh-lightbox-thumb lazy" width="1090" data-original="https://pic4.zhimg.com/v2-af799e6f6d8671dced73ada5f557107b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-af799e6f6d8671dced73ada5f557107b_b.png"/></figure><p>最后一种兔子叫<em>绝望（Despair）</em>。它们的平均身高45厘米，但体重较轻，平均只有2.5公斤。一百只绝望的数据用黄色圆圈表示。</p><figure><noscript><img src="https://pic4.zhimg.com/v2-f5940e7a6f1a1af8bf8aa0ecce72bc97_b.png" data-rawwidth="1099" data-rawheight="745" class="origin_image zh-lightbox-thumb" width="1099" data-original="https://pic4.zhimg.com/v2-f5940e7a6f1a1af8bf8aa0ecce72bc97_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-f5940e7a6f1a1af8bf8aa0ecce72bc97_b.png" data-rawwidth="1099" data-rawheight="745" class="origin_image zh-lightbox-thumb lazy" width="1099" data-original="https://pic4.zhimg.com/v2-f5940e7a6f1a1af8bf8aa0ecce72bc97_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-f5940e7a6f1a1af8bf8aa0ecce72bc97_b.png"/></figure><p>在这些数据中，(身高,体重) 的二元组叫做特征（features），兔子的品种则是分类标签（class label）。我们想解决的问题是，给定一个未知分类的新样本的所有特征，通过已知数据来判断它的类别。</p><p>北京十八环外有一个小树林里经常出现这三种兔子。为了了解它们的生态环境，某研究团队想知道三种兔子的数量比例；可是这些兔子又太过危险，不能让人亲自去做，所以要设计一个全自动的机器人，让它自己去树林里识别它遇到的每一个兔子的种类。啊，为了把故事讲圆，还要假设他们经费不足，所以机器只有测量兔子的身高和体重的能力。</p><p>那么现在有一迷之兔子，我们想判断它的类别，要怎么做呢？按照最普通的直觉，应该在已知数据里找出几个和我们想探究的兔子最相似的几个点，然后看看那些兔子都是什么个情况；如果它们当中大多数都属于某一类别，那么迷之兔子大概率也就是那个类别了。</p><p>于是乎，我们给机器人预设一个整数 k，让它去寻找距离最近的k个数据样本进行分析。好，机器发现了一只兔子，它长着八条腿，三十二只眼睛，毛茸茸的小尾巴，齐刷刷的八十六颗獠牙，面相狰狞，散发着噩梦般的腐臭，发出来自地狱底处的咆哮… 差不多就是这个样子（作者手绘）：</p><figure><noscript><img src="https://pic4.zhimg.com/v2-8330c2e172b18f9d85c52a413db510d7_b.png" data-rawwidth="879" data-rawheight="657" class="origin_image zh-lightbox-thumb" width="879" data-original="https://pic4.zhimg.com/v2-8330c2e172b18f9d85c52a413db510d7_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-8330c2e172b18f9d85c52a413db510d7_b.png" data-rawwidth="879" data-rawheight="657" class="origin_image zh-lightbox-thumb lazy" width="879" data-original="https://pic4.zhimg.com/v2-8330c2e172b18f9d85c52a413db510d7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-8330c2e172b18f9d85c52a413db510d7_b.png"/></figure><p>可我们的机器才识别不了那么多，它只测量出这只兔子身长 40 厘米，体重 2.7 公斤，就是下面图中那颗闪闪发亮的红星</p><figure><noscript><img src="https://pic2.zhimg.com/v2-4252e63dc15617db5f6365febaa23711_b.png" data-rawwidth="1068" data-rawheight="741" class="origin_image zh-lightbox-thumb" width="1068" data-original="https://pic2.zhimg.com/v2-4252e63dc15617db5f6365febaa23711_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-4252e63dc15617db5f6365febaa23711_b.png" data-rawwidth="1068" data-rawheight="741" class="origin_image zh-lightbox-thumb lazy" width="1068" data-original="https://pic2.zhimg.com/v2-4252e63dc15617db5f6365febaa23711_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-4252e63dc15617db5f6365febaa23711_b.png"/></figure><p>kNN 算法如何对这次观测进行分类要取决于k的大小。直觉告诉我们迷之兔像是一只绝望，因为除了最近的蓝色三角外，附近其他都是黄色圆圈。的确，如果设 k=15，算法会判断这只兔子是一只绝望。但是如果设 k=1，那么由于距离最近的是蓝色三角，会判断迷之兔子是一只痛苦。</p><p>如果按照15NN和1NN的方法对这个二维空间上的每一个点进行分类，会形成以下的分割</p><figure><noscript><img src="https://pic1.zhimg.com/v2-2fbb822962699124ed1c5ad3655da000_b.png" data-rawwidth="1086" data-rawheight="754" class="origin_image zh-lightbox-thumb" width="1086" data-original="https://pic1.zhimg.com/v2-2fbb822962699124ed1c5ad3655da000_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-2fbb822962699124ed1c5ad3655da000_b.png" data-rawwidth="1086" data-rawheight="754" class="origin_image zh-lightbox-thumb lazy" width="1086" data-original="https://pic1.zhimg.com/v2-2fbb822962699124ed1c5ad3655da000_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-2fbb822962699124ed1c5ad3655da000_b.png"/></figure><figure><noscript><img src="https://pic1.zhimg.com/v2-15fb8650357d68839332797aa32789c8_b.png" data-rawwidth="1095" data-rawheight="768" class="origin_image zh-lightbox-thumb" width="1095" data-original="https://pic1.zhimg.com/v2-15fb8650357d68839332797aa32789c8_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-15fb8650357d68839332797aa32789c8_b.png" data-rawwidth="1095" data-rawheight="768" class="origin_image zh-lightbox-thumb lazy" width="1095" data-original="https://pic1.zhimg.com/v2-15fb8650357d68839332797aa32789c8_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-15fb8650357d68839332797aa32789c8_b.png"/></figure><p>在两组分类中，1NN 的分类边界明显更“崎岖”，但是对历史样本没有误判；而 15NN 的分类边界更平滑，但是对历史样本有发生误判的现象。选择k的大小取决于对偏差和方差之间的权衡，本篇不进行更深探讨，读者在使用 kNN 时凭感觉选一个 k 就好。</p><p>距离函数</p><p>我们在上面的例子中把一个很重要的概念隐藏了起来，在选择一个数量k还只是小问题，更重要的是距离的计算方法。毕竟，当我们说“最近的k个点”时，这个“近”是怎么衡量的？</p><p>在数学中，一个空间上距离的严格定义如下：</p><p>设 MM 为一个空间，MM 上的一个距离函数是一个函数 d:M×M→R，满足</p><p>∙ d(x,y)≥0 ∀x,y∈M</p><p>∙ d(x,y)=0⟺（等价于符号，不知为何是方块。。）x=y</p><p>∙ d(x,y)=d(y,x) ∀x,y∈M</p><p>∙ d(x,z)≤d(x,y)+d(y,z) ∀x,y,z∈M</p><p>两个点 x,y 之间的距离就是 d(x,y)。</p><p>我们一般最常用的距离函数是欧氏距离，也称作 L2 距离。如果 x=(x1,x2,…,xn) 和 y=(y1,y2,…,yn) 是 nn 维欧式空间 Rn 上的两个点，那它们之间的 L2 距离是</p><figure><noscript><img src="https://pic2.zhimg.com/v2-e97be97bc2ef170b5fc56b9a6ad935cd_b.png" data-rawwidth="312" data-rawheight="105" class="content_image" width="312"/></noscript><img src="https://pic2.zhimg.com/v2-e97be97bc2ef170b5fc56b9a6ad935cd_b.png" data-rawwidth="312" data-rawheight="105" class="content_image lazy" width="312" data-actualsrc="https://pic2.zhimg.com/v2-e97be97bc2ef170b5fc56b9a6ad935cd_b.png"/></figure><p>L2 是更普遍的 Lp 距离在 p=2 时的特列。Lp 距离的函数 dp 定义如下：对于 1≤p&lt;∞，有</p><figure><noscript><img src="https://pic1.zhimg.com/v2-1db31d0c8aa97c68c95eb80850bab030_b.png" data-rawwidth="303" data-rawheight="87" class="content_image" width="303"/></noscript><img src="https://pic1.zhimg.com/v2-1db31d0c8aa97c68c95eb80850bab030_b.png" data-rawwidth="303" data-rawheight="87" class="content_image lazy" width="303" data-actualsrc="https://pic1.zhimg.com/v2-1db31d0c8aa97c68c95eb80850bab030_b.png"/></figure><p>还有 L∞ 距离</p><figure><noscript><img src="https://pic1.zhimg.com/v2-27b8b17f638a00d27fb3afb2a2747d68_b.png" data-rawwidth="311" data-rawheight="83" class="content_image" width="311"/></noscript><img src="https://pic1.zhimg.com/v2-27b8b17f638a00d27fb3afb2a2747d68_b.png" data-rawwidth="311" data-rawheight="83" class="content_image lazy" width="311" data-actualsrc="https://pic1.zhimg.com/v2-27b8b17f638a00d27fb3afb2a2747d68_b.png"/></figure><p>在实际应用中，距离函数的选择应该根据数据的特性和分析的需要而定，本篇就不进行更深入的探讨，一般情况下使用最常用的 L2 函数即可。</p><p>但是！注意！使用 kNN 时需要根据特征数据的取值区间来调整坐标轴的比例，这个做法叫作标准化或者归一化。为什么要这么做呢？拿上面的例子来说，一只兔子的身长（cm）数值平均是它的体重（kg）的 10 倍左右，如果我们在这组数值上直接使用 L2 距离函数的话就会导致横轴的距离比重明显放大，分类结果也不合理，如下图所示</p><figure><noscript><img src="https://pic2.zhimg.com/v2-b84820e871eebe5fa89d6e64560d98d5_b.png" data-rawwidth="1115" data-rawheight="761" class="origin_image zh-lightbox-thumb" width="1115" data-original="https://pic2.zhimg.com/v2-b84820e871eebe5fa89d6e64560d98d5_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-b84820e871eebe5fa89d6e64560d98d5_b.png" data-rawwidth="1115" data-rawheight="761" class="origin_image zh-lightbox-thumb lazy" width="1115" data-original="https://pic2.zhimg.com/v2-b84820e871eebe5fa89d6e64560d98d5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-b84820e871eebe5fa89d6e64560d98d5_b.png"/></figure><p>如果把坐标轴成其他的单位，比如毫米和吨，并用相应的新数值来计算距离，又会得到完全不同的分类标准。甚至，在极端情况下，如果身高用纳米并且体重用吨计量，那么相比之下身高的数值会奇高无比，以至于两点之间的距离是完全由身高决定的，体重则没有任何权重。为了解决这个问题，我们应该在计算距离时把所有坐标轴进行归一化。</p><p>在之前的例子中，由于横轴数值大约是竖轴的 10 倍左右，所以我们将横轴（身高）的数值压缩 10 倍，即计算距离时使用</p><figure><noscript><img src="https://pic4.zhimg.com/v2-b7445ca8ba357ebb6af3d7592695f4bb_b.png" data-rawwidth="536" data-rawheight="76" class="origin_image zh-lightbox-thumb" width="536" data-original="https://pic4.zhimg.com/v2-b7445ca8ba357ebb6af3d7592695f4bb_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-b7445ca8ba357ebb6af3d7592695f4bb_b.png" data-rawwidth="536" data-rawheight="76" class="origin_image zh-lightbox-thumb lazy" width="536" data-original="https://pic4.zhimg.com/v2-b7445ca8ba357ebb6af3d7592695f4bb_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b7445ca8ba357ebb6af3d7592695f4bb_b.png"/></figure><p>就可以得出合理的 kNN 分类。</p><p>一般来说，假设进行 kNN 分类使用的样本的特征是 ，取每一轴上的最大值减最小值</p><figure><noscript><img src="https://pic2.zhimg.com/v2-7a960e18da37c94365be00661ddafa8d_b.png" data-rawwidth="339" data-rawheight="74" class="content_image" width="339"/></noscript><img src="https://pic2.zhimg.com/v2-7a960e18da37c94365be00661ddafa8d_b.png" data-rawwidth="339" data-rawheight="74" class="content_image lazy" width="339" data-actualsrc="https://pic2.zhimg.com/v2-7a960e18da37c94365be00661ddafa8d_b.png"/></figure><p>并且在计算距离时将每一个坐标轴除以相应的 Mj 以进行归一化，即</p><figure><noscript><img src="https://pic3.zhimg.com/v2-64701f1a43e2cd7a4e14bf316e6706fe_b.png" data-rawwidth="537" data-rawheight="103" class="origin_image zh-lightbox-thumb" width="537" data-original="https://pic3.zhimg.com/v2-64701f1a43e2cd7a4e14bf316e6706fe_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-64701f1a43e2cd7a4e14bf316e6706fe_b.png" data-rawwidth="537" data-rawheight="103" class="origin_image zh-lightbox-thumb lazy" width="537" data-original="https://pic3.zhimg.com/v2-64701f1a43e2cd7a4e14bf316e6706fe_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-64701f1a43e2cd7a4e14bf316e6706fe_b.png"/></figure><p>便可以规避坐标轴比例失衡的问题。</p><p>概率 kNN</p><p>上面的kNN算法返回的是对一组特征的绝对分类，告诉我们这只兔子被判断为哪一个类别。可有时我们并不想知道一个确切地分类，而想知道它属于某个分类的概率是多大。比如我们发现一只身长 37 体重 4.8 的小兔兔，在下图五角星的位置。</p><figure><noscript><img src="https://pic2.zhimg.com/v2-c822018632b5547cc9072dfe2c8a2809_b.png" data-rawwidth="1090" data-rawheight="738" class="origin_image zh-lightbox-thumb" width="1090" data-original="https://pic2.zhimg.com/v2-c822018632b5547cc9072dfe2c8a2809_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-c822018632b5547cc9072dfe2c8a2809_b.png" data-rawwidth="1090" data-rawheight="738" class="origin_image zh-lightbox-thumb lazy" width="1090" data-original="https://pic2.zhimg.com/v2-c822018632b5547cc9072dfe2c8a2809_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c822018632b5547cc9072dfe2c8a2809_b.png"/></figure><p>这只兔子的特征数据在悲伤和痛苦的分界处，机器不论判断它属于哪个类别都很有可能是错的。这时，类似“它有一半可能性是痛苦，一半可能性是悲伤”的反馈会更有意义。</p><p>为了这个目的，我们同样找找出距离问题特征最近的 k 个样本，但与其寻找数量最多的分类，我们统计其中每个类别的分别有多少个，再除以 kk 得到一个属于每一个类别概率值。比如在上面的图里，距离五角星最近的 15 个样本中，有 8 只悲伤和 7 只痛苦，由此判断：它有 53% 的可能性是悲伤，47% 的可能性是痛苦，0%的可能性是绝望。</p><p>在整个二维空间中的每一个点上进行概率 kNN 算法，可以得到每个特征点是属于某个类别的概率热力图，图中颜色越深代表概率越大。</p><figure><noscript><img src="https://pic3.zhimg.com/v2-6f30943815fa89d72671af3c4608d3ea_b.png" data-rawwidth="1128" data-rawheight="768" class="origin_image zh-lightbox-thumb" width="1128" data-original="https://pic3.zhimg.com/v2-6f30943815fa89d72671af3c4608d3ea_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-6f30943815fa89d72671af3c4608d3ea_b.png" data-rawwidth="1128" data-rawheight="768" class="origin_image zh-lightbox-thumb lazy" width="1128" data-original="https://pic3.zhimg.com/v2-6f30943815fa89d72671af3c4608d3ea_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-6f30943815fa89d72671af3c4608d3ea_b.png"/></figure><figure><noscript><img src="https://pic3.zhimg.com/v2-240f4c56df69af4bbeeefc75f46e0a72_b.png" data-rawwidth="1112" data-rawheight="755" class="origin_image zh-lightbox-thumb" width="1112" data-original="https://pic3.zhimg.com/v2-240f4c56df69af4bbeeefc75f46e0a72_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-240f4c56df69af4bbeeefc75f46e0a72_b.png" data-rawwidth="1112" data-rawheight="755" class="origin_image zh-lightbox-thumb lazy" width="1112" data-original="https://pic3.zhimg.com/v2-240f4c56df69af4bbeeefc75f46e0a72_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-240f4c56df69af4bbeeefc75f46e0a72_b.png"/></figure><figure><noscript><img src="https://pic4.zhimg.com/v2-be973a36ec3484887162f646362432b7_b.png" data-rawwidth="1110" data-rawheight="762" class="origin_image zh-lightbox-thumb" width="1110" data-original="https://pic4.zhimg.com/v2-be973a36ec3484887162f646362432b7_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-be973a36ec3484887162f646362432b7_b.png" data-rawwidth="1110" data-rawheight="762" class="origin_image zh-lightbox-thumb lazy" width="1110" data-original="https://pic4.zhimg.com/v2-be973a36ec3484887162f646362432b7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-be973a36ec3484887162f646362432b7_b.png"/></figure><p>相比于绝对的分类，这些概率的计算会给我们更有效的表述以及更多的应用空间。比如说，我们知道悲伤兔子喜欢向我们的机器人上喷洒奇怪的粘液，毫无作用毫无意义的绿色的粘液，就像这样（作者手绘）：</p><figure><noscript><img src="https://pic3.zhimg.com/v2-51bd02e0ea50c7c92a39ae4e1076a9de_b.png" data-rawwidth="1032" data-rawheight="737" class="origin_image zh-lightbox-thumb" width="1032" data-original="https://pic3.zhimg.com/v2-51bd02e0ea50c7c92a39ae4e1076a9de_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-51bd02e0ea50c7c92a39ae4e1076a9de_b.png" data-rawwidth="1032" data-rawheight="737" class="origin_image zh-lightbox-thumb lazy" width="1032" data-original="https://pic3.zhimg.com/v2-51bd02e0ea50c7c92a39ae4e1076a9de_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-51bd02e0ea50c7c92a39ae4e1076a9de_b.png"/></figure><p>倒不是因为别的，我们就是觉得这种粘液很恶心，清洗起来也很麻烦，所以我们想让机器人在测量并发现是悲伤之后马上掉头逃跑。但是如果机器发现了一只体型接近痛苦的悲伤，并且普通的 kNN 算法发生误判，没有马上逃跑，那么最后就会被喷了。所以我们使用概率 kNN 的算法并且使用以下风控原则：只要发现兔子有 30% 以上的概率是悲伤，就马上逃跑。从此之后，机器人就再也没被喷过。</p><p>结语</p><p>不知你有没有发现，我跟你讲了这么多关于兔子的事，却丝毫没有提及如何用代码计算 kNN。这是因为 kNN 虽然思路简单，但实现起来有一个问题，那就是计算量很大；当数据量很多时，拿一组特征来和所有样本依次计算距离并选取最近的 k 个，是非常耗费时间的。所以，在量化课堂接下来的文章中，我们将讲解 kNN 的一个高效算法—kd树。</p><p>想看代码的到这里哦：<a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//www.joinquant.com/post/2227%3Ff%3Dzh%26m%3Dzl22874873" target="_blank" rel="nofollow noreferrer">一只兔子帮你理解 机器学习的kNN算法-JoinQuant量化交易平台</a></p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
