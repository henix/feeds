<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CVPR 2018 | 使用CNN生成图像先验，实现更广泛场景的盲图像去模糊</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1522157012&amp;src=3&amp;ver=1&amp;signature=0cvnXkkPLGuvkvGJnYTgmGJG4Vbdc8RdJI9F2W695UJajlELoYpdLc3HH6LC1TQ-I9lo*nbnzEN7Jn7XwXQIbAZhO98noLnQJ7RKwsexRNif7Xud5lpcFeQbNZfd5B5dOtBi5xZuwXspbf1tqoofN2b9cSqHGJDPNT0v-506uXc=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    CVPR 2018 | 使用CNN生成图像先验，实现更广泛场景的盲图像去模糊                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-18</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;"></span></p><section style="white-space: normal;max-width: 100%;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;text-align: center;color: rgb(62, 62, 62);max-width: 100%;min-height: 1em;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="font-size: 14px;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="color: rgb(62, 62, 62);font-size: 16px;max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="color: rgb(62, 62, 62);font-size: 16px;max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">参与：<strong style="color: rgb(62, 62, 62);font-family: 微软雅黑;font-size: 16px;text-align: center;white-space: normal;background-color: rgb(255, 255, 255);max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">Nurhachu Null、刘晓坤</span></strong></span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p><br></p><blockquote><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(136, 136, 136);">现有的最优方法在文本、人脸以及低光照图像上的盲图像去模糊效果并不佳，主要受限于图像先验的手工设计属性。本文研究者将图像先验表示为二值分类器，训练 CNN 来分类模糊和清晰图像。实验表明，该图像先验比目前最先进的人工设计先验更具区分性，可实现更广泛场景的盲图像去模糊。</span></p></blockquote><p style="text-align: center;margin-bottom: 20px;"><strong>简介</strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">盲图像去模糊（blind image deblurring）是图像处理和计算机视觉领域中的一个经典问题，它的目标是将模糊输入中隐藏的图像进行恢复。当模糊形状满足空间不变性的时候，模糊过程可以用以下的方式进行建模：</span></p><p style="margin-bottom: 20px;"><img class="" data-ratio="0.15571776155717762" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnvvE7IibBa0ythjsSWUkawiadfTMyfjCOhKq62cUkPveYbdwAXlO8Y6Yw/640?wx_fmt=png" data-type="png" data-w="411" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">其中⊗代表的是卷积算子，B、I、k 和 n 分别代表模糊图像、隐藏的清晰图像、模糊核以及噪声。式（1）中的问题是不适定性，因为 I 和 k 都是未知的，存在无穷多个解。为了解决这个问题，关于模糊核和图像的额外约束和先验知识都是必需的。</span></p><p><img class="" data-ratio="0.9463171036204744" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnIib2XwwP0HErGanHI8ckbWAw1tltiaj1hF0PeaicOwQpDVsCsSmBRr7nw/640?wx_fmt=png" data-type="png" data-w="801" style=""></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 1： 一个去模糊的例子。本文提出了一个判别图像先验，它是从用于图像去模糊的深度二分类网络中学习得到的。</em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">最近的去模糊方法的成功主要来自于有效图像先验和边缘检测策略方面的研究进展。然而，基于边缘预测的方法常常会涉及到启发式的边缘选择步骤，当边缘不可预测的时候，这种方法表现不佳。为了避免启发式的边缘选择步骤，人们提出了很多基于自然图像先验的算法，包括稀疏性归一化（normalized sparsity）[16]、L0 梯度 [38] 和暗通道先验（dark channel prior）[27]。这些算法在一般的自然图像上表现良好，但是并不适用于特殊的场景，例如文本 [26]、人脸 [25] 以及低光照图像 [11]。大多数上述的图像先验都有相似的效果，它们更加适用于清晰的图像，而不是模糊的图像，这种属性有助于基于 MAP（最大后验）的盲图像去模糊方法的成功。然而，大多数先验都是手工设计的，它们主要是基于对特定图像统计的有限观察。这些算法不能很好地泛化以处理自然环境中的多种场景。所以，开发能够使用 MAP 框架来处理不同场景的图像先验是很有意义的。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">为达到这个目的，研究者将图像先验表示为能够区分清晰图像和模糊图像的二值分类器。具体来说，他们训练深度卷积神经网络来分类模糊图像 (标记为 1 ) 和清晰图像 (标记为 0 )。由于基于 MAP（最大后验）的去模糊方法通常使用 coarse-to-fine（由粗到精）策略，因此在 MAP 框架中插入具有全连接层的 CNN 无法处理不同大小的输入图像。为了解决这个问题，他们在 CNN 中采用了全局平均池化层 [ 21 ]，以允许学习的分类器处理不同大小的输入。此外，为了使分类器对不同输入图像尺寸具有更强的鲁棒性，他们还采用多尺度训练策略。然后将学习到的 CNN 分类器作为 MAP（最大后验）框架中潜在图像对应的正则项。如图 1 所示，本文提出的图像先验比目前最先进的人工设计的先验 [ 27 ] 更具区分性。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">然而，使用学习到的图像先验去优化这个去模糊方法是很困难的，因为这里涉及到了一个非线性 CNN。因此，本文提出了一种基于半二次方分裂法（half-quadratic splitting method）和梯度下降算法的高效数值算法。这个算法在实际使用中可以快速地收敛，并且可以应用在不同的场景中。此外，它还可以直接应用在非均匀去模糊任务中。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">本文的主要贡献如下：</span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;margin-bottom: 5px;"><span style="font-size: 14px;">提出了一种高效判别图像先验，它可以通过深度卷积神经网络学习到，用于盲图像去模糊。为了保证这个先验（也就是分类器）能够处理具有不同大小的输入图像，研究者利用全局平均池化和多尺度训练策略来训练这个卷积神经网络。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;margin-bottom: 5px;"><span style="font-size: 14px;">将学习到的分类器作为 MAP（最大后验）框架中潜在图像对应的正则化项，并且提出了一种能够求解去模糊模型的高效优化算法。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;margin-bottom: 5px;"><span style="font-size: 14px;">研究者证明，与当前最佳算法相比，这个算法在广泛使用的自然图像去模糊基准测试和特定领域的去模糊任务中都具备有竞争力的性能。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">研究者展示了这个方法可以直接泛化到非均匀去模糊任务中。</span></p></li></ul><p style="text-align: center;margin-bottom: 20px;"><strong>二分类网络</strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">我们的目标是通过卷积神经网络来训练一个二分类器。这个网络以图像作为输入，并输出一个标量数值，这个数值代表的是输入图像是模糊图像的概率。因为我们的目标是将这个网络作为一种先验嵌入到由粗到精的 MAP（最大后验）框架中，所以这个网络应该具备处理不同大小输入图像的能力。所以，我们将分类其中常用的全连接层用全局平均池化层代替 [21]。全局平均池化层在 sigmoid 层之前将不同大小的特征图转换成一个固定的大小。此外，全局平均池化层中没有额外的参数，这样就消除了过拟合问题。图 2 展示了整个网络架构和二分类网络的细节参数。</span></p><p><img class="" data-ratio="0.29071911493546404" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnVCfiasa2dTglibUJ6LsPhTP4Xb2XhE2P2dBtvvN5gHGIXmxZvBEJfYtA/640?wx_fmt=png" data-type="png" data-w="1627" style=""></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 2. 本文中使用的二分类网络的架构和参数，其中使用了全局平均池化层取代全连接层来应对不同大小的输入。CR 代表的是后面跟着一个 ReLU 非线性函数的卷积层，M 代表的是最大池化层，C 代表的是卷积层，G 指的是全局平均池化层，S 代表的是 Sigmoid 非线性函数。</span></em></span></p><p><img class="" data-ratio="0.2246376811594203" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnfVBMBPnpQ3JdPD4R9LspicpkGYcnDlPPIEfurjgruRar2BXUugdxs6g/640?wx_fmt=png" data-type="png" data-w="1656" style=""></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 4. 数据集 [15] 中的一个很具挑战性的例子。本文提出的方法以更少的边缘振荡效应和更好的视觉愉悦度恢复了模糊图像。</span></em></span></p><p><img class="" data-ratio="0.7661188369152971" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGn9MG7TS4B1s7ZRIGG7nGEzManlXYiaN1Eqs2wJE5E4picCmmAgkMqbfAg/640?wx_fmt=png" data-type="png" data-w="791" style=""></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 5. 在实际的模糊图像中的去模糊结果。本文的结果更加清晰，失真较少。</span></em></span></p><p><img class="" data-ratio="0.3079019073569482" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnztZQS9jReIMSMQ1JNgmbiaN9kA4JhVqEjWWd5w0LV4icpBG7dV3322JA/640?wx_fmt=png" data-type="png" data-w="734" style=""></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 6. 文本图像上的去模糊结果。与目前最先进的去模糊算法 [26] 相比，本文的方法生成了更加尖锐的去模糊图像，其中的字符更加清晰。</span></em></span></p><p><img class="" data-ratio="1.457142857142857" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGncFkInPgFicqbvpQdBCMLmLh4oA9icSj3U84Dy5U2TJZYhcrcr39SaZbg/640?wx_fmt=png" data-type="png" data-w="560" style=""></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 12. 去模糊结果和中间结果。作者在图 (a)-(d) 中与目前最先进的方法 [40, 27] 比较了去模糊结果，并在 (e)-(h) 中展示了迭代中的（从左至右）中间隐藏图像。本文的判别先验恢复了用于核估计的具有更强边缘的中间结果。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">论文：Learning a Discriminative Prior for Blind Image Deblurring（学习用于盲图像去模糊的判别先验）</span></strong></p><p style="margin-bottom: 20px;"><img class="" data-ratio="0.2870201096892139" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGn1CicaWVMkZvicuhpxH5PdKQJqpKWMSaRKcffhg6t0c6CLnfvomNDP0NQ/640?wx_fmt=png" data-type="png" data-w="1094" style=""></p><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1803.03363</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">我们提出了一种基于数据驱动的判别先验的盲图像去模糊方法。我们的工作是基于这样一个事实:一个好的图像先验应该有利于清晰的图像而不是模糊的图像。在本文中，我们将图像先验表示为一个二值分类器，它可以通过一个深度卷积神经网络 ( CNN ) 来实现。学习到的先验能够区分输入图像是否清晰。嵌入到最大后验 ( MAP ) 框架中之后，它有助于在各种场景 (包括自然图像、人脸图像、文本图像和低照明图像) 中进行盲去模糊。然而，由于去模糊方法涉及非线性 CNN，因此很难优化具有学习已图像先验的去模糊方法。为此，本文提出了一种基于半二次分裂法和梯度下降法的数值求解方法。此外，该模型易于推广到非均匀去模糊任务中。定性和定量的实验结果表明，与当前最优的图像去模糊算法以及特定领域的图像去模糊方法相比，该方法具备有竞争力的性能。<img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOd6kYzKSNx7GpKhf1OMhgW30B8WEsyibXYuvBogNHE5TQTpUQGLsWmeQ/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);text-align: justify;white-space: normal;font-size: 16px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 51px !important;" width="51px"></span></p><p><br></p><p style="margin-bottom: 20px;white-space: normal;line-height: 1.75em;"><strong style="text-align: justify;max-width: 100%;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="1274311416" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
