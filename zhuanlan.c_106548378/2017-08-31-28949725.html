<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>lasso/gbm/randomForest/deep learn商品高频量化对比</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/28949725">原文</a></p>
<p>----------------------------更新---------------------</p><p>原来有笔误，看看程序就知道，应该是3层，每层11个节点，我对节点数进行了优化，11是最好的，更多和更少效果都更差。</p><p><br></p><p>-----------------------------------------</p><p>之前我说用deep learning研究金融交易不大靠谱，貌似有很多研究人工智能的人不大服气。今天我特地实验了一下深度学习，为了对比也使用其他简单的模型和复杂的模型，然后看看这个深度学习的魔力到底如何。</p><p>比如一开始先简单准备一下数据，这是螺纹钢某段历史的数据，150天吧，40天训练，10天验证，100天测试，够意思了吧。</p><code lang="text">train.range &lt;- 1:40
valid.range &lt;- 41:50
test.range &lt;- 51:150
train.mat &lt;-
prepare.tick.data(signal.list, y.str, product, all.dates[train.range])
valid.mat &lt;-
prepare.tick.data(signal.list, y.str, product, all.dates[valid.range])
test.mat &lt;-
prepare.tick.data(signal.list, y.str, product, all.dates[test.range])</code><p>然后看看数据量</p><code lang="text">&gt; dim(train.mat)
[1] 1609232 12
&gt; dim(valid.mat)
[1] 407031 12
&gt; dim(test.mat)
[1] 4017980 12</code><p>还是挺大的，比如训练集160万，验证集40万，测试集400万，因子11个，还有一个因变量。</p><p>然后再看看它的表现，先用lasso，对lambda进行regularization，</p><code lang="text">&gt; n.coef &lt;-100
&gt; grid=10^seq(-4,-7,length=n.coef)
&gt; x &lt;- as.matrix(train.mat[1:(ncol(train.mat)-1)])
&gt; x.valid &lt;- as.matrix(valid.mat[1:(ncol(valid.mat)-1)])
&gt; x.test &lt;- as.matrix(test.mat[1:(ncol(test.mat)-1)])
&gt; fit.lasso &lt;- glmnet(x,train.mat$y,intercept=FALSE, lambda = grid,alpha=1)
&gt; coef.mat &lt;- coef(fit.lasso)[-1,]
&gt; if (dim(coef.mat)[2]&lt;100) n.coef &lt;- dim(coef.mat)[2]
&gt; oos.mat &lt;- rep(0,n.coef)
&gt; for (i in 1:n.coef) {
+ cur.coef &lt;- coef.mat[,i]
+ pred &lt;- x.valid%*%cur.coef
+ oos.mat[i] &lt;- R2(pred,valid.mat$y)
+ }
&gt; best &lt;- which.max(oos.mat)
&gt; best
[1] 62
</code><p>大概100组参数优化一下，然后找一个最佳的，貌似是第62个，上图：</p><p><br></p><img src="https://pic3.zhimg.com/v2-e8c4afe360908257d904c6234d5b3e76_r.png" data-rawwidth="411" data-rawheight="386"><p>应该还是欠拟合的，反正实验一下不管了，我最主要的是看看测试机的表现。这里纵坐标是验证集的拟合优度R平方。</p><p>&gt; pred.test &lt;- x.test %*%<br>coef.mat[,best]</p><p>&gt; R2(pred.test, test.mat$y)</p><p><b>[1] 0.0108247</b></p><p>大家记得这个结果，样本外R^2是1.08%，以后作为一个参考的基准。</p><p>然后我们看看一些复杂的模型，首先当仁不让的显然是gradient boosting machine (gbm)，华丽登场。。。</p><code lang="text">&gt; set.seed(100)
&gt; n.tree &lt;- 300
&gt; depth &lt;- 10
&gt; n.tree &lt;- 600
&gt; depth &lt;- 10
&gt; system.time(gbm.model &lt;- gbm(y~.,
data=train.mat, shrinkage = 0.01, 
+ interaction.depth = depth, distribution="gaussian",n.trees=n.tree,
verbose=FALSE))
用户 系统 流逝 
5899.52 1.08 5928.26 </code><p>运行了整整5928.25秒！写一篇日志容易么我。。。看看我做科研多认真。。。嘻嘻</p><code lang="text">&gt; best &lt;- which.max(gbm.r2)
&gt; best
[1] 282</code><p>省略一部分代码，我们主要优化的是树的数目，看多少棵树合适，我一共600棵，其中它说最好是282，哈哈，不错，上图：</p><p><br></p><img src="https://pic1.zhimg.com/v2-930a750db31a20c762c3686074c8da77_r.png" data-rawwidth="411" data-rawheight="386"><code lang="text">&gt; gbm.test.pred &lt;- 
predict.gbm(gbm.model,newdata=test.mat[,1:(ncol(valid.mat)-1)],n.trees = best)
&gt; R2(gbm.test.pred, test.mat$y)
[1] 0.01765722
</code><p>然后看看样本外，同样的数据这次是<b>0.01765722</b>，比lasso好多了，5900多秒不是吃干饭的。。。</p><p>然后看看随机森林，说实话这个我不大熟，貌似好慢。。。那就试试h2o这个包吧，还是挺高达上的，很多斯坦福统计系教授是它的顾问，包括Elements of Statistical Learning的几个作者。。。</p><code lang="text">&gt; library(h2o)
&gt; h2o.init(
+ nthreads=-1,
+ max_mem_size = "8G")
Connection successful!

R is connected to the H2O cluster: 
H2O cluster uptime: 4
hours 4 minutes 
H2O cluster version: 3.10.5.3 
H2O cluster version age: 1
month and 30 days 
H2O cluster name: H2O_started_from_R_pc_igo044 
H2O cluster total nodes: 1 
H2O cluster total memory: 6.42
GB 
H2O cluster total cores: 20 
H2O cluster allowed cores: 20 
H2O cluster healthy: TRUE 
H2O Connection ip: localhost 
H2O Connection port: 54321 
H2O Connection proxy: NA 
H2O Internal Security: FALSE 
R
Version: R version 3.3.3
(2017-03-06) 

&gt; h2o.removeAll() # Clean slate - just in
case the cluster was already running
[1] 0
</code><p>我的电脑配置一般啦，人穷志短，莫笑。。。。还要对数据进行处理</p><code lang="text">&gt; train.frame &lt;- as.h2o(train.mat,
destination_frame = "train.frame")
|===========================================================================================================================================|
100%
&gt; valid.frame &lt;- as.h2o(valid.mat,
destination_frame = "valid.frame")
|===========================================================================================================================================|
100%
&gt; test.frame &lt;- as.h2o(test.mat,
destination_frame = "test.frame")
|===========================================================================================================================================|
100%
&gt; dim(train.frame)
[1] 1609232 12</code><p>然后就可以建模了。。。其实对随机森林真心不大熟，就对深度进行优化吧，省略一些过程：</p><code lang="text">&gt; best &lt;- which.max(rf.r2)*2
&gt; best
[1] 10
可见最优的模型是深度为10的情况，我们看看样本外的结果：
&gt; h2o.rf.model &lt;-
h2o.randomForest(x=1:11,
+  y=12,
+ training_frame = train.frame,
+ seed=100,
+ ntrees=500,
+ max_depth =
best)
|===========================================================================================================================================|
100%
&gt; h2o.test.pred &lt;-
as.vector(h2o.predict(h2o.rf.model, newdata=test.frame))
|===========================================================================================================================================|
100%
&gt; R2(h2o.test.pred, test.mat$y)
[1] 0.0133576</code><p>最后10层最优，跟gbm貌似一样，gbm我随便取的10层哈。。。结果是<b>0.0133576</b>，比lasso好，比gbm差，这是样本外的。。。。</p><p><b>好了，最后关键时刻到了，deep learning粉末登场！！！</b></p><p>其实这个deep learning参数有点多，我先放代码再解释：</p><code lang="text">n.level &lt;- 13
dl.r2 &lt;- rep(0,n.level)
for (i in 2:n.level) {
h2o.dl.model &lt;- h2o.deeplearning(x=1:11,
y=12,
training_frame = train.frame,
 seed=100,
input_dropout_ratio = 0.2,
activation="RectifierWithDropout",
hidden =
rep(i,3),
hidden_dropout_ratios
= rep(0.3,3),
distribution
= "gaussian")
h2o.valid.pred &lt;- as.vector(h2o.predict(h2o.dl.model,
newdata=valid.frame))
dl.r2[i] &lt;- R2(h2o.valid.pred, valid.mat$y)
cat(i,dl.r2[i],"\n")
}</code><p>比如它有hidden和dropout，就是隐层结构和省略的比例，regularization防止过度拟合用的，我也充分参考了Andrew Ng，俺也是上了他三门deep learning的好吗。。。</p><p><br></p><img src="https://pic2.zhimg.com/v2-55b36ed8474c4bd8f3e4f678c47d25bf_r.png" data-rawwidth="1034" data-rawheight="744"><p>调参还行吧，反正一共3层，然后我对每层的节点数进行优化，2、3、4...13，都试一下，大概这样吧，dropout概率就0.3吧，参数太多了，这可如何是好啊。。。</p><code lang="text">&gt; best &lt;- which.max(dl.r2)
&gt; best
[1] 11
模型说11层最好，还是够deep啊，然后看看结果：
&gt; h2o.test.pred &lt;-
as.vector(h2o.predict(h2o.dl.model, newdata=test.frame))
|==========================================================================================================================================|
100%
&gt; R2(h2o.test.pred, test.mat$y)
[1] 0.01183622</code><p>算的这么辛苦最后才<b>0.01183622</b>！</p><p><b>lasso 是0.010, deep learning是0.0118，random forest是0.013，gbm是0.01765....</b></p><p>貌似看不出deep learning有什么出彩的地方哈。。。</p><p>当然有同胞弄出了xgboost，要说支持不支持，我肯定是支持的，可惜效果不大好，就不显摆了。。。</p><p>总之谁再跟我说deep learning在金融有用我就不知说什么好了。。。我觉得还是学学我的知乎live，找一份工作吧：</p><p><a href="https://zhihu.com/lives/876135312345137153">国内量化交易工作的求职指南</a></p><p>已经结束了，评价5颗星：</p><p><br></p><img src="https://pic2.zhimg.com/v2-c1f6921be07bccda93b0a4eeedbb185d_r.png" data-rawwidth="802" data-rawheight="291"><p><br></p><p>就这样吧。。。</p><p></p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
