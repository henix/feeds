<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Bayesian Neural Networks：贝叶斯神经网络</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/81170602">原文</a></p>
<div class="title-image"><img src="https://pic2.zhimg.com/v2-848a90741883acb348dd7b1baa3e16ab_b.jpg" alt=""></div><blockquote><b>FBI WARNING</b>：本文讨论的是<i>贝叶斯<b>神经</b>网络，</i>而非<i>贝叶斯网络</i>。<br/><b>FBI WARNING</b>：鉴于近期知乎上一些睿智发言，本文将所有术语翻译成了中文，请谨慎食用。</blockquote><p>贝叶斯神经网络，简单来说可以理解为通过为神经网络的权重引入不确定性进行正则化（regularization），也相当于集成（ensemble）某权重分布上的无穷多组神经网络进行预测。</p><p>本文主要基于 Charles at al. 2015<sup data-text="Weight Uncertainty in Neural Networks" data-url="https://arxiv.org/abs/1505.05424" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>。</p><p><a href="https://link.zhihu.com/?target=https%3A//medium.com/neuralspace/bayesian-convolutional-neural-networks-with-bayes-by-backprop-c84dcaaf086e" class=" wrap external" target="_blank" rel="nofollow noreferrer">题图来源</a></p><h2>0. 神经网络的概率模型</h2><p>众所周知，一个神经网络模型可以视为一个条件分布模型 <img src="https://www.zhihu.com/equation?tex=P%28%5Cmathbf%7By%7D%7C%5Cmathbf%7Bx%7D%2C%5Cmathbf%7Bw%7D%29" alt="P(\mathbf{y}|\mathbf{x},\mathbf{w})" eeimg="1"/> ：输入 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D" alt="\mathbf{x}" eeimg="1"/> ，输出预测值 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D" alt="\mathbf{y}" eeimg="1"/> 的分布， <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 为神经网络中的权重。在分类问题中这个分布对应各类的概率，在回归问题中一般认为是（标准差固定的）高斯（Gaussian）分布并取均值作为预测结果。相应地，神经网络的学习可以视作是一个最大似然估计（Maximum Likelihood Estimation, MLE）：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cmathbf%7Bw%7D%5E%5Cmathrm%7BMLE%7D%26%3D%5Carg%5Cmax_%5Cmathbf%7Bw%7D%5Clog+P%28%5Cmathcal%7BD%7D%7C%5Cmathbf%7Bw%7D%29%5C%5C+%26%3D%5Carg%5Cmax_%5Cmathbf%7Bw%7D%5Csum_i%5Clog+P%28%5Cmathbf%7By%7D_i%7C%5Cmathbf%7Bx%7D_i%2C%5Cmathbf%7Bw%7D%29+%5Cend%7Balign%7D" alt="\begin{align} \mathbf{w}^\mathrm{MLE}&amp;=\arg\max_\mathbf{w}\log P(\mathcal{D}|\mathbf{w})\\ &amp;=\arg\max_\mathbf{w}\sum_i\log P(\mathbf{y}_i|\mathbf{x}_i,\mathbf{w}) \end{align}" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D+" alt="\mathcal{D} " eeimg="1"/> 对应我们用来训练的数据集（dataset）。回归问题中我们代入高斯分布就可以得到平均平方误差（Mean Squared Error, MSE），分类问题则代入逻辑函数（logistic）可以推出交叉熵（cross-entropy）。求神经网络的极小值点一般使用梯度下降，基于反向传播（back-propagation， BP）实现。</p><p>MLE 中不对 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 的先验概率作假设，也就是认为 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 取什么值的机会都均等。如果为 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 引入先验，那就变成了最大后验估计（Maximum Posteriori, MAP）：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cmathbf%7Bw%7D%5E%5Cmathrm%7BMAP%7D%26%3D%5Carg%5Cmax_%5Cmathbf%7Bw%7D%5Clog+P%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29%5C%5C+%26%3D%5Carg%5Cmax_%5Cmathbf%7Bw%7D%5Clog+P%28%5Cmathcal%7BD%7D%7C%5Cmathbf%7Bw%7D%29+%2B+%5Clog+P%28%5Cmathbf%7Bw%7D%29+%5Cend%7Balign%7D" alt="\begin{align} \mathbf{w}^\mathrm{MAP}&amp;=\arg\max_\mathbf{w}\log P(\mathbf{w}|\mathcal{D})\\ &amp;=\arg\max_\mathbf{w}\log P(\mathcal{D}|\mathbf{w}) + \log P(\mathbf{w}) \end{align}" eeimg="1"/> </p><p>代入高斯分布可以推出 L2 正则化（倾向于取小值），代入拉普拉斯分布（Laplace）可以推出 L1 正则化（倾向于取 0 使权重稀疏）。</p><h2>1. 贝叶斯起来了！</h2><p>贝叶斯估计（bayesian estimation）同样引入先验假设，与 MAP 的区别是贝叶斯估计求出 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 的后验分布 <img src="https://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29" alt="P(\mathbf{w}|\mathcal{D})" eeimg="1"/> ，而不限于 argmax 值，这样我们就可以为神经网络的预测引入不确定性。由于我们求得的是分布，基于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D" alt="\mathbf{w}" eeimg="1"/> 由输入 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7Bx%7D%7D" alt="\hat{\mathbf{x}}" eeimg="1"/> 预测 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7By%7D%7D" alt="\hat{\mathbf{y}}" eeimg="1"/> 的概率模型就变成了：</p><p><img src="https://www.zhihu.com/equation?tex=P%28%5Chat%7B%5Cmathbf%7By%7D%7D%7C%5Chat%7B%5Cmathbf%7Bx%7D%7D%29%3D%5Cmathbb%7BE%7D_%7BP%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29%7D%5BP%28%5Chat%7B%5Cmathbf%7By%7D%7D%7C%5Chat%7B%5Cmathbf%7Bx%7D%7D%2C%5Cmathbf%7Bw%7D%29%5D" alt="P(\hat{\mathbf{y}}|\hat{\mathbf{x}})=\mathbb{E}_{P(\mathbf{w}|\mathcal{D})}[P(\hat{\mathbf{y}}|\hat{\mathbf{x}},\mathbf{w})]" eeimg="1"/> </p><p>这样我们每次预测 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7By%7D%7D" alt="\hat{\mathbf{y}}" eeimg="1"/> 都得求个期望，问题是这个期望我们并不可能真的算出来，因为这就相当于要计算在 <img src="https://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29" alt="P(\mathbf{w}|\mathcal{D})" eeimg="1"/> 上的所有可能的神经网络的预测值。</p><p>另一方面，求后验分布 <img src="https://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29" alt="P(\mathbf{w}|\mathcal{D})" eeimg="1"/> 也是件麻烦的事情。众所周知，根据贝叶斯理论，求 <img src="https://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29" alt="P(\mathbf{w}|\mathcal{D})" eeimg="1"/> 需要通过：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+P%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29%3D+%5Cfrac%7BP%28%5Cmathbf%7Bw%7D%2C%5Cmathcal%7BD%7D%29%7D%7BP%28%5Cmathcal%7BD%7D%29%7D+%3D%5Cfrac%7BP%28%5Cmathcal%7BD%7D%7C%5Cmathbf%7Bw%7D%29P%28%5Cmathbf%7Bw%7D%29%7D%7BP%28%5Cmathcal%7BD%7D%29%7D+%5Cend%7Balign%7D" alt="\begin{align} P(\mathbf{w}|\mathcal{D})= \frac{P(\mathbf{w},\mathcal{D})}{P(\mathcal{D})} =\frac{P(\mathcal{D}|\mathbf{w})P(\mathbf{w})}{P(\mathcal{D})} \end{align}" eeimg="1"/> </p><p> 这东西也是难解（intractable）的。</p><p>所以，为了在神经网络中引入贝叶斯估计，需要找到方法近似这些东西，并且最好能转化成为求解优化（optimization）问题的形式，这样比较符合我们炼丹师的追求。</p><h2>2. 变分估计</h2><p>利用变分（variational）的方法，我们可以使用一个由一组参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> 控制的分布 <img src="https://www.zhihu.com/equation?tex=q%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29" alt="q(\mathbf{w}|\theta)" eeimg="1"/> 去逼近真正的后验 <img src="https://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29" alt="P(\mathbf{w}|\mathcal{D})" eeimg="1"/> ，比如用高斯来近似的话 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> 就是 <img src="https://www.zhihu.com/equation?tex=%28%5Cmu%2C%5Csigma%29" alt="(\mu,\sigma)" eeimg="1"/>，这样就把求后验分布的问题转化成了求最好的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="\theta" eeimg="1"/> 这样的优化问题。这个过程可以通过最小化两个分布的 KL 散度（Kullback-Leibler divergence）实现：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Ctheta%5E%2A%26%3D%5Carg%5Cmin_%5Ctheta+D_%5Cmathrm%7BKL%7D%5Bq%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29%7C%7CP%28%5Cmathbf%7Bw%7D%7C%5Cmathcal%7BD%7D%29%5D%5C%5C+%26%3D%5Carg%5Cmin_%5Ctheta+%5Cint+q%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29%5Clog+%5Cfrac%7Bq%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29%7D%7BP%28%5Cmathbf%7Bw%7D%29P%28%5Cmathcal%7BD%7D%7C%5Cmathbf%7Bw%7D%29%7D+d%5Cmathbf%7Bw%7D%5C%5C+%26%3D%5Carg%5Cmin_%5Ctheta+D_%5Cmathrm%7BKL%7D%5Bq%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29%7C%7CP%28%5Cmathbf%7Bw%7D%29%5D+-+%5Cmathbb%7BE%7D_%7Bq%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29%7D%5B%5Clog+P%28%5Cmathcal%7BD%7D%7C%5Cmathbf%7Bw%7D%29%5D+%5Cend%7Balign%7D" alt="\begin{align} \theta^*&amp;=\arg\min_\theta D_\mathrm{KL}[q(\mathbf{w}|\theta)||P(\mathbf{w}|\mathcal{D})]\\ &amp;=\arg\min_\theta \int q(\mathbf{w}|\theta)\log \frac{q(\mathbf{w}|\theta)}{P(\mathbf{w})P(\mathcal{D}|\mathbf{w})} d\mathbf{w}\\ &amp;=\arg\min_\theta D_\mathrm{KL}[q(\mathbf{w}|\theta)||P(\mathbf{w})] - \mathbb{E}_{q(\mathbf{w}|\theta)}[\log P(\mathcal{D}|\mathbf{w})] \end{align}" eeimg="1"/> </p><p>这样看起来比前式好多了。写成目标函数（objective function）的形式就是：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%28%5Cmathcal%7BD%7D%7C%5Ctheta%29%3DD_%5Cmathrm%7BKL%7D%5Bq%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29%7C%7CP%28%5Cmathbf%7Bw%7D%29%5D-%5Cmathbb%7BE%7D_%7Bq%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29%7D%5B%5Clog+P%28%5Cmathcal%7BD%7D%7C%5Cmathbf%7Bw%7D%29%5D" alt="\mathcal{F}(\mathcal{D}|\theta)=D_\mathrm{KL}[q(\mathbf{w}|\theta)||P(\mathbf{w})]-\mathbb{E}_{q(\mathbf{w}|\theta)}[\log P(\mathcal{D}|\mathbf{w})]" eeimg="1"/> (1)</p><p>这个其实仍然没法算出来，但是至少长得更像能算出来的东西。第一项就是我们的变分后验与先验的 KL 散度；第二项的取值依赖了训练数据。<sup data-text="Weight Uncertainty in Neural Networks" data-url="https://arxiv.org/abs/1505.05424" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>把第一项叫作复杂性代价（complexity cost），描述的是权重和先验的契合程度；把第二项叫作似然代价（likelihood cost），描述对样本的拟合程度。优化这个目标函数可以看作是炼丹师们最熟悉的正则化，在两种代价中取平衡。</p><p>对于 <img src="https://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bw%7D%29" alt="P(\mathbf{w})" eeimg="1"/> 的形式，<sup data-text="Weight Uncertainty in Neural Networks" data-url="https://arxiv.org/abs/1505.05424" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>给出了一个混合尺度高斯先验（scale mixture gaussian prior）：</p><p><img src="https://www.zhihu.com/equation?tex=P%28%5Cmathbf%7Bw%7D%29%3D%5Cprod_%7Bj%7D+%5Cpi+%5Cmathcal%7BN%7D%5Cleft%28%5Cmathbf%7Bw%7D_%7Bj%7D+%7C+0%2C+%5Csigma_%7B1%7D%5E%7B2%7D%5Cright%29%2B%281-%5Cpi%29+%5Cmathcal%7BN%7D%5Cleft%28%5Cmathbf%7Bw%7D_%7Bj%7D+%7C+0%2C+%5Csigma_%7B2%7D%5E%7B2%7D%5Cright%29" alt="P(\mathbf{w})=\prod_{j} \pi \mathcal{N}\left(\mathbf{w}_{j} | 0, \sigma_{1}^{2}\right)+(1-\pi) \mathcal{N}\left(\mathbf{w}_{j} | 0, \sigma_{2}^{2}\right)" eeimg="1"/> </p><p>即每个权重其分布的先验都是两种相同均值、不同标准差的高斯分布的叠加。</p><p>下一步要做的就是继续对目标函数取近似，直到能求出来为止。</p><h2>3. 遇事不决，蒙特卡罗</h2><p>蒙特卡罗方法（Monte Carlo method）是刻在炼丹师 DNA 里的方法。(1) 中有一个期望不好求，可以使用这种喜闻乐见的办法弄出来。</p><p>众所周知，同样利用贝叶斯估计推导出来的变分自编码器（Variational Auto-Encoder, VAE）<sup data-text="Auto-Encoding Variational Bayes" data-url="https://arxiv.org/abs/1312.6114" data-draft-node="inline" data-draft-type="reference" data-numero="2">[2]</sup>引入了一个妙不可言的重参数化（reparameterize）操作：对于 <img src="https://www.zhihu.com/equation?tex=z%5Csim+%5Cmathcal%7BN%7D%28%5Cmu%2C%5Csigma%5E2%29" alt="z\sim \mathcal{N}(\mu,\sigma^2)" eeimg="1"/> ，直接从 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BN%7D%28%5Cmu%2C%5Csigma%5E2%29" alt="\mathcal{N}(\mu,\sigma^2)" eeimg="1"/> 采样（sample）会使得 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="\sigma" eeimg="1"/> 变得不可微；为了得到它们的梯度，将 <img src="https://www.zhihu.com/equation?tex=z" alt="z" eeimg="1"/> 重写为 <img src="https://www.zhihu.com/equation?tex=z%3D%5Csigma+%5Cepsilon%2B%5Cmu" alt="z=\sigma \epsilon+\mu" eeimg="1"/> ，其中 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon%5Csim+%5Cmathcal%7BN%7D%280%2C1%29" alt="\epsilon\sim \mathcal{N}(0,1)" eeimg="1"/> ，这样便可以先从标准高斯分布采样出随机量，然后可导地引入 <img src="https://www.zhihu.com/equation?tex=%5Cmu" alt="\mu" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="\sigma" eeimg="1"/> 。</p><p><sup data-text="Weight Uncertainty in Neural Networks" data-url="https://arxiv.org/abs/1505.05424" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>对此进行了推广，证明了对一个随机变量 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon" alt="\epsilon" eeimg="1"/> 和概率密度 <img src="https://www.zhihu.com/equation?tex=q%28%5Cepsilon%29" alt="q(\epsilon)" eeimg="1"/> ，只要能满足 <img src="https://www.zhihu.com/equation?tex=q%28%5Cepsilon%29d%5Cepsilon%3Dq%28%5Cmathbf%7Bw%7D%7C%5Ctheta%29d%5Cmathbf%7Bw%7D" alt="q(\epsilon)d\epsilon=q(\mathbf{w}|\theta)d\mathbf{w}" eeimg="1"/> ，则对于期望也可以使用类似操作得到可导的对期望偏导的无偏估计：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Cmathbb%7BE%7D_%7Bq%28%5Cmathbf%7Bw%7D+%7C+%5Ctheta%29%7D%5Bf%28%5Cmathbf%7Bw%7D%2C+%5Ctheta%29%5D%3D%5Cmathbb%7BE%7D_%7Bq%28%5Cepsilon%29%7D%5Cleft%5B%5Cfrac%7B%5Cpartial+f%28%5Cmathbf%7Bw%7D%2C+%5Ctheta%29%7D%7B%5Cpartial+%5Cmathbf%7Bw%7D%7D+%5Cfrac%7B%5Cpartial+%5Cmathbf%7Bw%7D%7D%7B%5Cpartial+%5Ctheta%7D%2B%5Cfrac%7B%5Cpartial+f%28%5Cmathbf%7Bw%7D%2C+%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta%7D%5Cright%5D" alt="\frac{\partial}{\partial \theta} \mathbb{E}_{q(\mathbf{w} | \theta)}[f(\mathbf{w}, \theta)]=\mathbb{E}_{q(\epsilon)}\left[\frac{\partial f(\mathbf{w}, \theta)}{\partial \mathbf{w}} \frac{\partial \mathbf{w}}{\partial \theta}+\frac{\partial f(\mathbf{w}, \theta)}{\partial \theta}\right]" eeimg="1"/> </p><p>利用这一点可以得到 (1) 的蒙特卡罗近似：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cmathcal%7BF%7D%28%5Cmathcal%7BD%7D%2C+%5Ctheta%29+%5Capprox+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Clog+q%5Cleft%28%5Cmathbf%7Bw%7D%5E%7B%28i%29%7D+%7C+%5Ctheta%5Cright%29-%5Clog+P%5Cleft%28%5Cmathbf%7Bw%7D%5E%7B%28i%29%7D%5Cright%29+-%5Clog+P%5Cleft%28%5Cmathcal%7BD%7D+%7C+%5Cmathbf%7Bw%7D%5E%7B%28i%29%7D%5Cright%29+%5Cend%7Balign%7D" alt="\begin{align} \mathcal{F}(\mathcal{D}, \theta) \approx \sum_{i=1}^{n} \log q\left(\mathbf{w}^{(i)} | \theta\right)-\log P\left(\mathbf{w}^{(i)}\right) -\log P\left(\mathcal{D} | \mathbf{w}^{(i)}\right) \end{align}" eeimg="1"/> (2)</p><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bw%7D%5E%7B%28i%29%7D" alt="\mathbf{w}^{(i)}" eeimg="1"/> 是处理第 <img src="https://www.zhihu.com/equation?tex=i" alt="i" eeimg="1"/> 个数据点时的权重采样。</p><p><sup data-text="Weight Uncertainty in Neural Networks" data-url="https://arxiv.org/abs/1505.05424" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>中提出的这个近似把 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%28%5Cmathcal%7BD%7D%7C%5Ctheta%29" alt="\mathcal{F}(\mathcal{D}|\theta)" eeimg="1"/> 的 KL 项也给蒙特卡罗了，而其实对于很多先验形式这个 KL 项是可以有解析解的。<sup data-text="Weight Uncertainty in Neural Networks" data-url="https://arxiv.org/abs/1505.05424" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>这么做的理由是为了配适更复杂的先验/后验形式。另一篇文章<sup data-text="Variational Dropout and the Local Reparameterization Trick" data-url="https://arxiv.org/abs/1506.02557" data-draft-node="inline" data-draft-type="reference" data-numero="3">[3]</sup>只考虑高斯先验，于是在同样的证据下界中取了 KL 项的解析解。实践中可以根据使用的先验不同来取不同的近似。</p><h2>4. 贝叶斯小批梯度下降</h2><p>(1) 中的目标函数及 (2) 中的近似都是模型在整个数据集上的下界。实践中的现代炼丹都是采用的小批梯度下降（mini-batch gradient descent），所以需要相应地缩放复杂性代价。假设整个数据集被分为 <img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1"/> 批，最简单的形式就是对每个小批作平均：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BF%7D_%7Bi%7D%5E%7B%5Cmathrm%7BEQ%7D%7D%5Cleft%28%5Cmathcal%7BD%7D_%7Bi%7D%2C+%5Ctheta%5Cright%29%3D%5Cfrac%7B1%7D%7BM%7D+%5Cmathrm%7BKL%7D%5Bq%28%5Cmathbf%7Bw%7D+%7C+%5Ctheta%29+%5C%7C+P%28%5Cmathbf%7Bw%7D%29%5D+-%5Cmathbb%7BE%7D_%7Bq%28%5Cmathbf%7Bw%7D+%7C+%5Ctheta%29%7D%5Cleft%5B%5Clog+P%5Cleft%28%5Cmathcal%7BD%7D_%7Bi%7D+%7C+%5Cmathbf%7Bw%7D%5Cright%29%5Cright%5D+%5Cend%7Baligned%7D" alt="\begin{aligned} \mathcal{F}_{i}^{\mathrm{EQ}}\left(\mathcal{D}_{i}, \theta\right)=\frac{1}{M} \mathrm{KL}[q(\mathbf{w} | \theta) \| P(\mathbf{w})] -\mathbb{E}_{q(\mathbf{w} | \theta)}\left[\log P\left(\mathcal{D}_{i} | \mathbf{w}\right)\right] \end{aligned}" eeimg="1"/> </p><p>这样可以使得 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%7D+%5Cmathcal%7BF%7D_%7Bi%7D%5E%7B%5Cmathrm%7BEQ%7D%7D%5Cleft%28%5Cmathcal%7BD%7D_%7Bi%7D%2C+%5Ctheta%5Cright%29%3D%5Cmathcal%7BF%7D%28%5Cmathcal%7BD%7D%2C+%5Ctheta%29" alt="\sum_{i} \mathcal{F}_{i}^{\mathrm{EQ}}\left(\mathcal{D}_{i}, \theta\right)=\mathcal{F}(\mathcal{D}, \theta)" eeimg="1"/> 成立。在此基础上<sup data-text="Weight Uncertainty in Neural Networks" data-url="https://arxiv.org/abs/1505.05424" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>还提出了另一种缩放：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BF%7D_%7Bi%7D%5E%7B%5Cpi%7D%5Cleft%28%5Cmathcal%7BD%7D_%7Bi%7D%2C+%5Ctheta%5Cright%29%3D%5Cpi_%7Bi%7D+%5Cmathrm%7BKL%7D%5Bq%28%5Cmathbf%7Bw%7D+%7C+%5Ctheta%29%5C%7C+P%28%5Cmathbf%7Bw%7D%29%5D+-%5Cmathbb%7BE%7D_%7Bq%28%5Cmathbf%7Bw%7D+%7C+%5Ctheta%29%7D+%26%5Cleft%5B%5Clog+P%5Cleft%28%5Cmathcal%7BD%7D_%7Bi%7D+%7C+%5Cmathbf%7Bw%7D%5Cright%29%5Cright%5D+%5Cend%7Baligned%7D" alt="\begin{aligned} \mathcal{F}_{i}^{\pi}\left(\mathcal{D}_{i}, \theta\right)=\pi_{i} \mathrm{KL}[q(\mathbf{w} | \theta)\| P(\mathbf{w})] -\mathbb{E}_{q(\mathbf{w} | \theta)} &amp;\left[\log P\left(\mathcal{D}_{i} | \mathbf{w}\right)\right] \end{aligned}" eeimg="1"/> </p><p>只要取 <img src="https://www.zhihu.com/equation?tex=%5Cpi+%5Cin%5B0%2C1%5D%5E%7BM%7D" alt="\pi \in[0,1]^{M}" eeimg="1"/> 并保证 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7BM%7D+%5Cpi_%7Bi%7D%3D1" alt="\sum_{i=1}^{M} \pi_{i}=1" eeimg="1"/> ，那么 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7BM%7D+%5Cmathcal%7BF%7D_%7Bi%7D%5E%7B%5Cpi%7D%5Cleft%28%5Cmathcal%7BD%7D_%7Bi%7D%2C+%5Ctheta%5Cright%29" alt="\sum_{i=1}^{M} \mathcal{F}_{i}^{\pi}\left(\mathcal{D}_{i}, \theta\right)" eeimg="1"/> 就是 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%28%5Cmathcal%7BD%7D%2C+%5Ctheta%29" alt="\mathcal{F}(\mathcal{D}, \theta)" eeimg="1"/> 的无偏估计。特别地，取 <img src="https://www.zhihu.com/equation?tex=%5Cpi_%7Bi%7D%3D%5Cfrac%7B2%5E%7BM-i%7D%7D%7B2%5E%7BM%7D-1%7D" alt="\pi_{i}=\frac{2^{M-i}}{2^{M}-1}" eeimg="1"/> ，可以使在一轮（epoch）训练的初期着重于契合先验、后期着重于拟合数据，带来（玄学的）性能提升。</p><h2>5. 局部重参数化</h2><p>至此为止的在神经网络权重中引入的不确定性可以看作是全局的（global）不确定性。在神经网络中引入全局不确定性意味着在推理计算（inference）过程中要对全局所有参数进行采样操作，这个代价其实要比想象中高昂——比如一个 <img src="https://www.zhihu.com/equation?tex=1000%5Ctimes1000" alt="1000\times1000" eeimg="1"/> 的全连接层（fully connected layer），对于 <img src="https://www.zhihu.com/equation?tex=M%5Ctimes1000" alt="M\times1000" eeimg="1"/> 的输入需要 <img src="https://www.zhihu.com/equation?tex=M%5Ctimes1000%5Ctimes1000" alt="M\times1000\times1000" eeimg="1"/> 个不同的采样，并且更致命的是，一般的神经网络中这样的全连接层，由于参数是同一个矩阵，可以转换为一个 <img src="https://www.zhihu.com/equation?tex=%28M%2C1000%29" alt="(M,1000)" eeimg="1"/> 矩阵和 <img src="https://www.zhihu.com/equation?tex=%281000%2C1000%29" alt="(1000,1000)" eeimg="1"/> 矩阵之间的乘法；而引入了不确定性后，需要采样 <img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1"/> 组不同的 <img src="https://www.zhihu.com/equation?tex=%281000%2C1000%29" alt="(1000,1000)" eeimg="1"/> 参数，进行 <img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1"/> 次 <img src="https://www.zhihu.com/equation?tex=%281%2C1000%29" alt="(1,1000)" eeimg="1"/> 与 <img src="https://www.zhihu.com/equation?tex=%281000%2C1000%29" alt="(1000,1000)" eeimg="1"/> 的矩阵乘法，对于一般的矩阵并行库而言这是两件完全不同的事情。</p><p>针对这个问题，<sup data-text="Variational Dropout and the Local Reparameterization Trick" data-url="https://arxiv.org/abs/1506.02557" data-draft-node="inline" data-draft-type="reference" data-numero="3">[3]</sup>观察到，如果所有参数都是独立高斯分布，那么进行矩阵乘法后的结果也都会是独立高斯分布。也就是说，对于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BY%7D%3D%5Cmathbf%7BX%7D%5Cmathbf%7BW%7D" alt="\mathbf{Y}=\mathbf{X}\mathbf{W}" eeimg="1"/> ，若有</p><p><img src="https://www.zhihu.com/equation?tex=q%5Cleft%28w_%7Bi%2C+j%7D%5Cright%29%3DN%5Cleft%28%5Cmu_%7Bi%2C+j%7D%2C+%5Csigma_%7Bi%2C+j%7D%5E%7B2%7D%5Cright%29+%5Cforall+w_%7Bi%2C+j%7D+%5Cin+%5Cmathbf%7BW%7D" alt="q\left(w_{i, j}\right)=N\left(\mu_{i, j}, \sigma_{i, j}^{2}\right) \forall w_{i, j} \in \mathbf{W}" eeimg="1"/> </p><p>那么对于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BY%7D" alt="\mathbf{Y}" eeimg="1"/> 就会有</p><p><img src="https://www.zhihu.com/equation?tex=q%5Cleft%28y_%7Bm%2C+j%7D+%7C+%5Cmathbf%7BX%7D%5Cright%29%3DN%5Cleft%28%5Cgamma_%7Bm%2C+j%7D%2C+%5Cdelta_%7Bm%2C+j%7D%5Cright%29" alt="q\left(y_{m, j} | \mathbf{X}\right)=N\left(\gamma_{m, j}, \delta_{m, j}\right)" eeimg="1"/> </p><p>其中 <img src="https://www.zhihu.com/equation?tex=%5Cgamma_%7Bm%2C+j%7D%3D%5Csum_%7Bi%7D+a_%7Bm%2C+i%7D+%5Cmu_%7Bi%2C+j%7D" alt="\gamma_{m, j}=\sum_{i} a_{m, i} \mu_{i, j}" eeimg="1"/> 且 <img src="https://www.zhihu.com/equation?tex=%5Cdelta_%7Bm%2C+j%7D%3D%5Csum_%7Bi%7D+a_%7Bm%2C+i%7D%5E%7B2%7D+%5Csigma_%7Bi%2C+j%7D%5E%7B2%7D" alt="\delta_{m, j}=\sum_{i} a_{m, i}^{2} \sigma_{i, j}^{2}" eeimg="1"/> 。</p><p>有了这个结论，我们就没有必要每次都采样参数 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BW%7D" alt="\mathbf{W}" eeimg="1"/> 了，可以直接计算出结果 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BY%7D" alt="\mathbf{Y}" eeimg="1"/> 的均值和方差进行采样，然后反向传播到 <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BW%7D" alt="\mathbf{W}" eeimg="1"/> 上。这样每次计算进行的采样都是相应数据点的局部（local）采样，<sup data-text="Variational Dropout and the Local Reparameterization Trick" data-url="https://arxiv.org/abs/1506.02557" data-draft-node="inline" data-draft-type="reference" data-numero="3">[3]</sup>将这个技巧称为局部重参数化（local reparameterization）。</p><h2>效果</h2><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-ffd3917b7b8dc2c55c42fe130697eecd_b.jpg" data-size="normal" data-rawwidth="926" data-rawheight="498" class="origin_image zh-lightbox-thumb" width="926" data-original="https://pic2.zhimg.com/v2-ffd3917b7b8dc2c55c42fe130697eecd_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-ffd3917b7b8dc2c55c42fe130697eecd_b.jpg" data-size="normal" data-rawwidth="926" data-rawheight="498" class="origin_image zh-lightbox-thumb lazy" width="926" data-original="https://pic2.zhimg.com/v2-ffd3917b7b8dc2c55c42fe130697eecd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-ffd3917b7b8dc2c55c42fe130697eecd_b.jpg"/><figcaption>[1]中的噪声数据回归结果。左图为贝叶斯神经网络，右图为普通神经网络。红色标记表示预测值中位数；蓝色紫色区域为四分位区域。</figcaption></figure><p><sup data-text="Weight Uncertainty in Neural Networks" data-url="https://arxiv.org/abs/1505.05424" data-draft-node="inline" data-draft-type="reference" data-numero="1">[1]</sup>中给出了一个玩具级噪声数据回归任务的示意图。图中可以看出贝叶斯神经网络的不确定性特点：在数据较多的区域可以做到准确拟合，而在数据稀疏的区域会保留较大的不确定性。</p><p><a href="https://link.zhihu.com/?target=https%3A//github.com/JavierAntoran/Bayesian-Neural-Networks" class=" wrap external" target="_blank" rel="nofollow noreferrer">Gayhub</a> 上有一个 PyTorch 实现多种贝叶斯神经网络的仓库，如果对其实现有兴趣可以参考。</p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
