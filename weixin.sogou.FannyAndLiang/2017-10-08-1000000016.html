<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>AI学习笔记：[3]梯度下降和反向传播</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1553851092&amp;src=3&amp;ver=1&amp;signature=z6gi9LfFJD5JWquMmV6Owh3Vb2MxzbujYIJzlVLtAJbqFPhoTydoRSp8cRFLWkuxhdt4kP8USY5*uGEjLR5zfqGI9*kFftWOPpxU9U3KotKXBzn3l4T2YwCmGbzPhtdoRilGKBzQfHe-HI0XlHWFPyd2Fa*nsmJQd31Kr8dFuu4=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    AI学习笔记：[3]梯度下降和反向传播                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                        <span id="copyright_logo" class="rich_media_meta meta_original_tag">原创</span>
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2017-10-08</em>

                                        <em class="rich_media_meta rich_media_meta_text">Liang</em>
                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">ImFanny</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">ImFanny</span>

                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">ImFanny</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value"></span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">Hello, I'm Fanny. Nice to meet you.</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <h2 style="box-sizing: border-box;"><strong style="font-size: 24px;"></strong></h2><blockquote style="box-sizing: border-box;padding: 20px;margin-bottom: 25px;font-size: 16px;border-left-width: 6px;border-left-color: rgb(180, 180, 180);background-color: rgb(247, 247, 247);line-height: 30px;color: rgb(47, 47, 47);"><p style="box-sizing: border-box;line-height: 1.7;word-break: break-word !important;">上一篇回顾：<a href="http://mp.weixin.qq.com/s?__biz=MzUxMjAxMzU4Mw==&amp;mid=2247483707&amp;idx=1&amp;sn=7871bdeb3123506250cc774f26d55d34&amp;chksm=f96ba55bce1c2c4db2c17d3e23fd4a08252d9b149dfa0d95432d46343475d573e3a3336c09d3&amp;scene=21#wechat_redirect" target="_blank">《AI学习笔记：[2]神经网络向量化和成本函数》</a></p><p style="box-sizing: border-box;line-height: 1.7;word-break: break-word !important;"><br></p><p style="box-sizing: border-box;line-height: 1.7;word-break: break-word !important;"><span style="font-size: 14px;">如需获取超链接内容请前往http://www.jianshu.com/p/541d67c6dded 或者点击文末的“阅读原文”。</span></p></blockquote><p><strong style="font-size: 24px;"><br></strong></p><h2 style="box-sizing: border-box;"><strong style="font-size: 24px;">回忆一下：要求解的问题</strong><br></h2><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">上一节学到成本函数 J(w,b) ，我们的目标是：<span style="box-sizing: border-box;font-weight: 700;">求解 w 和 b 使得全局的成本J(w,b) 最小</span>。<br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">回顾一下成本函数 J(w,b)：</p><p><img class="" data-ratio="0.10952380952380952" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVH9ZiaXiaHtsblfHCJRribDOWXaq9GbjdkOQ8dWKc8JToB7LPYo7goteHsA/0?wx_fmt=png" data-type="png" data-w="210" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><img class="" data-ratio="0.05665024630541872" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVH4yicNWgnSgYlA7svW1DR6zqSAmFvqDiaNBGAJ3hrFFkHKc1S8MIVLTXw/0?wx_fmt=png" data-type="png" data-w="406" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">再回顾一下神经网络的递推公式：</p><p><img class="" data-ratio="0.11764705882352941" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVH3D3m0dA3UmCZqbHmbiaTakmgeaFqw0Df9QZBHE9djgVj4UYVaTDbnSQ/0?wx_fmt=png" data-type="png" data-w="153" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><img class="" data-ratio="0.1941747572815534" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHI4NkerhDK2bg69y9B8YnP9ZGZoDIGSW8zA5zK3cC2gGbzibvoA2VDkg/0?wx_fmt=png" data-type="png" data-w="103" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">这里:</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度为 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l-1]</span>)</p></li><li><p>b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度为 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>, 1)</p></li></ol><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">因此，对于 L 层神经网络，我们要求出 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[0]</span> * n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span> + n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span> * n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[2]</span> + ... + n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[L-1]</span> * n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[L]</span>) 个 w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l](i)</span>以及 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>+...+n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[L]</span>) 个 b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l](i)</span>，这里的 w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 和 b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l](i)</span> 都是实数。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">以如下这个2层的神经网络为例：</p><p><img class="" data-ratio="0.5831932773109244" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHYR3SHmmY9lA71A0RtmtFIrVgP5L4vAqbFgyGc6oBMzWKicEJwJNsBeA/0?wx_fmt=png" data-type="png" data-w="595" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="text-align: center;"><span style="color: rgb(136, 136, 136);font-size: 14px;">图：2层神经网络</span></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">其中：</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>L=2，n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[0]</span>=2, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>=4, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[2]</span>=1</p></li><li><p>要求解的对象 w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span> 为 (2, 4) 的矩阵，w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[2]</span> 为 (4, 1) 的矩阵，共 8 个 w；</p></li><li><p>要求解的对象 b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span> 为 (4, 1) 的列向量，b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[2]</span> 为 (1, 1) 的列向量，共 5 个 b。</p></li></ol><h2 style="box-sizing: border-box;"><span style="font-size: 24px;"><strong><br></strong></span></h2><h2 style="box-sizing: border-box;"><span style="font-size: 24px;"><strong>梯度下降法</strong></span></h2><p><span style="font-size: 24px;"><strong><br></strong></span></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">梯度下降法是一种求最优解（但并不一定是真正的解，只是无限接近真正解）的算法。用负梯度方向为搜索方向，最速下降法越接近目标值，步长越小，前进越慢。详见 <span style="color: rgb(0, 128, 255);">维基百科-梯度下降法</span></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">举个例子可能更好理解。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">比如函数：f(x)=x<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">2</span>-2x，高中数学告诉我们，这个函数的顶点为 (1, -1)，其最小值为-1。</p><p style="text-align: center;"><img class="" data-ratio="0.8469387755102041" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHVicJgTVVQ4P33noqR9lrl0Y2RibIowuHLZ6nnTE4atSiaB3j8mibo5JicPA/0?wx_fmt=png" data-type="png" data-w="294" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="text-align: center;"><span style="color: rgb(136, 136, 136);font-size: 14px;">f(x)=x^2-2x</span></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">首先求解f <span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">'</span>(x)：</p><p><img class="" data-ratio="0.15527950310559005" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHVh10ibyjEDdgnAR0ncddYzNunYFESLXpjRFickzdK3ccpzxPiaaRQff3Q/0?wx_fmt=png" data-type="png" data-w="161" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">我们尝试用梯度下降法求解其最小值的方法 Python 代码为：</p><pre class="hljs python" style="box-sizing: border-box;overflow: auto;"><code class="python" style="box-sizing: border-box;"><span style="font-size: 12px;"><span class="hljs-function" style="font-size: 12px;box-sizing: border-box;">def f<span class="hljs-params" style="font-size: 12px;box-sizing: border-box;">(x)</span>:</span>
  <span class="hljs-keyword" style="font-size: 12px;box-sizing: border-box;color: rgb(133, 153, 0);">return</span> x**<span class="hljs-number" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">2</span> - <span class="hljs-number" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">2</span>*x<br><br><span class="hljs-function" style="font-size: 12px;box-sizing: border-box;">def df<span class="hljs-params" style="font-size: 12px;box-sizing: border-box;">(x)</span>:</span>
  <span class="hljs-keyword" style="font-size: 12px;box-sizing: border-box;color: rgb(133, 153, 0);">return</span> <span class="hljs-number" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">2</span>*x - <span class="hljs-number" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">2<br><br></span><span class="hljs-function" style="font-size: 12px;box-sizing: border-box;">def gd<span class="hljs-params" style="font-size: 12px;box-sizing: border-box;">(start_x, step_x, iter_cnt, df)</span>:</span>
    <span class="hljs-string" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">"""
    Args:
        start_x  起点
        step_x   迭代步长，也称为learing rate
        iter_cnt 迭代次数
        df       函数f(x)的导数
    """</span>
    x = start_x<br>    <span class="hljs-keyword" style="font-size: 12px;box-sizing: border-box;color: rgb(133, 153, 0);">for</span> i <span class="hljs-keyword" style="font-size: 12px;box-sizing: border-box;color: rgb(133, 153, 0);">in</span> range(iter_cnt):
        g = df(x)
        x -= g * step_x
        print(<span class="hljs-string" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">'[%2d] x=%f df(x)=%f f(%f)=%f'</span> %(i, x, g, x, f(x)))<br>        <span class="hljs-keyword" style="font-size: 12px;box-sizing: border-box;color: rgb(133, 153, 0);">if</span> abs(g) &lt; <span class="hljs-number" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">1e-6</span>:        <br>            <span class="hljs-keyword" style="font-size: 12px;box-sizing: border-box;color: rgb(133, 153, 0);">break</span>;<br>    <span class="hljs-keyword" style="font-size: 12px;box-sizing: border-box;color: rgb(133, 153, 0);">return</span> x<span class="hljs-comment" style="font-size: 12px;box-sizing: border-box;color: rgb(147, 161, 161);"># 从x=5起步，每次往前走0.05，迭代100次，前面定义的df当做导数函数传入<br><br></span>gd(<span class="hljs-number" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">5</span>, <span class="hljs-number" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">0.05</span>, <span class="hljs-number" style="font-size: 12px;box-sizing: border-box;color: rgb(42, 161, 152);">100</span>, df)</span></code></pre><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">输出结果为：</p><pre class="hljs undefined" style="box-sizing: border-box;overflow: auto;"><code style="box-sizing: border-box;"><span style="font-size: 12px;color: rgb(136, 136, 136);">[ 0] x=4.600000 df(x)=8.000000 f(4.600000)=11.960000
[ 1] x=4.240000 df(x)=7.200000 f(4.240000)=9.497600
[ 2] x=3.916000 df(x)=6.480000 f(3.916000)=7.503056
[ 3] x=3.624400 df(x)=5.832000 f(3.624400)=5.887475
[ 4] x=3.361960 df(x)=5.248800 f(3.361960)=4.578855
[ 5] x=3.125764 df(x)=4.723920 f(3.125764)=3.518873
[ 6] x=2.913188 df(x)=4.251528 f(2.913188)=2.660287
[ 7] x=2.721869 df(x)=3.826375 f(2.721869)=1.964832
...
[24] x=1.287159 df(x)=0.638132 f(1.287159)=-0.917540
...
[35] x=1.090114 df(x)=0.200252 f(1.090114)=-0.991880
...
[99] x=1.000106 df(x)=0.000236 f(1.000106)=-1.000000</span></code></pre><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">可以看到，开始的时候，梯度较大，因此下降速度很快，0到24步就逼近了1.9，而后面的35到99步才缓慢逼近到1.0，f(x) 找到的点(1.000106, -1.0) 非常接近于(1, -1)。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">在神经网络中，由于变量 w 和 b 都是多维度的，因此问题从“求一个自变量函数的导数”转变成“多变量函数的偏导数”。如下图所示：</p><p><img class="" data-ratio="0.8987179487179487" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVH9FzpmeEOuWMvicELBwrF3VBmicVtYR88IdTtRh3ddax8XTWwicgp6OTIw/0?wx_fmt=png" data-type="png" data-w="780" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;width: 100%;height: auto;"><br style="box-sizing: border-box;"></p><p style="text-align: center;"><span style="color: rgb(136, 136, 136);font-size: 14px;">梯度下降法</span></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">用三维的图形表示就是，实际上神经网络远远比三维图形要复杂，用直观的方式无法表达。</p><p><img class="" data-ratio="0.495" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHBPGsJ7ia8NzdEn9ic4aFHZejq0vYTZvba2NoLbo25FKic174UNSb7oh6A/0?wx_fmt=png" data-type="png" data-w="600" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="text-align: center;"><span style="font-size: 14px;color: rgb(136, 136, 136);">梯度下降法示意图</span></p><h2 style="box-sizing: border-box;"><br></h2><h2 style="box-sizing: border-box;"><span style="font-size: 24px;"><strong><br></strong></span></h2><h2 style="box-sizing: border-box;"><span style="font-size: 24px;"><strong>数学回顾（1）：导数以及链式法则</strong></span></h2><p><span style="font-size: 24px;"><strong><br></strong></span></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">梯度下降法里面很重要的一步是求导数，然而并不是所有的函数都像x<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">2</span>-2x这么规范、这么容易求导的，这时候就要用到反向传播。但在学习反向传播之前，我们先重拾一下 <span style="box-sizing: border-box;font-weight: 700;">导数</span> 以及在神经网络中非常重要的 <span style="box-sizing: border-box;font-weight: 700;">求导链式法则</span>。</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p><span style="color: rgb(0, 128, 255);">浅谈导数的意义</span></p></li><li><p><span style="color: rgb(0, 128, 255);">如何理解导数的概念 ?</span></p></li><li><p><span style="color: rgb(0, 128, 255);">微分和导数的关系是什么？两者的几何意义有什么不同？为什么要定义微分 ?</span></p></li></ol><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">导数的计算公式和四则运算法则：</p><p><img class="" data-ratio="0.5802310654685494" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHgmPEPkaQeRaFgjh5hpeDBxaYibVgZz9IQZFGk0duYefVHicCEtZ486HA/0?wx_fmt=png" data-type="png" data-w="779" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">导数的链式法则：</p><p><img class="" data-ratio="0.4948717948717949" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHPWFAuDVSTq3KAHjSmib20V6hGBiaqqQ3D4LpqMMoCVBMTnrpBY63aficw/0?wx_fmt=png" data-type="png" data-w="780" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;width: 100%;height: auto;"><br style="box-sizing: border-box;"></p><p><br></p><h2 style="box-sizing: border-box;"><strong><span style="font-size: 24px;"><br></span></strong></h2><h2 style="box-sizing: border-box;"><strong><span style="font-size: 24px;"><br></span></strong></h2><h2 style="box-sizing: border-box;"><strong><span style="font-size: 24px;">数学回顾（2）：偏导数以及计算方法</span></strong></h2><p><strong><span style="font-size: 24px;"><br></span></strong></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">偏导数的定义详见：<span style="color: rgb(0, 128, 255);">偏导数</span></p><p><img class="" data-ratio="0.9538461538461539" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHUxXyYNA9usxn1ylztckCVibTctDdqQ7yIPgArVF9jJcgiaEVSwlsibFrA/0?wx_fmt=png" data-type="png" data-w="780" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p><strong><span style="font-size: 24px;background-color: rgb(255, 255, 255);color: rgb(47, 47, 47);"><br></span></strong></p><p><strong><span style="font-size: 24px;background-color: rgb(255, 255, 255);color: rgb(47, 47, 47);">正向传播以及反向传播<br></span></strong></p><p><strong><span style="font-size: 24px;background-color: rgb(255, 255, 255);color: rgb(47, 47, 47);"><br></span></strong></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">我们再来看一下整个神经网络的计算过程，先以 Logistic Regression 为例。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><em style="box-sizing: border-box;">始终牢记在心里：我们最终是为了求解 w 和 b，使得 J(w,b) 最小。</em>因此，我们要先求解 dw 和 db，然后通过梯度下降法，每个迭代中通过　w=w-dw，不断地往前找，一直找到　dw 接近０为止（梯度为０，为最小值）。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">再强调一下，在吴恩达的课程中，dw表示的是dL(a,y)/dw，可以认为只取了分母而忽略了分子。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">如下图所示，正向传播分成三个步骤：</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>计算z</p></li><li><p>计算a</p></li><li><p>计算L(a, y)</p></li></ol><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">相反，我们需要计算的 dw, db，使用链式法则可以计算出，如图所示：<br></p><p><img class="" data-ratio="0.7370967741935484" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVH4xGcJXqNxbicAxXicGvA7iaxbicJ3mWsTNiaRFanDSm9aMicyoRUETvItGrg/0?wx_fmt=png" data-type="png" data-w="1240" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="text-align: center;"><span style="color: rgb(136, 136, 136);font-size: 14px;">逻辑斯蒂回归反向传播</span></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">对图中的公式进行整理，如下：</p><p><img class="" data-ratio="0.11504424778761062" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHOk5B6DiaziamavAldtUdY4PMv3ice0SWq8aFb0ZJug2Y2tIftX9xIGOLg/0?wx_fmt=png" data-type="png" data-w="226" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br style="box-sizing: border-box;color: rgb(47, 47, 47);"></p><p><img class="" data-ratio="0.11981566820276497" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHZzf5RZWO9pycwByhibpSQQqZH8YXNC8ic29OicENFVVjRKy75ZiavicpJiag/0?wx_fmt=png" data-type="png" data-w="217" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">所以，我们最终需要计算如下四个算子：</p><p><img class="" data-ratio="0.17557251908396945" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHyZAPWicIdTV1PYRUCMvHJmvHH6lF2yuF51SorKGp3k2zSzesKMsvNnw/0?wx_fmt=png" data-type="png" data-w="262" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">da、dz、dw、db的计算步骤如下图所示，注意其中计算 dσ(z)/dz 的计算过程和前文中的一样。</p><p><img class="" data-ratio="0.9943548387096774" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHibQ3SMYXywa9alFTSkmkUTGcnNeaSvTUKCeCicj84d6uAQYBSiaojXwLA/0?wx_fmt=png" data-type="png" data-w="1240" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;width: 100%;height: auto;"><br style="box-sizing: border-box;"></p><p><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">最终得到：</p><p><img class="" data-ratio="0.2857142857142857" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVH4PnJPNj9GmhFyPRxpdKY8k0YoMmdH1XwqP1O56bU9tDShxtLAcM75w/0?wx_fmt=png" data-type="png" data-w="161" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><img class="" data-ratio="0.2062780269058296" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHK6bys5YAtpaVgGU3uoOV6hjqayJj0mKOKwxpHWympPkuxCcfRoQagQ/0?wx_fmt=png" data-type="png" data-w="223" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><img class="" data-ratio="0.1509433962264151" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHYr400VGT1moTmno5u93ebLzAdeYhXEZeK2tZ373xyCS1hKpPoakbpQ/0?wx_fmt=png" data-type="png" data-w="106" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><img class="" data-ratio="0.22535211267605634" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcuJjickwFXEJ6GHG2sH5YVHia2faLZYsxErBG8ibsxicQj3r3MM2yibdFqHIoictpk66o8x3ibrvZyvFH9A/0?wx_fmt=png" data-type="png" data-w="71" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">最终，我们得到了 dw 和 db 的计算公式，将其代入到梯度下降算法，当 dw 趋近于0 以及 db 趋近于 0 时对应的 w 和 b 就是我们要找的值。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">不知不觉中又写了不少了，这里只讲解到逻辑斯蒂回归的反向传播，多层神经网络网络的反向传播以及对应的伪代码就留到下次继续了。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">提到反向传播，最近有一个比较有意思的新闻是<span style="color: rgb(0, 128, 255);">《被Geoffrey Hinton抛弃，反向传播为何饱受质疑？（附BP推导）》</span>，至少学习完这个课程，你可以看懂这篇新闻了。的确，像Hinton所说的那样，人类大脑的原理并不是反向传播，其实吴恩达也在各种各样的场合反复撇清深度学习和人类大脑的关系，虽然人工智能的概念被炒作得非常火，但是我们还是应该意识到，深度学习只是一种更好的机器学习方法而已，至于人类大脑是怎么运作的，我们还知之甚少，更不用说去模拟或者仿造了。但是我们也应该感到自豪的是，人类已经开始往这个方向努力，就和文艺复兴一样，多年以后回首会认为这是一个非常了不起的时代。</p><h2 style="box-sizing: border-box;"><strong><span style="font-size: 24px;">下一步</span></strong></h2><p><strong><span style="font-size: 24px;"><br></span></strong></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">学到这里，一直是比较枯燥的数学推导，吴恩达非常讲究实战与理论结合，因此下一周首先将反向传播学习完，然后开始做课后练习了。我看GitHub上有人将课后<span style="color: rgb(0, 128, 255);">Programming Assignments &amp; Quiz Solutions</span>，也是非常贴心的。</p><p><br></p>
                </div>
                <script nonce="1979193919" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_reward_area" style="display:none;">
                                        <p>
                        <a class="reward_access" id="js_reward_link" href="##">赞赏</a>
                    </p>
                    <div id="js_reward_inner" class="reward_area_inner" style="display:none;">
                        <p class="tips_global reward_user_tips"><a href="##" id="js_reward_total"></a>人赞赏</p>
                        <div id="js_reward_list" class="reward_user_list"></div>
                    </div>
                </div>
                                <div class="reward_qrcode_area reward_area tc" id="js_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                                        <p class="reward_tips"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" id="js_reward_qrcode_img"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                                            </div><div class="rich_media_tool" id="js_toobar3">
                                            <a class="media_tool_meta meta_primary" id="js_view_source" href="##">阅读原文</a>
                                <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                                <a class="media_tool_meta meta_primary" href="http://www.jianshu.com/p/541d67c6dded" target="_blank">阅读原文</a>
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
