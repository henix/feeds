<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>高效灵活的概率建模方法基于Python</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/31919315">原文</a></p>
<div class="title-image"><img src="https://pic4.zhimg.com/v2-6c5e5f1d8b4c6c5c9af27794ce4cc63b_r.jpg" alt=""></div><p><b>前言</b></p><p>在今天给大家介绍一个研究工具：<b>pomegranate。</b>它比其他软件包更加灵活，更快，直观易用，并且可以在多线程中并行完成。</p><img src="https://pic2.zhimg.com/v2-cd3bd018f8f458216df7e20f8954154b_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="80"><img src="https://pic2.zhimg.com/v2-9c5ebf96cf78836ea5d6cc1d6fe79f01_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="350"><p><b>The API</b></p><p><b>主要模型介绍</b></p><ul><li>一般混合模型</li><li>隐马尔可夫模型</li><li>贝叶斯网络</li><li>贝叶斯分类器</li></ul><p><b>所有模型使用做多的方法</b></p><code lang="python">model.log_probability(X) / model.probability(X) 
model.sample() 
model.fit(X, weights, inertia) 
model.summarize(X, weights) 
model.from_summaries(inertia) 
model.predict(X) model.predict_proba(X) 
model.predict_log_proba(X) 
model.from_samples(X, weights)</code><p><b>支持很多分布函数</b></p><blockquote><b>单变量分布</b><br>    1. UniformDistribution<br>    2. BernoulliDistribution<br>    3. NormalDistribution<br>    4. LogNormalDistribution<br>    5. ExponentialDistribution<br>    6. BetaDistribution<br>    7. GammaDistribution<br>    8. DiscreteDistribution<br>    9. PoissonDistribution<br><b>内核密度</b><br>   1. GaussianKernelDensity<br>    2. UniformKernelDensity<br>    3. TriangleKernelDensity<br><b>多变量分布</b><br>   1. IndependentComponentsDistribution<br>    2. MultivariateGaussianDistribution<br>    3. DirichletDistribution<br>    4. ConditionalProbabilityTable<br>    5. JointProbabilityTable</blockquote><p><b>模型可以从已知值中创建</b></p><img src="https://pic1.zhimg.com/v2-39f3ebc01717f7e1e67709cb05f5e5d7_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="267"><p><b>模型也可以从数据直接学习</b></p><img src="https://pic1.zhimg.com/v2-be1f2e053d71280ac7969268ef9ba55f_r.jpg" data-caption="" data-size="normal" data-rawwidth="517" data-rawheight="289"><p><b>pomegranate 比 numpy 快</b></p><img src="https://pic3.zhimg.com/v2-03c150a1477dd66453d4e9519ae07190_r.jpg" data-caption="" data-size="normal" data-rawwidth="567" data-rawheight="309"><img src="https://pic2.zhimg.com/v2-37c430db5708161211b1bbd8256105f4_r.jpg" data-caption="" data-size="normal" data-rawwidth="557" data-rawheight="285"><p>只需要一次数据集（适用于所有模型）。以下是正态分布统计示例：</p><img src="https://pic3.zhimg.com/v2-5205e02f1c2eb8c0c30824c37c7d1ad8_r.jpg" data-caption="" data-size="normal" data-rawwidth="635" data-rawheight="212"><p><b>支持核心学习</b></p><p>由于使用了足够多的统计数据，因此可以支持外核/在线学习。 </p><img src="https://pic2.zhimg.com/v2-c97aae99db71d03cc83cf227082f3dd2_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="207"><p><b>pomegranate 比 scipy 快</b></p><img src="https://pic3.zhimg.com/v2-b4397126631ee84952651e0c84b3b6cb_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="310"><p><b>The API</b></p><p><b>主要模型介绍</b></p><ul><li><b>一般混合模型</b></li><li>隐马尔可夫模型</li><li>贝叶斯网络</li><li>贝叶斯分类器</li></ul><p><b>通用混合模型（GMM）可以对多组分布进行建模</b></p><img src="https://pic2.zhimg.com/v2-58a67a9def939da778b5ab5695e0306d_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="290"><p>GMM使用期望最大化（EM）来拟合</p><p>   1、使用kmeans ++或kmeans ||初始化集群</p><p>   2、对于等于后P（M | D）（E步）的所有点分配权重</p><p>   3、使用加权点更新分布（M步）</p><p>   4、重复2和3，直到收敛</p><img src="https://pic1.zhimg.com/v2-dc5e4f8a263baf64b7b76e2ae296c0ce_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="270"><code lang="python">model = GeneralMixtureModel.from_samples(NormalDistribution, 2, X)</code><p><b>GMM不限于高斯分布</b></p><img src="https://pic1.zhimg.com/v2-ace1f7c29ec091af71a6ffd4a094c1b4_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="329"><p><b>单个指数分布不能很好的数据进行建模</b></p><img src="https://pic4.zhimg.com/v2-815343bf7b55069e19f70927a6785120_r.jpg" data-caption="" data-size="normal" data-rawwidth="639" data-rawheight="364"><code lang="python">model = ExponentialDistribution.from_samples(X)</code><p><b>两个指数混合使数据更好的模拟</b></p><img src="https://pic1.zhimg.com/v2-2725d7413a894c7d54cd1012b264232c_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="320"><p><br></p><code lang="python">model = GeneralMixtureModel.from_samples(ExponentialDistribution, 2, X)</code><p><b>Heterogeneous mixtures are natively supported</b></p><code lang="python">model = GeneralMixtureModel.from_samples([ExponentialDistribution, UniformDistribution], 2, X)</code><p><b>一般混合模型比sklearn快</b></p><img src="https://pic3.zhimg.com/v2-57ded422f35c45e2990e266c9b27144a_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="228"><p><b>The API</b></p><p><b>主要模型介绍</b></p><ul><li>一般混合模型</li><li><b>隐马尔可夫模型</b></li><li>贝叶斯网络</li><li>贝叶斯分类器</li></ul><p><b>CG enrichment detection HMM</b> </p><p>GACTACGACTCGCGCTCGCGCGACGCGCTCGACATCATCGACACGACACTC </p><img src="https://pic4.zhimg.com/v2-4836e5b7b256ae2b0504562bf8ffea5e_r.jpg" data-caption="" data-size="normal" data-rawwidth="549" data-rawheight="318"><img src="https://pic3.zhimg.com/v2-4f253fa7236a8c56a1790ef853eaa3d8_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="286"><img src="https://pic1.zhimg.com/v2-980b868f79aaf0b4ab051583f2464d10_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="257"><img src="https://pic4.zhimg.com/v2-642b0ffd63be10fb6d03b43a00fe0044_r.jpg" data-caption="" data-size="normal" data-rawwidth="293" data-rawheight="55"><p><b>GMM-HMM</b></p><img src="https://pic2.zhimg.com/v2-4c640fd002eee7d8e3ef4e3842b9ed6e_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="268"><p><b>HMM比hmmlearn快</b></p><img src="https://pic4.zhimg.com/v2-385e33346d28525187025baab1be6c14_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="439"><p><b>The API</b></p><p><b>主要模型介绍</b></p><ul><li>一般混合模型</li><li>隐马尔可夫模型</li><li><b>贝叶斯网络</b></li><li><b>贝叶斯分类器</b></li></ul><code lang="python">P(M|D)= P(D|M)P(M) / P(D)
Posterior = Likelihood * Prior / Normalization</code><p><b>基于数据建立一个简单的分类器</b></p><img src="https://pic2.zhimg.com/v2-a6cfd99b6a9d6768be127b33c22de3dd_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="215"><p><b>似然函数本身忽略了类不平衡</b></p><img src="https://pic2.zhimg.com/v2-07e232d0ad150590ab880285658c3a1c_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="222"><p><b>先验概率可以模拟分类不平衡</b></p><img src="https://pic4.zhimg.com/v2-7bc8d8fbb6fa279cd1afd63b2f6979a2_r.jpg" data-caption="" data-size="normal" data-rawwidth="338" data-rawheight="446"><p><b>后验模型更真实地对原始数据进行建模</b></p><img src="https://pic1.zhimg.com/v2-136ef58c245331487ac59c24592df929_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="218"><p><b>后者的比例是一个很好的分类器</b></p><img src="https://pic2.zhimg.com/v2-f59fca547945f6d0ae329bf3323699c9_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="214"><code lang="python">model = NaiveBayes.from_samples(NormalDistribution, X, y)
posteriors = model.predict_proba(idxs)

P(M|D)= ∏P(D|M) P(M) / P(D)
Posterior = Likelihood * Prior / Normalization

Naive Bayes does not need to be homogenous</code><img src="https://pic1.zhimg.com/v2-87842c06f0baa3ce93d208343abe8e1c_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="247"><p><b>不同的功能属于不同的分布</b></p><img src="https://pic3.zhimg.com/v2-980be468709c1e5966efc51d1305c345_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="218"><img src="https://pic1.zhimg.com/v2-ab997d8a4a2b70ecfca10f164828783e_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="221"><p>Gaussian Naive Bayes: 0.798<br>sklearn Gaussian Naive Bayes: 0.798<br>Heterogeneous Naive Bayes: 0.844</p><p><b>与sklearn一样快</b></p><img src="https://pic3.zhimg.com/v2-05e84eabca7f4277dc58b93884df7336_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="287"><code lang="python">P(M|D)= P(D|M) P(M) / P(D)
Posterior = Likelihood * Prior / Normalization

mc_a = MarkovChain.from_samples(X[y == 0])
mc_b = MarkovChain.from_samples(X[y == 1])
model_b = BayesClassifier([mc_a, mc_b], weights=numpy.array([1-y.mean(), y.mean()]))
hmm_a = HiddenMarkovModel…
hmm_b = HiddenMarkovModel...
model_b = BayesClassifier([hmm_a, hmm_b], weights=numpy.array([1-y.mean(), y.mean()]))
bn_a = BayesianNetwork.from_samples(X[y == 0])
bn_b = BayesianNetwork.from_samples(X[y == 1])
model_b = BayesClassifier([bn_a, bn_b], weights=numpy.array([1-y.mean(), y.mean()]))</code><p><b>并行</b> </p><img src="https://pic1.zhimg.com/v2-caaafccd5482aaf1e922d13be380df82_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="290"><img src="https://pic2.zhimg.com/v2-2dbbc25eeb4f0b00a86f7c52a0322058_r.jpg" data-caption="" data-size="normal" data-rawwidth="640" data-rawheight="233"><p><br></p><img src="https://pic3.zhimg.com/v2-a283cc2b67804cefe5c2b7a6d86dfc21_r.jpg" data-caption="" data-size="normal" data-rawwidth="1141" data-rawheight="697"><p></p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
