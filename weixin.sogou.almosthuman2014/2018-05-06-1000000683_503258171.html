<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>入门 | 从零开始，了解元学习</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1526411416&amp;src=3&amp;ver=1&amp;signature=WA2zn6QYtPvoXuNa17yexM4FDdXrZbS6TyvlUW-Fq42d2wOqWkXLnPVQmMbzujUZNyg*EPOqYB2gQfFfLI*E4WvNfGqGMiueIsibbJF2VMlFMyhAU1HMGjdL5BpWOnY1T1-WW7rqjROZSQqS2KCa-w2lMPL2b7IqL1EkeNwsRbU=">原文</a></p>
<div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        <div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    入门 | 从零开始，了解元学习                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-05-06</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="font-size: 16px;white-space: normal;max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="max-width: 100%;font-size: 14px;">Medium</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 12px;"><span style="max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">Thomas Wolf</strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 12px;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：Tianci LIU、路</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;"><br></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">本文介绍了元学习，一个解决「学习如何学习」的问题。</span></p></blockquote><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">元学习是目前机器学习领域一个令人振奋的研究趋势，它解决的是学习如何学习的问题。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">传统的机器学习研究模式是：获取特定任务的大型数据集，然后用这个数据集从头开始训练模型。很明显，这和人类利用以往经验，仅仅通过少量样本就迅速完成学习的情况相差甚远。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">因为人类学习了「如何学习」。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在这篇文章中，我将从一个非常直观的元学习简介入手，从它最早的起源一直谈到如今的元学习研究现状。然后，我会从头开始，在 PyTorch 中实现一个元学习模型，同时会分享一些从该项目中学到的经验教训。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: center;"><strong>首先，什么是学习？</strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们先来简单了解一下，当我们训练一个用来实现猫狗图像分类的简单神经网络时，到底发生了什么。假设我们现在有一张猫的图像，以及对应的表示「这是一只猫」的标签。为简洁起见，我做了一个简单的动画来展示训练的过程。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.4582637729549249" src="https://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW94cJJdDiaIlvRZ5icSgSnSuUtCQlPP0mfOQ5mopRzoP1t6jGXOrzqkDROWNYicenbaNKWIxmmqmUrAA/640?wx_fmt=gif" data-type="gif" data-w="1198"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">神经网络训练过程的单步。该网络用来实现猫狗图像分类。</span></em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">反向传播是神经网络训练中很关键的一步。因为神经网络执行的计算和损失函数都是可微函数，因此我们能够求出网络中每一个参数所对应的梯度，进而减少神经网络当前给出的预测标签与真实/目标标签之间的差异（这个差异是用损失函数度量的）。在反向传播完成后，就可以使用优化器来计算模型的更新参数了。而这正是使神经网络的训练更像是一门「艺术」而不是科学的原因：因为有太多的优化器和优化设置（超参数）可供选择了。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们把该「单个训练步」放在一张图中展示，如下所示：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: center;"><img class="" data-copyright="0" data-ratio="0.47354497354497355" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW94cJJdDiaIlvRZ5icSgSnSuUO3KdwS989fbapaZQEZW4Xd4q6SyR7Y6JnZkFl3uJbibuo073YVMn28A/640?wx_fmt=png" data-type="png" data-w="378"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">现在，训练图像是一只🐈，表示图像是一只猫的标签是 🔺。最大的这些 △ 表示我们的神经网络，里面的 ■ 表示参数和梯度，标有 L 的四边形表示损失函数，标有 O 的四边形表示优化器。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">完整的学习过程就是不断地重复这个优化步，直到神经网络中的参数收敛到一个不错的结果上。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.3396414342629482" src="https://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW94cJJdDiaIlvRZ5icSgSnSuU2huX2wXhBLU9oURwDMj9MvBOMjCbQDky0grC70cJtvrH1RpKtkmTrA/640?wx_fmt=gif" data-type="gif" data-w="1004"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">上图表示神经网络的训练过程的三步，神经网络（用最大的 △ 表示）用于实现猫狗图像分类。</span></em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: center;"><strong>元学习</strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">元学习的思想是学习「学习（训练）」过程。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">元学习有好几种实现方法，不过本文谈到的两种「学习『学习』过程」的方法和上文介绍的方式很类似。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在我们的训练过程中，具体而言，可以学习到两点：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: center;"><img class="" data-copyright="0" data-ratio="0.6617647058823529" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW94cJJdDiaIlvRZ5icSgSnSuUjIbu6Zvb5msTxPZzobl9viaGZ3icSJKlbj2qvHFg5jH7W050l3RW3UTA/640?wx_fmt=png" data-type="png" data-w="272"></p><p style="white-space: normal;"><br></p><ul class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">神经网络的初始参数（图中的蓝色■）；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">优化器的参数（粉色的★）。</span></p></li></ul><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我会介绍将这两点结合的情况，不过这里的每一点本身也非常有趣，而且可获得到简化、加速以及一些不错的理论结果。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">现在，我们有两个部分需要训练：</span></p><p style="white-space: normal;"><br></p><ul class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">用「模型（M）」这个词来指代我们之前的神经网络，现在也可以将其理解为一个低级网络。有时，人们也会用「优化对象（optimizee）」或者「学习器（learner）」来称呼它。该模型的权重在图中用 ■ 表示。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">用「优化器（O）」或者「元学习器」来指代用于更新低级网络（即上述模型）权重的高级模型。优化器的权重在图中用 ★ 表示。</span></p></li></ul><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">如何学习这些元参数？</span></strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">事实上，我们可以将训练过程中的元损失的梯度反向传播到初始的模型权重和/或优化器的参数。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">现在，我们有了两个嵌套的训练过程：优化器/元学习器上的元训练过程，其中（元）前向传输包含模型的多个训练步：我们之前见过的前馈、反向传播以及优化步骤。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">现在我们来看看元训练的步骤：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.5934883720930233" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW94cJJdDiaIlvRZ5icSgSnSuUP41ufy5lBKpjRlHSwpDH1SYbpILRca3HlWR7yuzKyLxAspkj1YMJGg/640?wx_fmt=png" data-type="png" data-w="1075"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">元训练步（训练优化器 O）包含 3 个模型（M）的训练步。</span></em></span><br><span style="font-size: 15px;"></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在这里，元训练过程中的单个步骤是横向表示的。它包含模型训练过程中的两个步骤（在元前馈和元反向传播的方格中纵向表示），模型的训练过程和我们之前看到的训练过程完全一样。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">可以看到，元前向传输的输入是在模型训练过程中依次使用的一列样本/标签（或一列批次）。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: center;"><img class="" data-copyright="0" data-ratio="0.16083916083916083" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW94cJJdDiaIlvRZ5icSgSnSuUvGSibvCIVx79pcQ4ibhbIWib6xPIaBn9z8cyk3ee6VnI0aVyhHHCsjzXA/640?wx_fmt=png" data-type="png" data-w="286"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">元训练步中的输入是一列样本（🐈、🐕）及其对应的标签（🔺、🔻）。</span></em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们应该如何使用元损失来训练元学习器呢？在训练模型时，我们可以直接将模型的预测和目标标签做比较，得到误差值。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;"><br></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(136, 136, 136);">在训练元学习器时，我们可以用元损失来度量元学习器在目标任务——训练模型——上的表现。</span></p></blockquote><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">一个可行的方法是在一些训练数据上计算模型的损失：损失越低，模型就越好。最后，我们可以计算出元损失，或者直接将模型训练过程中已经计算得到的损失结合在一起（例如，把它们直接加起来）。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们还需要一个元优化器来更新优化器的权重，在这里，问题就变得很「meta」了：我们可以用另一个元学习器来优化当前的元学习器……不过最终，我们需要人为选择一个优化器，例如 SGD 或者 ADAM（不能像「turtles all the way down」一样（注：turtles all the way down 这里大概是说，「不能一个模型套一个模型，这样无限的套下去」）。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这里给出一些备注，它们对于我们现在要讨论的实现而言非常重要：</span></p><p style="white-space: normal;"><br></p><ul class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">二阶导数：将元损失通过模型的梯度进行反向传播时，需要计算导数的导数，也就是二阶导数（在最后一个动画中的元反向传播部分，这是用绿色的 ▲ 穿过绿色的 ■ 来表示的）。我们可以使用 TensorFlow 或 PyTorch 等现代框架来计算二阶导数，不过在实践中，我们通常不考虑二阶导数，而只是通过模型权重进行反向传播（元反向传播图中的黄色 ■），以降低复杂度。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">坐标共享：如今，深度学习模型中的参数数量非常多（在 NLP 任务中，很容易就有将近 3000 万 ～２亿个参数）。当前的 GPU 内存无法将这么多参数作为单独输入传输给优化器。我们经常采用的方法是「坐标共享」（coordinate sharing），这表示我们为一个参数设计一个优化器，然后将其复制到所有的参数上（具体而言，将它的权重沿着模型参数的输入维度进行共享）。在这个方法中，元学习器的参数数量和模型中的参数数量之间并没有函数关系。如果元学习器是一个记忆网络，如 RNN，我们依然可以令模型中的每个参数都具有单独的隐藏状态，以保留每个参数的单独变化情况。</span></p></li></ul><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: center;"><strong>在 PyTorch 中实现元学习</strong></p><p style="white-space: normal;"><span style="font-size: 15px;text-align: justify;"><br></span></p><p style="white-space: normal;"><span style="font-size: 15px;text-align: justify;">我们来尝试写些代码，看看真实情况如何吧。</span><br></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">现在我们有了一个模型，它包含一个我们想要进行训练的权重集合，我们将使用该集合解决这两项任务：</span></p><p style="white-space: normal;"><br></p><ul class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在元前馈步骤中：我们使用这个模型计算（损失函数的）梯度，并作为优化器的输入来更新模型参数；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在元反向传播步骤中：我们使用这个模型作为反向传播优化器参数梯度（从元损失中计算得到）的路径。</span></p></li></ul><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在 PyTorch 中完成这个任务最简单的方法是：使用两个一样的模块来表示模型，每个任务一个。我们把存储元前馈步骤中使用的模型梯度的模块称为前向模型（forward model），把元反向传播步骤中将参数存储为反向传播优化器梯度的连续路径的模块称为后向模型（backward model）。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">两个模块之间会使用共享的 Tensor，以防止重复占用内存（Tensor 是内存中真正有意义的部分）；但同时，也会保留各自的 Variable，以明确区分模型的梯度和元学习器的梯度。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">PyTorch 中的一个简单元学习器类</span></strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在 PyTorch 中共享张量非常直接：只需要更新 Variable 类中的指针，让它们指向相同的 Tensor 就可以了。但如果模型已经是内存优化模型，例如 AWD-LSTM 或 AWD-QRNN 这类共享 Tensors（输入和输出嵌入）的算法时，我们就会遇到问难。这时，我们在更新两个模块中的模型参数时，需要很小心，以确保我们保留的指针是正确的。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在这里给出一个实现方法：设置一个简单的辅助程序来完成遍历参数的任务，并返回更新 Parameter 指针（而不只是 Tensor）所需的全部信息，并保持共享参数同步。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">以下是一个实现函数：</span></p><p style="white-space: normal;"><br></p><pre style="margin-top: 0px;margin-bottom: 0px;padding: 0px;background-color: rgb(255, 255, 255);font-size: 16px;box-sizing: border-box;color: rgb(62, 62, 62);line-height: inherit;"><code class="python language-python hljs" style="margin-right: 2px;margin-left: 2px;padding: 0.5em;box-sizing: border-box;font-size: 14px;color: rgb(169, 183, 198);line-height: 18px;border-radius: 0px;background-color: rgb(40, 43, 46);font-family: Consolas, Inconsolata, Courier, monospace;display: block;overflow-x: auto;letter-spacing: 0px;word-wrap: normal !important;word-break: normal !important;overflow-y: auto !important;"><span class="hljs-function" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><span class="hljs-keyword" style="word-wrap: inherit !important;word-break: inherit !important;">def</span> <span class="hljs-title" style="line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">get_params(module, memo=None, pointers=None)</span>:</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-string" style="box-sizing: border-box;font-size: inherit;color: rgb(238, 220, 112);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">""" Returns an iterator over PyTorch module parameters that allows to update parameters<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        (and not only the data).<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    ! Side effect: update shared parameters to point to the first yield instance<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        (i.e. you can update shared parameters and keep them shared)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    Yields:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        (Module, string, Parameter): Tuple containing the parameter's module, name and pointer<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    """</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">if</span> memo <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">is</span> <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">None</span>:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        memo = set()<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        pointers = {}<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> name, p <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> module._parameters.items():<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">if</span> p <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">not</span> <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> memo:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            memo.add(p)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            pointers[p] = (module, name)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">yield</span> module, name, p<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">elif</span> p <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">is</span> <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">not</span> <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">None</span>:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            prev_module, prev_name = pointers[p]<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            module._parameters[name] = prev_module._parameters[prev_name] <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># update shared parameter pointer</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> child_module <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> module.children():<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> m, n, p <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> get_params(child_module, memo, pointers):<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">yield</span> m, n, p<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"></code></pre><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">通过这个函数，我们可以嵌入任何模型，并且很整洁地遍历元学习器的模型参数。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">现在，我们来写一个简单的元学习器类。我们的优化器是一个模块：在前馈阶段，它可以将前向模型（及其梯度）和后向模型作为输入接受，并遍历它们的参数来更新后向模型中的参数，同时允许元梯度反向传播（通过更新 Parameter 指针，而不仅仅是 Tensor 指针）。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><pre style="margin-top: 0px;margin-bottom: 0px;padding: 0px;background-color: rgb(255, 255, 255);font-size: 16px;box-sizing: border-box;color: rgb(62, 62, 62);line-height: inherit;"><code class="python language-python hljs" style="margin-right: 2px;margin-left: 2px;padding: 0.5em;box-sizing: border-box;font-size: 14px;color: rgb(169, 183, 198);line-height: 18px;border-radius: 0px;background-color: rgb(40, 43, 46);font-family: Consolas, Inconsolata, Courier, monospace;display: block;overflow-x: auto;letter-spacing: 0px;word-wrap: normal !important;word-break: normal !important;overflow-y: auto !important;"><span class="hljs-class" style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><span class="hljs-keyword" style="line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">class</span> <span class="hljs-title" style="line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">MetaLearner(nn.Module)</span>:</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-string" style="box-sizing: border-box;font-size: inherit;color: rgb(238, 220, 112);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">""" Bare Meta-learner class<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        Should be added: intialization, hidden states, more control over everything<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    """</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-function" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><span class="hljs-keyword" style="word-wrap: inherit !important;word-break: inherit !important;">def</span> <span class="hljs-title" style="line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">__init__(self, model)</span>:</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        super(MetaLearner, self).__init__()<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        self.weights = Parameter(torch.Tensor(<span class="hljs-number" style="box-sizing: border-box;font-size: inherit;color: rgb(174, 135, 250);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">1</span>, <span class="hljs-number" style="box-sizing: border-box;font-size: inherit;color: rgb(174, 135, 250);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">2</span>))<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-function" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><span class="hljs-keyword" style="word-wrap: inherit !important;word-break: inherit !important;">def</span> <span class="hljs-title" style="line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">forward(self, forward_model, backward_model)</span>:</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        <span class="hljs-string" style="box-sizing: border-box;font-size: inherit;color: rgb(238, 220, 112);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">""" Forward optimizer with a simple linear neural net<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        Inputs:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            forward_model: PyTorch module with parameters gradient populated<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            backward_model: PyTorch module identical to forward_model (but without gradients)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">              updated at the Parameter level to keep track of the computation graph for meta-backward pass<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        """</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        f_model_iter = get_params(forward_model)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        b_model_iter = get_params(backward_model)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">        <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> f_param_tuple, b_param_tuple <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> zip(f_model_iter, b_model_iter): <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># loop over parameters</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Prepare the inputs, we detach the inputs to avoid computing 2nd derivatives (re-pack in new Variable)</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            (module_f, name_f, param_f) = f_param_tuple<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            (module_b, name_b, param_b) = b_param_tuple<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            inputs = Variable(torch.stack([param_f.grad.data, param_f.data], dim=<span class="hljs-number" style="box-sizing: border-box;font-size: inherit;color: rgb(174, 135, 250);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">-1</span>))<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Optimization step: compute new model parameters, here we apply a simple linear function</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            dW = F.linear(inputs, self.weights).squeeze()<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            param_b = param_b + dW<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Update backward_model (meta-gradients can flow) and forward_model (no need for meta-gradients).</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            module_b._parameters[name_b] = param_b<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">            param_f.data = param_b.data<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"></code></pre><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这样一来，我们就可以像在第一部分中看到的那样来训练优化器了。以下是一个简单的要点示例，展示了前文描述的元训练过程：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><pre style="margin-top: 0px;margin-bottom: 0px;padding: 0px;background-color: rgb(255, 255, 255);font-size: 16px;box-sizing: border-box;color: rgb(62, 62, 62);line-height: inherit;"><code class="python language-python hljs" style="margin-right: 2px;margin-left: 2px;padding: 0.5em;box-sizing: border-box;font-size: 14px;color: rgb(169, 183, 198);line-height: 18px;border-radius: 0px;background-color: rgb(40, 43, 46);font-family: Consolas, Inconsolata, Courier, monospace;display: block;overflow-x: auto;letter-spacing: 0px;word-wrap: normal !important;word-break: normal !important;overflow-y: auto !important;"><span class="hljs-function" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><span class="hljs-keyword" style="word-wrap: inherit !important;word-break: inherit !important;">def</span> <span class="hljs-title" style="line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">train(forward_model, backward_model, optimizer, meta_optimizer, train_data, meta_epochs)</span>:</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">  <span class="hljs-string" style="box-sizing: border-box;font-size: inherit;color: rgb(238, 220, 112);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">""" Train a meta-learner<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">  Inputs:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    forward_model, backward_model: Two identical PyTorch modules (can have shared Tensors)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    optimizer: a neural net to be used as optimizer (an instance of the MetaLearner class)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    meta_optimizer: an optimizer for the optimizer neural net, e.g. ADAM<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    train_data: an iterator over an epoch of training data<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    meta_epochs: meta-training steps<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">  To be added: intialization, early stopping, checkpointing, more control over everything<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">  """</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">  <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> meta_epoch <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> range(meta_epochs): <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Meta-training loop (train the optimizer)</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    optimizer.zero_grad()<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    losses = []<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> inputs, labels <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> train_data:   <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Meta-forward pass (train the model)</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">      forward_model.zero_grad()         <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Forward pass</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">      inputs = Variable(inputs)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">      labels = Variable(labels)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">      output = forward_model(inputs)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">      loss = loss_func(output, labels)  <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Compute loss</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">      losses.append(loss)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">      loss.backward()                   <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Backward pass to add gradients to the forward_model</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">      optimizer(forward_model,          <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Optimizer step (update the models)</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">                backward_model)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    meta_loss = sum(losses)             <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Compute a simple meta-loss</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    meta_loss.backward()                <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Meta-backward pass</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    meta_optimizer.step()               <span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Meta-optimizer step</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"></code><p><span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><br></span></p></pre><p style="white-space: normal;"><strong><span style="font-size: 15px;text-align: justify;">避免内存爆炸——隐藏状态记忆</span></strong><br></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">有时，我们想要学习一个可在非常庞大的（可能有几千万个参数的）模型上运行的优化器；同时，我们还希望可以在大量步骤上实现元训练，以得到优质梯度；就像我们在论文《Meta-Learning a Dynamical Language Model》中所实现的那样。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在实践中，这意味着，我们想要在元前馈中包含一个很长的训练过程，以及很多时间步；同时我们还需要将每一步的参数（黄色■）和梯度（绿色■）保存在内存中，这些参数和梯度会在元反向传播中使用到。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们如何在不让 GPU 内存爆炸的情况下做到这一点呢？</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">一个办法是，使用梯度检查点（gradient checkpointing）来用内存换取计算，这个方法也叫「隐藏状态记忆」（Hidden State Memorization）。在我们的案例中，梯度检查点表示，将我们连续计算的元前馈和元反向传播切分成片段。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">来自 Open AI 的 Yaroslav Bulatov 有一篇很好的介绍梯度检查点的文章，如果你感兴趣，可以了解一下：</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;line-height: 1.75em;"><span style="font-size: 15px;">Fitting larger networks into memory（https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9）</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这篇文章非常长，所以我没有给出一个完整的梯度检查点代码示例，建议大家使用已经很完善的 TSHadley 的 PyTorch 实现，以及当前还在开发的梯度检查点的 PyTorch 本地实现。</span></p><p style="white-space: normal;"><span style="font-size: 15px;text-align: justify;"><br></span></p><p style="white-space: normal;"><strong><span style="font-size: 15px;text-align: justify;">元学习中的其他方法</span></strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">元学习中还有另外两个很有前景的研究方向，但本文没有时间来讨论了。在这里我给出一些提示，这样，当你知道了它们大致的原理后，就可以自己查阅相关资料了：</span></p><p style="white-space: normal;"><br></p><ul class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">循环神经网络：我们之前给出了神经网络的标准训练过程。还有一个方法：将连续的任务作为一个输入序列，然后建立一个循环模型，并用它提取、构建一个可用于新任务的序列表征。在这种方法中，对于某个带有记忆或注意力的循环神经网络，我们通常只使用一个训练过程。这个方法的效果也很不错，尤其是当你设计出适合任务的嵌入时。最近的这篇 SNAIL 论文是一个很好的例子：A Simple Neural Attentive Meta-Learner（https://openreview.net/forum?id=B1DmUzWAW）。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">强化学习：优化器在元前馈过程中完成的计算和循环神经网络的计算过程很类似：在输入序列（学习过程中模型的权重序列和梯度序列）上重复使用相同的参数。在真实场景下，这表示我们会遇到循环神经网络经常遇到的一个问题：一旦模型出错，就很难返回安全路径，因为我们并没有训练模型从训练误差中恢复的能力；同时，当遇到一个比元学习过程中使用的序列更长的序列时，模型难以泛化。为了解决这些问题，我们可以求助于强化学习方法，让模型学习一个和当前训练状态相关的动作策略。</span></p></li></ul><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: center;"><strong>自然语言处理中的元学习</strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">元学习和用于自然语言处理（NLP）的神经网络模型（如循环神经网络）之间有一个非常有趣的相似之处。在上一段中，我们曾提到：</span></p><p style="white-space: normal;"><br></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(136, 136, 136);">用于优化神经网络模型的元学习器的行为和循环神经网络类似。</span><br></p></blockquote><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">和 RNN 类似，元学习器会提取一系列模型训练过程中的参数和梯度作为输入序列，并根据这个输入序列计算得到一个输出序列（更新后的模型参数序列）。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们的论文《Meta-Learning a Dynamical Language Model》中详细论述了该相似性，并研究了将元学习器用于神经网络语言模型中，以实现中期记忆：经过学习，元学习器能够在标准 RNN（如 LSTM）的权重中，编码中期记忆（除了短期记忆在 LSTM 隐藏状态中的传统编码方式以外）。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.4139908256880734" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW94cJJdDiaIlvRZ5icSgSnSuUcMPUVrv37fYgMejqxBYicUqcicufFLXMEgDHHCjEen7ibpdbhUrHiby0tw/640?wx_fmt=png" data-type="png" data-w="872"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们的元学习语言模型由 3 层记忆层级组成，自下而上分别是：标准 LSTM、用于更新 LSTM 权重以存储中期记忆的元学习器，以及一个长期静态记忆。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们发现，元学习语言模型可以通过训练来编码最近输入的记忆，就像一篇维基百科文章的开始部分对预测文章的结尾部分非常有帮助一样。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.44166666666666665" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW94cJJdDiaIlvRZ5icSgSnSuUicqvbVHQamy8FbQ8RxZ3XADMTwW5AktfP994uFQ1iayTIeMqRCIT6Q7Q/640?wx_fmt=png" data-type="png" data-w="1200"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">上图中的曲线展示了在给定一篇维基百科文章开始部分的情况下（A, …, H 是连续的维基百科文章），模型预测文章词汇的效果。单词颜色表示的意思相同：蓝色表示更好，红色表示更差。当模型在阅读一篇文章时，它从文章的开始部分进行学习，读到结尾部分的时候，它的预测效果也变得更好了（更多细节，请阅读我们的论文）。</span></em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">以上是我对元学习的介绍，希望对大家有所帮助！<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);text-align: justify;white-space: normal;font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">参考文献</span></strong></p><p style="white-space: normal;"><br></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">1. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#afeb) As such, meta-learning can be seen as a generalization of「transfer learning」and is related to the techniques for fine-tuning model on a task as well as techniques for hyper-parameters optimization. There was an interesting workshop on meta-learning (https://nips.cc/Conferences/2017/Schedule?showEvent=8767) at NIPS 2017 last December.</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">2. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#dc5a) Of course in a real training we would be using a mini-batch of examples.</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">3. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#e0bb) More precisely:「most of」these operations are differentiable.</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">4. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#d640) Good blog posts introducing the relevant literature are the BAIR posts: Learning to learn (http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/) by Chelsea Finn and Learning to Optimize with Reinforcement Learning (http://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/) by Ke Li.</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">5. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#930c) Good examples of learning the model initial parameters are Model-Agnostic Meta-Learning (https://arxiv.org/abs/1703.03400) of UC Berkeley and its recent developments (https://openreview.net/forum?id=BJ_UL-k0b) as well as the Reptile algorithm (https://blog.openai.com/reptile/) of OpenAI. A good example of learning the optimizer』s parameters is the Learning to learn by gradient descent by gradient descent (https://arxiv.org/abs/1606.04474) paper of DeepMind. A paper combining the two is the work Optimization as a Model for Few-Shot Learning (https://openreview.net/forum?id=rJY0-Kcll) by Sachin Ravi and Hugo Larochelle. An nice and very recent overview can be found in Learning Unsupervised Learning Rules (https://arxiv.org/abs/1804.00222).</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">6. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#d094) Similarly to the way we back propagate through time in an unrolled recurrent network.</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">7. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#725d) Initially described in DeepMind』s Learning to learn by gradient descent by gradient descent (https://arxiv.org/abs/1606.04474) paper.</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">8. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#4e23) We are using coordinate-sharing in our meta-learner as mentioned earlier. In practice, it means we simply iterate over the model parameters and apply our optimizer broadcasted on each parameters (no need to flatten and gather parameters like in L-BFGS for instance).</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">9. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#d029) There is a surprising under-statement of how important back-propagating over very long sequence can be to get good results. The recent paper An Analysis of Neural Language Modeling at Multiple Scales (https://arxiv.org/abs/1803.08240) from Salesforce research is a good pointer in that direction.</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">10. ^ (https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a#6c6f) Gradient checkpointing is described for example in Memory-Efficient Backpropagation Through Time (https://arxiv.org/abs/1606.03401) and the nice blog post (https://medium.com/@yaroslavvb/fitting-larger-networks-into-memory-583e3c758ff9) of Yaroslav Bulatov.</span></em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">原文链接：https://medium.com/huggingface/from-zero-to-research-an-introduction-to-meta-learning-8e16e677f78a</span></em></span></p><p style="white-space: normal;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 15px;"></span></em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;"><br></p><p style="white-space: normal;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;"></span></em></span></p><p style="white-space: normal;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="912816608" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                        
            <ul id="js_hotspot_area" class="article_extend_area"></ul>


            
                        <div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>

        
        <div class="rich_media_area_extra">

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_friend_cmt_area" style="display:none">
              
              
              
            </div>

                        <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_area" style="display:none">
            </div>
                    </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
