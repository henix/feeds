<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>展望未来</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/37515535">原文</a></p>
<p>在我拿起那估计比火箭西决G7三分球命中率还铁的水晶球去预测未来编程语言的走向以前，我想先翻翻我那堆历史书，总结下三条规律。</p><p><br></p><p>0：摩尔定律。GPU，内存，硬盘，网络的指数增长在肉眼可见范围里面还远远没到头。CPU上的增长也还有一段时间 - CPU已经在摩尔定律要凉的论调下已经可持续崩溃式发展了10多年了。如果再假设我们会有基础性进展，这些东西只怕会续命更久。当然，有些常数我们越不过，比如硬盘网络的延迟，还有光速一样，但是没关系。</p><p><br></p><p>1：量变引起质变。就如同利威尔阿克曼跟十五米高的进击巨人行动方式有着天壤之别一样，如果一个参数改变了一个数量级，规则跟各种定律都得大改。跟摩尔定律结合起来，这对定律就能多次把不可能化为无聊透顶 - 你手上拿着的手机，在1970年就有原型，而你现在在看着的显示器，在几年前最顶尖的研究所才能量SRI之财力买下几个。时间快进50年，这已经成为人手一抓丢了都不怎么心疼的常见物了。例子远远不止这一个：互联网，个人计算机，甚至计算机，全部都是靠摩尔定律续起来的。软件也是如此 - 打败世界第零的国际象棋AI从‘横竖怎么看反正都不可能’，到‘哈？我手机在这，拿去’之间的差距，除了优化一下暴力搜索以外，就是‘我的硬件快得暴力就能解决问题了’。事实上，暴力搜索已经成为使用计算机的标准操作了 - Intel很早就采用了SAT solver来验证&amp;排电路，Z3出现了10年了，而深度学习需要的计算量一点也不虚这几位。。支撑起这些应用的计算力提升何止兵长变巨人，是成千上百万啊。把现在的计算机跟几十年前的视为同一种东西，比把还在爬的熊孩子跟喷气式飞机对比还离谱，毕竟后者只有100倍速度差。</p><p><br></p><p>2：编译器大法好。Fortran老爹之所以有资格拿图灵奖，是因为天下之间，只有Fortran做出‘跟手写效率在一个数量级的高级编程语言’。在此之前，什么是编程语言？学术界做出的又慢又没用的东西而已。如果我们再把时间往前拨一下，我们甚至可以看到海军准将在跟她上司据理力争‘老大你看，COBOL才不是什么理论上不可能实现的东西，听我说啊’。再启动一下bite the dust呢？我们可以看到老冯在怒斥下属‘你在搞什么，我很angry，怎么可以用汇编器，你浪费了多少CPU cycle你知道吗？’。事实上，在我们黑啥语言效率不如C以前，我们是在黑C效率不如Fortran，Fortran 效率不如汇编，汇编浪费CPU Cycle。这也是为啥我认为计算机科学的理论深度无可比拟 - 计算机实在太难以估摸了，纵使通天彻地如老冯，发明完一套集合论搞量子力学，再转去给计算机发明出一套沿用至今的架构，也无法猜出计算机的最基本用法。</p><p><br></p><p>回到正文。好，我们在开始以前，再看看一个东西：Javascript。我们看看这两个post：</p><a href="https://www.breck-mckye.com/blog/2018/05/why-is-front-end-development-so-unstable/" data-draft-node="block" data-draft-type="link-card">Why is Front-End Development So Unstable?</a><a href="https://news.ycombinator.com/item?id=17191872" data-draft-node="block" data-draft-type="link-card">Web frameworks are churn-y because they are incredibly leaky abstractions coveri...</a><p>基本上就两个问题。</p><ul><li>0：库太多，不知道用啥</li><li>1：Impedance Mismatch。举个例子，当你用LINQ的时候很多不好的事情事情可能发生：你传进LINQ的高阶函数可能无法编译成SQL（用了闭包，或者副作用等），可能无法表示想要的东西（SQL的NULL），可能语义对不上（SQL NULL上的操作不会err而是传递NULL），而最严重的问题是，你要懂SQL。</li></ul><p><br></p><p>如果我们把上面的三个定律融合一下，这两个问题将不复存在。</p><p><br></p><p><b>编译器将消除绝大部分的Impedance Mismatch。</b></p><p>我们来看看要写出高效，可以处理大量数据的程序，我们需要知道啥：</p><ul><li>分布式计算</li><li>数据库</li><li>并发编程</li><li>GPU</li></ul><p>而这个Stack里面有什么是必要的？</p><p>我们试试看描述每个东西的用途：</p><ul><li>分布式计算保证你的数据有多份储存，提高redundancy，并且提速</li><li>数据库效率快，并且保证redundancy</li><li>并发编程提高效率</li><li>GPU提高效率</li></ul><p>我们可以发现，这一切额外的复杂度，其实是为了效率。而既然人类能在写程序的时候推理出如何做这些优化，凭啥编译器推不出来？大不了暴力搜索。靠计算机软件隐藏leaky abstraction又不是没发生过，上古时期码农还得写代码来swap暂时用不上的memory进磁盘呢。</p><p>事实上，有些地方，编译器优化就是各种各样暴力搜索，<a href="https://en.wikipedia.org/wiki/Superoptimization">Superoptimization - Wikipedia</a>是， <a href="https://zhuanlan.zhihu.com/p/37181530">AutoTVM：让AI来编译优化AI系统底层算子</a> 是，<a href="https://facebookresearch.github.io/TensorComprehensions/tutorials/index.html">Tensor Comprehensions Tutorials</a> 也是。</p><p><br></p><p><a href="http://grappa.io/">自动分布式计算</a> 有（TensorFlow也应该算），单机并发更不在话下，那还要数据库做啥？数据库的ACID跟recovery，也只是自动分布式计算下的ACID跟recovery，交给自动分布式就好了啊！</p><p>当然，如果你眼尖，你会提出一个问题，‘程序的行数，远远比摩尔定律带来的倍数多啊，指数会爆炸啊。’这个问题的答案得益于我们很蠢。笨得一时间只能想起撑死十几个东西。所以我们强迫自己写代码需要写得更模块化，一段代码做一件东西。所以我们抛弃掉goto。</p><p>换句话说编译器能对程序一小段一小段的进行推理，最后compose起来。</p><p>如果我们激进点，可以假设这个分布式计算网络覆盖了全球，那其实我们就剩下一台计算机了。或者说，到了这个地步，计算机这个词已经过时，我们应该用‘算力’来代表这种无处不在的物体了。至于支付跟安全性问题？我想想。。。区块链？<br></p><p>这还有一个副作用：以后大家写Haskell需要用IO的地方都能去掉一大半了。数据库，并发，网络，通通不需要，连文件也能顺带扫进历史的垃圾桶了‘啥叫读写文件。。。你不能。。用参数吗？’。计算机的三种交互中，计算机&lt;-&gt;计算机会完全消失，计算机&lt;-&gt;人也许会被<a href="http://conal.net/blog/posts/tangible-functional-programming-a-modern-marriage-of-usability-and-composability">Tangible Functional Programming: a modern marriage of usability and composability</a>代替，那只剩下不常见的计算机&lt;-&gt;机器臂了。The future is superfunctional Aka <a href="http://conal.net/blog/posts/can-functional-programming-be-liberated-from-the-von-neumann-paradigm">Can functional programming be liberated from the von Neumann paradigm?</a></p><p><br></p><p><b>既然编译器能做这么多优化，离自动编程还有多远？</b></p><p>自动编程的尝试有一大堆，Program Synthesis/Genetic Programming/Differentiable Programming等全是。如果限制Domain，也有<a href="https://cozy.uwplse.org/">取得不错成绩</a>的。</p><p>这里很重要的一点是，<a href="https://www.zhihu.com/question/56250357/answer/148934031">别看到AI，就不成功便成仁，不是没用，就是抢掉我的岗位</a> - <a href="https://www.youtube.com/watch?v=er_lLvkklsk">这些东西完全可以整合进IDE中，提高编程生产力。</a></p><p>而最近，<a href="https://blogs.msdn.microsoft.com/visualstudio/2018/05/07/introducing-visual-studio-intellicode/">微软就在往这个方向前进</a>。</p><p>而我们已经可以在这个5分钟的demo中，看出一下可以怎么extend下去了：最简单的，设一个hotkey，按一下则会选取当前的autocomplete，然后跳到下一个需要写代码的地方（hole）。或者，拿最近历史记录&amp;当前函数名&amp;类型作hint，这样啥也不写也能开始autocomplete。又或者，autocomplete会爬Github，找出最关联的函数，然后给你pull该项目为dependency下来。这样怕啥百万package，我有autocomplete。而这背后需要啥，才能变得实用？啥也不需要，预测准确度一步步提升而已。量变引起质变。</p><p><br></p><p><b>最后说下我写的时候意料不及的东西</b></p><p>其实JS这种package management style（相对来说）有点接近<a href="https://www.zhihu.com/question/266108521">《Why do we need modules at all?》？</a> 。当然，没有好的metadata/工具造成的混像我们都看见了。</p><p>从另一个角度来看，很巧妙的是，风马牛不相及（Compiler，Pure Function）的东西竟然互相影响了（Autocomplete，Module System）。我认为这是因为一个东西work不work很可能只是有暂时的问题，并不一定是因为从跟不上不可行。而这些问题可能随着某些奇奇怪怪的advance被奇奇怪怪的解决掉。预测未来真难啊。</p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
