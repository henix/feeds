<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CVPR 2018 | 商汤科技提出GeoNet：用无监督学习感知3D场景几何</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1522794345&amp;src=3&amp;ver=1&amp;signature=XOHKEx9q8tv0N6g1sqUp20p9bzDYtcIFXGY1amHPNsW6VRu6Zx2bQ0d7NKT0YBb2TZp-wsb3QjetXr15m11yHYvSBQ9P47krnYAkkXaeN1c6H8pIw82Pjr3kitEI4BwVCuPJaRxKGGGX4pULI9VRne70Y9YWnGLZGKyaUcFdsVs=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    CVPR 2018 | 商汤科技提出GeoNet：用无监督学习感知3D场景几何                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-25</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="max-width: 100%;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">作者：Zhichao Yin等</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">参与：<strong style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">Panda</span></strong></span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span><span style="font-size: 14px;"> </span></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">有效的无监督学习方法能缓解对有标注数据的需求，无监督学习技术与视觉感知领域的结合也有助于推动自动驾驶等高价值技术的发展。近日，商汤科技的一篇 CVPR 2018 论文提出了一种可以联合学习深度、光流和相机姿态的无监督学习框架 GeoNet，其表现超越了之前的无监督学习方法并可比肩最佳的监督学习方法。本论文的作者为 Zhichao Yin 和 Jianping Shi。机器之心对该论文进行了简要介绍，更多详情请参阅原论文。</span></p></blockquote><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解视频中的 3D 场景几何是视觉感知领域内的一项基本主题。其中包括很多经典的计算机视觉任务，比如深度恢复、流估计、视觉里程计（visual odometry）。这些技术有广泛的工业应用，包括自动驾驶平台、交互式协作机器人以及定位与导航系统等。</span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">传统的根据运动恢复结构（SfM：Structure from Motion）方法是以一种集成式的方式来解决这些任务，其目标是同时重建场景结构和相机运动。在稳健的和鉴别式的特征描述系统、更有效的跟踪系统以及更好地利用形义层面的信息等方面最近已经取得了一些进展。尽管如此，容易受到异常值和无纹理区域故障的影响的问题仍然还未完全消除，因为它们本质上依赖于高质量的低层面特征对应。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了突破这些局限，研究者将深度模型应用到了每个低层面子问题上，并且相对于传统方法实现了显著的增益。其中主要的优势来自于大数据，这有助于为低层面的线索学习获取高层面的形义对应，因此相比于传统方法，即使在不适定（ill-posed）的区域，深度模型也能取得优良的表现。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽管如此，为了保证在更一般的场景中也能表现优良，深度学习通常需要大量基本真值数据（groundtruth data）。在大多数情况下，需要基于昂贵的激光的设置和差分 GPS，这就限制了对大规模数据的获取。此外，之前的深度模型大都是为解决单个特定任务而设计的，比如深度、光流、相机姿态等。它们没有探索这些任务之间固有的冗余性（redundancy），这可以通过几何规律根据 3D 场景构建的本质性质来形式化。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">最近已有工作研究了将这些问题与深度学习一起形式化。但所有这些工作都存在固有的局限性。比如，它们需要大量激光扫描的深度数据来进行监督学习，需要立体相机作为获取数据的额外设备或不能明确处理非刚性（non-rigidity）和遮挡问题。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在这篇论文中，我们提出了一种无监督学习框架 GeoNet，可用于根据视频联合估计单眼深度、光流和相机运动。我们的方法基于 3D 场景几何的本质性质。直观的解释就是大多数自然场景都由刚性的静态表面组成，即道路、房屋、树木等。它们投射在视频帧之间的 2D 图像运动完全由深度结构和相机运动决定。同时，行人和车辆等动态目标通常存在于这样的场景中，而且通常具有大位移和扰乱性（disarrangement）的特点。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">由此，我们使用了深度卷积网络来理解上述直观现象。具体来说，我们的范式使用了一种 分治策略（divide-and-conquer strategy）。我们设计了一种全新的二级式级联架构来适应地处理场景刚性流和目标运动。因此这个全局的运动域（motion field）可以逐步得到细化，让我们的整个学习流程变成一种分解的且更易于学习的形式。由这种融合的运动域引导的视图合成损失（view synthesis loss）可以为无监督学习实现自然的正则化。图 1 给出了预测示例。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.47982708933717577" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnsSP5Ruy083MOls3nzmRiaAaZDCSHqaj0s0uPVGvnUOVc9lsaGoD9kVg/640?wx_fmt=png" data-type="png" data-w="694"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 1：我们方法在 KITTI 2015 上所得到的预测示例。从上到下：输入图像（序列中的一张）、深度图和光流。我们的模型是完全无监督式的，并且可以明确地处理动态目标和遮挡。</em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">第二个贡献是我们引入了一种全新的自适应几何一致性损失（geometric consistency loss）来克服纯视图合成目标中未包含的因素，比如遮挡处理和照片不一致问题。通过模仿传统的前向-反向（即向左-向右）一致性检查，我们的方法可以自动滤除可能的异常值和遮挡。预测一致的地方会在无遮挡区域中的不同视图之间得到强化，而错误的预测则会被平滑处理掉，尤其是被遮挡的区域。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">最后，我们在 KITTI 数据集的全部三项任务上全面地评估了我们的模型。我们的无监督方法的表现优于之前的无监督方法，并且可媲美监督方法的结果，这体现了我们的范式的有效性和优势。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">GeoNet 概述</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们提出的 GeoNet 能以一种无监督的方式通过 3D 场景几何的本质性质来感知 3D 场景几何。特别需要指出，我们分别使用了刚性结构重建器和非刚性运动定位器来分开学习刚性流和目标运动。我们采用了图像外观相似度来引导无监督学习，这无需任何标注成本就能泛化到无限多的视频序列上。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">图 2 给出了我们的 GeoNet 的概览图。它包含两个阶段：刚性结构推理阶段和非刚性运动细化阶段。第一个推理场景布局的阶段由两个子网络构成，即 DepthNet 和 PoseNet。深度图和相机姿态分别经过回归处理后再融合到一起，得到刚性流。此外，第二个阶段通过 ResFlowNet 实现，用于处理动态目标。ResFlowNet 学习得到的残差非刚性流再与刚性流相结合，就推导出了我们的最终流预测。因为我们的每个子网络的目标都是解决一个特定的子任务，因此复杂的场景几何理解目标就分解成了一些更简单的目标。我们将不同阶段的视图合成用作我们的无监督学习范式的基本监督。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">最后但并非不重要的是，我们会在训练期间执行几何一致性检查，这能显著提升我们的预测一致性并得到出色的表现。</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.43433652530779754" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnib4icmQibcOmuZCOk0pZK0oK0U2kXBGarFelictsmaC6QgK2HEOH4JbMUg/640?wx_fmt=png" data-type="png" data-w="1462"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 2：GeoNet 概览。它由用于估计静态场景几何的刚性结构重建器和用于捕捉动态目标的非刚性运动定位器构成。为了解决遮挡和非朗伯（non-Lambertian）表面问题，我们在任意双向流预测对内部使用了一致性检查。</em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em><img class="" data-copyright="0" data-ratio="0.43711340206185567" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnDuKFRU6whasJ9cIJowTEQNp7IhxzdSpCYLLggMGibvXH0R7pou7G77w/640?wx_fmt=png" data-type="png" data-w="1455"></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 4：Eigen et al. [9]（通过深度进行监督）、Zhou et al. [56]（无监督）和我们的方法（无监督）的单眼深度估计结果比较。为了可视化，中间插入了基本真值结果。我们的方法能取得精细结构的细节，并且在近和远的区域都能始终保持高质量的预测。</em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.3506056527590848" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnvibD7ojwn4XShyiatAuzzbSSlumSwLc7VHsToZju4gtiavyJfmErr5YJg/640?wx_fmt=png" data-type="png" data-w="1486"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>表 1：根据 Eigen et al. [9] 中的指标，在 KITTI 2015 上得到的单眼深度结果。对于训练，K 是指 KITTI 数据集，CS 是指 Cityscapes。其它方法的误差取自 [15, 56]。我们用粗体标出了仅在 KITTI 上训练所得到的最佳结果。Garg et al. [14] 的结果是在 50m 上限条件下得到的，我们将其单独列出来比较。</em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><img class="" data-copyright="0" data-ratio="0.30506155950752395" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnbx7pd8SOFglWGGJkM8yZStK1595XtZfyxF1HGfABYibttzQ6Gv4y9eg/640?wx_fmt=png" data-type="png" data-w="1462"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 5：直接流学习方法 DirFlowNetS（使用了几何一致性损失）和我们的 GeoNet 框架的比较。如图所示，GeoNet 在有遮挡、纹理模糊的区域优势明显，甚至在暗淡的阴影区域也有优势。</em></span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">论文：GeoNet：密集的深度、光流和相机姿态的无监督学习（GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose）</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.17666891436277815" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibGPYqOvuA5ib7rQ6drIibWGnbRib5qiaCMGpo265JeTBjDah9bpVrIelhzNyw6ndeku3MWQA0aUN7h7g/640?wx_fmt=png" data-type="png" data-w="1483"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1803.02276</span></p><p style="white-space: normal;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们提出了 GeoNet，这是一种可以从视频中联合学习单眼深度、光流和自我运动估计的无监督学习框架。这三个分量可以根据 3D 场景几何的本质性质而组合到一起，通过我们的框架以一种端到端的方式联合学习得到。具体而言，该框架可以根据单个模块的预测提取几何关系，然后可以将这些几何关系组合成一个图像重建损失，可用来分别推理静态和动态的场景部分。此外，我们还提出了一种自适应几何一致性损失，用以提升模型对异常值和非朗伯区域的稳健性，这能有效地解决遮挡和纹理模糊问题。我们在 KITTI 驾驶数据集上进行了实验，结果表明我们的方法能在所有三项任务上实现当前最佳的结果，表现优于之前的无监督方法，并可与监督方法媲美。</span><img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOd6kYzKSNx7GpKhf1OMhgW30B8WEsyibXYuvBogNHE5TQTpUQGLsWmeQ/640?wx_fmt=png" data-type="png" data-w="73" width="51px" style="background-color: rgb(255, 255, 255);color: rgb(62, 62, 62);font-size: 16px;box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 51px !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="margin-bottom: 20px;font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="300609719" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
