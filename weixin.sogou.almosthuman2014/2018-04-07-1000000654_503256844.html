<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CVPR 2018 | 腾讯AI Lab、MIT等机构提出TVNet：可端到端学习视频的运动表征</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1523931056&amp;src=3&amp;ver=1&amp;signature=9RnjtdTicaq9qc7UOlcEBCpNpprKhgkAouUv50AzZQWb2ylfcUT7XO3ICfGkIhZIKxJ7B1OyWNj4M9gPlioTD6NhCKt*huFwy0GCp759pa3rhEe8XQ0FtVMIT5LU2WIShGbuxStN08cTblpaIgPRtmYeUZNmzyA8tOCgBY7rWKM=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    CVPR 2018 | 腾讯AI Lab、MIT等机构提出TVNet：可端到端学习视频的运动表征                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-07</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;caret-color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="color: rgb(62, 62, 62);font-size: 16px;margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自arXiv</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color:#888888;"><span style="font-size: 12.000000953674316px;"><strong>作者：Lijie Fan、Wenbing Huang、Chuang Gan、Stefano Ermon、Boqing Gong、Junzhou Huang</strong></span></span></p><p style="color: rgb(62, 62, 62);font-size: 16px;max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="color: rgb(62, 62, 62);font-size: 16px;text-align: center;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">参与：Panda<br></span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p><br></p><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">尽管端到端的特征学习已经取得了重要的进展，但是人工设计的光流特征仍然被广泛用于各类视频分析任务中。为了弥补这个不足，由来自腾讯 AI Lab、MIT、清华、斯坦福大学的研究者完成并入选 CVPR 2018 Spotlight 论文的一项研究提出了一种能从数据中学习出类光流特征并且能进行端到端训练的神经网络：TVNet。机器之心对本论文进行了摘要介绍，详情请参阅原论文。另外，该研究成果的 TensorFlow 实现已经发布在 GitHub 上。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/abs/1804.00413</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">代码地址：https://github.com/LijieFan/tvnet</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">图像分类和目标检测等基于图像的任务已经在深度学习（尤其是卷积神经网络（CNN））的推动下实现了革命性的发展。但是，视频分析方面的进展却并不尽如人意，这说明学习时空数据的表征是非常困难的。我们认为其中主要的难点是：寻找视频中明显的运动信息（motion cue）需要某种新型网络设计，而这些设计尚未被找到和测试。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽管已经有些研究在尝试通过在空间和时间维度上同时执行卷积运算来学习特征，但是人工设计的光流（optical flow）特征在视频分析上仍有广泛和有效的应用。光流，顾名思义，是指两个连续帧之间的像素位移。因此，将光流应用到视频理解任务上可以明确而方便地实现运动线索的建模。然而，这种方法很低效，估计光流的计算和存储成本往往很高。目前成功将光流应用于视频理解的重要方法之一是 two-stream model [33]，其在光流数据上训练了一个用于学习动作模式的卷积网络。研究者们已经提出了一些不同的 two-stream model 的扩展，并在动作识别和动作检测等多种任务上实现了当前最佳水平。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽管表现出色，但当前的基于光流的方法存在一些显著缺陷：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">训练是一种双阶段过程。第一个阶段是通过基于优化的方法（比如 TVL1 [42]）提取每两个连续帧的光流。第二个阶段是基于提取出的光流数据上训练一个 CNN。这两个阶段是分开的，而且来自第二个阶段的信息（比如梯度）无法被用于调节第一个阶段的过程。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">光流提取的空间和时间成本很高。提取出的光流必须写入到磁盘上，以便训练和测试。对于包含大约 1 万段视频的 UCF101 数据集而言，通过 TVL1 方法提取所有数据的光流需要消耗一个 GPU 一整天时间，而将原来的场作为浮点数来存储这些光流需要至少 1TB 的存储空间（为了节省存储成本，通常需要线性归一化成 JPEG）。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了解决上述问题，我们提出了一种全新的神经网络设计，可以端到端的方式学习类光流的特征。这个名叫 TVNet 的网络是通过模仿和展开 TV-L1 的迭代优化过程而获得的。尤其值得一提的是，我们将 TV-L1 方法中的迭代形式化为了神经网络的自定义层。因此，我们的 TVNet 的设定基础良好，无需任何额外训练就能直接被使用。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">此外，我们的 TVNet 是可以端到端地训练的，因此可以自然地连接到特定任务的网络上（比如动作分类网络），进而实现「更深度」的可端到端训练的架构。因此，无需再预计算或存储光流特征。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">最后，通过执行端到端学习，被初始化为标准的光流特征提取器的 TVNet 的权重可以得到进一步的调节。这让我们可以发现更丰富且更针对任务的特征（相比于原始的光流），从而实现更优的表现。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了验证我们提出的架构的有效性，我们在两个动作识别基准（HMDB51 和 UCF101）上执行了实验，比较了我们提出的 TVNet 和几种互相竞争的方法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">总体而言，本论文有以下贡献：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们通过将 TV-L1 方法的迭代展开成自定义的神经层，开发了一种学习视频中的运动的全新神经网络。这个网络名叫 TVNet，具有良好的初始化。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽管我们提出的 TVNet 是以特定的 TV-L1 架构初始化的，但相比于标准的光流，它可以在进一步微调之后学习到更丰富和更面向任务的特征。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">相比于其它动作表征学习的方法（比如 TV-L1、FlowNet2.0 和 3D Convnets），我们的 TVNet 在两个动作识别基准上实现了更优的准确度，即在 HMDB51 上实现了 72.6% 的准确度、在 UCF101 上实现了 95.4% 的准确度。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.9861286254728878" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic0KN1ly9CicWUy5OB4s5bk29Ik4Gxapy37AXicRz1iauicmBhRbmpiaBl6JzWRw8UkjtoibV0frf3S4H3Q/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">算法 1：用于光流提取的 TV-L1 方法</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.7515762925598991" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic0KN1ly9CicWUy5OB4s5bk2XjC86pz6pEB4qg2t3TPrMREfHuFKyrbvibBZgEp3icJKsxLULfvQ5PaA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 1：由 TV-L1、TVNet（无训练）、TVNet（有训练）得到的类光流运动特征的可视化结果</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.31651954602774274" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic0KN1ly9CicWUy5OB4s5bk28zydibCjclDKRp0fHvzDibfsW44BVGL0cER11Ws7lVibmuzpGdM3AaibPA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 2：（a）将 TV-L1 展开成 TVNet 的过程示意图。对于 TV-L1，我们只描述了算法 1 中的单次迭代。我们将 TV-L1 中的双三次翘曲（bicubic warping）、梯度和散度计算重新形式化为了 TVNet 中的双线性翘曲（bilinear warping）和卷积运算。（b）用于动作识别的端到端模型。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.8649940262843488" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic0KN1ly9CicWUy5OB4s5bk20WBNpJU9ZpajBcKdDUQM3yfzbKIPLqqR8wIejWgePcRJmbHxW3Eoww/640?wx_fmt=png" data-type="png" data-w="837" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 3：TV-L1 和 TVNet-50 在 MiddleBurry 上估计得到的光流示例。经过训练后，TVNet-50 可以提取出比 TV-L1 更精细的特征。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5280095351609059" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic0KN1ly9CicWUy5OB4s5bk2okzOEqB34908xz70rTuNpvzVUWkH4lg1Oaj4BwNgeA0NOTpQoAU8hA/640?wx_fmt=png" data-type="png" data-w="839" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 3：在 HMDB51 和 UCF101 上的各种运动描述器的分类准确度。上面一部分是之前最佳的动作表征方法的结果；中间一部分给出了 4 种基准方法的准确度；下面的结果表明我们的 TVNet-50 模型在这两个数据集上都得到了最佳表现。</span></em><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5851197982345523" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic0KN1ly9CicWUy5OB4s5bk28PYuVbdgAYvgLkS7NWgRfDhjPpoOhmSPo4XgvbVqPAGpibU8ejz8TTA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">表 4：在 HMDB51 和 UCF101 上的平均分类准确度</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.416141235813367" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic0KN1ly9CicWUy5OB4s5bk2yohmUjct7skuicBYG5HWic143nX1jxKuXrCfdNibUibMWh5b0n00B1KRRw/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 4：TV-L1 和 TVNet-50 在 UCF101 数据集上得到的运动特征。从第一列到最后一列，我们分别展示了输入图像对（仅第一张图像）、TV-L1 得到的运动特征、无训练和有训练的 TVNet-50 得到的运动特征。有意思的是，使用训练后，TVNet-50 可以得到比 TV-L1 及  TVNet-50 的非训练版本更抽象的运动特征。这些特征不仅自动移除了背景的运动（参见「punch」示例），而且还捕捉到了运动物体的轮廓。 </span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">论文：用于视频理解的运动表征的端到端学习（End-to-End Learning of Motion Representation for Video Understanding）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="0.2459016393442623" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWic0KN1ly9CicWUy5OB4s5bk2Hly4lx6l8ciadqI347dHjm7KX0BHxRj2yTmSgDEWtAkPUZdJiaIMkqMA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽管端到端学习的表征近来取得了成功，但视频分析任务广泛使用的仍然还是人工设计的光流特征。为了弥补这一不足，我们提出了一种全新的可端到端训练的神经网络 TVNet，可从数据中学习类光流特征。TVNet 是一种特定的光流求解器 TV-L1 方法，并且通过将 TV-L1 中的优化迭代展开成神经层而进行了初始化。因此，TVNet 无需任何额外训练就能直接使用。此外，它还可以自然地嫁接到其它特定任务的网络上，从而得到端到端的架构；因此，这种方法无需预计算和在磁盘上预存储特征，比之前的多阶段方法更加高效。最后，TVNet 的参数可以通过端到端训练进一步优化。这让 TVNet 不仅可以学习光流，而且还可以学习到更丰富的且更针对任务的动作模式。我们在两个动作识别基准上进行了广泛的实验，结果表明了我们提出的方法的有效性。我们的 TVNet 实现了比所有同类方法更优的准确度，同时在特征提取时间方面也能与最快的同类方法媲美。</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></p><p style="margin-bottom: 20px;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 14px;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></span></p><p style="max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="620494318" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
