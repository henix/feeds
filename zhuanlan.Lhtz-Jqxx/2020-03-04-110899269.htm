<div class="title-image"><img src="https://pic3.zhimg.com/v2-47a5e80b07d48ea6920b557bd3055386_b.jpg" alt=""></div><p>作者：Gray      编译：1+1=6</p><h2><u><i><b>1</b></i></u></h2><h2><b>前言</b></h2><p>金融市场大多是随机的。然而，它们并不是完全随机的。市场中存在着许多小的低效和模式，它们可以被识别出来，并被用来在市场上获得微弱的优势。</p><p>这些优势很少大到足以单独交易，交易成本和间接费用很容易覆盖我们的收益。但是<b>，当我们能够将许多这样的小优势结合在一起时，其收益可能是巨大的！</b></p><p>在本文中，我们将给各位读者提供一个框架<b>Stacked Generalization</b>，中文叫<b>堆栈泛化</b></p><p>集成学习（Ensemble Learning）中除了Bagging和Boosting对数据的横向划分划分之外，还有一个纵向划分（加深）的方法， 一般称为Stacked Generalization（SG）。</p><p>SG指训练一个用于组合（combine）其他多个不同模型的模型，具体是说首先我们<b>使用不同的算法或者其他方法能够训练出多个不同的模型，然后将这些模型的输出作为新的数据集，即将这些训练的模型的输出再作为为输入训练一个模型，最后得到一个最终的输出。</b></p><h2><u><i><b>2</b></i></u></h2><h2><b>堆栈泛化</b></h2><p>堆叠泛化框架最初是由Wolpert在1992年的一篇学术论文中提出的。自从它被首次提出以来，堆叠泛化已经得到了来自机器学习领域的关注。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-93f1c3623c8428caa70a85c6a70e91e5_b.jpg" data-caption="" data-size="normal" data-rawwidth="780" data-rawheight="482" class="origin_image zh-lightbox-thumb" width="780" data-original="https://pic2.zhimg.com/v2-93f1c3623c8428caa70a85c6a70e91e5_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-93f1c3623c8428caa70a85c6a70e91e5_b.jpg" data-caption="" data-size="normal" data-rawwidth="780" data-rawheight="482" class="origin_image zh-lightbox-thumb lazy" width="780" data-original="https://pic2.zhimg.com/v2-93f1c3623c8428caa70a85c6a70e91e5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-93f1c3623c8428caa70a85c6a70e91e5_b.jpg"/></figure><p><a href="https://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/summary%3Fdoi%3D10.1.1.56.1533" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">http://</span><span class="visible">citeseerx.ist.psu.edu/v</span><span class="invisible">iewdoc/summary?doi=10.1.1.56.1533</span><span class="ellipsis"></span></a></p><p><b>堆栈泛化是一种集成建模技术</b>。其核心概念是通过：</p><p>1、建立多个不同的模型（具有不同的学习算法、不同的超参数或不同的特征）来预测。</p><p>2、训练一个”元模型”或”混合模型”来确定如何结合每个这些多模型的预测，从而为一个回归或分类任务生成一个单一的、最佳的稳健预测。</p><p>下面是一种可视化的好方法。每一个R1到Rm模型都根据历史数据进行训练，并用于做出P1到Pm的预测。然后，这些预测就变成了用来训练元模型以确定如何组合这些预测的特征。</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-bf7413498f16897a66f26a1e5d205f85_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="942" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-bf7413498f16897a66f26a1e5d205f85_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-bf7413498f16897a66f26a1e5d205f85_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="942" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic2.zhimg.com/v2-bf7413498f16897a66f26a1e5d205f85_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-bf7413498f16897a66f26a1e5d205f85_b.jpg"/></figure><p>打个比方。想象一下，有一组投资分析师，他们的经理要求他们每个人对同一公司在多个季度的收益进行预测。然后经理来“学习”哪些分析师历来是最准确的。当然有些是准确的，有些是不准确的。当需要对未来进行预测时，经理可以为每个分析师的预测分配更多或更少的权重（在某些情况下是零权重）。</p><p>这就是为什么它被称为堆栈的原因。但<b>为什么还要泛化呢？</b>通过弱化那些看起来与数据过度匹配的模型，使模型对样本外（即不可见的）数据具有更大的泛化能力。这是通过允许元模型来学习哪些基础模型的预测在样本外表现良好（或差)），并适当地对模型进行加权来实现的。</p><h2><u><i><b>3</b></i></u></h2><h2><b>我们这么做的动机是什么？</b></h2><p>在我们看来，<b>堆栈泛化非常适合在嘈杂、非平稳、不稳定的金融市场中进行预测时所面临的挑战，且有助于解决过拟合问题</b>。这是几乎所有应用机器学习在投资的从业者都认同的！</p><p>更好的是，堆栈泛化允许我们以一种不会被更强的信号淹没的方式将相对微弱（正交）的信号混合在一起。</p><p>为了说明这一点，我们考虑一个典型的趋势跟踪策略，策略以12个月减去1个月的价格变动为基础。也许我们还认为，month-of-year或近期的IBIS收益趋势对价格变化有微弱但仍然有用的影响。如果我们要训练一个将显性特征（12-1动量）和较弱特征（季节性或IBIS趋势）集中在一起的模型，我们的模型可能会错过这些微妙的信息，因为显性特征掩盖了它们。</p><p>一个堆栈的模型，其中一个组成部分（例如一个基本模型）只关注动量特征，另一个部分只关注季节性特征，第三个部分只关注分析师修正特征，这些部分可以捕捉和使用更微妙的影响和更显著的动量效应。</p><h2><u><i><b>4</b></i></u></h2><h2><b>成功的关键</b></h2><p>这有两个具体的原则，将使你在很长一段路稳健的结果。</p><p><b>1、样本外训练</b></p><p>首先，<b>用于训练元模型的 P1到 Pm 预测需完全脱离样本预测</b>，这一点非常关键。为什么？因为，为了确定哪些模型可能更好地泛化到样本外（即那些超拟合最小的模型），我们必须根据过去的预测（这些预测本身就是样本外的）来判断。</p><p>假设你使用不同的算法（如逻辑回归和决策树）来训练两个模型。这两种方法都是非常有用的（样本外），但是决策树更倾向于过拟合训练数据。如果我们使用样本内预测作为元学习者的特征，我们可能会给模型更大的权重，并倾向于过拟合。</p><p>有几种方法可以用于此目的。一些人建议将训练数据分解为Train1和Train2集，以便在Train1上训练基本模型，然后对Train2数据进行预测，用于训练集成模型。当然，集成模型的预测必须在另一个数据集上进行评估。</p><p>其他的则在基础模型上使用K-fold交叉验证预测（例如scikit的cross_val_predict）来模拟样本外（ish）预测，从而将其输入集成层。</p><p>然而，在我们看来，<b>金融时间序列数据的最佳方法是在基础模型上使用walk-forward训练和预测</b>。除了确保每一个基本预测都是真实的样本外，它还模拟了非平稳性随时间的影响。</p><p><b>2、Non-Negativity</b></p><p>这不是一个严格的规则，是使用ElasticNet或lasso这样的算法，将元模型限制为只学习非负系数，允许非负约束。</p><p>这项技术很重要，因为通常输入到元模型中的特征会有非常高的共线性（P1到Pm）。在高共线性时期，学习算法可以做一些古怪的事情，比如通过给一个模型分配一个高的正系数，而给另一个模型分配一个大的负系数，从而找到对过去数据的稍微更好的拟合。这不是我们真正想要的。</p><p><b>但如果一个模型只有在不断预测错误结果的情况下才有用，那它可能就是一个我们不愿相信的模型。</b></p><h2><u><i><b>5</b></i></u></h2><h2><b>数据准备</b></h2><p>对于这个简单的示例，我们将创建合成数据，不使用真实的市场价格来消除关于从模型中提取最大价值所需的特征和转换的模糊性。</p><p><b>注意：</b>为了使数据集更真实，我们使用quandl的API从实际股票价格中提取一个索引，但是所有特征和目标值将在下面构造。</p><p>有了索引，我们将生成四个“隐藏因子”。这些是目标变量的非随机驱动因子，是我们理想情况下想要学习的“信号”。</p><p>为了确保这些因子是有意义的，我们将使用这些因子的组合来创建目标变量(y)。前两个隐藏因子与目标呈线性关系。后两个隐藏因子的关系更为复杂，涉及变量之间的相互作用。最后，我们将添加一个噪声组件，使我们的学习者为它工作。</p><p>最后，我们将创建几个与一个或多个隐藏因子相关的特征，包括大量的噪声和偏差。</p><p><b>重点：</b>我们已经创建了X和y数据，有些是线性的，有些不是。这就是我们的建模将要学习的内容。</p><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="n">pd</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">common</span><span class="o">.</span><span class="n">is_list_like</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">api</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">is_list_like</span>
<span class="kn">import</span> <span class="nn">pandas_datareader.data</span> <span class="kn">as</span> <span class="nn">web</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span><span class="n">Image</span>
<span class="n">HTML</span><span class="p">(</span><span class="s1">&#39;&lt;style&gt;{}&lt;/style&gt;&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">CSS</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">get_symbols</span><span class="p">(</span><span class="n">symbols</span><span class="p">,</span><span class="n">data_source</span><span class="p">,</span> <span class="n">begin_date</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">end_date</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">symbol</span> <span class="ow">in</span> <span class="n">symbols</span><span class="p">:</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">web</span><span class="o">.</span><span class="n">DataReader</span><span class="p">(</span><span class="n">symbol</span><span class="p">,</span> <span class="n">data_source</span><span class="p">,</span><span class="n">begin_date</span><span class="p">,</span> <span class="n">end_date</span><span class="p">)[[</span><span class="s1">&#39;AdjOpen&#39;</span><span class="p">,</span><span class="s1">&#39;AdjHigh&#39;</span><span class="p">,</span><span class="s1">&#39;AdjLow&#39;</span><span class="p">,</span><span class="s1">&#39;AdjClose&#39;</span><span class="p">,</span><span class="s1">&#39;AdjVolume&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
        <span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">,</span><span class="s1">&#39;open&#39;</span><span class="p">,</span><span class="s1">&#39;high&#39;</span><span class="p">,</span><span class="s1">&#39;low&#39;</span><span class="p">,</span><span class="s1">&#39;close&#39;</span><span class="p">,</span><span class="s1">&#39;volume&#39;</span><span class="p">]</span> <span class="c1">#my convention: always lowercase</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;symbol&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">symbol</span> <span class="c1"># add a new column which contains the symbol so we can keep multiple symbols in the same dataframe</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">set_index</span><span class="p">([</span><span class="s1">&#39;date&#39;</span><span class="p">,</span><span class="s1">&#39;symbol&#39;</span><span class="p">])</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">out</span><span class="p">,</span><span class="n">df</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#stacks on top of previously collected data</span>
    <span class="k">return</span> <span class="n">out</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>
        
<span class="n">idx</span> <span class="o">=</span> <span class="n">get_symbols</span><span class="p">([</span><span class="s1">&#39;AAPL&#39;</span><span class="p">,</span><span class="s1">&#39;CSCO&#39;</span><span class="p">,</span><span class="s1">&#39;MSFT&#39;</span><span class="p">,</span><span class="s1">&#39;INTC&#39;</span><span class="p">],</span><span class="n">data_source</span><span class="o">=</span><span class="s1">&#39;quandl&#39;</span><span class="p">,</span><span class="n">begin_date</span><span class="o">=</span><span class="s1">&#39;2012-01-01&#39;</span><span class="p">,</span><span class="n">end_date</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span><span class="o">.</span><span class="n">index</span>
<span class="c1"># note, we&#39;re only using quandl prices to generate a realistic multi-index of dates and symbols</span>

<span class="n">num_obs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="n">split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_obs</span><span class="o">*.</span><span class="mi">80</span><span class="p">)</span>

<span class="c1">## First, create factors hidden within feature set</span>
<span class="n">hidden_factor_1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span>
<span class="n">hidden_factor_2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span>
<span class="n">hidden_factor_3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span>
<span class="n">hidden_factor_4</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span>

<span class="c1">## Next, generate outcome variable y that is related to these hidden factors</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">hidden_factor_1</span> <span class="o">+</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">hidden_factor_2</span> <span class="o">+</span>  <span class="c1"># factors linearly related to outcome</span>
     <span class="n">hidden_factor_3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">hidden_factor_4</span><span class="p">)</span> <span class="o">+</span> <span class="n">hidden_factor_4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">hidden_factor_3</span><span class="p">)</span><span class="o">+</span> <span class="c1"># factors with non-linear relationships</span>
      <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">))</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span> <span class="c1"># noise</span>

<span class="c1">## Generate features which contain a mix of one or more hidden factors plus noise and bias</span>

<span class="n">f1</span> <span class="o">=</span> <span class="mf">0.25</span><span class="o">*</span><span class="n">hidden_factor_1</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span>
<span class="n">f2</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">hidden_factor_1</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">f3</span> <span class="o">=</span> <span class="mf">0.25</span><span class="o">*</span><span class="n">hidden_factor_2</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="mf">2.0</span>
<span class="n">f4</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">hidden_factor_2</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="o">-</span> <span class="mf">2.0</span>
<span class="n">f5</span> <span class="o">=</span> <span class="mf">0.25</span><span class="o">*</span><span class="n">hidden_factor_1</span> <span class="o">+</span> <span class="mf">0.25</span><span class="o">*</span><span class="n">hidden_factor_2</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span>
<span class="n">f6</span> <span class="o">=</span> <span class="mf">0.25</span><span class="o">*</span><span class="n">hidden_factor_3</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span>
<span class="n">f7</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">hidden_factor_3</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">f8</span> <span class="o">=</span> <span class="mf">0.25</span><span class="o">*</span><span class="n">hidden_factor_4</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="o">+</span> <span class="mf">2.0</span>
<span class="n">f9</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">hidden_factor_4</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span> <span class="o">-</span> <span class="mf">2.0</span>
<span class="n">f10</span> <span class="o">=</span> <span class="n">hidden_factor_3</span> <span class="o">+</span> <span class="n">hidden_factor_4</span>  <span class="o">+</span>  <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_obs</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">idx</span><span class="p">)</span>

<span class="c1">## From these features, create an X dataframe</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">f1</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f1&#39;</span><span class="p">),</span><span class="n">f2</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f2&#39;</span><span class="p">),</span><span class="n">f3</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f3&#39;</span><span class="p">),</span><span class="n">f4</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f4&#39;</span><span class="p">),</span><span class="n">f5</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f5&#39;</span><span class="p">),</span>
               <span class="n">f6</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f6&#39;</span><span class="p">),</span><span class="n">f7</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f7&#39;</span><span class="p">),</span><span class="n">f8</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f8&#39;</span><span class="p">),</span><span class="n">f9</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f9&#39;</span><span class="p">),</span><span class="n">f10</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;f10&#39;</span><span class="p">)],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code></pre></div><h2><u><i><b>6</b></i></u></h2><h2><b>探索性数据分析</b></h2><p>1、特征和目标变量的分布。</p><div class="highlight"><pre><code class="language-python"><span class="n">X</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;green&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="o">+</span><span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Distributions - Features and Target&#39;</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span> <span class="c1"># target</span></code></pre></div><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-f98b2c1cdd2a0bb043385712e4fa6884_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="727" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-f98b2c1cdd2a0bb043385712e4fa6884_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-f98b2c1cdd2a0bb043385712e4fa6884_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="727" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic1.zhimg.com/v2-f98b2c1cdd2a0bb043385712e4fa6884_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-f98b2c1cdd2a0bb043385712e4fa6884_b.jpg"/></figure><p>2、与目标变量相比，十个特征中的每一个都是简单的单变量回归。</p><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&#34;dark&#34;</span><span class="p">)</span>

<span class="c1"># Set up the matplotlib figure</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Rotate the starting point around the cubehelix hue circle</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">flat</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">cubehelix_palette</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">light</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">s</span><span class="p">]</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">regplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span><span class="n">fit_reg</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">,</span> <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;s&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;salmon&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span>
            <span class="o">**</span><span class="p">{</span><span class="s1">&#39;ha&#39;</span><span class="p">:</span> <span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="s1">&#39;va&#39;</span><span class="p">:</span> <span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="s1">&#39;family&#39;</span><span class="p">:</span> <span class="s1">&#39;sans-serif&#39;</span><span class="p">},</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&#34;Univariate Regressions for Features&#34;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</code></pre></div><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-9ecb2654fa76c17e84dc65bab4bbfa79_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="878" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic2.zhimg.com/v2-9ecb2654fa76c17e84dc65bab4bbfa79_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-9ecb2654fa76c17e84dc65bab4bbfa79_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="878" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic2.zhimg.com/v2-9ecb2654fa76c17e84dc65bab4bbfa79_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-9ecb2654fa76c17e84dc65bab4bbfa79_b.jpg"/></figure><p>3、显示特征之间相关性的clustermap。</p><div class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">scipy.cluster</span> <span class="kn">import</span> <span class="n">hierarchy</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="kn">import</span> <span class="n">distance</span>

<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">correlations_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">)</span>
<span class="n">linkage</span> <span class="o">=</span> <span class="n">hierarchy</span><span class="o">.</span><span class="n">linkage</span><span class="p">(</span><span class="n">distance</span><span class="o">.</span><span class="n">pdist</span><span class="p">(</span><span class="n">correlations_array</span><span class="p">),</span> \
                            <span class="n">method</span><span class="o">=</span><span class="s1">&#39;average&#39;</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">clustermap</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span><span class="n">row_linkage</span><span class="o">=</span><span class="n">linkage</span><span class="p">,</span><span class="n">col_linkage</span><span class="o">=</span><span class="n">linkage</span><span class="p">,</span>\
                   <span class="n">row_cluster</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">col_cluster</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span><span class="p">,</span><span class="n">center</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ax_heatmap</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">get_majorticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">label_order</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">g</span><span class="o">.</span><span class="n">dendrogram_row</span><span class="o">.</span><span class="n">reordered_ind</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span>
</code></pre></div><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-e9d8ff953e9126fa76d05586735fe306_b.jpg" data-caption="" data-size="normal" data-rawwidth="788" data-rawheight="792" class="origin_image zh-lightbox-thumb" width="788" data-original="https://pic3.zhimg.com/v2-e9d8ff953e9126fa76d05586735fe306_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-e9d8ff953e9126fa76d05586735fe306_b.jpg" data-caption="" data-size="normal" data-rawwidth="788" data-rawheight="792" class="origin_image zh-lightbox-thumb lazy" width="788" data-original="https://pic3.zhimg.com/v2-e9d8ff953e9126fa76d05586735fe306_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-e9d8ff953e9126fa76d05586735fe306_b.jpg"/></figure><h2><u><i><b>7</b></i></u></h2><h2><b>基础模型</b></h2><p>堆栈泛化的第一步是生成基础模型，即从我们输入特征中学习的模型。我们将创建两个基本模型用于我们的集成：</p><p>1、一组简单的线性回归模型。</p><p>2、树模型的集合，在这种情况下，使用ExtraTrees算法。</p><p>如上所述，建立能够提供真实的样本外预测的模型是绝对重要的，我们将应用在Walk-forward 建模中提出的方法。简而言之，这将在每个季度末进行重新训练，只使用当时可用的数据。预测是使用最新的训练模型。</p><p>为了更容易理解，我们将定义一个名为make_walkforward_model的简单函数，该函数在不同时间点训练一系列模型，并使用这些训练好的模型生成样本外预测。</p><div class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">clone</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>


<span class="k">def</span> <span class="nf">make_walkforward_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">outcome</span><span class="p">,</span><span class="n">algo</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">()):</span>
    <span class="n">recalc_dates</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s1">&#39;Q&#39;</span><span class="p">,</span><span class="n">level</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">values</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1">## Train models</span>
    <span class="n">models</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">recalc_dates</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">date</span> <span class="ow">in</span> <span class="n">recalc_dates</span><span class="p">:</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">xs</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="n">date</span><span class="p">),</span><span class="n">level</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span><span class="n">drop_level</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">xs</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span><span class="n">date</span><span class="p">),</span><span class="n">level</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span><span class="n">drop_level</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1">#print(f&#39;Train with data prior to: {date} ({y_train.count()} obs)&#39;)</span>
        
        <span class="n">model</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">algo</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
        <span class="n">models</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">date</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span>

    <span class="n">begin_dates</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">index</span>
    <span class="n">end_dates</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">([</span><span class="s1">&#39;2099-12-31&#39;</span><span class="p">]))</span>

    <span class="c1">## Generate OUT OF SAMPLE walk-forward predictions</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">features</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">model</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">models</span><span class="p">):</span> <span class="c1">#loop thru each models object in collection</span>
        <span class="c1">#print(f&#39;Using model trained on {begin_dates[i]}, Predict from: {begin_dates[i]} to: {end_dates[i]}&#39;)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">xs</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="n">begin_dates</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">end_dates</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="n">level</span><span class="o">=</span><span class="s1">&#39;date&#39;</span><span class="p">,</span><span class="n">drop_level</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
        <span class="n">predictions</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">p</span>
    
    <span class="k">return</span> <span class="n">models</span><span class="p">,</span><span class="n">predictions</span>
</code></pre></div><p>要创建一系列walk-forward模型，只需传入 x 和 y 数据以及一个 scikit estimator 对象。它返回一系列模型和一系列预测。在这里，我们将针对所有特征创建两个基本模型，一个使用线性回归模型，另一个使用Extra Trees。</p><div class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesRegressor</span>

<span class="n">linear_models</span><span class="p">,</span><span class="n">linear_preds</span> <span class="o">=</span> <span class="n">make_walkforward_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">algo</span><span class="o">=</span><span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">tree_models</span><span class="p">,</span><span class="n">tree_preds</span> <span class="o">=</span> <span class="n">make_walkforward_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">algo</span><span class="o">=</span><span class="n">ExtraTreesRegressor</span><span class="p">())</span>
</code></pre></div><p>注意，在第一个训练好的模型之前不能进行任何预测，<b>因此在使用之前对预测进行</b>dropna()<b>非常重要。</b></p><div class="highlight"><pre><code class="language-python"><span class="k">print</span><span class="p">(</span><span class="s2">&#34;Models:&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">linear_models</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&#34;Predictions:&#34;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">linear_preds</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="n">Models</span><span class="p">:</span>
<span class="mi">2012</span><span class="o">-</span><span class="mo">03</span><span class="o">-</span><span class="mi">31</span>    <span class="n">LinearRegression</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="n">Tr</span><span class="o">...</span>
<span class="mi">2012</span><span class="o">-</span><span class="mo">06</span><span class="o">-</span><span class="mi">30</span>    <span class="n">LinearRegression</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="n">Tr</span><span class="o">...</span>
<span class="mi">2012</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">30</span>    <span class="n">LinearRegression</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="n">Tr</span><span class="o">...</span>
<span class="mi">2012</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>    <span class="n">LinearRegression</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="n">Tr</span><span class="o">...</span>
<span class="mi">2013</span><span class="o">-</span><span class="mo">03</span><span class="o">-</span><span class="mi">31</span>    <span class="n">LinearRegression</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="n">Tr</span><span class="o">...</span>
<span class="n">dtype</span><span class="p">:</span> <span class="nb">object</span>

<span class="n">Predictions</span><span class="p">:</span>
<span class="n">date</span>        <span class="n">symbol</span>
<span class="mi">2012</span><span class="o">-</span><span class="mo">04</span><span class="o">-</span><span class="mo">02</span>  <span class="n">AAPL</span>     <span class="o">-</span><span class="mf">0.786846</span>
            <span class="n">CSCO</span>     <span class="o">-</span><span class="mf">1.518537</span>
            <span class="n">INTC</span>      <span class="mf">0.145496</span>
            <span class="n">MSFT</span>     <span class="o">-</span><span class="mf">0.677892</span>
<span class="mi">2012</span><span class="o">-</span><span class="mo">04</span><span class="o">-</span><span class="mo">03</span>  <span class="n">AAPL</span>      <span class="mf">0.403579</span>
<span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</code></pre></div><p>看看线性模型系数是如何随着时间演变的，这是很有启发的：</p><div class="highlight"><pre><code class="language-python"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">linear_models</span><span class="p">],</span>
             <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="n">linear_models</span><span class="o">.</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Weighting Coefficients for </span><span class="se">\n</span><span class="s1">Linear Model&#39;</span><span class="p">)</span>
</code></pre></div><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-1273de24f57e5709640267b918035f9a_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="746" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-1273de24f57e5709640267b918035f9a_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-1273de24f57e5709640267b918035f9a_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="746" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic3.zhimg.com/v2-1273de24f57e5709640267b918035f9a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-1273de24f57e5709640267b918035f9a_b.jpg"/></figure><p>接下来，我们将创建一个简单的函数来评估多个模型性能指标，称为calc_scorecard。</p><div class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span><span class="p">,</span><span class="n">mean_absolute_error</span>

<span class="k">def</span> <span class="nf">calc_scorecard</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_true</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">make_df</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_true</span><span class="p">):</span>
        <span class="n">y_pred</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;y_pred&#39;</span>
        <span class="n">y_true</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;y_true&#39;</span>

        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_true</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;sign_pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;sign_true&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">y_true</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;is_correct&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">sign_pred</span> <span class="o">*</span> <span class="n">df</span><span class="o">.</span><span class="n">sign_true</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="p">,</span><span class="s1">&#39;is_correct&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="c1"># only registers 1 when prediction was made AND it was correct</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;is_incorrect&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">sign_pred</span> <span class="o">*</span> <span class="n">df</span><span class="o">.</span><span class="n">sign_true</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span><span class="s1">&#39;is_incorrect&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="c1"># only registers 1 when prediction was made AND it was wrong</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;is_predicted&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">is_correct</span> <span class="o">+</span> <span class="n">df</span><span class="o">.</span><span class="n">is_incorrect</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;result&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sign_pred</span> <span class="o">*</span> <span class="n">df</span><span class="o">.</span><span class="n">y_true</span>
        <span class="k">return</span> <span class="n">df</span>
    
    <span class="n">df</span> <span class="o">=</span> <span class="n">make_df</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_true</span><span class="p">)</span>
    
    <span class="n">scorecard</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">()</span>
    <span class="c1"># building block metrics</span>
    <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;RSQ&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">y_true</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">y_true</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;directional_accuracy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">is_correct</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">*</span><span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">is_predicted</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">*</span><span class="mf">1.</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
    <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;edge&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">result</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;noise&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">y_pred</span><span class="o">.</span><span class="n">diff</span><span class="p">()</span><span class="o">.</span><span class="nb">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="c1"># derived metrics</span>
    <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;edge_to_noise&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;edge&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;noise&#39;</span><span class="p">]</span>
    <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;edge_to_mae&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;edge&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">scorecard</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;MAE&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">scorecard</span>

<span class="n">calc_scorecard</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">linear_preds</span><span class="p">,</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;Linear&#39;</span><span class="p">)</span>

<span class="n">RSQ</span>                      <span class="mf">0.027760</span>
<span class="n">MAE</span>                      <span class="mf">1.784532</span>
<span class="n">directional_accuracy</span>    <span class="mf">53.788634</span>
<span class="n">edge</span>                     <span class="mf">0.278431</span>
<span class="n">noise</span>                    <span class="mf">0.530620</span>
<span class="n">edge_to_noise</span>            <span class="mf">0.524727</span>
<span class="n">edge_to_mae</span>              <span class="mf">0.156024</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">Linear</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</code></pre></div><p>由于我们不仅关心平均性能，还关心周期与周期之间的一致性，所以我们将创建一个简单的函数，它按季度重新计算我们的指标。</p><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">scores_over_time</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_true</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_true</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">resample</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">)</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">df</span><span class="p">:</span> <span class="n">calc_scorecard</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">y_pred</span><span class="o">.</span><span class="n">name</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">y_true</span><span class="o">.</span><span class="n">name</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">scores</span>

<span class="n">scores_by_year</span> <span class="o">=</span> <span class="n">scores_over_time</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">linear_preds</span><span class="p">,</span><span class="n">y_true</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">scores_by_year</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">scores_by_year</span><span class="p">[</span><span class="s1">&#39;edge_to_mae&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Prediction Edge vs. MAE&#39;</span><span class="p">)</span>

<span class="n">date</span>                  <span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">312017</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">312018</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">31</span>
<span class="n">RSQ</span>                     <span class="mf">0.0285390</span><span class="o">.</span><span class="mo">017</span><span class="mi">950</span><span class="o">-</span><span class="mf">0.006912</span>
<span class="n">MAE</span>                     <span class="mf">1.7777841</span><span class="o">.</span><span class="mf">7261961.779631</span>
<span class="n">directional_accuracy</span>   <span class="mf">55.85317552</span><span class="o">.</span><span class="mf">10420853.813559</span>
<span class="n">edge</span>                    <span class="mf">0.2743540</span><span class="o">.</span><span class="mf">2548430.254830</span>
<span class="n">noise</span>                   <span class="mf">0.5149290</span><span class="o">.</span><span class="mf">5028130.503823</span>
<span class="n">edge_to_noise</span>           <span class="mf">0.5327990</span><span class="o">.</span><span class="mf">5068350.505792</span>
<span class="n">edge_to_mae</span>             <span class="mf">0.1543230</span><span class="o">.</span><span class="mf">1476330.143193</span>
</code></pre></div><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-5bd8a90307a3f14a8300c90cc61a42f7_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="778" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic4.zhimg.com/v2-5bd8a90307a3f14a8300c90cc61a42f7_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-5bd8a90307a3f14a8300c90cc61a42f7_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="778" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic4.zhimg.com/v2-5bd8a90307a3f14a8300c90cc61a42f7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-5bd8a90307a3f14a8300c90cc61a42f7_b.jpg"/></figure><h2><u><i><b>8</b></i></u></h2><h2><b>进阶模型</b></h2><p>既然我们已经训练了基本模型并生成了样本外预测，现在就该训练堆栈泛化的集成模型了。</p><p>训练集成模型只需将基本模型的预测作为X数据流输入即可。为了清理数据并确保X和y具有兼容的维度，我们创建了一个简短的数据准备函数。</p><p>在这里，我们将使用Lasso来训练集合，因为它是少数几个可以positive = True的线性模型之一。这将确保集成将为每个模型分配一个正的或0权重。</p><div class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>
<span class="k">def</span> <span class="nf">prepare_Xy</span><span class="p">(</span><span class="n">X_raw</span><span class="p">,</span><span class="n">y_raw</span><span class="p">):</span>
    <span class="s1">&#39;&#39;&#39; Utility function to drop any samples without both valid X and y values&#39;&#39;&#39;</span>
    <span class="n">Xy</span> <span class="o">=</span> <span class="n">X_raw</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">y_raw</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span><span class="bp">None</span><span class="p">,</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">:</span><span class="bp">None</span><span class="p">})</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">Xy</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">Xy</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span><span class="n">y</span>
<span class="n">X_ens</span><span class="p">,</span> <span class="n">y_ens</span> <span class="o">=</span> <span class="n">prepare_Xy</span><span class="p">(</span><span class="n">X_raw</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">linear_preds</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">),</span><span class="n">tree_preds</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;tree&#39;</span><span class="p">)],</span>
                                          <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">y_raw</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">ensemble_models</span><span class="p">,</span><span class="n">ensemble_preds</span> <span class="o">=</span> <span class="n">make_walkforward_model</span><span class="p">(</span><span class="n">X_ens</span><span class="p">,</span><span class="n">y_ens</span><span class="p">,</span><span class="n">algo</span><span class="o">=</span><span class="n">LassoCV</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">ensemble_preds</span> <span class="o">=</span> <span class="n">ensemble_preds</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;ensemble&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ensemble_preds</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="n">date</span>        <span class="n">symbol</span>
<span class="mi">2012</span><span class="o">-</span><span class="mo">07</span><span class="o">-</span><span class="mo">02</span>  <span class="n">AAPL</span>      <span class="mf">0.464468</span>
            <span class="n">CSCO</span>      <span class="mf">0.238618</span>
            <span class="n">INTC</span>     <span class="o">-</span><span class="mf">0.008967</span>
            <span class="n">MSFT</span>      <span class="mf">0.864243</span>
<span class="mi">2012</span><span class="o">-</span><span class="mo">07</span><span class="o">-</span><span class="mo">03</span>  <span class="n">AAPL</span>      <span class="mf">0.437890</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">ensemble</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</code></pre></div><p>请注意，集成的预测要到7月才开始，因为最早的训练集成模型要到第二季度末才可用。这对于确保集成模型在样本外数据上得到训练是必要的——而且它的预测也是样本外的。</p><p>再一次，我们可以观察集合模型随时间变化的系数。请记住，集成模型的系数表示每个基本模型的权重。在这种情况下，我们的树模型似乎比线性模型更有用，尽管线性模型正在逐渐赶上。</p><div class="highlight"><pre><code class="language-python"><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">ensemble_models</span><span class="p">],</span>
             <span class="n">columns</span><span class="o">=</span><span class="n">X_ens</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="n">ensemble_models</span><span class="o">.</span><span class="n">index</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Weighting Coefficients for </span><span class="se">\n</span><span class="s1">Simple Two-Model Ensemble&#39;</span><span class="p">)</span>
</code></pre></div><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-18fc011cf0c006226db68857e127fbdc_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="768" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-18fc011cf0c006226db68857e127fbdc_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-18fc011cf0c006226db68857e127fbdc_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="768" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic1.zhimg.com/v2-18fc011cf0c006226db68857e127fbdc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-18fc011cf0c006226db68857e127fbdc_b.jpg"/></figure><h2><u><i><b>9</b></i></u></h2><h2><b>进阶模型与基本模型性能比较</b></h2><div class="highlight"><pre><code class="language-python"><span class="c1"># calculate scores for each model</span>
<span class="n">score_ens</span> <span class="o">=</span> <span class="n">calc_scorecard</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">ensemble_preds</span><span class="p">,</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_ens</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;Ensemble&#39;</span><span class="p">)</span>
<span class="n">score_linear</span> <span class="o">=</span> <span class="n">calc_scorecard</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">linear_preds</span><span class="p">,</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_ens</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;Linear&#39;</span><span class="p">)</span>
<span class="n">score_tree</span> <span class="o">=</span> <span class="n">calc_scorecard</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">tree_preds</span><span class="p">,</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_ens</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s1">&#39;Tree&#39;</span><span class="p">)</span>


<span class="n">scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">score_linear</span><span class="p">,</span><span class="n">score_tree</span><span class="p">,</span><span class="n">score_ens</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;edge_to_noise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span><span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;edge&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span><span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">scores</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;noise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</code></pre></div><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-58eb47bc58197779ec66935f50c19ed2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="714" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic3.zhimg.com/v2-58eb47bc58197779ec66935f50c19ed2_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-58eb47bc58197779ec66935f50c19ed2_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="714" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic3.zhimg.com/v2-58eb47bc58197779ec66935f50c19ed2_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-58eb47bc58197779ec66935f50c19ed2_b.jpg"/></figure><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-963d48ab30d807039116f170307d84d5_b.jpg" data-caption="" data-size="normal" data-rawwidth="838" data-rawheight="286" class="origin_image zh-lightbox-thumb" width="838" data-original="https://pic2.zhimg.com/v2-963d48ab30d807039116f170307d84d5_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-963d48ab30d807039116f170307d84d5_b.jpg" data-caption="" data-size="normal" data-rawwidth="838" data-rawheight="286" class="origin_image zh-lightbox-thumb lazy" width="838" data-original="https://pic2.zhimg.com/v2-963d48ab30d807039116f170307d84d5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-963d48ab30d807039116f170307d84d5_b.jpg"/></figure><p>观察结果：树型模型具有较高的精度和较高的edge，但其噪声较大。线性模型的精度和edge较低，但噪声小得多。将两个模型集成在一起可以保持树模型的准确性，但将预测噪声降低了近一半。虽然这不是最有意义的测量指标，但令人惊讶的是，该合集的RSQ是两个基本模型的两倍多。在真实的交易中，了解一致性如何表现，以及表现的趋势是好是坏，也是非常重要的。下面，我们将按年份绘制四个性能统计数据：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-8293f53ec389c353dff2d128d41c4d1c_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="715" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-8293f53ec389c353dff2d128d41c4d1c_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-8293f53ec389c353dff2d128d41c4d1c_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="715" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic1.zhimg.com/v2-8293f53ec389c353dff2d128d41c4d1c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8293f53ec389c353dff2d128d41c4d1c_b.jpg"/></figure><p>我们可以看到：</p><p>1、总体比任何一个基本模型都要有效。</p><p>2、随着时间的推移，所有的模型似乎都在变得更好，而且它们有更多的数据可供训练。</p><p>3、随着时间的推移，整体效果似乎更加一致。就像一个多样化的股票投资组合应该比其中的个股波动更小一样，一个多样化的模型组合往往会在一段时间内表现得更稳定。</p><p>后面我们要做的：</p><p>堆栈泛化具有高度的灵活性，我们可以采取包括：</p><p>1、更多模型类型添加SVM、深度学习模型、正则化回归和降维模型。</p><p>2、更多超参数组合：在特定算法上尝试多组超参数。</p><p>3、正交特征集：尝试在不同的特征子集上训练基模型。通过将每个基本模型限制在适当数量的特征上来避免“维数灾难”。</p><p>本文到此结束，希望大家有所收获。</p><p>量化投资与机器学习微信公众号，是业内垂直于<b>Quant、MFE、Fintech、AI、ML</b>等领域的<b>量化类主流自媒体。</b>公众号拥有来自<b>公募、私募、券商、期货、银行、保险资管、海外</b>等众多圈内<b>18W+</b>关注者。每日发布行业前沿研究成果和最新量化资讯。</p>