<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>RNN最大化条件概率参数模型预测时间序列</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/26434409">原文</a></p>
<div class="title-image"><img src="https://pic4.zhimg.com/v2-6a2b16adf26adacbe7558bb86e831743_r.jpg" alt=""></div>假想股市为交响乐合奏，多种信号交织出股票涨跌，则可以通过RNN拟合最大化条件概率的参数模型预测股票未来收益率。<p><img src="https://pic4.zhimg.com/v2-cc2ab1bc327bfd419809c71450426fda_r.png" data-rawwidth="1263" data-rawheight="302">假设股票收益率可以通过多种信号进行一定程度的解释和估计，也就是进行多参数模型拟合，则可以通过拟合如上图所示的信号流设定的数据训练集使RNN模型对应训练集的条件概率最大化，对求得的参数模型使用最新的输入变量进行预估。</p><h2>最大条件概率</h2><p>设定训练序列对输入序列<equation>X_{train} =(x_1, x_2, x_3, ...,x_T)</equation>,输出序列为<equation>Y_{train}=(y_1, y_2, y_3, ..., y_{T'}) </equation>(两个序列长度不固定相等)。一个标准的RNN计算序列到序列通过迭代下公式：<br></p><equation>h_t = f(x_t, h_{t-1})  ;   
y_t = W_{out}h_t</equation><br><p>其中<equation>f</equation>为激活函数，迭代上面的公式序列到序列的RNN参数<equation>\theta</equation>模型估计最大的序列生成条件概率，即：</p><equation>p(Y_{train}|X_{train};\theta) = \prod_{i=1}^T p(y_i|y_1,...,y_{i-1}, x_1,x_1,...,x_i;\theta)
</equation><br><equation>\theta^* = \arg\max_{\theta} \sum_{X_{train},Y_{train}}log(p(Y_{train}|X_{train};\theta))
</equation><br><p>通过推断，给定的输入序列<equation>P
</equation>学习到的参数<equation>\theta^*</equation>概率最大的估计序列被输出<equation>\hat C^P=\arg\max_{\hat P^C} p(C^P|P,\theta) </equation><br></p><p>对于一个全A股市场收益率序列预测模型寻找最优的序列意味着巨大的计算资源需求，按照直接算法这意味着巨大的计算量，由于这里不是介绍数值优化器的短文，关于具体求解技巧略。<br></p><h2>节点激活函数</h2><p>最初<equation>RNN
</equation>模型的激活函数<equation>f(x)</equation>采用sigmoid函数但对于长序列依赖问题和序列到序列的非单调对应关系使得模型性能表现较差。<br></p>1) Long Short Term Memory<p><equation>LSTM</equation>通过设计门控制单元处理长期关系依赖问题，在本文构件使用原始<equation>LSTM
</equation>控制单元,公式如下：<br></p><equation>f(x_t,h_{t-1})=\begin{bmatrix}
i_t = tanh(W_ix_t + R_ih_{t-1} + b_i)\\
j_t = \sigma(W_jx_t + R_jh_{t-1} + b_j)\\
f_t = \sigma(W_fx_t + R_fh_{t-1} + b_f)\\
o_t = tanh(W_ox_t+R_oh_{t-1} + b_o)\\
c_t = i_t \odot j_t + f_t \odot c_{t-1}\\
h_t = o_t \odot tanh(c_t)\\
\end{bmatrix}</equation><br><img src="https://pic3.zhimg.com/v2-db17916e48cc58cbc5c7bf5de48c4b79_r.png" data-rawwidth="752" data-rawheight="530"><br>2) Gated Recurrent Unit<p>GRU连接输入门和遗忘门控为更新门控，并连接单元状态和隐藏状态，在某些应用方面保证极高的学习效果的前提下非常高效的降低了LSTM的计算量。<br></p><equation>f(x_t,h_{t-1})=\begin{bmatrix}
r = \sigma(W_rx + U_r h_{t-1})\\
z = \sigma(W_z + U_z h_{t-1})\\
\widetilde h_t =  \phi(Wx + U(r\odot h_{t-1})) \\
h_t = z h_{t-1} + (1-z) \widetilde h_t\\
\end{bmatrix}</equation><br>3) Neural Turing Machines<p>NTM通过添加外缓存结构有效解决了LSTM中长序列信息丢失问题。由于本文后面的内容与NTM联系不多，这里不对NTM过多描述。<br></p><img src="https://pic1.zhimg.com/v2-071b21c464d9339db3035864cffed5ce_r.png" data-rawwidth="702" data-rawheight="486"><br><img src="https://pic4.zhimg.com/v2-5ace60399eaf80782f2821b8ffbb9dcd_r.png" data-rawwidth="945" data-rawheight="688"><h2>网络推断模型</h2>1) ResNet<p>stacked RNN结构就是多层RNN模型叠加，下一层的输出作为上一层的输入，在encoder-decoder可以上一层用输出或者隐藏状态当作下一层的输入。</p><equation>Stacked RNN: h_t^{(l)} = f_h^{(l)}(h_t^{(l-1)}, h_{t-1}^{(l)} ) </equation><br><img src="https://pic1.zhimg.com/v2-c468ce871d57249aab99fd9d762f649f_r.png" data-rawwidth="494" data-rawheight="512"><br><p>ResNet:设i layer 的输入为X,输出为<equation>F(X)</equation>, 则下一层的输入为 <equation>X + F(x)</equation>这种残差结构设计可以有效加深stacked LSTM的纵向层数。<br></p><img src="https://pic3.zhimg.com/v2-e77f4c657a25ce215bd85258e9082fdc_r.png" data-rawwidth="1047" data-rawheight="729"><br>2) Gird2d LSTM<p>Grid LSTM 是吸收Stacked LSTM和Multidimensional LSTM两种LSTM单元间网络结构形成的新算法设计。相对于Stacked结构，Grid2d使用维度概念代替Stacked里面的深度概念(层数)，将深度构建为另一个维度的序列，即转换多层LSTM为x，y维度的多维度LSTM网格结构。实验显示，Grid相对于Stacked效果有明显提升。</p><p>具体操作方法如下图，将多维度LSTM放在一个格子里面算成一步，在第一个维度运行LSTM激活函数然后将维度一的输出和时刻i的输入合并为维度二的在时刻i的输入，更多维度依此进行。Grid2d LSTM网络拓扑结构设计的特点是相对于常规Stacked RNN结构在深度方向的上添加了记忆流动渠道，为双通道信息流。</p><img src="https://pic1.zhimg.com/v2-8a8a5e0e483c850f40fe6e4401486f28_r.png" data-rawwidth="506" data-rawheight="508"><br>3 Adaptive Computation Time<p>自适应计算时间算法是一种允许神经网络常态性的学习在输入和输出之间需要执行多少计算步骤的算法，根据具体问题的复杂度动态调整模型使用的计算量，该网络结构主要针对如下问题设计：</p><ul><li><p>RNN的计算时间由给定问题的序列长度和设计者指定，无法自行变动适应具体问题</p></li><li><p>具有高度复杂变化的序列数据处理</p></li><li><p>研究表明，在条件受限无法加深RNN模型深度的情况下，拉成RNN时间轴长度也能够提升RNN精度</p></li></ul><img src="https://pic2.zhimg.com/v2-39b6d7d4a6a2a000daf210d4b5faaba1_r.png" data-rawwidth="916" data-rawheight="526"><p>上图显示了ACT模型通过嵌套包裹RNN单元来实现自适应计算时间的调整的操作方法，两条虚线之间的时间展开对应传统RNN模型的时刻i的一步计算，相当于传统RNN模型在时刻i也就是序列位置i只进行一次计算，而ACT在时刻i或者序列位置i可以自行调整计算次数提升模型泛化效果。</p><p>标准RNN迭代公式如下：</p><equation>s_t = S(s_{t-1}, x_t)\\
y_t = W_y s_t + b_y</equation><br><p>ACT修改RNN迭代公式为：<br></p><equation>s_t^n = \begin{cases}
S(s_{t-1}, x_t^1)  \;\;\; if \;\; n = 1  \\ 
 S(s_t^{n-1},x_t^n) \;\;\; otherwise\\
\end{cases}</equation><br><p>即在时刻t进行n次迭代计算，并使用halting probability unit决定每个输入步的更新量：<br></p><equation>p_t^n =\begin{cases}
R(t) \;\;\; if \;\; n=N(t)\\
h_t^n \;\;\; otherwise
\end{cases}\\
</equation><br><equation>h_t^n = \sigma(W_hs_t^n + b_h)\\</equation><br><p>并设置最大迭代限制N防止网络在某步进程中无限制迭代计算,<br></p><equation>N(t) = \min  { n':\sum_{n=1}^{n'}} h_t^n &gt;=1-\epsilon\\
R(t) = 1- \sum_{n=1}^{N(t)-1}h_t^n\\</equation><br><p>最终在时刻t的更新量为：<br></p><equation>s_t = \sum _{n=1}^{N(t)}p_t^n s_t^n\\
y_t = \sum_{n=1}^{N(t)}p_t^ny_t^n\\</equation><br><p>通过使用包含思考代价的复合损失函数<equation>\hat L(x,y) = L(x,y) + \tau P(x)</equation>,该模型被设计为鼓励神经网络快速进行判断而不是一味的追求精度消耗大量计算资源。<br></p><equation>P(x) = \sum _{t=1}^T p_t
p_t = N(t) + R(t)</equation><br>4)  Neural GPU<p>从数据集学习具体的算法是神经网络设计的初衷，对于能够学习算法的神经模型，必须能够运行必要数量的计算步骤。上述的模型可以输入序列对中学习到需要线性计算步骤(时间)的算法，但是对于需要超线性运算时间的算法表现很差，在处理多位数字乘法算法学习的测试中，GridLSTM和NerualGPU表现较佳。但是NerualGPU和GridLSTM的具体使用网络模型往往非常深入，并且具有精细的架构，这使得最小化训练误差的问题对于随机梯度下降是非常具有挑战性的。</p><p>需要超线性数量的计算操作的任务不能通过只能执行线性数计算操作的神经体系来解决，表列出了几种不同的神经网络架构可以执行的计算操作的数量。</p><p><img src="https://pic2.zhimg.com/v2-79d51a97c6fc0b9ed5ccb0fcd153a264_r.png" data-rawwidth="852" data-rawheight="100">GridLSTM和NeruralGPU对输入序列长度为n的任务执行<equation>O(n^2)</equation>的计算量，这是一个理想的属性，因为这意味着该模型可以学习对输入序列计算运行时间超线性增长的算法，例如整数乘法。<br></p><img src="https://pic2.zhimg.com/v2-7359e1dab46d2be566a8248351b76942_r.png" data-rawwidth="949" data-rawheight="304"><br><equation>CGRU(s) = \mu \odot s + (1-\mu) \odot tanh(U*(r \odot s) + B)\\
\mu = \sigma(U' \odot s + B')\\
r = \sigma(U''*s + B'')\\
U*s = conv(input=s, \;\;\; filter=U )</equation><br><p>深度拓展(传统RNN中的叠加多层概念)结构NerualGPU迭代公式为<br></p><equation>s_{t+1}  = CGRU_l(CGRU_{l-1}(...(CGRU_1(s_t))))\\
s_{fin} = s_n
</equation><br><p>其中的假想图片(mental image)是对一个标准的conv2d卷积的数据操作格式为 输入数据[batch, in_height, in_width, in_channels] 相对应的卷积核数据形状为[filter_height, filter_width, in_channels, out_channels]，在上面的公式中卷积<equation>U*x</equation>中是将RNN序列中在时刻t的输入变量后置于卷积通道数据位上了，如RNN输入数据格式为[batch, in_length, in_width] 其中in_width为股票i在时刻t对应的多因子归一化数据，调整为[batch, in_length, w, in_width]数据格式，其中w维度扩展初置0并且在最终只提取w维度第一列的数据。<br></p><img src="https://pic4.zhimg.com/v2-ac8519f4e2b26ac9f9a00a9f533eaef7_r.jpg" data-rawwidth="600" data-rawheight="450"><br>The Extended Nerual GPU<img src="https://pic2.zhimg.com/v2-2f08cad197300e9d4eb5a71d33cb030a_r.png" data-rawwidth="1078" data-rawheight="363">NerualGPU模型在后续研究中被添加了主动记忆解码过程，将原始算法的最终假想图片<equation>s_{fin}</equation>设定为decoder解码器的起始点。在主动记忆解码器(active memory decoder)中，使用一个单独的输出缓存张量<equation>p</equation>(output tape tensor)将解码器设计为<equation>s_t = f(s_{t-1}, y_{t-1}, x_t)</equation>,t时刻的隐藏状态<equation>y_{t-1}</equation>受到上一时刻输出值:<equation>CGRU^d(s,p) = \mu \odot s + (1-\mu)\odot tanh(U*(r\odot s) + W * p  +B)\\
\mu = \sigma(U' * s + W' *p + B')\\
r = \sigma (U'' *s + W'' *p + B'')</equation><p>解码器在时刻t的运算公式为：</p><equation>d_{t+1} = CGRU_l^d(CGRU_{l-1}^d(...CGRU_1^d(s_t, p_t)),pt)</equation><p>在解码器计算时刻k输出缓存张量p更新k位置的tensor值为该时刻的输出列向量。</p><p>注意，The Extended Nerual GPU结构设计中虽然使用tape结构引入RNN attention 机制，但是在tape数据格式具体操作中采用在未得到数据前置0的方式进行处理，不会使用时间序列中的未来数据。</p><p>5) Pointer Network</p><p>Pointer Network本意是 简化attention机制的计算公式，但是这个公式组合简化之后可以剥离encoder-decoder结构，可以直接拿到多层LSTM上面使用，我试了一下，效果还不错。</p><equation>u_j^i = v^T tanh (W_1e_j +W_2 d_i)\\
p(C_i|C_1,...,C_{i-1},P)= softmax(u_i)\\</equation><p>注意上面的公式，encoder在decoder时间步i将所有的encoder 序列j全面编码计算指针网络，这里如果直接使用时间序列会用到未来数据，注意这里可以参考上面NeuralGPU论文里面的处理方法空置方法或者在编码的地方设置一个条件选择即可。当然也可以用作横面数据回归，不过根据因素排序股票通常没有特别明显的嬗变关系，可以适当使用分组的方法处理。<br><img src="https://pic2.zhimg.com/v2-18b4111a18468f4af3343a1d3ef8099c_r.png" data-rawwidth="943" data-rawheight="441"><br><br><br></p><h3>附录</h3>Encoder&amp;Decoder<img src="https://pic4.zhimg.com/v2-44532e7d03bd1b2da3fe4bfd8af7b087_r.png" data-rawwidth="379" data-rawheight="385">Encoder-Decoder 结构一般用来处理神经机器翻译方面，如上图所示，对一个英-汉句子互译的任务可以认为是sequence to sequence的具体案例，通过encoder将输入序列进行编码处理，将RNN最后一个隐藏状态矩阵当作信息缓存传送到decoder进行RNN解码翻译成对应的语句词向量。由上图可见对于传统的encoder-decoder结构，encoder只是将最后一个隐藏状态矩阵传递到decoder这造成了神经翻译模型的性能瓶颈，attention机制通过软对齐方法将更多的encoder信息流向decoder，提高模型整体性能。<p><img src="https://pic4.zhimg.com/v2-0fa2313f5c90e93ccd381ba558edb6dd_r.png" data-rawwidth="159" data-rawheight="224">由于在结构设计Encoder-Decoder提取全输入序列的信息，不便于直接用于时间序列的预测，这里附录是考虑多数的sequence to sequence是基于encoder-decoder结构处理NLP为假设应用进行设计的，所以单从时间序列角度考虑可能不容易快速理解。</p><p>代码示例</p><p><a href="https://zhuanlan.zhihu.com/p/25593926" data-title="Grid2dLSTM 构建高抽象的多因子股票时间序列预测模型" class="">Grid2dLSTM 构建高抽象的多因子股票时间序列预测模型</a><br></p><p><a href="https://zhuanlan.zhihu.com/p/25464454">Adaptive Computation Time for Recurrent Neural Networks</a><br></p><p><a href="https://zhuanlan.zhihu.com/p/25757053">NeuralGPU卷积与RNN结合超线性计算时间多因子时间序列预</a><br></p><p><a href="https://www.ricequant.com/community/topic/2524//2">tensorflow 笔记6 RNN 时间序列预测</a><br></p><br><h2>引用</h2><p><a href="http://papers.nips.cc/paper/5866-pointer-networks.pdf">Pointer Networks</a><br></p><p><a href="https://arxiv.org/pdf/1511.08228.pdf" data-title="NEURAL GPUS LEARN ALGORITHMS" class="">NEURAL GPUS LEARN ALGORITHMS</a><br></p><p><a href="https://arxiv.org/pdf/1611.00736.pdf" data-title="EXTENSIONS AND LIMITATIONS OF THE NEURAL GPU" class="">EXTENSIONS AND LIMITATIONS OF THE NEURAL GPU</a><br></p><p><a href="https://pdfs.semanticscholar.org/a283/d012cf89a039ca21a4155b127680a65eef0e.pdf">Can Active Memory Replace Attention?</a><br></p><p><a href="https://arxiv.org/pdf/1409.0473.pdf" data-title="NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE" class="">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a><br></p><p><a href="https://arxiv.org/pdf/1507.01526.pdf" data-title="GRID LONG SHORT-TERM MEMORY" class="">GRID LONG SHORT-TERM MEMORY</a><br></p><p><a href="https://arxiv.org/pdf/1611.05104.pdf" data-title="A WAY OUT OF THE ODYSSEY: ANALYZING AND COBINING RECENT INSIGHTS FOR LSTM" class="">A WAY OUT OF THE ODYSSEY: ANALYZING AND COBINING RECENT INSIGHTS FOR LSTM</a><br></p><p><a href="https://arxiv.org/pdf/1312.4400.pdf">Network In Network</a><br></p><p><a href="https://arxiv.org/pdf/1603.08983.pdf" data-title="Adaptive Computation Time for Recurrent Neural Networks" class="">Adaptive Computation Time for Recurrent Neural Networks</a><br></p><p><a href="https://arxiv.org/pdf/1506.00019.pdf">A Critical Review of Recurrent Neural Networks</a><br></p><p><a href="http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf">Sequence to Sequence Learning with Neural Networks</a><br></p><p><a href="https://arxiv.org/pdf/1406.1078.pdf">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</a></p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
