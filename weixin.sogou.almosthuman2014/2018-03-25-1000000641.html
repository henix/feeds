<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>专访MIT教授Tomaso Poggio：表达、优化与泛化——数学视角里的深度学习</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1522794345&amp;src=3&amp;ver=1&amp;signature=XOHKEx9q8tv0N6g1sqUp20p9bzDYtcIFXGY1amHPNsW6VRu6Zx2bQ0d7NKT0YBb2TZp-wsb3QjetXr15m11yHYvSBQ9P47krnYAkkXaeN1e3l3YVozfmffpY5VifrsWUV6mzs1QQi5N5x1TBo2*1c2z86zv5xMRRHnkEksyTKeE=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    专访MIT教授Tomaso Poggio：表达、优化与泛化——数学视角里的深度学习                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                        <span id="copyright_logo" class="rich_media_meta meta_original_tag">原创</span>
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-25</em>

                                        <em class="rich_media_meta rich_media_meta_text">邱陆陆</em>
                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="font-size: 16px;white-space: normal;max-width: 100%;line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);">机器之心原创</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">作者：邱陆陆</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">  </span></p><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">三月，受腾讯 AI Lab 学术论坛邀请，机器之心在深圳采访了深度学习理论研究著名学者 Tomaso Poggio。他以平直易懂的语言介绍了自己的「长篇系列工作」，也谈了谈他对理论指导实践以及仿生学指导深度学习算法发展等观点的看法。</span></p></blockquote><p><br></p><p><img class="" data-copyright="0" data-ratio="0.66640625" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibIcXJR7CpF13q7eQN4G4rQ70D1sHxdfnZvCj07BmFdkCyS6yYNo7icPJJMypalyiaia34XRYXSnjf8w/640?wx_fmt=jpeg" data-type="jpeg" data-w="1280" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Tomaso Poggio 的知名度，有相当一部分来源于他异常出色的导师身份：DeepMind 创始人及 CEO Demis Hassabis 和 Mobileye 创始人及 CTO Amnon Shashua，都是他的学生。这两家公司一个创造出了击败了围棋世界冠军、重新定义这个项目的 AlphaGo，另一个将辅助驾驶系统装进了全球超过 1500 万辆车里，制造了世界上第一款能在终端进行深度神经网络推理的量产车型的系统。Poggio 本人不仅鼓励他的学生们以创业的形式将深度学习带进现实世界，也亲身投入指导了这两家公司的早期创立。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然而在学术界，Poggio 的知名度更多来自于他的深度学习理论研究。他的论文非常好辨认——命名方式简单粗暴如同长篇系列小说的就是他，《深度学习理论 II》，《深度学习理论 IIIb》…… </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5625" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_jpg/KmXPKA19gWibIcXJR7CpF13q7eQN4G4rQ3oFSedJBOwQDaLKOPIQUUBCkYfawkdCAgGUVhe2X3UFFjjTNl1LOrw/640?wx_fmt=jpeg" data-type="jpeg" data-w="1280" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这个编号系统来自他对深度学习理论问题进行的拆分：在 Poggio 看来，深度学习理论研究问题分为三类：</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">第一类是表达（representation）问题：为什么深层网络比浅层网络的表达能力更好？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">第二类是优化（optimization）问题：为什么 SGD 能找到很好的极小值，好的极小值有什么特点？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">第三类是 泛化（generalization）问题：为什么参数比数据还多，仍然可以泛化、不过拟合？</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">对于每一类问题，他都尝试以应用数学工具为基础，通过举出能够用数学语言进行描述的例子然后给出解释的方式，用理论推导（也辅以一定的实验验证）来说明自己的观点。</span></p><p><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>深层网络表达组合函数的超强能力</strong></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">早在 2002 年，Poggio 和著名数学家 Steve Smale 就合著了一篇论文 [1]，总结了那些经典学习理论，它们的共同点是，都相当于具有单一隐藏层的网络。Poggio 是这样解释他研究「表达」的初衷：「当时我们就提出了一个问题：为什么大脑具有很多层？为什么当传统理论告诉我们使用单层网络的时候，大脑的视觉皮层其实在用许多层解决这一问题？」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">毫无疑问，目睹了深度网络的成功后，同样的问题再一次被摆上了台面。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Poggio 认为，事实上无论是深层网络还是单层网络，都能相当不错地近似任意连续函数——这也是上世纪 80 年代的学者们通常忽略多层网络而采用单层网络的原因。但是，问题的核心在于表达清楚一个函数所需要的维度：单层网络需要的单元数非常多，甚至比宇宙中的原子数还要多。这就是数学上所说的「维度灾难」：<strong>参数的个数需要是方程维度的指数倍</strong>。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了跳出维度灾难，过去的数学家尝试假设方程的光滑性：他们发现，维度灾难取决于「维度除以光滑性」。而深度学习给出了针对一类特定函数的独特方法：如果近似的对象是一个组合函数，换言之，是一个函数嵌套函数的大函数，那么<strong>深度网络拟合它所需的单元数和函数的维度是线性关系</strong>。换言之，无论维度多大，深度网络都能够摆脱维度灾难来拟合这个函数。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">现在，能够被深度神经网络很好地拟合的数据都具有组合函数的特点。以图像为例，想要分类一张图像，并不需要同时观察左上角和右下角两个相距甚远的像素，只需要观察每一小块，再将它们组合在一起。有了这种「组合」（compositional）的性质，当卷积神经网络被用来刻画图像，甚至不需要参数共享的帮助，就能轻易摆脱维度灾难。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">而那些现在还不能被神经网络刻画得很好的数据，通常都不是组合函数。但是知道「组合函数和神经网络很配」还远远不够，Poggio 说，「作为计算机科学学者，甚至作为数学家，我们能不能进一步解释一下组合函数，给出一些比『它是组合的』更明确的性质，从而更好地理解神经网络的工作机制。这对于我来说也是一个非常有趣的、希望更多研究者投入精力的开放问题。」</span></p><p><br></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">优化的致胜：取之不尽的参数和性质漂亮的 SGD</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">解线性方程组的时候，如果未知量的数量大于方程数，我们将收获很多解。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">拿神经网络处理数据的时候，以图像举例子，包含 6 万张训练数据的 CIFAR 数据集，通常会用一个包含数十万乃至上百万参数的神经网络进行处理——一个教科书般的过参数化（overparameterization）例子。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">将神经网络近似看成一个多项式——把每个具有非线性的小单元都用一个单变量多项式替代，得到一个由数十万乃至上百万多项式组成的大多项式，此时，想要在 CIFAR 上获得 0 训练误差，就转化成了一个解 6 万个线性方程的问题。根据贝祖定理（Bézout's theorem），此时的解的数量比宇宙中的原子数量还多。另外，参数多于数据量带来了「退化」（degenerate）性质：每一个解都对应着一个无限大的解集。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">因此，<strong>过参数化意味着，神经网络有无限多个退化的全局最优解，它们在损失空间里形成平坦的谷地</strong>。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">而众所周知，随机梯度下降（SGD）的特性就是会以较高的概率倾向于停留在退化的谷地里，即，停留在全局最优解上。二者的结合，就让神经网络的优化变得轻松：确定有全局最优、有很多，它们的特征明显，很容易被优化算法找到。</span></p><p><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>就算过拟合也能泛化：分类问题与交叉熵的绝妙搭配</strong></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">过参数化是优化的福音，同时也是泛化的噩梦。在经典机器学习里，随着优化进行，测试错误率会呈现一条先减后增的 U 型曲线，尤其是模型规模与数据规模不匹配的时候，后半段的过拟合是十分可怕的。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然而在深度学习里，泛化错误率却经常呈现一个「下降，然后停住不动」的态势，即使不是零，也能保持在一个相当低的水准上。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Poggio 对此的解释是：这是深度学习所进行的任务与所用的损失函数之间的特定组合带来的美好化学反应。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">具体来说，就是大多数神经网络都是用来解决分类问题（而不是回归问题）的，错误率通常以 0-1 损失计算，而目标函数却通常是交叉熵。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这种差异是因为 0-1 损失函数是好的效果衡量指标，却并不适合做优化的目标函数。拿手写数字分类器举例，神经网络分类器最后是通过 softmax 转 hardmax 来选择分类类别的，这也就意味着，即使模型认为一张「1」的图像是「1」的概率只有 30%，但只要这 30% 是所有 10 个可能性中最高的，模型仍然会将这张图像分类为「1」。一个信心水平只有 30% 的模型，即使分类正确，也远称不上一个好模型，需要继续优化。但是，如果选用 0-1 损失函数作为目标函数，只要分对了，该样本的损失就是 0 了，没办法计算梯度，也自然没办法进行反向传播来优化参数。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">选用交叉熵做损失函数就没有这个烦恼，你可以一直优化到信心水平无限接近 100%。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">而交叉熵函数与 0-1 损失这对组合的奇妙之处在于，即使<strong>测试集上的交叉熵过拟合了，分类误差也不会过拟合</strong>。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">几个月前，芝加哥大学的 Srebro 组的工作 [2] 证明了：对于单层线性网络来说，如果数据集噪声较小、可分，那么即使交叉熵过拟合了，分类误差也不会过拟合。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「这是一个非常优美的，角度独特的工作。在此之上，我们用微分方程动力系统理论的工具证明了，在全局最小值附近，深度网络表现得就像一个线性网络。因此，我们可以将 Srebro 工作的结果用在深度学习上，说明即使神经网络分类器的交叉熵过拟合了，分类器本身也不会过拟合。」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">交叉熵的这一性质是最小平方误差（least square error）等其他损失函数所不具备的，拥有这一性质的最简单的损失函数是指数误差（exponential loss）。而当我询问究竟是交叉熵的哪些特质让它拥有了如此特别的性质，是否和它的不对称性有关，Poggio 表示这仍然是一个有待讨论的问题。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">以上就是 Poggio 的「深度学习理论三部曲」的内容概要了，详情请参阅 [3-7]。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">除了他的工作本身，我们也和他聊了一些关于深度学习理论工作的其他问题：</span></p><p><br></p><p style="text-align: center;line-height: 1.75em;"><strong><span style="font-size: 16px;">平坦的极小值意味着好的泛化能力吗？一个观点转变</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">关于极小值的形状与泛化之间的关系，Poggio 说，他的观点转变了：「确实有学者在工作中表示，平坦是有利于泛化的。大概一年多以前我也曾经发表过类似的观点，但是我现在不再这么认为了。」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在关于优化的研究中，Poggio 证明了平坦确实会让优化过程变得容易，平坦的最小值也有更大可能是全局最小值。「但是我不觉得它和泛化之间有直接的联系，起码现在没有。如今对于泛化能力的研究，依赖于分类问题、依赖于损失函数的选择，却不依赖于平坦。Bengio 兄弟两人都参与的一篇论文就证明了，陡峭的极小值也是可以泛化的 [8]，因为你完全可以通过改变不同层的参数，在不改变网络的输入输出关系的前提下，让一个平坦的极小值变得陡峭。」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">另外，他也认为完全平坦的极小值是不存在的，起码对于现在这种以多项式网络为基础添加非线性的神经网络来说，是不存在的。「我们都知道，一旦多项式在一个解集上都为 0，那么这个多项式处处为 0，因此，我不觉得存在完全平坦的极小值了。」</span></p><p><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>对应用侧的建议：小心过拟合</strong></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">致力于应用深度学习算法的工程师们最经常对深度学习的理论研究者提出的一个问题就是：「你的工作很棒，但请问这能如何帮助我训练我的模型？」了解更多的理论知识当然具有启发意义，但是理论研究范围广阔且往往十分艰深，究竟哪些理论研究有助于应用开发者，应用开发者应该了解理论到何种程度？</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">机器学习里的无免费午餐定理（No Free Lunch Theorem），也就是 Wolpert 在 1996 和 1997 年发表的两篇著名论文里 [9, 10] 所提到的，学习算法之间没有先验区别，对于任何两个算法 A 和 B 来说，都存在一样多的两堆目标，对一堆目标来说 A 的检验误差比 B 高，对另一堆来说 B 的检验误差比 A 高。Poggio 援引了无免费午餐定理到理论研究中：不存在一个适用于所有问题的算法，类似地，也很难给出一个普适性正确的理论陈述。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「理论通常给出的是通常情况或最坏情况的分析，他们给出建议，告诉你应该做/不做什么，以避免最坏情况的发生。但是理论无法告诉你，对于一个特定案例来说，最佳方案是什么。」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">而他对今天的深度学习应用者的建议是，小心过拟合。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「在过去几十年的机器学习发展史中，我们学到的一课是，如果你的数据集没有大到排除过拟合可能性，那么在一个特定数据集上的最佳方法通常是过拟合的、无法扩展到其他数据集上的。并不是说学者们『偷看』了验证集测试集，而是当一个社区的学者都在用不同的方法进行试错，那么一段时间后的最佳做法通常是过拟合了这个特定数据集的。」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「我曾经是一名物理学研究者，在我的学生时代，最普遍的经验法则是，如果你想建立一个参数为 n 的模型，那么至少要有规模为 2n 的数据，如果你想从统计的角度得出一些结论，更为推荐的数据规模是 10n。然而现在的深度学习研究者倾向于对所有问题都套用有数百万参数的模型。我们得出的『深度学习模型不受过拟合困扰』的论证只适用于特定问题（分类）、且要求数据集质量良好（可分），因此深度学习研究者应该对过拟合持有更谨慎的态度。」</span></p><p><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>如何看待先验？理论研究能够告诉我们哪些关于先验的结论呢？</strong></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">人是很好的学习者，既不需要数百万数据，也不需要数据有标签，而这部分取决于我们与生俱来的、写在基因里的先验知识。然而，关于先天与后天（nature versus nurture）的争论从未停止。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「模型需要多少先验，是一个不能一概而论的、没有简单答案的问题。」Poggio 总结道，「理论研究的目的是找到能够做出特定预测所需的先验的下限。」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">他以回归问题举例，「对于给定一些数据点来恢复一条曲线的任务来说，如果你什么都不告诉我，那么除非给我曲线上的所有点，否则我基本上什么也做不了。<strong>连续</strong>是一个必须的先验，但这还不够。我起码需要类似<strong>平滑</strong>（smothness）这样的性质，才能进行预测。而最重要的还是数据量，样本复杂度和先验之间，存在一个权衡取舍的关系。」</span></p><p><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>深度学习能从人脑的学习过程中学到什么？</strong></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">三十年前，「深度学习之父」Geoffrey Hinton 用利于优化且计算高效的「反向传播」将整个领域带入了高速发展，而近年来，他则致力于寻找比反向传播更有可能在仿生学（bionics）上成立的结构。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">MIT 对于深度学习的研究素来与脑神经科学结合紧密，Poggio 是如何看待这一问题的呢？</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「我认为从生物学上完成反向传播并非完全不可能（not impossible），只能说，根据我们现在对神经元以及信号传递机制的了解，可能性不大（unlikely）。然而我认为真正不可能的是对所有样本的标注。」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「因此一个有趣的研究课题是，大脑是如何『绕开』标注的。例如，一个有趣的假设是，我们的视觉系统是以学习给图像『填色』来进行预训练的，它接收到了颜色信息，却只给其他视觉皮层以黑白的灰度信息，以此训练一个能够预测颜色的网络。在这个设定下，你不需要『神谕』（oracle）来告诉你真实的颜色是什么，你是有这部分信息的，只不过通过把它藏起来而建立了一个可以进行优化的模型。」</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「类似的假设还有，大脑在不断地预测下一帧影像并进行优化等等。而能够预测颜色的、预测下一帧影像的视觉系统，是不是能够更好地进行其他视觉任务呢？是不是能够利用更少的数据就能学会识别物体呢？这都是有趣的开放问题，而且一旦得到答案后，将对深度学习产生巨大的推动。」</span><img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibIcXJR7CpF13q7eQN4G4rQ9PjPXKp2USib0YaRWicX7YY085GYTqEoKBiatvwAm1lrDMgtiahUEicg3Mg/640?wx_fmt=png" data-type="png" data-w="73" style="width: 44px;height: 20px;"></p><p><br></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">Reference</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">1. Cucker, F., &amp; Smale, S. (2002). On the mathematical foundations of learning. Bulletin of the American mathematical society, 39(1), 1-49.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">2. Neyshabur, B., Tomioka, R., Salakhutdinov, R., &amp; Srebro, N. (2017). Geometry of optimization and implicit regularization in deep learning. arXiv preprint arXiv:1705.03071.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">3. Poggio, T., Mhaskar, H., Rosasco, L., Miranda, B., &amp; Liao, Q. (2017). Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review. International Journal of Automation and Computing, 14(5), 503-519.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">4. Liao, Q., &amp; Poggio, T. (2017). Theory of Deep Learning II: Landscape of the Empirical Risk in Deep Learning. arXiv preprint arXiv:1703.09833.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">5. Zhang, C., Liao, Q., Rakhlin, A., Miranda, B., Golowich, N., &amp; Poggio, T. (2018). Theory of Deep Learning IIb: Optimization Properties of SGD. arXiv preprint arXiv:1801.02254.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">6. Poggio, T., Kawaguchi, K., Liao, Q., Miranda, B., Rosasco, L., Boix, X., ... &amp; Mhaskar, H. (2017). Theory of Deep Learning III: explaining the non-overfitting puzzle. arXiv preprint arXiv:1801.00173.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">7. Zhang, C., Liao, Q., Rakhlin, A., Sridharan, K., Miranda, B., Golowich, N., &amp; Poggio, T. (2017). Theory of deep learning iii: Generalization properties of sgd. Center for Brains, Minds and Machines (CBMM).</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">8. Dinh, L., Pascanu, R., Bengio, S., &amp; Bengio, Y. (2017). Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">9. Wolpert, D. H. (1996). The lack of a priori distinctions between learning algorithms. Neural computation, 8(7), 1341-1390.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">10. Wolpert, D. H., &amp; Macready, W. G. (1997). No free lunch theorems for optimization. IEEE transactions on evolutionary computation, 1(1), 67-82.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"></span></p><p style="margin-bottom: 20px;font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心原创，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="806542775" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                <p class="media_tool_meta tips_global meta_primary article_modify_tag">修改于<span id="js_modify_time"></span></p>
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                <p class="media_tool_meta tips_global meta_primary article_modify_tag">修改于<span id="js_modify_time"></span></p>
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
