<div class="title-image"><img src="https://pic4.zhimg.com/v2-99130ac172d0b32f0107c8fc68b18872_b.jpg" alt=""></div><p><b>摘要</b></p><p>GMM 是研究 asset pricing 时绕不过的工具。本文介绍 GMM 框架的强大之处，并阐述其背后的数学之美。</p><h2><b>01 引言</b></h2><p>好了，来写写 <b>GMM（Generalized Method of Moments，广义矩估计）</b>吧，站在 asset pricing 的视角。</p><p>前文<a href="https://zhuanlan.zhihu.com/p/88458489" class="internal">《理解资产价格》</a>已经提到，Hansen (1982) 提出的 GMM 在 empirical asset pricing 研究的历史上起到了举足轻重的作用，而如今无论是在经济学领域还是金融学领域，GMM 因其数学上的优雅和特性上的强大都被广泛的运用。</p><p>今天这篇文章算是我自己关于 GMM 的一个学习笔记，而我的学习资料（公众号的老朋友一定猜到了）正是 John Cochrane 教授的神书 Asset Pricing（Cochrane 2005）以及他在 UChicago 时做的 Online 课程中对 GMM 的介绍。Cochrane 教授讲的实在是太清楚、到位了，本文是我做对他所讲的内容的消化、梳理和再串联。</p><p><b>本文的目标是试图从 intuition 出发揭示 GMM 蕴含数学之美；试图把公式掰开揉碎讲清楚从而帮助感兴趣的朋友理解大公式背后的本质。</b>我会力争在行文中做到非常准确，在 population 和 sample 之间腾挪、把 variance 和 test statistic 都说清楚。Cochrane 教授说，学习 GMM 时最大的障碍就是它的 notation（数学符号）繁多；只要搞清楚 notation，其实 GMM 背后的数学精髓是非常简单的，因为 GMM 的核心最终能够归结为计算 the variance of the sample mean。希望本文能够带给你这种恍然大悟之感。</p><p>先来剧透一下，本文希望传达以下三方面内容：</p><ol><li>GMM 的框架包括 model、estimate 以及 test 三部分；</li><li>学习 GMM 时，最大的障碍往往来自 notation；搞清楚 notation 后，GMM 背后的数学非常容易理解；</li><li>GMM 不应被当作计量学的黑箱。</li></ol><p><b>以我一贯的风格，行文中会“死磕”数学公式，因此这注定是一篇十分 technical 的文章。</b>本文的技术性远超<a href="https://zhuanlan.zhihu.com/p/40984029" class="internal">《股票多因子模型的回归检验》</a>，因此同样建议在一个能静下心来思考的心境下和环境中阅读。对于不关心数学、仅想快速了解 GMM 是什么的读者来说，我强烈推荐<a href="https://www.zhihu.com/question/41312883/answer/91484566?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=728969207022125056&amp;from=singlemessage&amp;isappinstalled=0" class="internal">慧航大神关于 GMM 的回答</a>。那篇回答对读者非常友好，涉及到的数学恰到好处，深入浅出的介绍了 GMM 的原理。</p><p>如果本文能对你理解 GMM 起到一点帮助，那完全是 Cochrane 教授的功劳；如果你看完后依然困惑，那一定也必须是怪我没写好……</p><p>Cochrane 教授说 GMM 的核心最终能够归结为计算 the variance of the sample mean；让我们就从 the variance of the sample mean 说起。</p><h2><b>02 Variance of the Sample Mean</b></h2><p>考虑某随机变量  <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 。假设它在某个样本内的取值为 0，-1，3，3，-3。我们可以很容易的算出样本均值：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cbar+u+%3D+%5Cmbox%7BE%7D_T%5Bu_t%5D%3D0.4" alt="\displaystyle\bar u = \mbox{E}_T[u_t]=0.4" eeimg="1"/> </p><p>上式中， <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D_T+%3D+%281%2FT%29%5Csum%28.%29" alt="\mbox{E}_T = (1/T)\sum(.)" eeimg="1"/> 表示对样本数据求平均， <img src="https://www.zhihu.com/equation?tex=%5Cbar+u" alt="\bar u" eeimg="1"/> 表示 <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 的样本均值。由于 <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 是一个随机变量，因此其<b>样本均值  </b><img src="https://www.zhihu.com/equation?tex=%5Cbar+u_t" alt="\bar u_t" eeimg="1"/> <b>本身也是一个随机变量</b>。虽然它在我们这个样本中的取值为 0.4，但如果我们能够乘坐时光机回到过去“重写历史”，得到不同的样本，那么在不同样本中，样本均值的取值也会有所变化。比如在下面这个表中，假设除样本一（就是上面这个样本）之外，还有三个样本，而它们的样本均值 <img src="https://www.zhihu.com/equation?tex=%5Cbar+u" alt="\bar u" eeimg="1"/> 的取值分别为 -0.8，0.6 和 1.6。</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-2d988b6aebb6c5ba1c5a23ccb4d1ad54_b.jpg" data-caption="" data-size="normal" data-rawwidth="713" data-rawheight="317" class="origin_image zh-lightbox-thumb" width="713" data-original="https://pic1.zhimg.com/v2-2d988b6aebb6c5ba1c5a23ccb4d1ad54_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-2d988b6aebb6c5ba1c5a23ccb4d1ad54_b.jpg" data-caption="" data-size="normal" data-rawwidth="713" data-rawheight="317" class="origin_image zh-lightbox-thumb lazy" width="713" data-original="https://pic1.zhimg.com/v2-2d988b6aebb6c5ba1c5a23ccb4d1ad54_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-2d988b6aebb6c5ba1c5a23ccb4d1ad54_b.jpg"/></figure><p>既然样本均值本身也是一个随机变量，那么一个很自然的问题就是样本均值在不同的样本中是如何变化的，即 variance of the sample mean。从 variance 的定义出发可得：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7Bvar%7D%28%5Cbar+u%29%3D%5Cmbox%7Bvar%7D%5Cleft%28%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bt%3D1%7D%5ET+u_t%5Cright%29%3D%5Cfrac%7B1%7D%7BT%5E2%7D%5Cmbox%7Bvar%7D%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET+u_t%5Cright%29" alt="\displaystyle\mbox{var}(\bar u)=\mbox{var}\left(\frac{1}{T}\sum_{t=1}^T u_t\right)=\frac{1}{T^2}\mbox{var}\left(\sum_{t=1}^T u_t\right)" eeimg="1"/> </p><p>在最简单的情况中，假设 <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 序列满足 i.i.d.，则上式可以简化成：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7Bvar%7D%28%5Cbar+u%29%3D%5Cfrac%7B1%7D%7BT%5E2%7D%5Cmbox%7Bvar%7D%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET+u_t%5Cright%29%3D%5Cfrac%7B%5Csigma%5E2%28u_t%29%7D%7BT%7D" alt="\displaystyle\mbox{var}(\bar u)=\frac{1}{T^2}\mbox{var}\left(\sum_{t=1}^T u_t\right)=\frac{\sigma^2(u_t)}{T}" eeimg="1"/> </p><p>把两边开方就得到样本均值的 standard error：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7Bs.e.%7D%28%5Cbar+u%29%3D%5Cfrac%7B%5Csigma%28u_t%29%7D%7B%5Csqrt%7BT%7D%7D" alt="\displaystyle\mbox{s.e.}(\bar u)=\frac{\sigma(u_t)}{\sqrt{T}}" eeimg="1"/> </p><p>这大概是我们在统计课中学到的印象最深的一个式子（假设 <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 满足 i.i.d. 条件下样本均值的 standard error）。在更一般的情况中 —— 尤其是在金融数据（比如收益率）数据中 ——  <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/>  序列是前后是有非零的自相关的，即 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bcov%7D%28u_t%2C+u_%7Bt-j%7D%29+%E2%89%A0+0" alt="\mbox{cov}(u_t, u_{t-j}) ≠ 0" eeimg="1"/> ，因此需要得到更一般下样本均值  <img src="https://www.zhihu.com/equation?tex=%5Cbar+u" alt="\bar u" eeimg="1"/>  的 variance：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7Bvar%7D%28%5Cbar+u%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%5B%5Cmbox%7Bvar%7D%28u_t%29%2B%5Csum_%7Bj%3D1%7D%5ET%5Cfrac%7BT-j%7D%7BT%7D%5Cleft%28%5Cmbox%7Bcov%7D%28u_t%2C+u_%7Bt-j%7D%29%2B%5Cmbox%7Bcov%7D%28u_t%2C+u_%7Bt%2Bj%7D%29%5Cright%29%5Cright%5D" alt="\displaystyle\mbox{var}(\bar u)=\frac{1}{T}\left[\mbox{var}(u_t)+\sum_{j=1}^T\frac{T-j}{T}\left(\mbox{cov}(u_t, u_{t-j})+\mbox{cov}(u_t, u_{t+j})\right)\right]" eeimg="1"/> </p><p>当  <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/>  趋于无穷大时（即 sample size 越来越大）， <img src="https://www.zhihu.com/equation?tex=%28T+%E2%80%93+j%29%2FT" alt="(T – j)/T" eeimg="1"/> 趋于 1，就可以求出 <img src="https://www.zhihu.com/equation?tex=var%28%5Cbar+u%29" alt="var(\bar u)" eeimg="1"/> 的渐进（asymptotic）形式：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7Bvar%7D%28%5Cbar+u%29%5Crightarrow%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bj%3D-%5Cinfty%7D%5E%7B%5Cinfty%7D%5Cmbox%7Bvar%7D%28u_t%2C+u_%7Bt-j%7D%29" alt="\displaystyle\mbox{var}(\bar u)\rightarrow\frac{1}{T}\sum_{j=-\infty}^{\infty}\mbox{var}(u_t, u_{t-j})" eeimg="1"/> </p><p>下面再假设一个特殊的情况，即随机变量 <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 的<b>总体均值</b> <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5Bu_t%5D+%3D+0" alt="\mbox{E}[u_t] = 0" eeimg="1"/> ，并利用方差的定义  <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28X%2C+Y%29+%3D+%5Cmbox%7BE%7D%5BXY%5D+%E2%80%93+%5Cmbox%7BE%7D%5BX%5D%5Cmbox%7BE%7D%5BY%5D" alt="\mbox{var}(X, Y) = \mbox{E}[XY] – \mbox{E}[X]\mbox{E}[Y]" eeimg="1"/>  可得：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7Bvar%7D%28%5Cbar+u%29%5Crightarrow%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bj%3D-%5Cinfty%7D%5E%7B%5Cinfty%7D%5Cmbox%7BE%7D%5Bu_tu_%7Bt-j%7D%5D%3D%5Cfrac%7B1%7D%7BT%7DS" alt="\displaystyle\mbox{var}(\bar u)\rightarrow\frac{1}{T}\sum_{j=-\infty}^{\infty}\mbox{E}[u_tu_{t-j}]=\frac{1}{T}S" eeimg="1"/> </p><p>上式最后一项中的  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  代表了中间项中那一大坨求和项。在 GMM 的术语中， <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  被称作 spectral density matrix at frequency zero of <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 。</p><p>OK！整理一下。本小节从我们熟悉的样本均值出发指出样本均值本身也是一个随机变量，并推导出当 sample size <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/>  趋于无穷且假设 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5Bu_t%5D+%3D+0" alt="\mbox{E}[u_t] = 0" eeimg="1"/> 时，variance of the sample mean 渐进趋于 <img src="https://www.zhihu.com/equation?tex=S%2FT" alt="S/T" eeimg="1"/> ，其中  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  是无穷级数求和 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Cmbox%7BE%7D%5Bu_tu_%7Bt-j%7D%5D" alt="\sum\mbox{E}[u_tu_{t-j}]" eeimg="1"/> 。</p><p>千万不要小看这个 <img src="https://www.zhihu.com/equation?tex=var%28%5Cbar+u%29+%5Crightarrow+S%2FT" alt="var(\bar u) \rightarrow S/T" eeimg="1"/> 这个式子，它在下文 GMM 的数学推导中起着至关重要的作用。用 Cochrane 的话说，<b>GMM 中大绝大部分计量学均可归结到这个式子（most econometrics boils down to this）！</b></p><h2><b>03 GMM 框架</b></h2><p>回顾了 variance of the sample mean 之后，本小节就来直观的看看 GMM 到底是怎么回事儿。</p><p><b>GMM 的作用是为了检验模型。</b>模型到底对不对？模型的参数如何估计？误差是来自运气还是因为模型有误？GMM 提供了一个优雅而强大的计量学框架来回答这些问题。一般来说，GMM 框架分为以下三个部分：</p><ol><li><b>第一部分：把待研究的问题表达成一系列总体矩条件（population moment conditions） ——</b> <b>这是提出 model；</b></li><li><b>第二部分：使用样本数据得到对应的样本矩（sample moments），从而对参数进行估计 ——</b> <b>这是把 model 和 data 联系起来；</b></li><li><b>第三部分：计算 sample moments 的 variance，从而对 population moments 进行 statistical test ——</b> <b>这是检验 model。</b></li></ol><p><b>所以概括来说就是 GMM 就是用 sample moments 代替 population moments 然后对 population moments 进行统计检验。</b></p><p><b>3.1 GMM 第一部分</b></p><p>我们用  <img src="https://www.zhihu.com/equation?tex=x_t" alt="x_t" eeimg="1"/>  代表 data， <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/>  代表参数（这些都是 vectors），且存在一系列关于 <img src="https://www.zhihu.com/equation?tex=x_t" alt="x_t" eeimg="1"/> 和 b 的函数 <img src="https://www.zhihu.com/equation?tex=f%28x_t%2C+b%29" alt="f(x_t, b)" eeimg="1"/> 。 <img src="https://www.zhihu.com/equation?tex=f%28x_t%2C+b%29" alt="f(x_t, b)" eeimg="1"/> <b>的 expected value 即 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5Bf%28x_t%2C+b%29%5D" alt="\mbox{E}[f(x_t, b)]" eeimg="1"/> 就是 population moments。</b>在 GMM 中，我们要求 population moments 满足 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5Bf%28x_t%2C+b%29%5D+%3D+0" alt="\mbox{E}[f(x_t, b)] = 0" eeimg="1"/> 的约束，<b>这一系列 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5Bf%28x_t%2C+b%29%5D+%3D+0" alt="\mbox{E}[f(x_t, b)] = 0" eeimg="1"/><b> 约束就是 GMM 中的 population moment conditions（矩条件）。这些 moment conditions 是我们关于真实模型（true model）的猜测。</b></p><p><b>需要说明的是，期望符号 </b> <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D" alt="\mbox{E}" eeimg="1"/> <b> 表示对总体求均值；而前面使用的（接下来也将会继续使用的）期望符号 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D_T" alt="\mbox{E}_T" eeimg="1"/> <b>（有个下标 </b><img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> <b>）表示对样本求均值。</b></p><p><b>GMM 的第一部分是把待研究的问题转化成数据 </b> <img src="https://www.zhihu.com/equation?tex=x_t" alt="x_t" eeimg="1"/> <b> 和参数 </b><img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/><b> 的一系列方程 </b><img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"/> <b>、且假设在 true model 下这些方程的 moments 满足 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5Bf%28x_t%2C+b%29%5D+%3D+0" alt="\mbox{E}[f(x_t, b)] = 0" eeimg="1"/> <b>。</b></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Brll%7D+%5Cmbox%7BExpress+Model%7D%26%5Crightarrow%26%5Cdisplaystyle%5Cmbox%7BMean+of+%7Df%28%5Cmbox%7BData%2C+Parameter%7D%29%3D0%5C%5C+%26%5Crightarrow%26%5Cdisplaystyle%5Cmbox%7BE%7D%5Bf%28x_t%2C+b%29%5D%3D0+%5Cend%7Barray%7D+" alt="\begin{array}{rll} \mbox{Express Model}&amp;\rightarrow&amp;\displaystyle\mbox{Mean of }f(\mbox{Data, Parameter})=0\\ &amp;\rightarrow&amp;\displaystyle\mbox{E}[f(x_t, b)]=0 \end{array} " eeimg="1"/> </p><p>仍然晦涩？马上来看一些 asset pricing 中的例子。</p><p>从最基础的 <img src="https://www.zhihu.com/equation?tex=p+%3D+%5Cmbox%7BE%7D%5Bmx%5D" alt="p = \mbox{E}[mx]" eeimg="1"/> 出发（详见<a href="https://zhuanlan.zhihu.com/p/88458489" class="internal">《理解资产价格》</a>），其中 m 是 stochastic discount factor（由某些未知参数 <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/> 决定）、 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"/>  是回报、 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/>  是价格。如果把 <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"/> 换成超额收益（用 <img src="https://www.zhihu.com/equation?tex=R%5Ee" alt="R^e" eeimg="1"/> 表示）则有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7BE%7D%5Bm%28b%29R%5Ee%5D%3D0" alt="\displaystyle\mbox{E}[m(b)R^e]=0" eeimg="1"/> </p><p>如果把  <img src="https://www.zhihu.com/equation?tex=x" alt="x" eeimg="1"/>  换成 gross risk-free rate  <img src="https://www.zhihu.com/equation?tex=R_f" alt="R_f" eeimg="1"/> （即 <img src="https://www.zhihu.com/equation?tex=t" alt="t" eeimg="1"/> 时刻投入 1， <img src="https://www.zhihu.com/equation?tex=t+%2B+1" alt="t + 1" eeimg="1"/> 时刻得到  <img src="https://www.zhihu.com/equation?tex=R_f" alt="R_f" eeimg="1"/> ）则有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7BE%7D%5Bm%28b%29R_f-1%5D%3D0" alt="\displaystyle\mbox{E}[m(b)R_f-1]=0" eeimg="1"/> </p><p>这些都是 asset pricing 中常见的 moment conditions。</p><p><b>3.2 GMM 第二部分</b></p><p>第一部分虽然把问题描述清楚了，但它们都是 population moment conditions，只是我们对于真实模型的猜想，我们有的只是样本数据。<b>GMM 的第二部分就是用 sample moments 来代替 population moments，从而建立起模型和数据之间的联系，以进行参数估计。</b></p><p>和本文第二节一样，用  <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D_T" alt="\mbox{E}_T" eeimg="1"/>  代表对样本数据求平均，则 sample moments 可以写成：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7BE%7D_T%5Bf%28x_t%2C+b%29%5D%3D%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bt%3D1%7D%5ET+f%28x_t%2C+b%29%5Cequiv+g_T%28b%29" alt="\displaystyle\mbox{E}_T[f(x_t, b)]=\frac{1}{T}\sum_{t=1}^T f(x_t, b)\equiv g_T(b)" eeimg="1"/> </p><p>上式中最后引入符号 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 仅仅是为了下文中简化公式。怎么样？看着这个式子有没有什么感想？无论研究的具体问题是什么（我们研究的是 asset pricing，而别人也可以研究经济学或金融学中其他的问题），不管 <img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"/> 到底长什么样子或数据 <img src="https://www.zhihu.com/equation?tex=x_t" alt="x_t" eeimg="1"/> 和参数 <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/> 向量都是什么，<b>上面的 sample moments 也仅仅是 </b> <img src="https://www.zhihu.com/equation?tex=f%28x_t%2C+b%29" alt="f(x_t, b)" eeimg="1"/> <b> 在样本内取平均，因此也只是一种 sample mean！</b></p><p>从 sample moments 出发就可以进行参数估计。来自总体的 moment conditions 要求  <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5Bf%28x_t%2C+b%29%5D+%3D+0" alt="\mbox{E}[f(x_t, b)] = 0" eeimg="1"/> ；使用样本数据，GMM estimator 的核心是找到  <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/>  的估计 —— 记为 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> —— 使得所有 sample moments 都尽可能的等于零：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7BE%7D_T%5Bf%28x_t%2C+%5Chat+b%29%5D%5Csimeq+0" alt="\displaystyle\mbox{E}_T[f(x_t, \hat b)]\simeq 0" eeimg="1"/> </p><p>上式中之所以用了约等于而非等于，是因为<b>在实际问题中，sample moments 的个数往往超过参数的个数（这也被称为 overidentification）。</b>假设一共有 n 个 moments（即 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 是 <img src="https://www.zhihu.com/equation?tex=n+%C3%97+1" alt="n × 1" eeimg="1"/> 阶 vector）， <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/>  个参数（即  <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/>  是 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+1" alt="p × 1" eeimg="1"/> 阶 vector）。当 <img src="https://www.zhihu.com/equation?tex=n+%3E+p" alt="n &gt; p" eeimg="1"/> 时，我们无法让所有的 sample moments 都等于零，而是选择让这其中的  <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/>  个 sample moments 或者这些 sample moments 的 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/> 个线性组合等于 0。这就是 GMM estimator：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Chat+b%3A+%5Cmbox%7Bset+%7D+ag_T%28%5Chat+b%29%3D0" alt="\displaystyle\hat b: \mbox{set } ag_T(\hat b)=0" eeimg="1"/> </p><p>上式中， <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  是 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+n" alt="p × n" eeimg="1"/> 阶矩阵，每一行都代表一个 sample moments 的线性组合。在具体问题中，根据 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 的具体形式，上式可能有解析解或数值解。求解上式就可以获得 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 。不过有的小伙伴可能会说：<b>等一下，你还没说矩条件的线性组合矩阵 </b><img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/><b> 是什么！不同的 </b><img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/><b> 显然会得到不同的参数估计。</b>没错，在 GMM 的框架下，我们可以自由选择  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 。然而，纯从计量学的角度，有一个特殊的矩阵  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  会让 GMM estimator 成为 efficient estimator。下文第 3.3 节和第 5 节将会就 efficiency 进行说明。</p><p>为了加深理解，仍然用 asset pricing 来举例子。假设 consumption-based CAPM（CCAPM）是真正的模型，因此随机折现因子 <img src="https://www.zhihu.com/equation?tex=m" alt="m" eeimg="1"/> 由两个参数 <img src="https://www.zhihu.com/equation?tex=%CE%B2" alt="β" eeimg="1"/> 和  <img src="https://www.zhihu.com/equation?tex=%CE%B3" alt="γ" eeimg="1"/>  决定（CCAPM 的介绍请见<a href="https://zhuanlan.zhihu.com/p/88458489" class="internal">《理解资产价格》</a>），即 <img src="https://www.zhihu.com/equation?tex=b%3D%5B%5Cbeta%2C%5Cgamma%5D%5E%5Cprime" alt="b=[\beta,\gamma]^\prime" eeimg="1"/> ；进一步假设我们有四个资产来检验 CCAPM，它们是 risk-free、市场组合以及 Fama and French (1993) 中的 HML 和 SMB。在这个例子中， <img src="https://www.zhihu.com/equation?tex=n+%3D+4" alt="n = 4" eeimg="1"/> 而 <img src="https://www.zhihu.com/equation?tex=p+%3D+2" alt="p = 2" eeimg="1"/> ，因此 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 是一个 <img src="https://www.zhihu.com/equation?tex=2+%C3%97+4" alt="2 × 4" eeimg="1"/> 阶矩阵，而 GMM estimator 可以写成：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcccc%7D+a_%7B11%7D%26a_%7B12%7D%26a_%7B13%7D%26a_%7B14%7D%5C%5C+a_%7B21%7D%26a_%7B22%7D%26a_%7B23%7D%26a_%7B24%7D+%5Cend%7Barray%7D+%5Cright%5D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+%5Cmbox%7BE%7D_T%5Bm%28%5Chat%5Cbeta%2C%5Chat%5Cgamma%29R%5Ee_m%5D%5C%5C+%5Cmbox%7BE%7D_T%5Bm%28%5Chat%5Cbeta%2C%5Chat%5Cgamma%29R_f-1%5D%5C%5C+%5Cmbox%7BE%7D_T%5Bm%28%5Chat%5Cbeta%2C%5Chat%5Cgamma%29R%5E%7B%5Cmbox%7BHML%7D%7D%5D%5C%5C+%5Cmbox%7BE%7D_T%5Bm%28%5Chat%5Cbeta%2C%5Chat%5Cgamma%29R%5E%7B%5Cmbox%7BSMB%7D%7D%5D+%5Cend%7Barray%7D+%5Cright%5D%3D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bc%7D+0%5C%5C0+%5Cend%7Barray%7D+%5Cright%5D" alt="\left[ \begin{array}{cccc} a_{11}&amp;a_{12}&amp;a_{13}&amp;a_{14}\\ a_{21}&amp;a_{22}&amp;a_{23}&amp;a_{24} \end{array} \right] \left[ \begin{array}{l} \mbox{E}_T[m(\hat\beta,\hat\gamma)R^e_m]\\ \mbox{E}_T[m(\hat\beta,\hat\gamma)R_f-1]\\ \mbox{E}_T[m(\hat\beta,\hat\gamma)R^{\mbox{HML}}]\\ \mbox{E}_T[m(\hat\beta,\hat\gamma)R^{\mbox{SMB}}] \end{array} \right]= \left[ \begin{array}{c} 0\\0 \end{array} \right]" eeimg="1"/> </p><p>根据上式就可以使用 sample moments 求出参数估计 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 。</p><p><b>3.3 GMM 第三部分</b></p><p>使用 GMM estimator 得到的 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 仅仅是真实但未知参数 <img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/> 的一个估计。从统计学的角度，我们自然关心估计的误差，即 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 。马上来回答上面遗留的矩阵 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 的选择的问题。<b>对于给定的 moments </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> <b>，从计量学的角度有一个特殊的矩阵 </b><img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/><b> 使得 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/><b> 最小，这就是 efficient 的含义。</b>Hansen (1982) 给出了这个  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  的形式。关于 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 的进一步讨论将放在本文第五节。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BVar%7D%28%5Chat+b%29" alt="\mbox{Var}(\hat b)" eeimg="1"/>  的大小仅仅告诉我们参数估计是否准确，而对于研究的问题来说，我们更加关注的是当给定 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 时，sample moments 的 variance，记为 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> ，的大小。从业务上说，在一般的 overidentification 问题下（moments 个数多于参数个数），sample moments 不可能都是零（如果 moments 个数  <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/>  等于参数个数  <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/> ，我们可以令每个 moment 都等于零从而求出全部  <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/>  个参数），因此我们关心 sample moments 联合起来相对于零的偏离的大小是多少。</p><p><b>我们必须搞清楚 </b><img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat+b%29" alt="g_T(\hat b)" eeimg="1"/><b> 联合起来相对于零的偏离是因为运气成分还是因为选择的 population moment condition 就是错的。</b>如果仅仅因为运气（即偏离的很小），那可以接受 population moment conditions —— 比如接受一个选择的 asset pricing 模型；如果不是因为运气（即偏离很大），那就只能拒绝 population moment conditions —— 即 reject 一个 asset pricing 模型。这就是 statistical test。唯有有了 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> ，才能够进行 statistical test。<b>计算 variance 并进行 statistical test 就是 GMM 的第三部分。</b></p><p>值得一提的是，由于 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat+b%29" alt="g_T(\hat b)" eeimg="1"/> 都是向量，因此 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 事实上都代表了它们各自的 <b>variance-covariance matrix</b>，其中 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 是 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+p" alt="p × p" eeimg="1"/> 阶矩阵（共有 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/> 个参数），而 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 是 <img src="https://www.zhihu.com/equation?tex=n+%C3%97+n" alt="n × n" eeimg="1"/> 阶矩阵（共有  <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/>  个 moments）。</p><p>有了 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 就可以写出 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat+b%29" alt="g_T(\hat b)" eeimg="1"/> 的分布。当 sample size  <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/>  趋于无穷时， <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 的满足以下渐进正态性：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Csqrt%7BT%7D%28%5Chat+b-b_0%29%5Crightarrow%5Cmathcal%7BN%7D%5Cleft%5B0%2C+%28ad%29%5E%7B-1%7DaSa%5E%5Cprime%28ad%29%5E%7B-1%5Cprime%7D%5Cright%5D" alt="\displaystyle\sqrt{T}(\hat b-b_0)\rightarrow\mathcal{N}\left[0, (ad)^{-1}aSa^\prime(ad)^{-1\prime}\right]" eeimg="1"/> </p><p>上式中， <img src="https://www.zhihu.com/equation?tex=-1" alt="-1" eeimg="1"/> 表示求逆， <img src="https://www.zhihu.com/equation?tex=%5Cprime" alt="\prime" eeimg="1"/> 表示转置，所以 <img src="https://www.zhihu.com/equation?tex=%28ad%29%5E%7B-1%5Cprime%7D" alt="(ad)^{-1\prime}" eeimg="1"/> 表示先求 <img src="https://www.zhihu.com/equation?tex=ad" alt="ad" eeimg="1"/> 的逆矩阵再转置。这个式子正是 Hansen (1982) 中的 Theorem 3.1。Hansen (1982) 给出了渐进分布成立需要满足的一系列假设。在实际应用中，我们需要记住的是<b>数据 </b><img src="https://www.zhihu.com/equation?tex=x_t" alt="x_t" eeimg="1"/><b> 需要满足弱平稳性，这是因为 GMM 的基础是随着 </b><img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/><b> 的增大，sample mean 向 population mean 收敛。</b></p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-45f6d1441ef969c244596d5bd9777482_b.jpg" data-caption="" data-size="normal" data-rawwidth="904" data-rawheight="116" class="origin_image zh-lightbox-thumb" width="904" data-original="https://pic3.zhimg.com/v2-45f6d1441ef969c244596d5bd9777482_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-45f6d1441ef969c244596d5bd9777482_b.jpg" data-caption="" data-size="normal" data-rawwidth="904" data-rawheight="116" class="origin_image zh-lightbox-thumb lazy" width="904" data-original="https://pic3.zhimg.com/v2-45f6d1441ef969c244596d5bd9777482_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-45f6d1441ef969c244596d5bd9777482_b.jpg"/></figure><p>此外， <img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat+b%29" alt="g_T(\hat b)" eeimg="1"/> 满足如下渐进正态性：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Csqrt%7BT%7Dg_T%28%5Chat+b%29%5Crightarrow%5Cmathcal%7BN%7D%5Cleft%5B0%2C%28I-d%28ad%29%5E%7B-1%7Da%29S%28I-d%28ad%29%5E%7B-1%7Da%29%5E%5Cprime%5Cright%5D" alt="\displaystyle\sqrt{T}g_T(\hat b)\rightarrow\mathcal{N}\left[0,(I-d(ad)^{-1}a)S(I-d(ad)^{-1}a)^\prime\right]" eeimg="1"/> </p><p>上式中， <img src="https://www.zhihu.com/equation?tex=I" alt="I" eeimg="1"/>  是 <img src="https://www.zhihu.com/equation?tex=n+%C3%97+n" alt="n × n" eeimg="1"/> 阶单位阵。这个式子正是 Hansen (1982) 中的 Lemma 4.1。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-79d44f2d157583534df26f3fd996edf6_b.png" data-caption="" data-size="normal" data-rawwidth="911" data-rawheight="106" class="origin_image zh-lightbox-thumb" width="911" data-original="https://pic3.zhimg.com/v2-79d44f2d157583534df26f3fd996edf6_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-79d44f2d157583534df26f3fd996edf6_b.png" data-caption="" data-size="normal" data-rawwidth="911" data-rawheight="106" class="origin_image zh-lightbox-thumb lazy" width="911" data-original="https://pic3.zhimg.com/v2-79d44f2d157583534df26f3fd996edf6_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-79d44f2d157583534df26f3fd996edf6_b.png"/></figure><p>看到这里，你大概在想：What the hell?! 这又  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  又  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  又  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  又求逆又转置，这都是什么“牛鬼蛇神”？有一种“每个字都认识、但是放在一起就看不懂了”的既视感。这里的 a 就是上面 sample moments 的线性组合矩阵，但是  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  和  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  还没有介绍。别着急，第四节将会把这些式子掰开了、揉碎了说清楚的。</p><p>看到  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  你是否想到什么？没错，本文第二节讲 the variance of the sample mean 的时候提到了  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> 。而上面 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 看似无比复杂，但它们的本质也都离不开 the variance of the sample mean！</p><p>在有了 <img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat+b%29" alt="g_T(\hat b)" eeimg="1"/> 的分布后，就可以对 GMM 第一部分中选择的模型进行检验，从而决定是接受还是拒绝它。以 asset pricing 为例，这些 moments 代表了不同资产或投资组合的 pricing errors。我们关心 pricing errors 是否联合起来显著不为零，这时可以用 <img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat+b%29" alt="g_T(\hat b)" eeimg="1"/> 的分布构建 chi-squared statistic 来检验。如果 test statistic 超过给定显著性水平的阈值，那么我们就可以拒绝该 asset pricing 模型。</p><p>总结一下，本小节介绍了 GMM 的三部分：</p><ol><li><b>第一部分是把关心的问题表述成一组 population moment conditions；</b></li><li><b>第二部分是用 sample moments 代替 population moments 从而把样本数据和模型联系起来，并进行参数估计；</b></li><li><b>第三部分是计算 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/><b> 和 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> <b>，从而进行 statistical test，决定是否接受第一部分中的模型。</b></li></ol><p>下一节就来看看 statistical test 背后的数学基础。</p><h2><b>04 数学基础</b></h2><p>OK! Let&#39;s do the math!</p><p>本节的目标是解释 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 里面的那些  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 、 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> 、 <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> 、求逆以及转置。我会力争把所有涉及到的公式都讲清楚。了解本节的内容无疑会更好的理解 GMM 背后的数学之美（汗），但是从阅读的角度，跳过本小节也不影响对后文的理解。</p><p>先说  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> ，这是一切的核心。从下式出发来解释  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> ：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+g_T%28b_0%29%5Cequiv%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bt%3D1%7D%5ET+f%28x_t%2C+b_0%29%3D%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bt%3D1%7D%5ET+U_t" alt="\displaystyle g_T(b_0)\equiv\frac{1}{T}\sum_{t=1}^T f(x_t, b_0)=\frac{1}{T}\sum_{t=1}^T U_t" eeimg="1"/> </p><p>上式中第一个等价符号是 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 的定义（参考 3.2 节），第二个等号是使用 <img src="https://www.zhihu.com/equation?tex=U_t" alt="U_t" eeimg="1"/> 来代表 <img src="https://www.zhihu.com/equation?tex=f%28x_t%2C+b_0%29" alt="f(x_t, b_0)" eeimg="1"/> 。<b>需要强调的是，上式中 </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 的参数是真实（但未知）的参数 </b><img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/> <b>。而</b> <img src="https://www.zhihu.com/equation?tex=+g_T%28b_0%29" alt=" g_T(b_0)" eeimg="1"/><b> 的方差 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/><b> 就表示 sample moments </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 在真实参数</b> <img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/> <b>下的 sampling variance。无论 </b> <img src="https://www.zhihu.com/equation?tex=f" alt="f" eeimg="1"/> <b> 长什么样子，sample moments 的数学形式都仅仅取平均，因此 </b> <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> <b> 正是 the variance of the sample mean！</b>（这就是本文第二节的价值所在。）</p><p>上面之所以用了 <img src="https://www.zhihu.com/equation?tex=U_t" alt="U_t" eeimg="1"/> ，一是为了简化表达式，二是为了和本文第二节中的小写 <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 呼应起来：这里的 <img src="https://www.zhihu.com/equation?tex=U_t" alt="U_t" eeimg="1"/> 对应第二节的 <img src="https://www.zhihu.com/equation?tex=u_t" alt="u_t" eeimg="1"/> 、这里的 <img src="https://www.zhihu.com/equation?tex=g_T%28b_0%29" alt="g_T(b_0)" eeimg="1"/> 就是第二节的 <img src="https://www.zhihu.com/equation?tex=%5Cbar+u" alt="\bar u" eeimg="1"/> ，因此马上得到（当  <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/>  趋于无穷）：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29%5Crightarrow%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bj%3D-%5Cinfty%7D%5E%7B%5Cinfty%7D%5Cmbox%7BE%7D%5BU_tU_%7Bt-j%7D%5E%5Cprime%5D%3D%5Cfrac%7B1%7D%7BT%7DS" alt="\displaystyle \mbox{var}(g_T(b_0))\rightarrow\frac{1}{T}\sum_{j=-\infty}^{\infty}\mbox{E}[U_tU_{t-j}^\prime]=\frac{1}{T}S" eeimg="1"/> </p><p><b>这正是 </b> <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> <b> 的定义</b>（实际中，它可以用样本数据来估计）。上面的计算中之所以能把方差和协方差写成 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5BXY%5D" alt="\mbox{E}[XY]" eeimg="1"/> 的形式是因为我们假设真实模型满足 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D%5Bf%28x_t%2C+b_0%29%5D+%3D+%5Cmbox%7BE%7D%5BU_t%5D+%3D+0" alt="\mbox{E}[f(x_t, b_0)] = \mbox{E}[U_t] = 0" eeimg="1"/> 。（预期符号 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7BE%7D" alt="\mbox{E}" eeimg="1"/> 没有下标 <img src="https://www.zhihu.com/equation?tex=T" alt="T" eeimg="1"/> ，表示 population expectation。）</p><p>重要的事情说三遍：</p><ul><li><b>上面求的 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/><b> 是 </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 在真实参数 </b><img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/><b>、而非估计量 </b><img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/><b> 下的 variance。</b></li><li><b>上面求的</b> <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> <b>是 </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 在真实参数 </b><img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/><b>、而非估计量 </b><img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/><b> 下的 variance。</b></li><li><b>上面求的</b> <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> <b>是 </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 在真实参数 </b><img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/><b>、而非估计量 </b><img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/><b> 下的 variance。</b></li></ul><p>当然，我们最终关心的是当 <img src="https://www.zhihu.com/equation?tex=b+%3D+%5Chat+b" alt="b = \hat b" eeimg="1"/> 时 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 的方差，即 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 。然而，一旦有了 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29+%3D+S%2FT" alt="\mbox{var}(g_T(b_0)) = S/T" eeimg="1"/> ，计算 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 以及 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 就变得迎刃而解。这就是为什么 Cochrane 教授说一切都可以归结为计算 the variance of the sample mean。</p><p>接下来就看看 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 如何计算。由 GMM estimator 可知， <img src="https://www.zhihu.com/equation?tex=ag_T%28%5Chat+b%29+%3D+0" alt="ag_T(\hat b) = 0" eeimg="1"/> 。将该式在真实参数 <img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/> 进行一阶泰勒展开有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+ag_T%28b_0%29%2Ba%5Cfrac%7B%5Cpartial+g_T%7D%7B%5Cpartial+b%5E%5Cprime%7D%28%5Chat+b-b_0%29%3D0" alt="\displaystyle ag_T(b_0)+a\frac{\partial g_T}{\partial b^\prime}(\hat b-b_0)=0" eeimg="1"/> </p><p>细心的小伙伴可能注意到了一阶偏导数  <img src="https://www.zhihu.com/equation?tex=%E2%88%82g_T%2F%E2%88%82b%5E%5Cprime" alt="∂g_T/∂b^\prime" eeimg="1"/>  的分母中 <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/> 右上角有个十分诡异的转置符号。在计算偏导数时， <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 是一个 <img src="https://www.zhihu.com/equation?tex=n+%C3%97+1" alt="n × 1" eeimg="1"/> 阶向量（ <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/>  个 moments），而  <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/>  是一个 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+1" alt="p × 1" eeimg="1"/> 阶向量（ <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/> 个参数），因此偏导数其实是一个矩阵（要么 <img src="https://www.zhihu.com/equation?tex=n+%C3%97+p" alt="n × p" eeimg="1"/> 阶、要么 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+n" alt="p × n" eeimg="1"/> 阶），而这类运算属于 matrix calculus。当转置符号出现在分母时，得到的矩阵是 <img src="https://www.zhihu.com/equation?tex=n+%C3%97+p" alt="n × p" eeimg="1"/> 阶，即每一行代表一个 moment，这种排列方式称作 numerator layout，也称作 Jacobian formulation。而这个一阶偏导数矩阵也正是我们的  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> ：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+d%5Cequiv%5Cmbox%7BE%7D%5Cleft%5B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+b%5E%5Cprime%7D%28x_t%2C+b%29%5Cright%5D%3D%5Cfrac%7B%5Cpartial+g_T%28b%29%7D%7B%5Cpartial+b%5E%5Cprime%7D" alt="\displaystyle d\equiv\mbox{E}\left[\frac{\partial f}{\partial b^\prime}(x_t, b)\right]=\frac{\partial g_T(b)}{\partial b^\prime}" eeimg="1"/> </p><p>严格来说， <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> 应该由 population moments 的一阶导数计算（上面的第一个等价条件）；但在应用中， <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> 的取值用 sample moments 和 <img src="https://www.zhihu.com/equation?tex=b+%3D+%5Chat+b" alt="b = \hat b" eeimg="1"/> 来估计（上面的第二的等式）。用  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  替换  <img src="https://www.zhihu.com/equation?tex=%E2%88%82g_T%2F%E2%88%82b%5E%5Cprime" alt="∂g_T/∂b^\prime" eeimg="1"/>  并代入上面的泰勒展开，进行简单的代数运算可得：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Brll%7D+%26ag_T%28b_0%29%2Bad%28%5Chat+b-b_0%29%3D0%5C%5C+%5CRightarrow%26%5Chat+b-b_0%3D-%28ad%29%5E%7B-1%7Dag_T%28b_0%29+%5Cend%7Barray%7D" alt="\begin{array}{rll} &amp;ag_T(b_0)+ad(\hat b-b_0)=0\\ \Rightarrow&amp;\hat b-b_0=-(ad)^{-1}ag_T(b_0) \end{array}" eeimg="1"/> </p><p>上式两边直接求 variance 就得到 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b-b_0%29" alt="\mbox{var}(\hat b-b_0)" eeimg="1"/> 。值得一提的是，上式右侧的 <img src="https://www.zhihu.com/equation?tex=%28ad%29%5E%7B-1%7Da" alt="(ad)^{-1}a" eeimg="1"/> 是系数矩阵而 <img src="https://www.zhihu.com/equation?tex=g_T%28b_0%29" alt="g_T(b_0)" eeimg="1"/> 的 variance 我们之前已经求出来了 —— 没错正是 <img src="https://www.zhihu.com/equation?tex=S%2FT" alt="S/T" eeimg="1"/> 。因此有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Cmbox%7Bvar%7D%28%5Chat+b-b_0%29%3D%5Cfrac%7B1%7D%7BT%7D%28ad%29%5E%7B-1%7DaSa%5E%5Cprime%28ad%29%5E%7B-1%5Cprime%7D" alt="\displaystyle\mbox{var}(\hat b-b_0)=\frac{1}{T}(ad)^{-1}aSa^\prime(ad)^{-1\prime}" eeimg="1"/> </p><p>数学上虽然通过泰勒展开顺理成章的从 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> 得到了 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b-b_0%29" alt="\mbox{var}(\hat b-b_0)" eeimg="1"/> ，但我们仍然希望从直觉上了解上面一顿操作猛如虎到底干了什么。考虑最简单的情况，即一个 moment 和一个参数（因此 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 是一个标量，令 <img src="https://www.zhihu.com/equation?tex=a+%3D+1" alt="a = 1" eeimg="1"/> ），我们可以画出 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b+-+b_0%29" alt="\mbox{var}(\hat b - b_0)" eeimg="1"/> 的关系。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-86baaf4c057fa96625436eafa0bd77c7_b.jpg" data-caption="" data-size="normal" data-rawwidth="853" data-rawheight="387" class="origin_image zh-lightbox-thumb" width="853" data-original="https://pic4.zhimg.com/v2-86baaf4c057fa96625436eafa0bd77c7_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-86baaf4c057fa96625436eafa0bd77c7_b.jpg" data-caption="" data-size="normal" data-rawwidth="853" data-rawheight="387" class="origin_image zh-lightbox-thumb lazy" width="853" data-original="https://pic4.zhimg.com/v2-86baaf4c057fa96625436eafa0bd77c7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-86baaf4c057fa96625436eafa0bd77c7_b.jpg"/></figure><p>图中，灰色曲线表示不同参数  <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/>  时的 population moment，用 <img src="https://www.zhihu.com/equation?tex=g%28b%29" alt="g(b)" eeimg="1"/> 表示。对于真实参数 <img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/> ，由假设有 <img src="https://www.zhihu.com/equation?tex=g%28b_0%29+%3D+%5Cmbox%7BE%7D%5Bf%28x_t%2C+b_0%29%5D+%3D+0" alt="g(b_0) = \mbox{E}[f(x_t, b_0)] = 0" eeimg="1"/> ，因此在图上，灰色曲线经过 <img src="https://www.zhihu.com/equation?tex=%28b_0%2C+0%29" alt="(b_0, 0)" eeimg="1"/> 这个点；蓝色曲线表示不同参数  <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/>  时的 sample moment，用 <img src="https://www.zhihu.com/equation?tex=g_T%28b%29" alt="g_T(b)" eeimg="1"/> 表示。图中 <img src="https://www.zhihu.com/equation?tex=g_T%28b_0%29" alt="g_T(b_0)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=g%28b_0%29+%3D+0" alt="g(b_0) = 0" eeimg="1"/> 之间的距离就是 <img src="https://www.zhihu.com/equation?tex=g_T%28b_0%29" alt="g_T(b_0)" eeimg="1"/> 的 sampling variation，即我们的样本可能由于 luck 或者 unluck，以至于 <img src="https://www.zhihu.com/equation?tex=g_T%28b_0%29+%E2%89%A0+0" alt="g_T(b_0) ≠ 0" eeimg="1"/> 而是较  <img src="https://www.zhihu.com/equation?tex=0" alt="0" eeimg="1"/>  有一定的偏离，在统计上它就是  <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> 。</p><p>接下来，对于 sample moment <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> ，我们令其等于 0 求出的参数估计为 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> ，因此蓝色曲线  <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/>  经过 <img src="https://www.zhihu.com/equation?tex=%28%5Chat+b%2C+0%29" alt="(\hat b, 0)" eeimg="1"/> 这个点。下面在蓝线上的 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 点计算其切线（红色）并计算切线的斜率 <img src="https://www.zhihu.com/equation?tex=d+%3D+%5Cmbox%7Bd%7Dg%2F%5Cmbox%7Bd%7Db" alt="d = \mbox{d}g/\mbox{d}b" eeimg="1"/> 。通过 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> 我们就可以把 <img src="https://www.zhihu.com/equation?tex=g_T%28b_0%29" alt="g_T(b_0)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=g%28b_0%29+%3D+0" alt="g(b_0) = 0" eeimg="1"/> 之间的 sampling variation 转换成 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=b_0" alt="b_0" eeimg="1"/> 之间的 sampling variation，即 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b+%E2%80%93+b_0%29" alt="\mbox{var}(\hat b – b_0)" eeimg="1"/> 。这正是 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> 和  <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b+%E2%80%93+b_0%29" alt="\mbox{var}(\hat b – b_0)" eeimg="1"/>  关系的几何含义。</p><p>下面我们如法炮制，利用一阶泰勒展开从 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> 求解 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> ：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+g_T%28%5Chat+b%29%3Dg_T%28b_0%29%2B%5Cfrac%7B%5Cpartial+g_T%7D%7B%5Cpartial+b%5E%5Cprime%7D%28%5Chat+b-b_0%29" alt="\displaystyle g_T(\hat b)=g_T(b_0)+\frac{\partial g_T}{\partial b^\prime}(\hat b-b_0)" eeimg="1"/> </p><p>由于 <img src="https://www.zhihu.com/equation?tex=%5Chat+b+%E2%80%93+b_0" alt="\hat b – b_0" eeimg="1"/> 已经在之前求出了，因此只需把它代入到上式就可得到：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Brll%7D+g_T%28%5Chat+b%29%26%3D%26%5Cdisplaystyle+g_T%28b_0%29-d%28ad%29%5E%7B-1%7Dag_T%28b_0%29%5C%5C+%26%3D%26%5Cleft%5BI-d%28ad%29%5E%7B-1%7Da%5Cright%5Dg_T%28b_0%29+%5Cend%7Barray%7D" alt="\begin{array}{rll} g_T(\hat b)&amp;=&amp;\displaystyle g_T(b_0)-d(ad)^{-1}ag_T(b_0)\\ &amp;=&amp;\left[I-d(ad)^{-1}a\right]g_T(b_0) \end{array}" eeimg="1"/> </p><p>两边同时求 variance（确切的说是 variance-covariance matrix）有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Brll%7D+%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29%26%3D%26%5Cdisplaystyle%5Cleft%5BI-d%28ad%29%5E%7B-1%7Da%5Cright%5D%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29%5Cleft%5BI-d%28ad%29%5E%7B-1%7Da%5Cright%5D%5E%5Cprime%5C%5C+%26%3D%26%5Cdisplaystyle%5Cfrac%7B1%7D%7BT%7D%5Cleft%5BI-d%28ad%29%5E%7B-1%7Da%5Cright%5DS%5Cleft%5BI-d%28ad%29%5E%7B-1%7Da%5Cright%5D%5E%5Cprime+%5Cend%7Barray%7D" alt="\begin{array}{rll} \mbox{var}(g_T(\hat b))&amp;=&amp;\displaystyle\left[I-d(ad)^{-1}a\right]\mbox{var}(g_T(b_0))\left[I-d(ad)^{-1}a\right]^\prime\\ &amp;=&amp;\displaystyle\frac{1}{T}\left[I-d(ad)^{-1}a\right]S\left[I-d(ad)^{-1}a\right]^\prime \end{array}" eeimg="1"/> </p><p>上式中的第二个等式用到了我们的老朋友： <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29+%3D+S%2FT" alt="\mbox{var}(g_T(b_0)) = S/T" eeimg="1"/> —— variance of sample mean！关于 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> 的关系也可以从直觉上解释两句。不难看出， <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 是 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> 乘以一个系数矩阵，这个系数矩阵是单位阵  <img src="https://www.zhihu.com/equation?tex=I" alt="I" eeimg="1"/>  减去这一大坨 <img src="https://www.zhihu.com/equation?tex=d%28ad%29%5E%7B-1%7Da" alt="d(ad)^{-1}a" eeimg="1"/> 。<b>因此从直觉上说，</b> <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 在 </b><img src="https://www.zhihu.com/equation?tex=b+%3D+%5Chat+b" alt="b = \hat b" eeimg="1"/><b> 时的方差 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> <b>会比 </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 在 </b><img src="https://www.zhihu.com/equation?tex=b+%3D+b_0" alt="b = b_0" eeimg="1"/><b> 时的方差 </b> <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> <b>要小一些。</b>这是因为在 GMM 估计时，我们要求 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 的 <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/> 个线性组合等于零 —— <img src="https://www.zhihu.com/equation?tex=ag_T%28%5Chat+b%29+%3D+0" alt="ag_T(\hat b) = 0" eeimg="1"/> —— 从而求出 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> ，因此求解 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 的过程用掉了 sample moments 的一些 variation，所以当 <img src="https://www.zhihu.com/equation?tex=b+%3D+%5Chat+b" alt="b = \hat b" eeimg="1"/> 时 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 方差小于当 <img src="https://www.zhihu.com/equation?tex=b+%3D+b_0" alt="b = b_0" eeimg="1"/> 时 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 的方差。</p><p><b>无论是 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/><b> 还是 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> <b>，上面一顿泰勒展开操作虽然非常热闹，但它们其实都仅是用了统计学中的 delta method。所以，其实我们只是用了 variance of the sample mean（</b> <img src="https://www.zhihu.com/equation?tex=S%2FT" alt="S/T" eeimg="1"/> <b>）+ delta method 就求出了我们关心的</b> <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/><b> 和 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> <b>。就是这么简单。</b></p><p>有了 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> ，就可以得到 <img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat+b%29" alt="g_T(\hat b)" eeimg="1"/> 的渐近分布（3.3 节介绍过），使用它的分布就可以构建 chi-squared statistic 来对模型进行检验：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Brll%7D+%26%5Cdisplaystyle+g_T%28%5Chat+b%29%5E%5Cprime%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29%5E%7B-1%7Dg_T%28%5Chat+b%29%5Csim%5Cchi%5E2_%7Bn-p%7D%5C%5C+%5CRightarrow%26%5Cdisplaystyle+Tg_T%28%5Chat+b%29%5E%5Cprime%5Cleft%5B%5Cleft%28I-d%28ad%29%5E%7B-1%7Da%5Cright%29S%5Cleft%28I-d%28ad%29%5E%7B-1%7Da%5Cright%29%5E%5Cprime%5Cright%5D%5E%7B-1%7D+g_T%28%5Chat+b%29%5Csim%5Cchi%5E2_%7Bn-p%7D+%5Cend%7Barray%7D" alt="\begin{array}{rll} &amp;\displaystyle g_T(\hat b)^\prime\mbox{var}(g_T(\hat b))^{-1}g_T(\hat b)\sim\chi^2_{n-p}\\ \Rightarrow&amp;\displaystyle Tg_T(\hat b)^\prime\left[\left(I-d(ad)^{-1}a\right)S\left(I-d(ad)^{-1}a\right)^\prime\right]^{-1} g_T(\hat b)\sim\chi^2_{n-p} \end{array}" eeimg="1"/> </p><p>上式的第二步是把  <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/>  的表达式代入并求逆；chi-squared statistic 的自由度是 moments 的个数减去参数的个数，即 <img src="https://www.zhihu.com/equation?tex=n+%E2%80%93+p" alt="n – p" eeimg="1"/> 。<b>由于估计 </b><img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/><b> 的时候用掉了 </b> <img src="https://www.zhihu.com/equation?tex=p" alt="p" eeimg="1"/> <b> 个自由度，所以 </b> <img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat+b%29" alt="g_T(\hat b)" eeimg="1"/> <b> 的 variance-covariance matrix 不是满秩的（这也体现在了 chi-squared statistic 的自由度 </b><img src="https://www.zhihu.com/equation?tex=n+%E2%80%93+p" alt="n – p" eeimg="1"/><b> 上），因此上式中对 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/><b> 求逆实际上是 pseudo-inverse。</b></p><p>如果你在 Wikipedia 或者其他书籍上查阅 GMM 的资料，也许看到的 chi-squared statistic 的表达式远没有上面这个复杂。<b>上述表达式是最 general 的情况</b>，因为我们尚未讨论那个使 GMM estimator 变得 efficient 的特殊的矩阵  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 。在那个  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  下， <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 矩阵以及 chi-squared statistic 表达式将被大大的简化。Efficient GMM 就是下一节的内容。</p><p>总结一下本小节。上面用了大量的文字和推导把 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 和 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 背后的数学含义呈现给各位，是希望这个过程能帮助小伙伴们加深对 GMM 的理解。站在 notation 的角度来说，虽然这些公式看上去很复杂（又是转置、又是求逆的），<b>但我们只需给 GMM 框架提供它需要的 </b><img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> <b>、</b> <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> <b>、</b> <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> <b> 和 </b> <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> <b>，剩下的“无脑”交给 GMM 就可以计算出各种想要的统计量并进行 test，非常方便。</b></p><h2><b>05 Efficient GMM</b></h2><p>本文的 3.2 小节给出的 GMM estimator 如下：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Chat+b%3A+%5Cmbox%7Bset+%7D+ag_T%28%5Chat+b%29%3D0" alt="\displaystyle\hat b: \mbox{set } ag_T(\hat b)=0" eeimg="1"/> </p><p>其中  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  是一个 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+n" alt="p × n" eeimg="1"/> 阶矩阵，每一行都代表一个 sample moments 的线性组合。本节关心的问题是，<b>如何选取矩阵 </b> <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> <b>？</b>回答这个问题可以从<b>业务上</b>和<b>统计上</b>两方面思考 —— <b>永远不要忘记业务层面的思考！从经济学或金融学原理出发，尤其是针对 asset pricing 的问题，我们可以选择一些最 economically important 的 moments，让它们或它们的线性组合等于零。我们不应让 GMM 成为一个 statistical 黑箱，代替我们的思考。</b>第七节将会进一步说明。</p><p>再来从统计上说，Hansen (1982) 指出了一个特殊的 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 矩阵，它能确保得到 efficient GMM estimator，即在给定的 moments <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 下，该矩阵  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  使得 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 最小。这个特殊的 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 矩阵为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+a%3Dd%5E%5Cprime+S%5E%7B-1%7D" alt="\displaystyle a=d^\prime S^{-1}" eeimg="1"/> </p><p>看到这里可能又有小伙伴会问： <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  见过、 <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  见过、转置明白、求逆矩阵清楚，但是这四个符号组合在一起得到的  <img src="https://www.zhihu.com/equation?tex=d%5E%5Cprime+S%5E%7B-1%7D" alt="d^\prime S^{-1}" eeimg="1"/>  是个什么鬼？？这个 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 到底有没有什么更直观的含义？别急，先来看看  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  的阶数。本文的 3.2 节已经指出  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  是一个 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+n" alt="p × n" eeimg="1"/> 阶矩阵，下面我们来验证一下。</p><p>前面首次提到 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> 的时候说过了，它是一阶偏导数  <img src="https://www.zhihu.com/equation?tex=%E2%88%82g_T%2F%E2%88%82b%5E%5Cprime" alt="∂g_T/∂b^\prime" eeimg="1"/> 。但由于 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> （ <img src="https://www.zhihu.com/equation?tex=n+%C3%97+1" alt="n × 1" eeimg="1"/> 阶）和  <img src="https://www.zhihu.com/equation?tex=b" alt="b" eeimg="1"/> （ <img src="https://www.zhihu.com/equation?tex=p+%C3%97+1" alt="p × 1" eeimg="1"/> 阶）都是 vectors，因此  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  是一个遵照 numerator layout（也称作 Jacobian formulation）排列的 <img src="https://www.zhihu.com/equation?tex=n+%C3%97+p" alt="n × p" eeimg="1"/> 阶矩阵。因此矩阵  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  的转置  <img src="https://www.zhihu.com/equation?tex=d%5E%5Cprime" alt="d^\prime" eeimg="1"/>  就是 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+n" alt="p × n" eeimg="1"/> 的矩阵，它同样也是一个一阶偏导数  <img src="https://www.zhihu.com/equation?tex=%E2%88%82g_T%5E%5Cprime%2F%E2%88%82b" alt="∂g_T^\prime/∂b" eeimg="1"/>  —— 这次转置在 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 上，遵循的是 denominator layout（也称 Hessian formulation），通常表示<b>求梯度（gradient）</b>。最后，由于 <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> 是 <img src="https://www.zhihu.com/equation?tex=n+%C3%97+n" alt="n × n" eeimg="1"/> 阶，因此 a 确实是 <img src="https://www.zhihu.com/equation?tex=p+%C3%97+n" alt="p × n" eeimg="1"/> 阶。</p><p>下面我们就来看看 <img src="https://www.zhihu.com/equation?tex=d%5E%5Cprime+S%5E%7B-1%7D" alt="d^\prime S^{-1}" eeimg="1"/> 到底是个什么鬼。其实它有着非常清晰的含义。为了解释 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 就不得不提 GMM estimator 的另一种表达式，这可能也是之前接触过 GMM 的小伙伴更熟悉的一种表达式：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Chat+b%3D+%5Carg%5Cmin+g_T%28b%29%5E%5Cprime+Wg_T%28b%29" alt="\displaystyle\hat b= \arg\min g_T(b)^\prime Wg_T(b)" eeimg="1"/> </p><p>上式中  <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/> <b> 是权重矩阵（weighting matrix）</b>，它是一个半正定矩阵。这个式子的含义是，在 overidentification 问题中，既然我们无法让所有的 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 都等于零，那么就让所有  <img src="https://www.zhihu.com/equation?tex=n" alt="n" eeimg="1"/>  个  <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/>  的范数的加权之和尽可能的接近零，以此来确定 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 。</p><p>正如在本文的第一种 GMM estimator 表达中我们可以随意选择矩阵  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  一样，在上面的第二种 GMM estimator 表达中我们可以随意选择权重矩阵  <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/> 。但是从 efficiency 的角度，最优的权重矩阵  <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/>  满足：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+W%3DS%5E%7B-1%7D" alt="\displaystyle W=S^{-1}" eeimg="1"/> </p><p>这从统计上非常好理解：我们有一组 moments <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> ，我们希望它们（非负）加权之和最接近零。使用 <img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> 即 <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> 的<b>逆矩阵</b>（别忘了 <img src="https://www.zhihu.com/equation?tex=S%2FT" alt="S/T" eeimg="1"/> 是 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28b_0%29%29" alt="\mbox{var}(g_T(b_0))" eeimg="1"/> ）<b>相当于给那些 sampling variation</b> <b>大的 </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 更低的权重、给那些 sampling variation</b> <b>小的 </b><img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/><b> 更高的权重（inverse 的意义）。换句话说，我们更愿意相信那些误差小的 moments 并使用它们来得到尽可能准确的参数估计 </b><img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> <b>，从而使 </b><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/><b> 最低，这也就是 efficient 的含义。</b></p><p>将 <img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> 代入上面第二个 GMM estimator 并求其 first order condition 有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cleft%28%5Cfrac%7B%5Cpartial+g_T%5E%5Cprime%7D%7B%5Cpartial+b%7DS%5E%7B-1%7D%5Cright%29g_T%28%5Chat+b%29%3D0" alt="\displaystyle \left(\frac{\partial g_T^\prime}{\partial b}S^{-1}\right)g_T(\hat b)=0" eeimg="1"/> </p><p>怎么样，看着眼熟不？括号里的第一项正是  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  的转置  <img src="https://www.zhihu.com/equation?tex=d%5E%5Cprime" alt="d^\prime" eeimg="1"/> ，第二项是 <img src="https://www.zhihu.com/equation?tex=S%5E%7B-1%7D" alt="S^{-1}" eeimg="1"/> ，这两个放一起  <img src="https://www.zhihu.com/equation?tex=d%5E%5Cprime+S%5E%7B-1%7D" alt="d^\prime S^{-1}" eeimg="1"/>  正是第一种 GMM estimator 下最优的矩阵 <img src="https://www.zhihu.com/equation?tex=a+%3D+d%5E%5Cprime+S%5E%7B-1%7D" alt="a = d^\prime S^{-1}" eeimg="1"/> ：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Brrl%7D+%26%5Cmbox%7BGMM+estimator+%7DW%3DS%5E%7B-1%7D%3A%26%5Cdisplaystyle%5Chat+b%3D+%5Carg%5Cmin+g_T%28b%29%5E%5Cprime+S%5E%7B-1%7Dg_T%28b%29%5C%5C+%5CRightarrow%26%5Cmbox%7Bfirst+order+condition%7D%3A%26%5Cdisplaystyle+%5Cleft%28%5Cfrac%7B%5Cpartial+g_T%5E%5Cprime%7D%7B%5Cpartial+b%7DS%5E%7B-1%7D%5Cright%29g_T%28%5Chat+b%29%3D0%5C%5C+%5CRightarrow%26%26%5Cdisplaystyle+%5Cleft%28%5Cleft%5B%5Cfrac%7B%5Cpartial+g_T%7D%7B%5Cpartial+b%5E%5Cprime%7D%5Cright%5D%5E%5Cprime+S%5E%7B-1%7D%5Cright%29g_T%28%5Chat+b%29%3D0%5C%5C+%5CRightarrow%26%26%5Cdisplaystyle+%5Cleft%28d%5E%5Cprime+S%5E%7B-1%7D%5Cright%29g_T%28%5Chat+b%29%3D0%5C%5C+%5CRightarrow%26%5Cmbox%7BGMM+estimator+%7Da%3Dd%5E%5Cprime+S%5E%7B-1%7D%3A%26%5Cdisplaystyle+ag_T%28%5Chat+b%29%3D0+%5Cend%7Barray%7D" alt="\begin{array}{rrl} &amp;\mbox{GMM estimator }W=S^{-1}:&amp;\displaystyle\hat b= \arg\min g_T(b)^\prime S^{-1}g_T(b)\\ \Rightarrow&amp;\mbox{first order condition}:&amp;\displaystyle \left(\frac{\partial g_T^\prime}{\partial b}S^{-1}\right)g_T(\hat b)=0\\ \Rightarrow&amp;&amp;\displaystyle \left(\left[\frac{\partial g_T}{\partial b^\prime}\right]^\prime S^{-1}\right)g_T(\hat b)=0\\ \Rightarrow&amp;&amp;\displaystyle \left(d^\prime S^{-1}\right)g_T(\hat b)=0\\ \Rightarrow&amp;\mbox{GMM estimator }a=d^\prime S^{-1}:&amp;\displaystyle ag_T(\hat b)=0 \end{array}" eeimg="1"/> </p><p>这就是最优矩阵  <img src="https://www.zhihu.com/equation?tex=a+%3D+d%5E%5Cprime+S%5E%7B-1%7D" alt="a = d^\prime S^{-1}" eeimg="1"/>  的含义。从上面的推导也不难看出这两种 GMM estimator 表达式是等价的：无论我们取何种  <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/>  权重矩阵，都有一个与之对应的 <img src="https://www.zhihu.com/equation?tex=a+%3D+d%5E%5Cprime+W" alt="a = d^\prime W" eeimg="1"/> 矩阵。当矩阵  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  或权重矩阵  <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/>  取统计上最优时， <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+b%29" alt="\mbox{var}(\hat b)" eeimg="1"/> 、 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29" alt="\mbox{var}(g_T(\hat b))" eeimg="1"/> 以及 chi-squared test statistic 的表达式均可以大大化简。Hansen (1982) 给出了它们的形式：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Brll%7D+%5Cmbox%7Bvar%7D%28%5Chat+b%29%26%3D%26%5Cdisplaystyle%5Cfrac%7B1%7D%7BT%7D%5Cleft%28d%5E%5Cprime+S%5E%7B-1%7Dd%5Cright%29%5E%7B-1%7D%5C%5C+%5Cmbox%7Bvar%7D%28g_T%28%5Chat+b%29%29%26%3D%26%5Cdisplaystyle%5Cfrac%7B1%7D%7BT%7D%5Cleft%28S-d%28d%5E%5Cprime+S%5E%7B-1%7Dd%29%5E%7B-1%7Dd%5E%5Cprime%5Cright%29%5C%5C+Tg_T%28%5Chat+b%29%5E%5Cprime+S%5E%7B-1%7Dg_T%28%5Chat+b%29%26%5Csim%26%5Cchi%5E2_%7Bn-p%7D+%5Cend%7Barray%7D" alt="\begin{array}{rll} \mbox{var}(\hat b)&amp;=&amp;\displaystyle\frac{1}{T}\left(d^\prime S^{-1}d\right)^{-1}\\ \mbox{var}(g_T(\hat b))&amp;=&amp;\displaystyle\frac{1}{T}\left(S-d(d^\prime S^{-1}d)^{-1}d^\prime\right)\\ Tg_T(\hat b)^\prime S^{-1}g_T(\hat b)&amp;\sim&amp;\chi^2_{n-p} \end{array}" eeimg="1"/> </p><p><b>需要强调的是，以上的这些大大简化了的表达式只有当 </b><img src="https://www.zhihu.com/equation?tex=a+%3D+d%5E%5Cprime+S%5E%7B-1%7D" alt="a = d^\prime S^{-1}" eeimg="1"/> <b>（或 </b><img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> <b>）时才成立！如果 </b> <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> <b> 或 </b> <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/> <b> 取别的值，则应该使用本文第 4 节中介绍的更 general 的形式。</b>很多关于 GMM 的材料中默认 <img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> 而给出了这些统计量的简化形式，使用时应搞清楚前提条件。</p><p>在实际估计中，因为必须先有 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 才能估计  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> ，并计算 <img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> （或最优的 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> ）；但另一方面只有使用 <img src="https://www.zhihu.com/equation?tex=S%5E%7B-1%7D" alt="S^{-1}" eeimg="1"/> 才能得到最优的 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 。这似乎是一个鸡生蛋、蛋生鸡的问题。因此，实际中往往采用 two-stage estimates：</p><ul><li><b>First Stage：</b>通常取 <img src="https://www.zhihu.com/equation?tex=W+%3D+I" alt="W = I" eeimg="1"/> 单位阵，估计出 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> ；</li><li><b>Second Stage：</b>使用  <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/>  估计 S，令 <img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> 进行再一次估计得到新的 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 。</li></ul><p>当然，如果愿意，也可以把上面的第二步<b>迭代多次</b>，得到最终的 <img src="https://www.zhihu.com/equation?tex=%5Chat+b" alt="\hat b" eeimg="1"/> 。以上就完成了关于 GMM 的全部介绍。</p><h2><b>06 GMM does OLS</b></h2><p>GMM 之所以如此强大，是因为它自带的“estimate、variance、test”三部曲能够干很多事儿！对于很多需要研究的问题，只要把它的模型塞进 GMM 的框架，就可以得到想要的分析结果。</p><p>本节就把我们熟悉的 OLS 放在 GMM 的框架下看看后者的强大之处。由于参数个数和 moments 个数相同，因此 OLS 不存在 overidentification 的问题，我们没有什么可以检验的。但是 GMM 仍然可以轻松的计算出参数的 variance（即完成 estimate 和 variance 两步），无论 OLS 的残差是否存在自相关或异方差。</p><p>想要使用 GMM 框架，只需要把 OLS 表述成 moment conditions。考虑 OLS 问题（截距被视作一个解释变量，不做区分；假设一共有  <img src="https://www.zhihu.com/equation?tex=k" alt="k" eeimg="1"/>  个解释变量，因此 <img src="https://www.zhihu.com/equation?tex=x_t" alt="x_t" eeimg="1"/> 表示 <img src="https://www.zhihu.com/equation?tex=k+%C3%97+1" alt="k × 1" eeimg="1"/> 阶向量）如下：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+y_t%3D%5Cbeta%5E%5Cprime+x_t%2B%5Cvarepsilon_t" alt="\displaystyle y_t=\beta^\prime x_t+\varepsilon_t" eeimg="1"/> </p><p>由 OLS 的性质可知，其解释变量和残差正交，因此 OLS 的 moment conditions 为：</p><p><img src="https://www.zhihu.com/equation?tex=g_T%28%5Chat%5Cbeta%29%3D%5Cmbox%7BE%7D_T%5Cleft%5Bx_t%28y_t-%5Chat%5Cbeta%5E%5Cprime+x_t%29%5Cright%5D%3D0" alt="g_T(\hat\beta)=\mbox{E}_T\left[x_t(y_t-\hat\beta^\prime x_t)\right]=0" eeimg="1"/> </p><p>由于 moments 个数和参数个数相同，因此我们只需要令所有 sample moments 都等于零即可，这意味着矩阵 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 是单位阵 <img src="https://www.zhihu.com/equation?tex=I" alt="I" eeimg="1"/> ，因此在上式的 GMM estimator 中省略了  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 。求解上述 sample moment conditions 就可以得到参数的估计：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Chat%5Cbeta%3D%5Cleft%5B%5Cmbox%7BE%7D_T%28x_tx_t%5E%5Cprime%29%5Cright%5D%5E%7B-1%7D%5Cmbox%7BE%7D_T%28x_ty_t%29" alt="\displaystyle\hat\beta=\left[\mbox{E}_T(x_tx_t^\prime)\right]^{-1}\mbox{E}_T(x_ty_t)" eeimg="1"/> </p><p>如果令  <img src="https://www.zhihu.com/equation?tex=X+%3D+%5Bx_1+x_2+%E2%80%A6+x_t%5D%5E%5Cprime" alt="X = [x_1 x_2 … x_t]^\prime" eeimg="1"/>  表示 data matrix，则有 <img src="https://www.zhihu.com/equation?tex=%281%2FT%29X%5E%5Cprime+X+%3D+%5Cmbox%7BE%7D_T%5Bx_tx_t%5E%5Cprime%5D" alt="(1/T)X^\prime X = \mbox{E}_T[x_tx_t^\prime]" eeimg="1"/> ，因此上式又可以写成：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle%5Chat%5Cbeta%3D%5Cleft%5B%5Cmbox%7BE%7D_T%28x_tx_t%5E%5Cprime%29%5Cright%5D%5E%7B-1%7D%5Cmbox%7BE%7D_T%28x_ty_t%29%3D%28X%5E%5Cprime+X%29%5E%7B-1%7DX%5E%5Cprime+Y" alt="\displaystyle\hat\beta=\left[\mbox{E}_T(x_tx_t^\prime)\right]^{-1}\mbox{E}_T(x_ty_t)=(X^\prime X)^{-1}X^\prime Y" eeimg="1"/> </p><p>这正是我们熟悉的 OLS estimator。GMM 的强大之处在于轻松的计算 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+%CE%B2%29" alt="\mbox{var}(\hat β)" eeimg="1"/> 。为此，我们需要给 GMM 框架提供它所需要的  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  和  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> （已经有了  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  和  <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> ）。根据  <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  和  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  的定义可得：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Brll%7D+d%26%3D%26-%5Cmbox%7BE%7D%5Bx_tx_t%5E%5Cprime%5D%5C%5C+f%28x_t%2C+%5Cbeta%29%26%3D%26x_t%5Cvarepsilon_t%5C%5C+S%26%3D%26%5Cdisplaystyle%5Csum_%7Bj%3D-%5Cinfty%7D%5E%7B%5Cinfty%7D%5Cmbox%7BE%7D%5B%5Cvarepsilon_tx_tx_%7Bt-j%7D%5E%5Cprime%5Cvarepsilon_%7Bt-j%7D%5D+%5Cend%7Barray%7D" alt="\begin{array}{rll} d&amp;=&amp;-\mbox{E}[x_tx_t^\prime]\\ f(x_t, \beta)&amp;=&amp;x_t\varepsilon_t\\ S&amp;=&amp;\displaystyle\sum_{j=-\infty}^{\infty}\mbox{E}[\varepsilon_tx_tx_{t-j}^\prime\varepsilon_{t-j}] \end{array}" eeimg="1"/> </p><p>有了 <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 、 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 、 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> 、 <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> ，直接利用 GMM 中的公式就可以求出 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+%CE%B2%29" alt="\mbox{var}(\hat β)" eeimg="1"/> ：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cmbox%7Bvar%7D%28%5Chat%5Cbeta%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cmbox%7BE%7D%5Bx_tx_t%5E%5Cprime%5D%5E%7B-1%7D%5Cleft%5B%5Csum_%7Bj%3D-%5Cinfty%7D%5E%7B%5Cinfty%7D%5Cmbox%7BE%7D%5B%5Cvarepsilon_tx_tx_%7Bt-j%7D%5E%5Cprime%5Cvarepsilon_%7Bt-j%7D%5D%5Cright%5D%5Cmbox%7BE%7D%5Bx_tx_t%5E%5Cprime%5D%5E%7B-1%7D" alt="\displaystyle \mbox{var}(\hat\beta)=\frac{1}{T}\mbox{E}[x_tx_t^\prime]^{-1}\left[\sum_{j=-\infty}^{\infty}\mbox{E}[\varepsilon_tx_tx_{t-j}^\prime\varepsilon_{t-j}]\right]\mbox{E}[x_tx_t^\prime]^{-1}" eeimg="1"/> </p><p>这正是广义 OLS 下 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+%CE%B2%29" alt="\mbox{var}(\hat β)" eeimg="1"/> 的表达式（请参考<a href="https://zhuanlan.zhihu.com/p/54913149" class="internal">《多因子回归检验中的 Newey-West 调整》</a>对比）。</p><p>We are done!</p><p>下面考察几种情况。首先如果残差满足 i.i.d.， <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+%CE%B2%29" alt="\mbox{var}(\hat β)" eeimg="1"/> 就可以简化成我们最熟悉的样子：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cmbox%7Bvar%7D%28%5Chat%5Cbeta%29%3D%5Cfrac%7B1%7D%7BT%7D%5Csigma_%7B%5Cvarepsilon%7D%5E2%5Cmbox%7BE%7D%5Bx_tx_t%5E%5Cprime%5D%5E%7B-1%7D%3D%5Csigma_%7B%5Cvarepsilon%7D%5E2%5Cleft%28X%5E%5Cprime+X%5Cright%29%5E%7B-1%7D" alt="\displaystyle \mbox{var}(\hat\beta)=\frac{1}{T}\sigma_{\varepsilon}^2\mbox{E}[x_tx_t^\prime]^{-1}=\sigma_{\varepsilon}^2\left(X^\prime X\right)^{-1}" eeimg="1"/> </p><p>通常来说，残差中可能存在异方差、自相关或者两者皆有。在 GMM 的框架下，为了计算 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+%CE%B2%29" alt="\mbox{var}(\hat β)" eeimg="1"/> 仅需要在  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  矩阵中考虑异方差和自相关造成的影响。当残差仅存在异方差时， <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/>  的表达式为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+S%3D%5Cmbox%7BE%7D%5B%5Cvarepsilon_t%5E2x_tx_t%27%5D" alt="\displaystyle S=\mbox{E}[\varepsilon_t^2x_tx_t&#39;]" eeimg="1"/> </p><p>因此 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+%CE%B2%29" alt="\mbox{var}(\hat β)" eeimg="1"/> 的表达式变成：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cmbox%7Bvar%7D%28%5Chat%5Cbeta%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cmbox%7BE%7D%5Bx_tx_t%5E%5Cprime%5D%5E%7B-1%7D%5Cmbox%7BE%7D%5B%5Cvarepsilon_t%5E2x_tx_t%5E%5Cprime%5D%5Cmbox%7BE%7D%5Bx_tx_t%5E%5Cprime%5D%5E%7B-1%7D" alt="\displaystyle \mbox{var}(\hat\beta)=\frac{1}{T}\mbox{E}[x_tx_t^\prime]^{-1}\mbox{E}[\varepsilon_t^2x_tx_t^\prime]\mbox{E}[x_tx_t^\prime]^{-1}" eeimg="1"/> </p><p>这正是大名鼎鼎的 White (1980) heteroscedasticity consistent estimator。</p><p>当残差即存在异方差又存在自相关时， <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> 可以写作：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+S%3D%5Csum_%7Bj%3D-k%7D%5Ek+w_j%5Cmbox%7BE%7D%5B%5Cvarepsilon_tx_tx_%7Bt-j%7D%27%5Cvarepsilon_%7Bt-j%7D%5D" alt="\displaystyle S=\sum_{j=-k}^k w_j\mbox{E}[\varepsilon_tx_tx_{t-j}&#39;\varepsilon_{t-j}]" eeimg="1"/> </p><p>而 <img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bvar%7D%28%5Chat+%CE%B2%29" alt="\mbox{var}(\hat β)" eeimg="1"/> 的表达式变成：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cmbox%7Bvar%7D%28%5Chat%5Cbeta%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cmbox%7BE%7D%5Bx_tx_t%5E%5Cprime%5D%5E%7B-1%7D%5Cleft%5B%5Csum_%7Bj%3D-k%7D%5Ek+w_j%5Cmbox%7BE%7D%5B%5Cvarepsilon_tx_tx_%7Bt-j%7D%27%5Cvarepsilon_%7Bt-j%7D%5D%5Cright%5D%5Cmbox%7BE%7D%5Bx_tx_t%5E%5Cprime%5D%5E%7B-1%7D" alt="\displaystyle \mbox{var}(\hat\beta)=\frac{1}{T}\mbox{E}[x_tx_t^\prime]^{-1}\left[\sum_{j=-k}^k w_j\mbox{E}[\varepsilon_tx_tx_{t-j}&#39;\varepsilon_{t-j}]\right]\mbox{E}[x_tx_t^\prime]^{-1}" eeimg="1"/> </p><p>这正是大名鼎鼎的 Newey and West (1987) autocorrelation consistent covariance estimator（见<a href="https://zhuanlan.zhihu.com/p/54913149" class="internal">《多因子回归检验中的 Newey-West 调整》</a>）。</p><p>无论残差具有什么特性，整个 OLS 的求解过程都可以很好的装到 GMM 的框架中。而当使用 GMM 框架时，只需按照它的要求来定义  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> 、 <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> 、 <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/>  以及  <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> ，就可以“无脑”的利用 GMM 给出的结果。这正是 GMM 的强大之处。</p><h2><b>07 不应成为黑箱</b></h2><p>在结束本文之前，再花一小节讨论一个很重要的问题：<b>GMM 不应该成为计量学黑箱。</b>这是我听完 Cochrane 教授的讲解后印象非常深刻的一点。</p><p>GMM 如此强大再加上现在各种编程语言（R、Stata 等）都能方便的计算，这种便捷性似乎把人们都惯坏了；<b>人们习惯于把问题描述成 moment conditions 然后一股脑塞进 GMM 并纯从统计的角度使用 efficient estimator（即 </b><img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> <b>）。</b>Cochrane 教授警告说这么做十分危险。</p><p><b>GMM 的强大之处在于它不仅仅是一个计量学工具来做 test，而是它足够 flexible 从而可以让我们研究我们真正关心的经济学或金融学问题，这体现在我们可以从“先验”出发去定义最适合待研究问题的矩阵 </b> <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> <b>（或权重矩阵 </b><img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/> <b>），而非无脑的选择 </b> <img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> <b>。</b></p><p>以 3.2 节中 asset pricing 的例子来说，我们有四个 moments，两个参数。这四个 moments 来自四个资产：risk-free、市场组合以及 HML 和 SMB，我们假设待检验的模型是 CCAPM。从经济学业务出发，我们可以选择如下的 <img src="https://www.zhihu.com/equation?tex=ag_T%28%5Chat+b%29+%3D+0" alt="ag_T(\hat b) = 0" eeimg="1"/> ：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cleft%5B+%5Cbegin%7Barray%7D%7Bcccc%7D+1%260%260%260%5C%5C+0%261%260%260+%5Cend%7Barray%7D+%5Cright%5D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bl%7D+%5Cmbox%7BE%7D_T%5Bm%28%5Chat%5Cbeta%2C%5Chat%5Cgamma%29R%5Ee_m%5D%5C%5C+%5Cmbox%7BE%7D_T%5Bm%28%5Chat%5Cbeta%2C%5Chat%5Cgamma%29R_f-1%5D%5C%5C+%5Cmbox%7BE%7D_T%5Bm%28%5Chat%5Cbeta%2C%5Chat%5Cgamma%29R%5E%7B%5Cmbox%7BHML%7D%7D%5D%5C%5C+%5Cmbox%7BE%7D_T%5Bm%28%5Chat%5Cbeta%2C%5Chat%5Cgamma%29R%5E%7B%5Cmbox%7BSMB%7D%7D%5D+%5Cend%7Barray%7D+%5Cright%5D%3D+%5Cleft%5B+%5Cbegin%7Barray%7D%7Bc%7D+0%5C%5C0+%5Cend%7Barray%7D+%5Cright%5D" alt="\left[ \begin{array}{cccc} 1&amp;0&amp;0&amp;0\\ 0&amp;1&amp;0&amp;0 \end{array} \right] \left[ \begin{array}{l} \mbox{E}_T[m(\hat\beta,\hat\gamma)R^e_m]\\ \mbox{E}_T[m(\hat\beta,\hat\gamma)R_f-1]\\ \mbox{E}_T[m(\hat\beta,\hat\gamma)R^{\mbox{HML}}]\\ \mbox{E}_T[m(\hat\beta,\hat\gamma)R^{\mbox{SMB}}] \end{array} \right]= \left[ \begin{array}{c} 0\\0 \end{array} \right]" eeimg="1"/> </p><p>在这个矩阵  <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/>  中，我们令市场超额收益和 <img src="https://www.zhihu.com/equation?tex=R_f+" alt="R_f " eeimg="1"/> 完美满足两个 sample moment conditions，并由此进行 CCAPM 的参数估计，求出两个参数，然后使用另外两个资产 HML 和 SMB 来检验 CCAPM。由 GMM 框架可知，最终的 chi-squared test statistic 的自由度为 2（因为一共 4 个资产，2 个被用来估计参数），因此联合检验的实际上正是 HML 和 SMB 在 CCAPM 这个定价模型下的 pricing errors。如果 pricing errors 联合显著不为零，那么就可以拒绝 CCAPM。这个例子说明，<b>从经济学原理出发选择合适的 a 或 W 能让我们回答最感兴趣的经济学问题。</b>GMM 的强大之处正在于此。</p><p>纯从统计学的角度来说， <img src="https://www.zhihu.com/equation?tex=W+%3D+S%5E%7B-1%7D" alt="W = S^{-1}" eeimg="1"/> 确实能够得到 efficient GMM。但不要忘记，这个 efficient 的是以给定的 moments 为前提的 —— 如果换了或者添加了更多的 moments，参数的 efficient 估计也会发生变化。在金融市场中，有无数的资产，包括股票、债券、外汇、商品等，还有无数的投资组合，这些资产可以构成无数的 moments。为了 efficiency，我们应该把这成千上万资产的 moments 都塞进 GMM 才能得到 efficient 的估计。但显然，从业务的角度来说这毫无意义。在研究资产定价的时候，我们应该使用最“clever”的资产，比如 HML、SMB 这些投资组合。它们才是我们真正关心的问题。</p><blockquote><i>The quest for efficiency doesn&#39;t really drive us as much as the quest for something that is robust and that expresses what the model is supposed to do. —— John Cochrane</i></blockquote><p><b>GMM 非常好使，但在 asset pricing 的研究中，我们不应追求使用 GMM 进行一个仅在统计上正式但模型却缺乏含义的 statistical test。GMM 的强大在于它让我们从经济学和金融学原理出发，去 measure 和 estimate 最合理的模型、并同时对 sampling error 保持足够的认识。</b></p><p><b>不要让 GMM 成为计量学的黑箱。</b></p><h2><b>08 结语</b></h2><p>呼！终于写完了！感谢你看到这里！</p><p>作为感谢，上点硬货 —— GMM 的 formula sheet（出自 Cochrane 2005）。它总结了前文解读的每一个公式。网上能找到的 Asset Pricing 的电子版还是 2000 年 6 月的版本，有不少 Typo。这张截图是来自 2005 年的修订版。怎么样？<b>GMM 其实并不复杂，我们只需要提供并计算 </b> <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> <b>，</b> <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> <b>，</b><img src="https://www.zhihu.com/equation?tex=d+" alt="d " eeimg="1"/> <b>和 </b> <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> <b>；有了它们，GMM 框架 takes care of everything else！</b></p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-3dae1f9979c74ebe0a02ae47a70661cc_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="993" class="origin_image zh-lightbox-thumb" width="1080" data-original="https://pic1.zhimg.com/v2-3dae1f9979c74ebe0a02ae47a70661cc_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-3dae1f9979c74ebe0a02ae47a70661cc_b.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="993" class="origin_image zh-lightbox-thumb lazy" width="1080" data-original="https://pic1.zhimg.com/v2-3dae1f9979c74ebe0a02ae47a70661cc_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-3dae1f9979c74ebe0a02ae47a70661cc_b.jpg"/></figure><p>最后对全文简要总结如下：</p><ol><li><b>GMM 的框架下包括 model、estimate 以及 test 三部分；它用 sample moments 代替 population moments 来检验后者；GMM 涉及的数学（不那么严谨的说）可以归结为 the variance of the sample mean + delta method。</b></li><li><b>从 notation 的角度，我们只需找到 </b> <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> <b>、</b> <img src="https://www.zhihu.com/equation?tex=g_T" alt="g_T" eeimg="1"/> <b>、</b> <img src="https://www.zhihu.com/equation?tex=d" alt="d" eeimg="1"/> <b> 和 </b> <img src="https://www.zhihu.com/equation?tex=S" alt="S" eeimg="1"/> <b>，剩下的交给 GMM 的公式；</b></li><li><b>GMM 允许我们自由挑选矩阵 </b> <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> <b>（或 </b> <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/> <b>）；从统计学的角度存在一个特定的 </b> <img src="https://www.zhihu.com/equation?tex=a" alt="a" eeimg="1"/> <b>（或 </b> <img src="https://www.zhihu.com/equation?tex=W" alt="W" eeimg="1"/> <b>）是最 efficient 的；但 GMM 不应被当作计量学的黑箱，理解你所研究的问题永远是最重要的。</b></li></ol><p>写完本文，我的感受和写完<a href="https://zhuanlan.zhihu.com/p/40984029" class="internal">《股票多因子模型的回归检验》</a>是一模一样的，对 Cochrane 教授崇拜的五体投地。关于 GMM 的内容，Cochrane 教授在其 UChicago 的课程中介绍的非常生动、到位，听完再结合他的书仔细体会，那收获就一个字 —— 爽！</p><p>最后，我想用和<a href="https://zhuanlan.zhihu.com/p/40984029" class="internal">《股票多因子模型的回归检验》</a>一文同样的结语作为本文的收尾。在介绍 Asset Pricing 这门课的时候，Cochrane 教授谈到：</p><blockquote><i>The math in real, academic, finance is not actually that hard. Understanding how to use the equations, and see what they really mean about the world... that&#39;s hard, and that&#39;s what I hope will be uniquely rewarding about this class.</i></blockquote><p>再一次的，我也真心希望本文在你理解 GMM 以及应用它研究 asset pricing 的道路上起到一点点帮助。</p><p class="ztext-empty-paragraph"><br/></p><p><b>参考文献</b></p><ul><li>Cochrane, J. H (2005). <i>Asset Pricing (revised edition)</i>. Princeton University Press.</li><li>Fama, E. F. and K. R. French (1993). Common Risk Factors in the Returns on Stocks and Bonds. <i>Journal of Financial Economics</i>, Vol. 33(1), 3 – 56.</li><li>Hansen, L. P. (1982). Large sample properties of generalized method of moments estimators. <i>Econometrica</i>, Vol. 50(4), 1029 – 1054.</li><li>Newey, W. K. and K. D. West (1987). A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance matrix. <i>Econometrica</i>, Vol. 55(3), 703 – 708.</li><li>White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. <i>Econometrica</i>, Vol. 48(4), 817 – 838.</li></ul><p class="ztext-empty-paragraph"><br/></p><p><b>免责声明：</b>文章内容不可视为投资意见。市场有风险，入市需谨慎。</p><p class="ztext-empty-paragraph"><br/></p><p>原创不易，请保护版权。如需转载，请联系获得授权，并注明出处。已委托“维权骑士”(<a href="https://link.zhihu.com/?target=http%3A//rightknights.com/" class=" wrap external" target="_blank" rel="nofollow noreferrer">维权骑士_免费版权监测/版权保护/内容多平台分发</a>) 为进行维权行动。</p>