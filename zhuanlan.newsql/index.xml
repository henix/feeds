<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Wed, 01 Jan 2020 16:03:43 +0800</lastBuildDate>
<item>
<title>「分布式系统前沿技术」专题 | 复杂分布式架构下的计算治理之路</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-31-100383665.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/100383665&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2ac2bbb8366274bf43bd936c96eb2bff_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;分布式技术的发展，深刻地改变了我们编程的模式和思考软件的模式。值 2019 岁末，PingCAP 联合 InfoQ 共同策划出品“分布式系统前沿技术 ”专题， 邀请众多技术团队共同参与，一起探索这个古老领域的新生机。本文出自微众银行大数据平台负责人邸帅。&lt;/blockquote&gt;&lt;p&gt;在当前的复杂分布式架构环境下，服务治理已经大行其道。但目光往下一层，从上层 APP、Service，到底层计算引擎这一层面，却还是各个引擎各自为政，Client-Server 模式紧耦合满天飞的情况。如何做好“计算治理”，让复杂环境下各种类型的大量计算任务，都能更简洁、灵活、有序、可控的提交执行，和保障成功返回结果？计算中间件 Linkis 就是上述问题的最佳实践。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;复杂分布式架构环境下的计算治理有什么问题？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;什么是复杂分布式架构环境？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;分布式架构，指的是系统的组件分布在通过网络相连的不同计算机上，组件之间通过网络传递消息进行通信和协调，协同完成某一目标。一般来说有水平（集群化）和垂直（功能模块切分）两个拆分方向，以解决高内聚低耦合、高并发、高可用等方面问题。&lt;/p&gt;&lt;p&gt;多个分布式架构的系统，组成分布式系统群，就形成了一个相对复杂的分布式架构环境。通常包含多种上层应用服务，多种底层基础计算存储引擎。如下图 1 所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4a7549d814033fe531f3da1b4f228b74_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;524&quot; data-rawheight=&quot;273&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;524&quot; data-original=&quot;https://pic1.zhimg.com/v2-4a7549d814033fe531f3da1b4f228b74_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4a7549d814033fe531f3da1b4f228b74_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;524&quot; data-rawheight=&quot;273&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;524&quot; data-original=&quot;https://pic1.zhimg.com/v2-4a7549d814033fe531f3da1b4f228b74_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-4a7549d814033fe531f3da1b4f228b74_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;什么是计算治理？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;就像《微服务设计》一书中提到的，如同城市规划师在面对一座庞大、复杂且不断变化的城市时，所需要做的规划、设计和治理一样，庞大复杂的软件系统环境中的各种区域、元素、角色和关系，也需要整治和管理，以使其以一种更简洁、优雅、有序、可控的方式协同运作，而不是变成一团乱麻。&lt;/p&gt;&lt;p&gt;在当前的复杂分布式架构环境下，大量 APP、Service 间的通信、协调和管理，已经有了从 SOA（Service-Oriented Architecture）到微服务的成熟理念，及从 ESB 到 Service Mesh 的众多实践，来实现其从服务注册发现、配置管理、网关路由，到流控熔断、日志监控等一系列完整的服务治理功能。服务治理框架的“中间件”层设计，可以很好的实现服务间的解耦、异构屏蔽和互操作，并提供路由、流控、状态管理、监控等治理特性的共性提炼和复用，增强整个架构的灵活性、管控能力、可扩展性和可维护性。&lt;/p&gt;&lt;p&gt;但目光往下一层，你会发现在从 APP、Service，到后台引擎这一层面，却还是各个引擎各自为政，Client-Server 模式紧耦合满天飞的情况。在大量的上层应用，和大量的底层引擎之间，缺乏一层通用的“中间件”框架设计。类似下图 2 的网状。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3714f165dbbea26a876da9cdcb2b95b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;549&quot; data-rawheight=&quot;286&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;549&quot; data-original=&quot;https://pic4.zhimg.com/v2-3714f165dbbea26a876da9cdcb2b95b7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3714f165dbbea26a876da9cdcb2b95b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;549&quot; data-rawheight=&quot;286&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;549&quot; data-original=&quot;https://pic4.zhimg.com/v2-3714f165dbbea26a876da9cdcb2b95b7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-3714f165dbbea26a876da9cdcb2b95b7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;计算治理，关注的正是上层应用和底层计算（存储）引擎之间，从 Client 到 Server 的连接层范围，所存在的紧耦合、灵活性和管控能力欠缺、缺乏复用能力、可扩展性、可维护性差等问题。要让复杂分布式架构环境下各种类型的计算任务，都能更简洁、灵活、有序、可控的提交执行，和成功返回结果。如下图 3 所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b013a8133477b16bac601afb96b14735_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;622&quot; data-rawheight=&quot;278&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;622&quot; data-original=&quot;https://pic2.zhimg.com/v2-b013a8133477b16bac601afb96b14735_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b013a8133477b16bac601afb96b14735_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;622&quot; data-rawheight=&quot;278&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;622&quot; data-original=&quot;https://pic2.zhimg.com/v2-b013a8133477b16bac601afb96b14735_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-b013a8133477b16bac601afb96b14735_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;计算治理问题描述&lt;/b&gt;&lt;/p&gt;&lt;p&gt;更详细的来看计算治理的问题，可以分为如下治（architecture，架构层面）和理（insight，细化特性）两个层面。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算治理之治（architecture）-架构层面问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;紧耦合问题，上层应用和底层计算存储引擎间的 CS 连接模式。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;所有 APP &amp;amp; Service 和底层计算存储引擎，都是通过 Client-Server 模式相连，处于紧耦合状态。以 Analytics Engine 的 Spark 为例，如下图 4：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b34f841e1a678fdc27b16ef868d15d7f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;321&quot; data-rawheight=&quot;298&quot; class=&quot;content_image&quot; width=&quot;321&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b34f841e1a678fdc27b16ef868d15d7f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;321&quot; data-rawheight=&quot;298&quot; class=&quot;content_image lazy&quot; width=&quot;321&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-b34f841e1a678fdc27b16ef868d15d7f_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;这种状态会带来如下问题：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;引擎 client 的任何改动（如版本升级），将直接影响每一个嵌入了该 client 的上层应用；当应用系统数量众多、规模庞大时，一次改动的成本会很高。&lt;/li&gt;&lt;li&gt;直连模式，导致上层应用缺乏，对跨底层计算存储引擎实例级别的，路由选择、负载均衡等能力；或者说依赖于特定底层引擎提供的特定连接方式实现，有的引擎有一些，有的没有。&lt;/li&gt;&lt;li&gt;随着时间推移，不断有新的上层应用和新的底层引擎加入进来，整体架构和调用关系将愈发复杂，可扩展性、可靠性和可维护性降低。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;重复造轮子问题，每个上层应用工具系统都要重复解决计算治理问题。&lt;/p&gt;&lt;p&gt;每个上层应用都要重复的去集成各种 client，创建和管理 client 到引擎的连接及其状态，包括底层引擎元数据的获取与管理。在并发使用的用户逐渐变多、并发计算任务量逐渐变大时，每个上层应用还要重复的去解决多个用户间在 client 端的资源争用、权限隔离，计算任务的超时管理、失败重试等等计算治理问题。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0ceab6c2e4a1e3eda633f7dd1ac0ce2a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;492&quot; data-rawheight=&quot;137&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;492&quot; data-original=&quot;https://pic3.zhimg.com/v2-0ceab6c2e4a1e3eda633f7dd1ac0ce2a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0ceab6c2e4a1e3eda633f7dd1ac0ce2a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;492&quot; data-rawheight=&quot;137&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;492&quot; data-original=&quot;https://pic3.zhimg.com/v2-0ceab6c2e4a1e3eda633f7dd1ac0ce2a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-0ceab6c2e4a1e3eda633f7dd1ac0ce2a_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;想象你有 10 个并发任务数过百的上层应用，不管是基于 Web 的 IDE 开发环境、可视化 BI 系统，还是报表系统、工作流调度系统等，每个接入 3 个底层计算引擎。上述的计算治理问题，你可能得逐一重复的去解决 10*3=30 遍，而这正是当前在各个公司不断发生的现实情况，其造成的人力浪费不可小觑。&lt;/p&gt;&lt;p&gt;&lt;b&gt;扩展难问题，上层应用新增对接底层计算引擎，维护成本高，改动大。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 CS 的紧耦合模式下，上层应用每新增对接一个底层计算引擎，都需要有较大改动。&lt;/p&gt;&lt;p&gt;以对接 Spark 为例，在上层应用系统中的每一台需要提交 Spark 作业的机器，都需要部署和维护好 Java 和 Scala 运行时环境和变量，下载和部署 Spark Client 包，且配置并维护 Spark 相关的环境变量。如果要使用 Spark on YARN 模式，那么你还需要在每一台需要提交 Spark 作业的机器上，去部署和维护 Hadoop 相关的 jar 包和环境变量。再如果你的 Hadoop 集群需要启用 Kerberos 的，那么很不幸，你还需要在上述的每台机器去维护和调试 keytab、principal 等一堆 Kerberos 相关配置。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e7235ce879acdb698e94ceea0af41a1e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;324&quot; data-rawheight=&quot;260&quot; class=&quot;content_image&quot; width=&quot;324&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e7235ce879acdb698e94ceea0af41a1e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;324&quot; data-rawheight=&quot;260&quot; class=&quot;content_image lazy&quot; width=&quot;324&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-e7235ce879acdb698e94ceea0af41a1e_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这还仅仅是对接 Spark 一个底层引擎。随着上层应用系统和底层引擎的数量增多，需要维护的关系会是个笛卡尔积式的增长，光 Client 和配置的部署维护，就会成为一件很令人头疼的事情。&lt;/p&gt;&lt;p&gt;&lt;b&gt;应用孤岛问题，跨不同应用工具、不同计算任务间的互通问题。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;多个相互有关联的上层应用，向后台引擎提交执行的不同计算任务之间，往往是有所关联和共性的，比如需要共享一些用户定义的运行时环境变量、函数、程序包、数据文件等。当前情况往往是一个个应用系统就像一座座孤岛，相关信息和资源无法直接共享，需要手动在不同应用系统里重复定义和维护。&lt;/p&gt;&lt;p&gt;典型例子是在数据批处理程序开发过程中，用户在数据探索开发 IDE 系统中定义的一系列变量、函数，到了数据可视化系统里往往又要重新定义一遍；IDE 系统运行生成的数据文件位置和名称，不能直接方便的传递给可视化系统；依赖的程序包也需要从 IDE 系统下载、重新上传到可视化系统；到了工作流调度系统，这个过程还要再重复一遍。不同上层应用间，计算任务的运行依赖缺乏互通、复用能力。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2c7323c38a9aac2b1fce94b1345c4bc2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;458&quot; data-rawheight=&quot;221&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;458&quot; data-original=&quot;https://pic3.zhimg.com/v2-2c7323c38a9aac2b1fce94b1345c4bc2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2c7323c38a9aac2b1fce94b1345c4bc2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;458&quot; data-rawheight=&quot;221&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;458&quot; data-original=&quot;https://pic3.zhimg.com/v2-2c7323c38a9aac2b1fce94b1345c4bc2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-2c7323c38a9aac2b1fce94b1345c4bc2_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;计算治理之理（insight）- 细化特性问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;除了上述的架构层面问题，要想让复杂分布式架构环境下，各种类型的计算任务，都能更简洁、灵活、有序、可控的提交执行，和成功返回结果，计算治理还需关注高并发，高可用，多租户隔离，资源管控，安全增强，计算策略等等细化特性问题。这些问题都比较直白易懂，这里就不一一展开论述了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;基于计算中间件 Linkis 的计算治理 - 治之路（Architecture）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Linkis 架构设计介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;核心功能模块与流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;计算中间件 Linkis，是微众银行专门设计用来解决上述紧耦合、重复造轮子、扩展难、应用孤岛等计算治理问题的。当前主要解决的是复杂分布式架构的典型场景-数据平台环境下的计算治理问题。&lt;/p&gt;&lt;p&gt;Linkis 作为计算中间件，在上层应用和底层引擎之间，构建了一层中间层。能够帮助上层应用，通过其对外提供的标准化接口（如 HTTP, JDBC, Java …），快速的连接到多种底层计算存储引擎（如 Spark、Hive、TiSpark、MySQL、Python 等），提交执行各种类型的计算任务，并实现跨上层应用间的计算任务运行时上下文和依赖的互通和共享。且通过提供多租户、高并发、任务分发和管理策略、资源管控等特性支持，使得各种计算任务更灵活、可靠、可控的提交执行，成功返回结果，大大降低了上层应用在计算治理层的开发和运维成本、与整个环境的架构复杂度，填补了通用计算治理软件的空白。（图 8、9）&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-579984210165a31aa22b8b81742f4900_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1730&quot; data-rawheight=&quot;687&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1730&quot; data-original=&quot;https://pic1.zhimg.com/v2-579984210165a31aa22b8b81742f4900_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-579984210165a31aa22b8b81742f4900_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1730&quot; data-rawheight=&quot;687&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1730&quot; data-original=&quot;https://pic1.zhimg.com/v2-579984210165a31aa22b8b81742f4900_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-579984210165a31aa22b8b81742f4900_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3ee1f891bf312f4a35f822c2d006a29b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1730&quot; data-rawheight=&quot;687&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1730&quot; data-original=&quot;https://pic4.zhimg.com/v2-3ee1f891bf312f4a35f822c2d006a29b_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3ee1f891bf312f4a35f822c2d006a29b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1730&quot; data-rawheight=&quot;687&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1730&quot; data-original=&quot;https://pic4.zhimg.com/v2-3ee1f891bf312f4a35f822c2d006a29b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-3ee1f891bf312f4a35f822c2d006a29b_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;要更详细的了解计算任务通过 Linkis 的提交执行过程，我们先来看看 Linkis 核心的“计算治理服务”部分的内部架构和流程。如下图 10：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fd84192b9f0281319906f7b9bb7632e1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;622&quot; data-rawheight=&quot;425&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;622&quot; data-original=&quot;https://pic2.zhimg.com/v2-fd84192b9f0281319906f7b9bb7632e1_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fd84192b9f0281319906f7b9bb7632e1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;622&quot; data-rawheight=&quot;425&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;622&quot; data-original=&quot;https://pic2.zhimg.com/v2-fd84192b9f0281319906f7b9bb7632e1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-fd84192b9f0281319906f7b9bb7632e1_b.jpg&quot;/&gt;&lt;figcaption&gt;图 10&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;计算治理服务&lt;/b&gt;：计算中间件的核心计算框架，主要负责作业调度和生命周期管理、计算资源管理，以及引擎连接器的生命周期管理。&lt;/p&gt;&lt;p&gt;&lt;b&gt;公共增强服务&lt;/b&gt;：通用公共服务，提供基础公共功能，可服务于 Linkis 各种服务及上层应用系统。&lt;/p&gt;&lt;p&gt;&lt;b&gt;其中计算治理服务的主要模块如下：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;入口服务 Entrance&lt;/b&gt;，负责接收作业请求，转发作业请求给对应的 Engine，并实现异步队列、高并发、高可用、多租户隔离。&lt;/li&gt;&lt;li&gt;&lt;b&gt;应用管理服务 AppManager&lt;/b&gt;，负责管理所有的 EngineConnManager 和 EngineConn，并提供 EngineConnManager 级和 EngineConn 级标签能力；加载新引擎插件，向 RM 申请资源， 要求 EM 根据资源创建 EngineConn；基于标签功能，为作业分配可用 EngineConn。&lt;/li&gt;&lt;li&gt;&lt;b&gt;资源管理服务 ResourceManager&lt;/b&gt;，接收资源申请，分配资源，提供系统级、用户级资源管控能力，并为 EngineConnManager 级和 EngineConn 提供负载管控。&lt;/li&gt;&lt;li&gt;&lt;b&gt;引擎连接器管理服务 EngineConn Manager&lt;/b&gt;，负责启动 EngineConn，管理 EngineConn 的生命周期，并定时向 RM 上报资源和负载情况。&lt;/li&gt;&lt;li&gt;&lt;b&gt;引擎连接器 EngineConn&lt;/b&gt;，负责与底层引擎交互，解析和转换用户作业，提交计算任务给底层引擎，并实时监听底层引擎执行情况，回推相关日志、进度和状态给 Entrance。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如图 10 所示，一个作业的提交执行主要分为以下 11 步：&lt;/p&gt;&lt;p&gt;1. 上层应用向计算中间件提交作业，微服务网关 SpringCloud Gateway 接收作业并转发给 Entrance。&lt;/p&gt;&lt;p&gt;2. Entrance 消费作业，为作业向 AppManager 申请可用 EngineConn。&lt;/p&gt;&lt;p&gt;3. 如果不存在可复用的 Engine，AppManager 尝试向 ResourceManager 申请资源，为作业启动一个新 EngineConn。&lt;/p&gt;&lt;p&gt;4. 申请到资源，要求 EngineConnManager 依照资源启动新 EngineConn&lt;/p&gt;&lt;p&gt;5. EngineConnManager 启动新 EngineConn，并主动回推新 EngineConn 信息。&lt;/p&gt;&lt;p&gt;6. AppManager 将新 EngineConn 分配给 Entrance，Entrance 将 EngineConn 分配给用户作业，作业开始执行，将计算任务提交给 EngineConn。&lt;/p&gt;&lt;p&gt;7. EngineConn 将计算任务提交给底层计算引擎。&lt;/p&gt;&lt;p&gt;8. EngineConn 实时监听底层引擎执行情况，回推相关日志、进度和状态给 Entrance，Entrance 通过 WebSocket，主动回推 EngineConn 传过来的日志、进度和状态给上层应用系统。&lt;/p&gt;&lt;p&gt;9. EngineConn 执行完成后，回推计算任务的状态和结果集信息，Entrance 将作业和结果集信息更新到 JobHistory，并通知上层应用系统。&lt;/p&gt;&lt;p&gt;10. 上层应用系统访问 JobHistory，拿到作业和结果集信息。&lt;/p&gt;&lt;p&gt;11. 上层应用系统访问 Storage，请求作业结果集。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算任务管理策略支持&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在复杂分布式环境下，一个计算任务往往不单会是简单的提交执行和返回结果，还可能需要面对提交失败、执行失败、hang 住等问题，且在大量并发场景下还需通过计算任务的调度分发，解决租户间互相影响、负载均衡等问题。&lt;/p&gt;&lt;p&gt;Linkis 通过对计算任务的标签化，实现了在任务调度、分发、路由等方面计算任务管理策略的支持，并可按需配置超时、自动重试，及灰度、多活等策略支持。如下图 11。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-dc0cf3f02a89ffdeca040d9627b14bb6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;367&quot; data-rawheight=&quot;177&quot; class=&quot;content_image&quot; width=&quot;367&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-dc0cf3f02a89ffdeca040d9627b14bb6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;367&quot; data-rawheight=&quot;177&quot; class=&quot;content_image lazy&quot; width=&quot;367&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-dc0cf3f02a89ffdeca040d9627b14bb6_b.jpg&quot;/&gt;&lt;figcaption&gt;图 11&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;基于 Spring Cloud 微服务框架&lt;/b&gt;&lt;/p&gt;&lt;p&gt;说完了业务架构，我们现在来聊聊技术架构。在计算治理层环境下，很多类型的计算任务具有生命周期较短的特征，如一个 Spark job 可能几十秒到几分钟就执行完，EngineConn（EnginConnector）会是大量动态启停的状态。前端用户和 Linkis 中其他管理角色的服务，需要能够及时动态发现相关服务实例的状态变化，并获取最新的服务实例访问地址信息。同时需要考虑，各模块间的通信、路由、协调，及各模块的横向扩展、负载均衡、高可用等能力。&lt;/p&gt;&lt;p&gt;基于以上需求，Linkis 实际是基于 Spring Cloud 微服务框架技术，将上述的每一个模块/角色，都封装成了一个微服务，构建了多个微服务组，整合形成了 Linkis 的完整计算中间件能力。如下图 12：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8fc1ab473801e54dc1306445c0de4849_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;455&quot; data-rawheight=&quot;230&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;455&quot; data-original=&quot;https://pic2.zhimg.com/v2-8fc1ab473801e54dc1306445c0de4849_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-8fc1ab473801e54dc1306445c0de4849_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;455&quot; data-rawheight=&quot;230&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;455&quot; data-original=&quot;https://pic2.zhimg.com/v2-8fc1ab473801e54dc1306445c0de4849_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-8fc1ab473801e54dc1306445c0de4849_b.jpg&quot;/&gt;&lt;figcaption&gt;图 12&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;从多租户管理角度，上述服务可区分为租户相关服务，和租户无关服务两种类型。租户相关服务，是指一些任务逻辑处理负荷重、资源消耗高，或需要根据具体租户、用户、物理机器等，做隔离划分、避免相互影响的服务，如 Entrance、 EnginConn（EnginConnector） Manager、EnginConn；其他如 App Manger、Resource Manager、Context Service 等服务，都是租户无关的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Eureka&lt;/b&gt; 承担了微服务动态注册与发现中心，及所有租户无关服务的负载均衡、故障转移功能。&lt;/p&gt;&lt;p&gt;Eureka 有个局限，就是在其客户端，对后端微服务实例的发现与状态刷新机制，是客户端主动轮询刷新，最快可设 1 秒 1 次（实际要几秒才能完成刷新）。这样在 Linkis 这种需要快速刷新大量后端 EnginConn 等服务的状态的场景下，时效得不到满足，且定时轮询刷新对 Eureka server、对后端微服务实例的成本都很高。&lt;/p&gt;&lt;p&gt;为此我们对 Spring Cloud Ribbon 做了改造，在其中封装了 Eureka client 的微服务实例状态刷新方法，并把它做成满足条件主动请求刷新，而不会再频繁的定期轮询。从而在满足时效的同时，大大降低了状态获取的成本。如下图 13：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-65e80a4ec01e43b53766b21d4506b7c5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;456&quot; data-rawheight=&quot;108&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;456&quot; data-original=&quot;https://pic2.zhimg.com/v2-65e80a4ec01e43b53766b21d4506b7c5_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-65e80a4ec01e43b53766b21d4506b7c5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;456&quot; data-rawheight=&quot;108&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;456&quot; data-original=&quot;https://pic2.zhimg.com/v2-65e80a4ec01e43b53766b21d4506b7c5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-65e80a4ec01e43b53766b21d4506b7c5_b.jpg&quot;/&gt;&lt;figcaption&gt;图 13&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;Spring Cloud Gateway&lt;/b&gt; 承担了外部请求 Linkis 的入口网关的角色，帮助在服务实例不断发生变化的情况下，简化前端用户的调用逻辑，快速方便的获取最新的服务实例访问地址信息。&lt;/p&gt;&lt;p&gt;Spring Cloud Gateway 有个局限，就是一个 WebSocket 客户端只能将请求转发给一个特定的后台服务，无法完成一个 WebSocket 客户端通过网关 API 对接后台多个 WebSocket 微服务，而这在我们的 Entrance HA 等场景需要用到。&lt;/p&gt;&lt;p&gt;为此 Linkis 对 Spring Cloud Gateway 做了相应改造，在 Gateway 中实现了 WebSocket 路由转发器，用于与客户端建立 WebSocket 连接。建立连接成功后，会自动分析客户端的 WebSocket 请求，通过规则判断出请求该转发给哪个后端微服务，然后将 WebSocket 请求转发给对应的后端微服务实例。详见 Github 上 Linkis 的 Wiki 中，“Gateway 的多 WebSocket 请求转发实现”一文。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9efd19acc21dc63b26a5aeda5e2fdbef_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1730&quot; data-rawheight=&quot;613&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1730&quot; data-original=&quot;https://pic4.zhimg.com/v2-9efd19acc21dc63b26a5aeda5e2fdbef_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9efd19acc21dc63b26a5aeda5e2fdbef_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1730&quot; data-rawheight=&quot;613&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1730&quot; data-original=&quot;https://pic4.zhimg.com/v2-9efd19acc21dc63b26a5aeda5e2fdbef_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-9efd19acc21dc63b26a5aeda5e2fdbef_b.jpg&quot;/&gt;&lt;figcaption&gt;图 14&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;Spring Cloud OpenFeign &lt;/b&gt;提供的 HTTP 请求调用接口化、解析模板化能力，帮助 Linkis 构建了底层 RPC 通信框架。&lt;/p&gt;&lt;p&gt;但基于 Feign 的微服务之间 HTTP 接口的调用，只能满足简单的 A 微服务实例根据简单的规则随机选择 B 微服务之中的某个服务实例，而这个 B 微服务实例如果想异步回传信息给调用方，是无法实现的。同时，由于 Feign 只支持简单的服务选取规则，无法做到将请求转发给指定的微服务实例，无法做到将一个请求广播给接收方微服务的所有实例。&lt;/p&gt;&lt;p&gt;Linkis 基于 Feign 实现了一套自己的底层 RPC 通信方案，集成到了所有 Linkis 的微服务之中。一个微服务既可以作为请求调用方，也可以作为请求接收方。作为请求调用方时，将通过 Sender 请求目标接收方微服务的 Receiver；作为请求接收方时，将提供 Receiver 用来处理请求接收方 Sender 发送过来的请求，以便完成同步响应或异步响应。如下图示意。详见 GitHub 上 Linkis 的 Wiki 中，“Linkis RPC 架构介绍”一文。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-dc560cfd06572228149212dc9c5c4cdd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1730&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1730&quot; data-original=&quot;https://pic2.zhimg.com/v2-dc560cfd06572228149212dc9c5c4cdd_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-dc560cfd06572228149212dc9c5c4cdd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1730&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1730&quot; data-original=&quot;https://pic2.zhimg.com/v2-dc560cfd06572228149212dc9c5c4cdd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-dc560cfd06572228149212dc9c5c4cdd_b.jpg&quot;/&gt;&lt;figcaption&gt;图 15&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;至此，Linkis 对上层应用和底层引擎的解耦原理，其核心架构与流程设计，及基于 Spring Cloud 微服务框架实现的，各模块微服务化动态管理、通信路由、横向扩展能力介绍完毕。&lt;/p&gt;&lt;p&gt;&lt;b&gt;解耦：Linkis 如何解耦上层应用和底层引擎&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Linkis 作为计算中间件，在上层应用和底层引擎之间，构建了一层中间层。上层应用所有计算任务，先通过 HTTP、WebSocket、Java 等接口方式提交给 Linkis，再由 Linkis 转交给底层引擎。原有的上层应用以 CS 模式直连底层引擎的紧耦合得以解除，因此实现了解耦。如下图 16 所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9c75d52920f43627461f91545d4fb442_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;429&quot; data-rawheight=&quot;327&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;429&quot; data-original=&quot;https://pic3.zhimg.com/v2-9c75d52920f43627461f91545d4fb442_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9c75d52920f43627461f91545d4fb442_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;429&quot; data-rawheight=&quot;327&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;429&quot; data-original=&quot;https://pic3.zhimg.com/v2-9c75d52920f43627461f91545d4fb442_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-9c75d52920f43627461f91545d4fb442_b.jpg&quot;/&gt;&lt;figcaption&gt;图 16&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;通过解耦，底层引擎的变动有了 Linkis 这层中间件缓冲，如引擎 client 的版本升级，无需再对每一个对接的上层应用做逐个改动，可在 Linkis 层统一完成。并能在 Linkis 层，实现对上层应用更加透明和友好的升级策略，如灰度切换、多活等策略支持。且即使后继接入更多上层应用和底层引擎，整个环境复杂度也不会有大的变化，大大降低了开发运维工作负担。&lt;/p&gt;&lt;p class=&quot;ztext-empty-paragraph&quot;&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;复用：对于上层应用，Linkis 如何凝练计算治理模块供复用，避免重复开发&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;上层应用复用 Linkis 示例（Scriptis）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有了 Linkis，上层应用可以基于 Linkis，快速实现对多种后台计算存储引擎的对接支持，及变量、函数等自定义与管理、资源管控、多租户、智能诊断等计算治理特性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;优点&lt;/b&gt;&lt;/p&gt;&lt;p&gt;以微众银行与 Linkis 同时开源的，交互式数据开发探索工具 Scriptis 为例，Scriptis 的开发人员只需关注 Web UI、多种数据开发语言支持、脚本编辑功能等纯前端功能实现，Linkis 包办了其从存储读写、计算任务提交执行、作业状态日志更新、资源管控等等几乎所有后台功能。基于 Linkis 的大量计算治理层能力的复用，大大降低了 Scriptis 项目的开发成本，使得 Scritpis 目前只需要有限的前端人员，即可完成维护和版本迭代工作。&lt;/p&gt;&lt;p&gt;如下图 17，Scriptis 项目 99.5% 的代码，都是前端的 JS、CSS 代码。后台基本完全复用 Linkis。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-74149e1cf67e66728cda3b8d99b04f8f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1727&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1727&quot; data-original=&quot;https://pic4.zhimg.com/v2-74149e1cf67e66728cda3b8d99b04f8f_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-74149e1cf67e66728cda3b8d99b04f8f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1727&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1727&quot; data-original=&quot;https://pic4.zhimg.com/v2-74149e1cf67e66728cda3b8d99b04f8f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-74149e1cf67e66728cda3b8d99b04f8f_b.jpg&quot;/&gt;&lt;figcaption&gt;图 17&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;快速扩展：对于底层引擎，Linkis 如何以很小的开发量，实现新底层引擎快速对接&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;模块化可插拔的计算引擎接入设计，新引擎接入简单快速。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于典型交互式模式计算引擎（提交任务，执行，返回结果），用户只需要 buildApplication 和 executeLine 这 2 个方法，就可以完成一个新的计算引擎接入 Linkis，代码量极少。示例如下。&lt;/p&gt;&lt;p&gt;(1) AppManager 部分：用户必须实现的接口是 ApplicationBuilder，用来封装新引擎连接器实例启动命令。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;1.  //用户必须实现的方法: 用于封装新引擎连接器实例启动命令

2.  def buildApplication(protocol:Protocol):ApplicationRequest&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(2) EngineConn部分：用户只需实现executeLine方法，向新引擎提交执行计算任务：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;1.  //用户必须实现的方法：用于调用底层引擎提交执行计算任务

2.  def executeLine(context: EngineConnContext,code: String): ExecuteResponse&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;引擎相关其他功能/方法都已有默认实现，无定制化需求可直接复用。 &lt;/p&gt;&lt;p&gt;&lt;b&gt;连通：Linkis 如何打通应用孤岛&lt;/b&gt;&lt;/p&gt;&lt;p&gt;通过 Linkis 提供的上下文服务，和存储、物料库服务，接入的多个上层应用之间，可轻松实现环境变量、函数、程序包、数据文件等，相关信息和资源的共享和复用，打通应用孤岛。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-429d8c2569cc0ea9f8d82ddc54ecd74a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1420&quot; data-rawheight=&quot;1005&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1420&quot; data-original=&quot;https://pic3.zhimg.com/v2-429d8c2569cc0ea9f8d82ddc54ecd74a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-429d8c2569cc0ea9f8d82ddc54ecd74a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1420&quot; data-rawheight=&quot;1005&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1420&quot; data-original=&quot;https://pic3.zhimg.com/v2-429d8c2569cc0ea9f8d82ddc54ecd74a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-429d8c2569cc0ea9f8d82ddc54ecd74a_b.jpg&quot;/&gt;&lt;figcaption&gt;图 18&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;Context Service 上下文服务介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Context Service（CS）为不同上层应用系统，不同计算任务，提供了统一的上下文管理服务，可实现上下文的自定义和共享。在 Linkis 中，CS 需要管理的上下文内容，可分为元数据上下文、数据上下文和资源上下文 3 部分。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1a033c37974cd5578f49e602cc6a0ace_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;455&quot; data-rawheight=&quot;78&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;455&quot; data-original=&quot;https://pic3.zhimg.com/v2-1a033c37974cd5578f49e602cc6a0ace_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1a033c37974cd5578f49e602cc6a0ace_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;455&quot; data-rawheight=&quot;78&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;455&quot; data-original=&quot;https://pic3.zhimg.com/v2-1a033c37974cd5578f49e602cc6a0ace_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-1a033c37974cd5578f49e602cc6a0ace_b.jpg&quot;/&gt;&lt;figcaption&gt;图 19&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;元数据上下文，定义了计算任务中底层引擎元数据的访问和使用规范，主要功能如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;提供用户的所有元数据信息读写接口（包括 Hive 表元数据、线上库表元数据、其他 NoSQL 如 HBase、Kafka 等元数据）。&lt;/li&gt;&lt;li&gt;计算任务内所需元数据的注册、缓存和管理。&lt;/li&gt;&lt;li&gt;数据上下文，定义了计算任务中数据文件的访问和使用规范。管理数据文件的元数据。&lt;/li&gt;&lt;li&gt;运行时上下文，管理各种用户自定义的变量、函数、代码段、程序包等。&lt;/li&gt;&lt;li&gt;同时 Linkis 也提供了统一的物料管理和存储服务，上层应用可根据需要对接，从而可实现脚本文件、程序包、数据文件等存储层的打通。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;基于计算中间件 Linkis 的计算治理 - 理之路（Insight）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Linkis 计算治理细化特性设计与实现介绍，在高并发、高可用、多租户隔离、资源管控、计算任务管理策略等方面，做了大量细化考量和实现，保障计算任务在复杂条件下成功执行。&lt;/p&gt;&lt;p&gt;&lt;b&gt;计算任务的高并发支持&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Linkis 的 Job 基于多级异步设计模式，服务间通过高效的 RPC 和消息队列模式进行快速通信，并可以通过给 Job 打上创建者、用户等多种类型的标签进行任务的转发和隔离来提高 Job 的并发能力。通过 Linkis 可以做到 1 个入口服务（Entrance）同时承接超 1 万+ 在线的 Job 请求。&lt;/p&gt;&lt;p&gt;多级异步的设计架构图如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ae35ae0c0f644a86446a6bc4be5e20b9_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;361&quot; data-rawheight=&quot;347&quot; class=&quot;content_image&quot; width=&quot;361&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ae35ae0c0f644a86446a6bc4be5e20b9_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;361&quot; data-rawheight=&quot;347&quot; class=&quot;content_image lazy&quot; width=&quot;361&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-ae35ae0c0f644a86446a6bc4be5e20b9_b.jpg&quot;/&gt;&lt;figcaption&gt;图 20&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如上图所示 Job 从 GateWay 到 Entrance 后，Job 从生成到执行，到信息推送经历了多个线程池，每个环节都通过异步的设计模式，每一个线程池中的线程都采用运行一次即结束的方式，降低线程开销。整个 Job 从请求—执行—到信息推送全都异步完成，显著的提高了 Job 的并发能力。&lt;/p&gt;&lt;p&gt;这里针对计算任务最关键的一环 Job 调度层进行说明，海量用户成千上万的并发任务的压力，在 Job 调度层中是如何进行实现的呢？&lt;/p&gt;&lt;p&gt;在请求接收层，请求接收队列中，会缓存前端用户提交过来的成千上万计算任务，并按系统/用户层级划分的调度组，分发到下游 Job 调度池中的各个调度队列；到 Job 调度层，多个调度组对应的调度器，会同时消费对应的调度队列，获取 Job 并提交给 Job 执行池进行执行。过程中大量使用了多线程、多级异步调度执行等技术。示意如下图 21：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-847b75dc94a7fc991f20ad3df3b81459_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;366&quot; data-rawheight=&quot;414&quot; class=&quot;content_image&quot; width=&quot;366&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-847b75dc94a7fc991f20ad3df3b81459_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;366&quot; data-rawheight=&quot;414&quot; class=&quot;content_image lazy&quot; width=&quot;366&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-847b75dc94a7fc991f20ad3df3b81459_b.jpg&quot;/&gt;&lt;figcaption&gt;图 21&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;其他细化特性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Linkis 还在高可用、多租户隔离、资源管控、计算任务管理策略等方面，做了很多细化考量和实现。篇幅有限，在这里不再详述每个细化特性的实现，可参见 Github 上 Linkis 的 Wiki。后继我们会针对 Linkis 的计算治理-理之路（Insight）的细化特性相关内容，再做专题介绍。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结语&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;基于如上解耦、复用、快速扩展、连通等架构设计优点，及高并发、高可用、多租户隔离、资源管控等细化特性实现，计算中间件 Linkis 在微众生产环境的应用效果显著。极大的助力了微众银行一站式大数据平台套件 WeDataSphere 的快速构建，且构成了 WeDataSphere 全连通、多租户、资源管控等企业级特性的基石。&lt;/p&gt;&lt;p&gt;Linkis 在微众应用情况如图 22：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f47c1a8c2068b456ffb02c6d4e1db9bb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;406&quot; data-rawheight=&quot;101&quot; class=&quot;content_image&quot; width=&quot;406&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f47c1a8c2068b456ffb02c6d4e1db9bb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;406&quot; data-rawheight=&quot;101&quot; class=&quot;content_image lazy&quot; width=&quot;406&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-f47c1a8c2068b456ffb02c6d4e1db9bb_b.jpg&quot;/&gt;&lt;figcaption&gt;图 22&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们已将 Linkis 开源，Github repo 地址：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/WeBankFinTech/Linkis&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/WeBankFinTec&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;h/Linkis&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;欢迎对类似计算治理问题感兴趣的同学，参与到计算中间件 Linkis 的社区协作中，共同把 Linkis 建设得更加完善和易用。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;：邸帅，微众银行大数据平台负责人，主导微众银行 WeDataSphere 大数据平台套件的建设运营与开源，具备丰富的大数据平台开发建设实践经验。&lt;/blockquote&gt;&lt;p&gt;本文是「分布式系统前沿技术」专题文章，目前该专题在持续更新中，欢迎大家保持关注！&lt;/p&gt;&lt;p&gt;&lt;b&gt;专题地址：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/theme/48&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/theme/48&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-31-100383665</guid>
<pubDate>Tue, 31 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>分布式系统 in 2010s ：软件构建方式和演化</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-31-100325641.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/100325641&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2ac2bbb8366274bf43bd936c96eb2bff_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄东旭 &lt;a class=&quot;member_mention&quot; href=&quot;https://www.zhihu.com/people/5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hash=&quot;5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hovercard=&quot;p$b$5940b1ec1c21a3538c6cfcf5711a75a6&quot;&gt;@Ed Huang&lt;/a&gt; &lt;/p&gt;&lt;blockquote&gt;分布式技术的发展，深刻地改变了我们编程的模式和思考软件的模式。值 2019 岁末，PingCAP 联合 InfoQ 共同策划出品“分布式系统前沿技术 ”专题， 邀请众多技术团队共同参与，一起探索这个古老领域的新生机。本文出自我司 CTO 黄东旭，为「分布式系统 in 2010s」 系列第二篇，第一篇请见《&lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/99587904&quot; class=&quot;internal&quot;&gt;分布式系统 in 2010s ：&lt;/a&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247490559%26idx%3D1%26sn%3D2c15756d09fa8582cde4197fb8dd1c35%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;存储之数据库篇&lt;/a&gt;&lt;/u&gt;》。&lt;/blockquote&gt;&lt;p&gt;我上大学的时候专业是软件工程，当时的软件工程是 CMM、瀑布模型之类。十几年过去了，看看现在我们的软件开发模式，尤其是在互联网行业，敏捷已经成为主流，很多时候老板说业务下周上线，那基本就是怎么快怎么来，所以现代架构师对于可复用性和弹性会有更多的关注。我所知道业界对 SOA 的关注是从 Amazon 的大规模 SOA 化开始， 2002 年 Bezos 要求 Amazon 的工程团队将所有的业务 API 和服务化，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.cio.com/article/3218667/have-you-had-your-bezos-moment-what-you-can-learn-from-amazon.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;几条原则&lt;/a&gt;放在今天仍然非常适用：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;i&gt;All teams will henceforth expose their data and functionality through service interfaces.&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;Teams must communicate with each other through these interfaces.&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;There will be no other form of inter-process communication allowed: no direct linking, no direct reads of another team’s data store, no shared-memory model, no back-doors whatsoever. The only communication allowed is via service interface calls over the network.&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;It doesn’t matter what technology they use.&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;i&gt;All service interfaces, without exception, must be designed from the ground up to be externalizable. That is to say, the team must plan and design to be able to expose the interface to developers in the outside world. No exceptions.&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;尤其最后一条，我个人认为对于后来的 AWS 的诞生有直接的影响，另外这条也间接地对工程团队的软件质量和 API 质量提出了更高的要求。亚马逊在 SOA 上的实践是组件化在分布式环境中的延伸，尽可能地将业务打散成最细粒度的可复用单元（Services），新的业务通过组合的方式构建。这样的原则一直发展到今天，我们提到的微服务、甚至 Serverless，都是这个思想的延伸。&lt;/p&gt;&lt;h2&gt;SOA 只是一个方法论&lt;/h2&gt;&lt;p&gt;很多人在思考 SOA 和微服务的区别时，经常有一些观点类似：「拆的粗就是 SOA，拆的细就是微服务 」，「使用 RESTful API 就是微服务，用 RPC 是 SOA」，「使用 XXX（可以是任何流行的开源框架） 的是微服务，使用 YYY 的是 SOA」... 这些观点我其实并不认可，我理解的 SOA 或者微服务只是一个方法论，核心在于有效地拆分应用，实现敏捷构建和部署，至于使用什么技术或者框架其实无所谓，甚至 SOA 本身就是反对绑定在某项技术上的。&lt;/p&gt;&lt;p&gt;对于架构师来说， 微服务化也并不是灵丹妙药，有一些核心问题，在微服务化的实践中经常会遇到：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;服务的拆分粒度到底多细？&lt;/li&gt;&lt;li&gt;大的单体服务如何避免成为单点，如何支持快速的弹性水平扩展？&lt;/li&gt;&lt;li&gt;如何进行流控和降级？防止调用者 DDoS？&lt;/li&gt;&lt;li&gt;海量服务背景下的 CI/CD (测试，版本控制，依赖管理)，运维（包括 tracing，分布式 metric 收集，问题排查）&lt;br/&gt;… …&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;上面几个问题都很大。熟悉多线程编程的朋友可能比较熟悉 Actor 模型，我认为 Actor 的思想和微服务还是很接近的，同样的最佳实践也可以在分布式场景下适用，事实上 Erlang OTP 和 Scala 的 Akka Framework 都尝试直接将 Actor 模型在大规模分布式系统中应用。其实在软件工程上这个也不是新的东西，Actor 和 CSP 的概念几乎在软件诞生之初就存在了，现在服务化的兴起我认为是架构复杂到一定程度后很自然的选择，就像当年 CSP 和 Actor 简化并发编程一样。&lt;/p&gt;&lt;h2&gt;服务化和云&lt;/h2&gt;&lt;p&gt;从服务化的大方向和基础设施方面来说，我们这几年经历了：本地单体服务 + 私有 API （自建数据中心，自己运维管理） -&amp;gt; 云 IaaS + 本地服务 + 云提供的 Managed Service (例如 EC2 + RDS) -&amp;gt; Serverless 的转变。其本质在于云的出现让开发者对于硬件控制力越来越低，算力和服务越来越变成标准化的东西。而容器的诞生，使得资源复用的粒度进一步的降低（物理机 -&amp;gt; VM -&amp;gt; Container），这无疑是云厂商非常希望看到的。对公有云厂商来说，资源分配的粒度越细越轻量，就越能精准地分配，以提升整体的硬件资源利用率，实现效益最大化。&lt;/p&gt;&lt;p&gt;这里暗含着一个我的观点：公有云和私有云在价值主张和商业模式上是不一样的：对公有云来说，只有不断地规模化，通过不断提升系统资源的利用率，获取收益（比如主流的公有云几乎对小型实例都会超卖）。而私有云的模式可以概括成降低运维成本（标准化服务 + 自动化运维），对于自己拥有数据中心的企业来说，通过云技术提升硬件资源的利用率是好事，只是这个收益并没有公有云的规模化收益来得明显。&lt;/p&gt;&lt;p&gt;在服务化的大背景下，也产生了另外一个趋势，就是基础软件的垂直化和碎片化，当然这也是和现在的 workload 变得越来越大，单一的数据库软件或者开发框架很难满足多变且极端的需求有关。数据库、对象存储、RPC、缓存、监控这几个大类，几乎每位架构师都熟悉多个备选方案，根据不同需求排列组合，一个 Oracle 包打天下的时代已经过去了。&lt;/p&gt;&lt;p&gt;这样带来的结果是数据或状态在不同系统之间的同步和传递成为一个新的普遍需求，这就是为什么以 Kafka，Pulsar 为代表的分布式的消息队列越来越流行。但是在异构数据源之间的同步，暗含了异步和不一致（如果需要一致性，那么就需要对消费者实现幂等的语义），在一些对一致性有极端需求的场景，仍然需要交给数据库处理。&lt;/p&gt;&lt;p&gt;在这种背景下，容器的出现将计算资源分配的粒度进一步的降低且更加标准化，硬件对于开发者来说越来越透明，而且随着 workload 的规模越来越大，就带来的一个新的挑战：海量的计算单元如何管理，以及如何进行服务编排。既然有编排这里面还隐含了另外一个问题：服务的生命周期管理。&lt;/p&gt;&lt;h2&gt;Kubernetes 时代开始了&lt;/h2&gt;&lt;p&gt;其实在 Kubernetes 诞生之前，很多产品也做过此类尝试，例如 Mesos。Mesos 早期甚至并不支持容器，主要设计的目标也是短任务（后通过 Marathon Framework 支持长服务），更像一个分布式的工作流和任务管理（或者是分布式进程管理）系统，但是已经体现了 Workload 和硬件资源分离的思想。&lt;/p&gt;&lt;p&gt;在前 Kubernetes 时代，Mesos 的设计更像是传统的系统工程师对分布式任务调度的思考和实践，而 K8s 的野心更大，从设计之初就是要在硬件层之上去抽象所有类型的 workload，构建自己的生态系统。如果说 Mesos 还是个工具的话，那么 K8s 的目标其实是奔着做一个分布式操作系统去的。简单做个类比：整个集群的计算资源统一管控起来就像一个单机的物理计算资源，容器就像一个个进程，Overlay network 就像进程通信，镜像就像一个个可执行文件，Controller 就像 Systemd，Kubectl 就像 Shell……同样相似的类比还有很多。&lt;/p&gt;&lt;p&gt;从另一方面看，Kubernetes 为各种 IaaS 层提供了一套标准的抽象，不管你底层是自己的数据中心的物理机，还是某个公有云的 VM，只要你的服务是构建在 K8s 之上，那么就获得了无缝迁移的能力。K8s 就是一个更加中立的云，在我的设想中，未来不管是公有云还是私有云都会提供标准 K8s 能力。对于业务来说，基础架构的上云，最安全的路径就是上 K8s，目前从几个主流的公有云厂商的动作上来看（GCP 的 GKE，AWS 的 EKS，Azure 的 AKS），这个假设是成立的。&lt;/p&gt;&lt;p&gt;不选择 K8s 的人很多时候会从性能角度来攻击 K8s，理由是：多一层抽象一定会损害性能。对于这个我是不太同意的。从网络方面说，大家可能有个误解，认为 Overlay Network 的性能一定不好，其实这不一定是事实。下面这张图来自 ITNEXT 的工程师对几个流行的 CNI 实现的&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-36475925a560&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;评测&lt;/a&gt;：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-18a35455a124b8114c974f3fb3afa1ee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1440&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1440&quot; data-original=&quot;https://pic3.zhimg.com/v2-18a35455a124b8114c974f3fb3afa1ee_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-18a35455a124b8114c974f3fb3afa1ee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1440&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1440&quot; data-original=&quot;https://pic3.zhimg.com/v2-18a35455a124b8114c974f3fb3afa1ee_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-18a35455a124b8114c974f3fb3afa1ee_b.jpg&quot;/&gt;&lt;figcaption&gt;Kubernetses CNI benchmark&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们其实可以看到，除了 WaveNet Encrypted 因为需要额外的加密导致性能不佳以外，其它的 CNI 实现几乎已经和 Bare metal 的 host network 性能接近，出现异常的网络延迟大多问题是出现在 iptable NAT 或者 Ingress 的错误配置上面。&lt;/p&gt;&lt;p&gt;所以软件的未来在哪里？我个人的意见是硬件和操作系统对开发者会更加的透明，也就是现在概念刚开始普及起来的 Serverless。我经常用的一个比喻是：如果自己维护数据中心，采购服务器的话，相当于买房；使用云 IaaS 相当于租房；而 Serverless，相当于住酒店。长远来看，这三种方案都有各自适用的范围，并不是谁取代谁的关系。目前看来 Serverless 因为出现时间最短，所以发展的潜力也是最大的。&lt;/p&gt;&lt;p&gt;从服务治理上来说，微服务的碎片化必然导致了管理成本上升，所以近年 Service Mesh （服务网格）的概念才兴起。 服务网格虽然名字很酷，但是其实可以想象成就是一个高级的负载均衡器或服务路由。比较新鲜的是 Sidecar 的模式，将业务逻辑和通信解耦。我其实一直相信未来在七层之上，会有一层以 Service Mesh 和服务为基础的「八层网络」，不过目前并没有一个事实标准出现。Istio 的整体架构过于臃肿，相比之下我更加喜欢单纯使用 Envoy 或者 Kong 这样更加轻量的 API Proxy。 不过我认为目前在 Service Mesh 领域还没有出现有统治地位的解决方案，还需要时间。&lt;/p&gt;&lt;blockquote&gt;本文是「分布式系统前沿技术」专题文章，目前该专题在持续更新中，欢迎大家保持关注。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-31-100325641</guid>
<pubDate>Tue, 31 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>如何打造可以无限扩展的分布式消息队列?——Pulsar 的设计哲学</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-27-99800206.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/99800206&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2ac2bbb8366274bf43bd936c96eb2bff_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;分布式技术的发展，深刻地改变了我们编程的模式和思考软件的模式。值 2019 岁末，PingCAP 联合 InfoQ 共同策划出品“分布式系统前沿技术 ”专题， 邀请众多技术团队共同参与，一起探索这个古老领域的新生机。本文出自 StreamNative 联合创始人 Sijie Guo。&lt;/blockquote&gt;&lt;p&gt;几十年前，消息队列开始兴起，它用于连接大型机和服务器应用程序，并逐渐在企业的服务总线与事件总线设计模式、应用间的路由和数据迁移中发挥至关重要的作用。自此，应用程序架构和数据角色经历了重大变化：例如，面向服务的架构、流处理、微服务、容器化、云服务和边缘计算，这些只是诸多变化中的冰山一角。这些变化创造了大量的新需求，这些新需求远远超出了原有消息队列的技术能力。&lt;/p&gt;&lt;p&gt;为了满足这些需求，处理消息队列的全新方法应运而生。现代应用对消息解决方案的要求不仅仅是主动连接、移动数据，而是要在持续增长的服务和应用中智能处理、分析和传输数据，并且在规模持续扩大的情况下不增加运营负担。&lt;/p&gt;&lt;p&gt;为了满足上述要求，新一代的消息传递和数据处理解决方案 Apache Pulsar 应运而生。Apache Pulsar 起初作为消息整合平台在 Yahoo 内部开发、部署，为 Yahoo Finance、Yahoo Mail 和 Flickr 等雅虎内部关键应用连接数据。2016 年 Yahoo 把 Pulsar 开源并捐给 Apache 软件基金会（ASF），2018 年 9 月 Pulsar 毕业成为 ASF 的顶级项目，逐渐从单一的消息系统演化成集消息、存储和函数式轻量化计算的流数据平台。&lt;/p&gt;&lt;p&gt;Pulsar 的设计是为了方便和现有的 Kafka 部署集成，同时也方便开发人员将其连接到应用程序。Pulsar 最初就是为连接 Kafka 构建的。Pulsar 提供和 Kafka 兼容的 API，无需更改代码，只要使用 Pulsar 客户端库重新编译，现有应用程序即可连接到 Kafka。Pulsar 还提供内置的 Kafka 连接器，可以消费 Kafka topic 的数据或将数据发布到 Kafka topic。&lt;/p&gt;&lt;p&gt;系统架构是软件最底层的设计决策，一旦实施，就很难改变。架构决定了软件特性和根本不同。Apache Pulsar 在功能上有很多优势，例如统一的消费模型，多租户，高可用性等等，但最本质、最重要的区别还是 Apache Pulsar 的系统架构。Apache Pulsar 的设计架构与其他消息传递解决方案（包括 Apache Kafka）的架构有着本质不同，Pulsar 从设计时就采用了分层分片式的架构，以提供更好的性能、可扩展性和灵活性。&lt;/p&gt;&lt;p&gt;现实生活中，存在的消息系统有很多，Yahoo 为什么研发自己的消息系统呢？因为已有的消息系统无法解决 Yahoo 遇到的问题和规模，Yahoo 需要多租户，能够支撑上百万的 topics，同时满足低延迟、持久化和跨地域复制要求。而现有的消息系统，存在如下诸多问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分区模型紧耦合存储和计算，不是云原生（Cloud Native）的设计。&lt;/li&gt;&lt;li&gt;存储模型过于简单，对文件系统依赖太强。&lt;/li&gt;&lt;li&gt;IO 不隔离，消费者在清除 Backlog 时会影响其他生产者和消费者。&lt;/li&gt;&lt;li&gt;运维复杂，替换机器、服务扩容需重新均衡数据。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;于是，我们决定开始研发 Pulsar来解决消息队列的扩展性问题。解决扩展性问题的核心思路是数据分片，Pulsar 从设计时就采用了分层分片式的架构，以提供更好的性能、可扩展性和灵活性。&lt;/p&gt;&lt;p&gt;下面我们从技术角度来详细解析 Apache Pulsar 的架构。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pulsar 的分层架构&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;从数据库到消息系统，大多数分布式系统采用了数据处理和数据存储共存于同一节点的方法。这种设计减少了网络上的数据传输，可以提供更简单的基础架构和性能优势，但其在系统可扩展性和高可用性上会大打折扣。&lt;/p&gt;&lt;p&gt;Pulsar 架构中数据服务和数据存储是单独的两层：数据服务层由无状态的 “Broker” 节点组成，而数据存储层则由 “Bookie” 节点组成。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d1620cf298a5d5d5626f2273ac6d9c26_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1178&quot; data-rawheight=&quot;402&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1178&quot; data-original=&quot;https://pic3.zhimg.com/v2-d1620cf298a5d5d5626f2273ac6d9c26_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d1620cf298a5d5d5626f2273ac6d9c26_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1178&quot; data-rawheight=&quot;402&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1178&quot; data-original=&quot;https://pic3.zhimg.com/v2-d1620cf298a5d5d5626f2273ac6d9c26_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-d1620cf298a5d5d5626f2273ac6d9c26_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1 传统单体架构 vs. Pulsar 存储计算分层架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这种存储和计算分离的架构给 Pulsar 带来了很多优势。首先，在 Pulsar 这种分层架构中，服务层和存储层都能够独立扩展，可以提供灵活的弹性扩容。特别是在弹性环境（例如云和容器）中能够自动扩容缩容，并动态适应流量的峰值。并且， Pulsar 这种分层架构显著降低了集群扩展和升级的复杂性，提高了系统可用性和可管理性。此外，这种设计对容器是非常友好的，这使 得Pulsar 也成为了流原生平台的理想选择。 &lt;/p&gt;&lt;p&gt;Pulsar 系统架构的优势也包括 Pulsar 分片存储数据的方式。Pulsar 将主题分区按照更小的分片粒度来存储，然后将这些分片均匀打散分布在存储层的 “bookie” 节点上。这种以分片为中心的数据存储方式，将主题分区作为一个逻辑概念，分为多个较小的分片，并均匀分布和存储在存储层中。这种架构设计为 Pulsar 带来了更好的性能，更灵活的扩展性和更高的可用性。&lt;/p&gt;&lt;p&gt;Pulsar 架构中的每层都可以单独设置大小，进行扩展和配置。根据其在不同服务中的作用不同，可灵活配置集群。对于需要长时间保留的用户数据，无需重新配置 broker，只要调整存储层的大小。如果要增加处理资源，不用重新强制配置存储层，只需扩展处理层。此外，可根据每层的需求优化硬件或容器配置选择，根据存储优化存储节点，根据内存优化服务节点，根据计算资源优化处理节点。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ae3a723b400016fb47c1288bd5c3fc20_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;534&quot; data-rawheight=&quot;581&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;534&quot; data-original=&quot;https://pic1.zhimg.com/v2-ae3a723b400016fb47c1288bd5c3fc20_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-ae3a723b400016fb47c1288bd5c3fc20_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;534&quot; data-rawheight=&quot;581&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;534&quot; data-original=&quot;https://pic1.zhimg.com/v2-ae3a723b400016fb47c1288bd5c3fc20_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-ae3a723b400016fb47c1288bd5c3fc20_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2 Apache Pulsar 系统架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;而大多数消息队列技术（包括 Apache Kafka）都采用单体架构，其消息处理和消息持久化（如果提供了的话）都在集群内的同一个节点上。这种体系结构在大多数传统的数据库平台以及 Hadoop 等大数据系统中也较为常见，与昂贵的外部存储阵列的常见替代方案相比，其设计目的在于将数据的计算与存储放到同一台机器上来处理，以减少网络流量和访问延迟，同时降低存储成本。这种方法在小型环境中很容易部署，但在性能、可伸缩性和灵活性方面存在明显问题。随着固态磁盘的广泛使用，网络带宽的迅速提升以及存储延迟的显著降低，已经没有必要采用单体架构进行这种权衡处理了。&lt;br/&gt;&lt;/p&gt;&lt;p&gt;接下来，我们结合数据处理中各种不同的 IO 访问模式来深入了解 Pulsar 系统架构的优势。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;IO 访问模式的优势&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;流系统中通常有三种 IO 访问模式：&lt;/p&gt;&lt;p&gt;1. &lt;b&gt;写（Writes）&lt;/b&gt;：将新数据写入系统中；&lt;/p&gt;&lt;p&gt;2. &lt;b&gt;追尾读（Tailing Reads）&lt;/b&gt;：读取最近写入的数据；&lt;/p&gt;&lt;p&gt;3. &lt;b&gt;追赶读（Catch-up Reads）&lt;/b&gt;：读取历史的数据。例如当一个新消费者想要从较早的时间点开始访问数据，或者当旧消费者长时间离线后又恢复时。&lt;/p&gt;&lt;p&gt;和大多数其他消息系统不同，Pulsar 中这些 IO 访问模式中的每一种都与其他模式隔离。在同样 IO 访问模式下，我们来对比下 Pulsar 和其他传统消息系统（存储和服务绑定在单个节点上，如 Apache Kafka）的不同。&lt;/p&gt;&lt;p&gt;传统消息系统（图 3 左侧图）中，每个 Broker 只能利用本地磁盘提供的存储容量，这会给系统带来一些限制：&lt;/p&gt;&lt;p&gt;1. Broker 可以存储和服务的数据量受限于单个节点的存储容量。因此，一旦 Broker 节点的存储容量耗尽，它就不能再提供写请求，除非在写入前先清除现有的部分数据。&lt;/p&gt;&lt;p&gt;2. 对于单个分区，如果需要在多个节点中存储多个备份，容量最小的节点将决定分区的最终大小。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-14e617d0740dd25f6ea7935a98e350fa_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1236&quot; data-rawheight=&quot;448&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1236&quot; data-original=&quot;https://pic3.zhimg.com/v2-14e617d0740dd25f6ea7935a98e350fa_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-14e617d0740dd25f6ea7935a98e350fa_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1236&quot; data-rawheight=&quot;448&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1236&quot; data-original=&quot;https://pic3.zhimg.com/v2-14e617d0740dd25f6ea7935a98e350fa_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-14e617d0740dd25f6ea7935a98e350fa_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 传统单体架构 vs. Pulsar 存储计算分层架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;相比之下，在 Apache Pulsar（图 3 右侧图）中，数据服务和数据存储是分离的，Pulsar 服务层的任意 Broker 都可以访问存储层的所有存储节点，并利用所有节点的整体存储容量。在服务层，从系统可用性的角度来看，这也有着深远的影响，只要任一个 Pulsar 的 Broker 还在运行，用户就可以通过这个 Broker 读取先前存储在集群中的任何数据，并且还能够继续写入数据。&lt;/p&gt;&lt;p&gt;下面我们来详细看一下在每种 IO 访问模式下的架构优势。&lt;/p&gt;&lt;p&gt;&lt;b&gt;写&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在传统消息系统架构中，一个分区的所有权会分配给 Leader Broker。对于写请求，该  Leader Broker 接受写入并将数据复制到其他 Broker。如图 4 左侧所示，数据首先写入 Leader Broker 并复制给其他 followers。数据的一次持久化写入的过程需要两次网络往返。&lt;/p&gt;&lt;p&gt;在 Pulsar 系统架构中，数据服务由无状态 Broker 完成，而数据存储在持久存储中。数据会发送给服务该分区的 Broker，该 Broker 并行写入数据到存储层的多个节点中。一旦存储层成功写入数据并确认写入，Broker 会将数据缓存在本地内存中以提供追尾读（Tailing Reads）。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-033160982a7d2ac120369ca425db66e0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1362&quot; data-rawheight=&quot;532&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1362&quot; data-original=&quot;https://pic1.zhimg.com/v2-033160982a7d2ac120369ca425db66e0_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-033160982a7d2ac120369ca425db66e0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1362&quot; data-rawheight=&quot;532&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1362&quot; data-original=&quot;https://pic1.zhimg.com/v2-033160982a7d2ac120369ca425db66e0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-033160982a7d2ac120369ca425db66e0_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 Writes 访问模式对比&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如图 4 所示，和传统的系统架构相比，Pulsar 的系统架构并不会在写入的 IO 路径上引入额外的网络往返或带宽开销。而存储和服务的分离则会显著提高系统的灵活性和可用性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;追尾读&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于读取最近写入的数据场景，在传统消息系统架构中，消费者从 Leader Broker 的本地存储中读取数据；在 Pulsar 的分层架中，消费者从 Broker 就可以读取数据，由于 Broker 已经将数据缓存在内存中，并不需要去访问存储层。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-02803cb122363cd68035b32a03af290c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1330&quot; data-rawheight=&quot;496&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1330&quot; data-original=&quot;https://pic1.zhimg.com/v2-02803cb122363cd68035b32a03af290c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-02803cb122363cd68035b32a03af290c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1330&quot; data-rawheight=&quot;496&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1330&quot; data-original=&quot;https://pic1.zhimg.com/v2-02803cb122363cd68035b32a03af290c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-02803cb122363cd68035b32a03af290c_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5 Tailing Read 访问模式对比&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这两种架构只需要一次网络往返就可以读取到数据。由于 Pulsar 在系统中自己管理缓存中的数据，没有依赖文件系统缓存，这样 Tailing Reads 很容易在缓存中命中，而无需从磁盘读取。传统的系统架构一般依赖于文件系统的缓存，读写操作不仅会相互竞争资源（包括内存），还会与代理上发生的其他处理任务竞争。因此，在传统的单片架构中实现缓存并扩展非常困难。&lt;/p&gt;&lt;p&gt;&lt;b&gt;追赶读&lt;/b&gt;&lt;/p&gt;&lt;p&gt;追赶读（&lt;b&gt;Catch-up Reads&lt;/b&gt;）非常有趣。传统的系统架构对 Tailing reads 和 Catch-up reads 两种访问模式进行了同样的处理。即使一份数据存在多个 Broker 中，所有的 Catch-up reads 仍然只能发送给 Leader Broker。&lt;/p&gt;&lt;p&gt;Pulsar 的分层架构中历史（旧）数据存储在存储层中。Catch-up 读可以通过存储层并行读取数据，而不会与 Write  和 Tailing Reads 两种 IO 模式竞争或干扰。&lt;/p&gt;&lt;p&gt;&lt;b&gt;三种 IO 模式放在一起看&lt;/b&gt;&lt;/p&gt;&lt;p&gt;最有趣的是当你把这些不同的模式放在一起时，也就是实际发生的情况。这也正是单体架构的局限性最令人痛苦的地方。传统的消息系统架构中，所有不同的工作负载都被发送到一个中心（Leader Broker）位置，几乎不可能在工作负载之间提供任何隔离。&lt;/p&gt;&lt;p&gt;然而，Pulsar 的分层架构可以很容易地隔离这些 IO 模式：服务层的内存缓存为 Tailing Reads 这种消费者提供最新的数据；而存储层则为历史处理和数据分析型的消费者提供数据读取服务。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-32848cb286ab0adff0a524f131f46552_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1336&quot; data-rawheight=&quot;528&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1336&quot; data-original=&quot;https://pic3.zhimg.com/v2-32848cb286ab0adff0a524f131f46552_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-32848cb286ab0adff0a524f131f46552_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1336&quot; data-rawheight=&quot;528&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1336&quot; data-original=&quot;https://pic3.zhimg.com/v2-32848cb286ab0adff0a524f131f46552_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-32848cb286ab0adff0a524f131f46552_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6 三种 IO 模式对比&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这种 IO 隔离是 Pulsar 和传统消息系统的根本差异之一，也是 Pulsar 可用于替换多个孤立系统的关键原因之一。Apache Pulsar 的存储架构读、写分离，能保证性能的一致性，不会引起数据发布和数据消费间的资源竞争。已发布数据的写入传递到存储层进行处理，而当前数据直接从 broker 内存缓存中读取，旧数据直接从存储层读取。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;超越传统消息系统&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;上面讨论了 Pulsar 的分层架构如何为不同类型的工作负载提供高性能和可扩展性。Pulsar 分层架构带来的好处远远不止这些。我举几个例子。&lt;/p&gt;&lt;p&gt;&lt;b&gt;无限的流存储&lt;/b&gt;&lt;/p&gt;&lt;p&gt;并行访问流式计算中的最新数据和批量计算中的历史数据，是业界一个普遍的需求。&lt;/p&gt;&lt;p&gt;由于 Pulsar 基于分片的架构，Pulsar 的一个主题在理论上可以达到无限大小。当容量不足时，用户只需要添加容器或存储节点即可轻松扩展存储层，而无需重新平衡数据；新添加的存储节点会被立即用于新的分片或者分片副本的存储。&lt;/p&gt;&lt;p&gt;Pulsar 将无界的数据看作是分片的流，分片分散存储在分层存储（tiered storage）、BookKeeper 集群和 Broker 节点上，而对外提供一个统一的、无界数据的视图。其次，不需要用户显式迁移数据，减少存储成本并保持近似无限的存储。因此，Pulsar 不仅可以存储当前数据，还可以存储完整的历史数据。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f88e6021b2bf14e6e751be20f997fe23_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1600&quot; data-rawheight=&quot;722&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1600&quot; data-original=&quot;https://pic4.zhimg.com/v2-f88e6021b2bf14e6e751be20f997fe23_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f88e6021b2bf14e6e751be20f997fe23_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1600&quot; data-rawheight=&quot;722&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1600&quot; data-original=&quot;https://pic4.zhimg.com/v2-f88e6021b2bf14e6e751be20f997fe23_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-f88e6021b2bf14e6e751be20f997fe23_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7 无限的流存储&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;数据查询和数据分析&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Pulsar 有能力存储数据流的完整历史记录，因此用户可以在其数据上使用各种数据工具。Pulsar 使用 Pulsar SQL 查询历史消息，使用 Presto 引擎高效查询 BookKeeper 中的数据。Presto 是用于大数据解决方案的高性能分布式 SQL 查询引擎，可以在单个查询中查询多个数据源的数据。Pulsar SQL 允许 Presto SQL 引擎直接访问存储层中的数据，从而实现交互式 SQL 查询数据，而不会干扰  Pulsar 的其他工作负载。Pulsar 与 Presto 的集成就是一个很好的例子，如下是使用 Pulsar SQL 查询的示例。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-801d5e57253b53aa668abe96aeaf6a7e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1540&quot; data-rawheight=&quot;1210&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1540&quot; data-original=&quot;https://pic3.zhimg.com/v2-801d5e57253b53aa668abe96aeaf6a7e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-801d5e57253b53aa668abe96aeaf6a7e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1540&quot; data-rawheight=&quot;1210&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1540&quot; data-original=&quot;https://pic3.zhimg.com/v2-801d5e57253b53aa668abe96aeaf6a7e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-801d5e57253b53aa668abe96aeaf6a7e_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8 Presto 与 Apache Pulsar 的集成&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;Pulsar 的周边生态&lt;/b&gt;&lt;/p&gt;&lt;p&gt;批处理是对有界的数据进行处理，通常数据以文件的形式存储在 HDFS 等分布式文件系统中。流处理将数据看作是源源不断的流，流处理系统以发布/订阅方式消费流数据。当前的大数据处理框架，例如 Spark、Flink 在 API 层和执行层正在逐步融合批、流作业的提交与执行，而 Pulsar 由于可以存储无限的流数据，是极佳的统一数据存储平台。Pulsar 还可以与其他数据处理引擎（例如 Apache Spark 或 Apache Flink）进行类似集成，作为批流一体的数据存储平台，这进一步扩展了 Pulsar 消息系统之外的角色。下图展示了 Pulsar 的周边生态。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f3af5ec2e9fc31a111a8df73fa1b796f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;797&quot; data-rawheight=&quot;334&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;797&quot; data-original=&quot;https://pic4.zhimg.com/v2-f3af5ec2e9fc31a111a8df73fa1b796f_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f3af5ec2e9fc31a111a8df73fa1b796f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;797&quot; data-rawheight=&quot;334&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;797&quot; data-original=&quot;https://pic4.zhimg.com/v2-f3af5ec2e9fc31a111a8df73fa1b796f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-f3af5ec2e9fc31a111a8df73fa1b796f_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9 Apache Pulsar 周边生态&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Apache Pulsar 是云原生的分布式消息流系统，采用了计算和存储分层的架构和以 Segment 为中心的分片存储，因此 Apache Pulsar 具有更好的性能、可扩展性和灵活性，是一款可以无限扩展的分布式消息队列。&lt;/p&gt;&lt;p&gt;Apache Pulsar 是一个年轻的开源项目，拥有非常多吸引人的特性。Pulsar 社区的发展迅猛，在不同的应用场景下不断有新的案例落地。期待大家能和 Apache Pulsar 社区深入合作，一起进一步完善、优化 Pulsar 的特性和功能。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;：Sijie Guo，StreamNative 联合创始人，Apache BookKeeper 和 Apache Pulsar PMC 成员和 Committer。之前是 Twitter 消息组的技术负责人，与他人共同创建了 Apache DistributedLog。加入 Twitter 之前，他曾在 Yahoo！从事推送通知基础架构工作。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;本文是「分布式系统前沿技术」专题文章，目前该专题在持续更新中，欢迎大家保持关注。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;专题地址：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/theme/48&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/theme/48&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-27-99800206</guid>
<pubDate>Fri, 27 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>微服务架构何去何从？</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-27-99698606.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/99698606&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2ac2bbb8366274bf43bd936c96eb2bff_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;分布式技术的发展，深刻地改变了我们编程的模式和思考软件的模式。值 2019 岁末，PingCAP 联合 InfoQ 共同策划出品“分布式系统前沿技术 ”专题， 邀请众多技术团队共同参与，一起探索这个古老领域的新生机。本文出自转转首席架构师孙玄。&lt;/blockquote&gt;&lt;p&gt;微服务架构模式经过 5 年多的发展，在各行各业如火如荼地应用和实践。如何在企业中优雅地设计微服务架构？是企业面对的一个重要问题。本文将讲述微服务架构 1.0 设计与实践以及面临问题和破局，最后讲述微服务架构 2.0 设计与实践等方面，尝试去回答这个难题。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;微服务架构 1.0 设计与实践&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;微服务架构定义&lt;/b&gt;&lt;/p&gt;&lt;p&gt;2014 年马丁福勒提出了微服务架构设计模式，微服务架构最核心的设计有二点（如图 1 绿框所示）：第一，把单体服务拆分成一系列小服务；第二，拆分后的这些小服务是去中心化的，即每个服务都可以使用不同的编程语言，也可以使用不同的数据库和缓存存储数据。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a5afe0b18d794a5178d00c848cbfa3cb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1298&quot; data-rawheight=&quot;382&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1298&quot; data-original=&quot;https://pic4.zhimg.com/v2-a5afe0b18d794a5178d00c848cbfa3cb_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a5afe0b18d794a5178d00c848cbfa3cb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1298&quot; data-rawheight=&quot;382&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1298&quot; data-original=&quot;https://pic4.zhimg.com/v2-a5afe0b18d794a5178d00c848cbfa3cb_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-a5afe0b18d794a5178d00c848cbfa3cb_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1 微服务架构模式&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;微服务架构拆分设计实践&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第一个问题是服务如何拆分的问题。架构拆分没有新鲜事，即不同领域的架构设计在道（哲学）的层面都是相通的。&lt;/p&gt;&lt;p&gt;我们来思考一下公司数据库集群遇到读写和存储的性能问题时，是如何解决的？假如公司电商业务包含用户、商品以及交易等数据，每种数据使用一张单独的表存储，这些数据放在一个数据库（DB4Global）中。随着请求量的增加和数据存储量的增加，单独的 DB4Global 数据库会遇到性能瓶颈。为了解决数据库的性能问题，需要对 DB4Global 库拆分，首先对 DB4Global 库按照业务领域进行垂直拆分，拆分为多个独立的用户库（DB4User）、商品库（DB4Info）、交易库（DB4Trade）等；其次为了进一步提升数据库的性能，再次根据功能对每个表进行水平方向的拆分，例如用户表 10 亿记录，主键为用户 UID。Partition Key 选择为 UID，按照 UID % 128 水平拆分。&lt;/p&gt;&lt;p&gt;架构设计之道是相通的，微服务拆分同样遵循业务领域的垂直拆分以及功能的水平拆分。继续以电商业务为例，首先按照业务领域的垂直拆分，分为用户微服务、商品微服务、搜索微服务、推荐微服务、交易微服务等。&lt;/p&gt;&lt;p&gt;继续思考一个问题，在垂直方向仅仅按照业务领域进行拆分是否满足所有的业务场景？答案是否定的。例如用户服务分为用户注册（写请求）和用户登陆（读请求）等。写请求的重要性往往是大于读请求，在互联网大流量下，读写比例 10:1，甚至更高的情况下，大量的读往往会直接影响写。为了避免大量的读对写请求的干扰，需要对服务进行读写分离，即用户注册为一个微服务，用户登陆为一个微服务。此时按照 API 的细粒度继续进行垂直方向的拆分。&lt;/p&gt;&lt;p&gt;在水平方向，按照请求的功能拆分，即对一个请求的生命周期继续进行拆分。请求从用户端发出，首先接受到请求的是网关服务，网关服务对请求进行请求鉴权、通用参数检查、协议转换以及路由转发等。接下来业务逻辑服务对请求进行业务逻辑的编排处理（比如微信发送消息，需要进行好友关系检查、对消息内容进行风控检查、进行消息的存储和推送等）。对业务数据进行存储和查询就需要数据访问服务，数据访问服务提供了基本的 CRUD 原子操作，并负责海量数据的 Sharding（分库分表）以及屏蔽底层存储的差异性等功能。最后是数据持久化和缓存服务，比如可以采用 NewSQL TiDB 以及 Redis Cluster 等。&lt;/p&gt;&lt;p&gt;通过以上的拆分，普适的微服务架构如图 2 所示。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e21c8b6d90ca16d1e9b250958586fd39_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1077&quot; data-rawheight=&quot;691&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1077&quot; data-original=&quot;https://pic2.zhimg.com/v2-e21c8b6d90ca16d1e9b250958586fd39_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e21c8b6d90ca16d1e9b250958586fd39_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1077&quot; data-rawheight=&quot;691&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1077&quot; data-original=&quot;https://pic2.zhimg.com/v2-e21c8b6d90ca16d1e9b250958586fd39_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-e21c8b6d90ca16d1e9b250958586fd39_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2 普适的微服务架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;微服务架构通过业务垂直拆分以及水平的功能拆分，服务演化成更小的颗粒度，各服务之间相互解耦，每个服务都可以快速迭代和持续交付，从而在公司层面能够达到降本增效的终极目标。但是服务粒度越细，服务之间的交互就会越来越多，更多的交互会使得服务之间的治理更复杂。服务之间的治理包括服务间的注册、通信、路由、负载均衡、重试、限流、降级、熔断、链路跟踪等。&lt;/p&gt;&lt;p&gt;微服务架构技术选型，包括微服务本身的研发框架以及服务治理框架。目前研发框架主流的 RPC 有两类：一种是 RPC Over TCP，典型代表是 Apache Dubbo；另外一种是 RPC Over HTTP，典型代表是 Spring Cloud。企业根据团队的研发基因二者选一即可。在服务治理方面包含了服务注册、服务配置、服务熔断、服务监控等方面，服务注册本质是 AP 的模型，可以选用 Nacos，服务配置可以选用 CTrip Apollo，服务熔断可以选用 Netflix Hystrix 组件，服务监控可以选用 Open-Falcon 等配套框架。&lt;/p&gt;&lt;p&gt;&lt;b&gt;微服务架构 1.0 面临问题以及破局&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在微服务架构 1.0 中每个服务包含了服务自身的功能设计以及服务治理的功能设计，他们耦合在一起，这些服务治理的功能和服务自身功能没有关系，业务方也不需要关注。使得微服务 1.0 架构不再是银弹，存在以下几个方面的问题：&lt;/p&gt;&lt;p&gt;第一，每一个业务服务为了和其他业务服务交互，都必须关注和引入服务间服务治理组件，使得业务服务迭代速度变慢，如图 3 所示。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7da6a3465f592941936e0944891cbcac_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1110&quot; data-rawheight=&quot;437&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1110&quot; data-original=&quot;https://pic1.zhimg.com/v2-7da6a3465f592941936e0944891cbcac_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-7da6a3465f592941936e0944891cbcac_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1110&quot; data-rawheight=&quot;437&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1110&quot; data-original=&quot;https://pic1.zhimg.com/v2-7da6a3465f592941936e0944891cbcac_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-7da6a3465f592941936e0944891cbcac_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 业务服务迭代速度慢&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;第二，服务治理组件和服务自身功能耦合在一个进程内，使得服务治理组件的升级强依赖于业务服务自身，造成基础设施研发团队的交付能力和交付速度大大降低。如图 4 所示，服务降级功能从 V1 升级到 V2，需要业务服务更换服务降级功能的组件，重新打包编译和发布。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1b55016052fea38497c54e67398d29b2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1163&quot; data-rawheight=&quot;436&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1163&quot; data-original=&quot;https://pic3.zhimg.com/v2-1b55016052fea38497c54e67398d29b2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-1b55016052fea38497c54e67398d29b2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1163&quot; data-rawheight=&quot;436&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1163&quot; data-original=&quot;https://pic3.zhimg.com/v2-1b55016052fea38497c54e67398d29b2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-1b55016052fea38497c54e67398d29b2_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 服务治理组件升级困难&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;第三，前文提到马丁福勒对微服务架构的期望是每个服务都可以使用业务团队熟悉的语言来编写，但是在服务自身和服务治理耦合在一起的情况下，每个语言都需要一套完整的服务治理组件，必然造成公司研发投入成本增大，ROI 不高。如图 5 所示，Java 语言编写的应用程序A和应用程序 C 交互，就需要一套完整的 Java 语言服务治理组件，同样，世界上最好语言编写的应用程序 B 和应用程序 C 交互，就需要一套完成的 PHP 语言服务治理组件。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4165456bd10d870c03dea10251c16ade_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1119&quot; data-rawheight=&quot;573&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1119&quot; data-original=&quot;https://pic3.zhimg.com/v2-4165456bd10d870c03dea10251c16ade_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4165456bd10d870c03dea10251c16ade_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1119&quot; data-rawheight=&quot;573&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1119&quot; data-original=&quot;https://pic3.zhimg.com/v2-4165456bd10d870c03dea10251c16ade_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-4165456bd10d870c03dea10251c16ade_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5 多套服务治理组件&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;那么造成这些问题的本质原因在于服务自身功能和服务治理功能的物理耦合，把服务治理功能完全解耦出来，变成一个独立的服务治理进程，从而以上三个问题得以彻底解决。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;微服务架构 2.0 设计与实践&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;Serive Mesh 定义&lt;/b&gt;&lt;/p&gt;&lt;p&gt;微服务架构 1.0 继续演进，就变成了微服务架构 2.0，即 Service Mesh 架构（Service Mesh）。Servie Mesh 架构最早由开发 Linkerd 的 Buoyant 公司提出，并在内部使用。2016 年 09 月 29 日第一次公开使用，2017 年初进入国内技术社区视野。Service Mesh 到底是什么？我们来看看 Linerd 公司 CEO Willian Morgan 对 Service Mesh 的定义如图 6 所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e349c1be6ecad631cbd18546e2bbd61e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1604&quot; data-rawheight=&quot;392&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1604&quot; data-original=&quot;https://pic3.zhimg.com/v2-e349c1be6ecad631cbd18546e2bbd61e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e349c1be6ecad631cbd18546e2bbd61e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1604&quot; data-rawheight=&quot;392&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1604&quot; data-original=&quot;https://pic3.zhimg.com/v2-e349c1be6ecad631cbd18546e2bbd61e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-e349c1be6ecad631cbd18546e2bbd61e_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6 Service Mesh 定义&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Service Mesh 是一个基础设施层，用于处理服务间交互。云原生应用有着复杂的服务拓扑，Service Mesh 负责在这些拓扑中实现请求的可靠传递。在线上实践中，Service Mesh 通常实现为一组轻量级的网络代理（Sidecar 边车），它们与应用程序部署在一起，并且对应用程序透明。&lt;/p&gt;&lt;p&gt;&lt;b&gt;微服务架构 2.0 破局&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3b53013c06c3959c303bd5ad180394b5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1230&quot; data-rawheight=&quot;426&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1230&quot; data-original=&quot;https://pic2.zhimg.com/v2-3b53013c06c3959c303bd5ad180394b5_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3b53013c06c3959c303bd5ad180394b5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1230&quot; data-rawheight=&quot;426&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1230&quot; data-original=&quot;https://pic2.zhimg.com/v2-3b53013c06c3959c303bd5ad180394b5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-3b53013c06c3959c303bd5ad180394b5_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7 Service Mesh 架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如图 7 所示，应用程序 A 和应用程序 B 交互，请求调用关系如下：应用程序 A 调用本地的 Sidecar A，Sidecar A 在通过网络交互调用远端的 Sidecar B，再由 Sidecar B 把请求传递给应用程序 B。请求回应关系也是类似：应用程序 B 调用 Sidecar B，Sidecar B 在通过网络交互调用远端的 Sidecar A，再由 Sidecar A 把请求回应传递给应用程序 A。通过把服务治理功能从服务自身中物理剥离出来，下沉形成独立的进程，从而物理解耦。&lt;/p&gt;&lt;p&gt;在这样的架构模式下，业务应用程序再也不需要关注服务治理的功能，服务治理的功能升级也不要依赖于服务自身，从而能够让业务迭代更快速和高效。同时由于服务治理功能变成一个独立的进程，只需要使用一种语言打造即可，业务服务自身可以选择业务团队擅长的语言进行编写，从而能够真正达到马丁福勒对微服务的期望。我们再深入分析下协议，在通信协议方面，业务应用程序和 Sidecar 的通信可以基于 TCP 长连接，也可以基于 HTTP 1.0 或者 2.0 的长连接（思考下：是否一定要使用长连接？），Sidecar 间的通信协议没有特殊要求；在数据传输协议方面，可以是 JSON／XML 等跨语言的文本协议，也可以选择 Protobuffers／MessagePack 等跨语言的二进制协议。&lt;/p&gt;&lt;p&gt;保证了通信协议和数据传输协议的跨语言，不同语言的应用程序就可以无缝地和 Sidecar 进行交与。在应用程序和对应的 Sidecar 部署层面，需要部署在同机（可以是同一台物理机／虚拟机，也可以是同一个 Pod），思考下，如果部署在不同的机器上，就会又引入服务通信交互的问题，那么就会变成无解的难题：为了解决通信交互的问题，又引入新的通信交互的问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;微服务架构 2.0 实践&lt;/b&gt;&lt;/p&gt;&lt;p&gt;按照新的微服务架构 2.0 打造，微服务架构 1.0 的升级演变如图 8 所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-34c972bd6701c5ea0185f58bc1b138fe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1022&quot; data-rawheight=&quot;742&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1022&quot; data-original=&quot;https://pic3.zhimg.com/v2-34c972bd6701c5ea0185f58bc1b138fe_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-34c972bd6701c5ea0185f58bc1b138fe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1022&quot; data-rawheight=&quot;742&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1022&quot; data-original=&quot;https://pic3.zhimg.com/v2-34c972bd6701c5ea0185f58bc1b138fe_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-34c972bd6701c5ea0185f58bc1b138fe_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8 微服务架构 2.0&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Service Mesh 架构框架方面，业内陆续开源了不少的优秀框架，Istio 是集大成者，由 Google、IBM、Lyft 等三家公司联合打造，并已经开源，社区版本也已经发展到 V1.4.2。IstioService Mesh 逻辑上分为数据面板（执行者）和控制面板（指挥者），数据面板由一组智能代理（Envoy）组成，代理部署为 Sidecar，调解和控制微服务之间所有的网络通信。控制面板负责管理和配置代理来路由流量，以及在运行时执行策略。如图 9 所示，控制面板（Pilot、Mixer、Citadel）加数据面板（Envoy Proxy）即是服务治理功能，svcA 和 svcB 是业务服务自身。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-23602333973159dada1103d6c4e6dba7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1059&quot; data-rawheight=&quot;574&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1059&quot; data-original=&quot;https://pic4.zhimg.com/v2-23602333973159dada1103d6c4e6dba7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-23602333973159dada1103d6c4e6dba7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1059&quot; data-rawheight=&quot;574&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1059&quot; data-original=&quot;https://pic4.zhimg.com/v2-23602333973159dada1103d6c4e6dba7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-23602333973159dada1103d6c4e6dba7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9 Istio 架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;未来展望&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;与纯粹的微服务架构相比，Service Mesh 又向前迈了一步。它最大的优势是解耦应用业务，企业能够彻底从业务角度考虑问题，同时还可以与容器编排部署平台的集成，成为企业级应用编排部署和服务治理的标准形态。&lt;/p&gt;&lt;p&gt;但是企业想要全面切换到 Service Mesh 并不是一件易事，还有一段路需要走。以 Istio 为例，如果要切换，会面临以下问题：&lt;/p&gt;&lt;p&gt;第一，老服务切换到 Istio 的过程中，由于历史服务使用的框架不同，如何保证老服务的平稳迁移以及新老服务如何无缝交互，是企业面临的第一个难题；&lt;/p&gt;&lt;p&gt;第二，切换到 Istio 后，由于通信链路会变长，必将增加请求的响应延迟，对请求响应延迟极其敏感的业务场景，比如量化交易等场景，增加的请求相应延迟对业务来说是致命的，如何进一步优化处理；&lt;/p&gt;&lt;p&gt;第三，Istio 的 Mixer 功能存在单点瓶颈问题，那么对高并发的业务场景如何突破，是公司需要考虑和解决的问题；&lt;/p&gt;&lt;p&gt;第四，切换到 Istio，将会增加基础设施团队的运维成本，并且遇到业务问题，定位问题涉及到业务研发团队和基础设施研发团队频繁沟通交互，自然成本也会相应增加。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;：孙玄，毕业于浙江大学，现任转转公司首席架构师，技术委员会主席，大中后台技术负责人（交易平台、基础服务、智能客服、基础架构、智能运维、数据库、安全、IT等方向）；前 58 集团技术委员会主席，高级系统架构师；前百度资深研发工程师；“架构之美” 〔beautyArch〕微信公众号作者；擅长系统架构设计，大数据，运维、机器学习、技术管理等领域；代表公司多次在业界顶级技术大会 CIO 峰会、Artificial Intelligence Conference、A2M、QCon、ArchSummit、SACC、SDCC、CCTC、DTCC、Top100、Strata + Hadoop World、WOT、GITC、GIAC、TID 等发表演讲，并为《程序员》杂志撰稿 2 篇。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;本文是「分布式系统前沿技术」专题文章，目前该专题在持续更新中，欢迎大家保持关注！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;专题地址：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/theme/48&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/theme/48&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-27-99698606</guid>
<pubDate>Fri, 27 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>分布式系统 in 2010s ：存储之数据库篇</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-26-99587904.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/99587904&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2ac2bbb8366274bf43bd936c96eb2bff_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄东旭&lt;/p&gt;&lt;blockquote&gt;经常思考一个问题，为什么我们需要分布式？很大程度或许是不得已而为之。如果摩尔定律不会失效，如果通过低成本的硬件就能解决互联网日益增长的计算存储需求，是不是我们也就不需要分布式了。&lt;br/&gt;过去的二三十年，是一场软件工程师们自我拯救的，浩浩荡荡的革命。分布式技术的发展，深刻地改变了我们编程的模式，改变了我们思考软件的模式。通过随处可见的 X86 或者 Arm 机器，构建出一个无限扩展的计算以及存储能力，这是软件工程师最浪漫的自我救赎。&lt;br/&gt;&lt;b&gt;值 2019 年末，PingCAP 联合 InfoQ 共同策划出品“分布式系统前沿技术”专题， 邀请转转、Pulsar、微众银行、UCloud、知乎、贝壳金服等技术团队共同参与，从数据库、硬件、测试、运维等角度，共同探索这个古老领域的新生机。&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;无论哪个时代，存储都是一个重要的话题，今天先聊聊数据库。在过去的几年，数据库技术上出现了几个很明显的趋势。&lt;/p&gt;&lt;h2&gt;存储和计算进一步分离&lt;/h2&gt;&lt;p&gt;我印象中最早的存储-计算分离的尝试是 Snowflake，Snowflake 团队在 2016 年发表的论文&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//pages.cs.wisc.edu/~remzi/Classes/739/Spring2004/Papers/p215-dageville-snowflake.pdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《The Snowflake Elastic Data Warehouse》&lt;/a&gt;是近几年我读过的最好的大数据相关论文之一，尤其推荐阅读。Snowflake 的架构关键点是在无状态的计算节点 + 中间的缓存层 + S3 上存储数据，计算并不强耦合缓存层，非常符合云的思想。从最近 AWS 推出的 RedShift 冷热分离架构来看，AWS 也承认 Snowflake 这个搞法是先进生产力的发展方向。另外这几年关注数据库的朋友不可能不注意到 Aurora。不同于 Snowflake，Aurora 应该是第一个将存储-计算分离的思想用在 OLTP 数据库中的产品，并大放异彩。Aurora 的成功在于将数据复制的粒度从 Binlog降低到 Redo Log ，极大地减少复制链路上的 IO 放大。而且前端复用了 MySQL，基本做到了 100% 的应用层 MySQL 语法兼容，并且托管了运维，同时让传统的 MySQL 适用范围进一步拓展，这在中小型数据量的场景下是一个很省心的方案。&lt;/p&gt;&lt;p&gt;虽然 Aurora 获得了商业上的成功，但是从技术上，我并不觉得有很大的创新。熟悉 Oracle 的朋友第一次见 Aurora 的架构可能会觉得和 RAC 似曾相识。Oracle 大概在十几年前就用了类似的方案，甚至很完美的解决了 Cache Coherence 的问题。另外，Aurora 的 Multi-Master 还有很长的路要走，从最近在 ReInvent 上的说法来看，目前 Aurora 的 Multi-Master 的主要场景还是作为 Single Writer 的高可用方案，本质的原因应该是目前 Multi-Writer 采用乐观冲突检测，冲突检测的粒度是 Page，在冲突率高的场合会带来很大的性能下降。&lt;/p&gt;&lt;p&gt;我认为 Aurora 是一个很好的迎合 90% 的公有云互联网用户的方案：100% MySQL 兼容，对一致性不太关心，读远大于写，全托管。但同时，Aurora 的架构决定了它放弃了 10% 有极端需求的用户，如全局的 ACID 事务+ 强一致，Hyper Scale（百 T 以上，并且业务不方便拆库），需要实时的复杂 OLAP。这类方案我觉得类似 TiDB 的以 Shared-nothing 为主的设计才是唯一的出路。作为一个分布式系统工程师，我对任何不能水平扩展的架构都会觉得不太优雅。&lt;/p&gt;&lt;h2&gt;分布式 SQL 数据库登上舞台，ACID 全面回归&lt;/h2&gt;&lt;p&gt;回想几年前 NoSQL 最风光的时候，大家恨不得将一切系统都使用 NoSQL 改造，虽然易用性、扩展性和性能都不错，但是多数 NoSQL 系统抛弃掉数据库最重要的一些东西，例如 ACID 约束，SQL 等等。NoSQL 的主要推手是互联网公司，对于互联网公司的简单业务加上超强的工程师团队来说当然能用这些简单工具搞定。&lt;/p&gt;&lt;p&gt;但最近几年大家渐渐发现低垂的果实基本上没有了，剩下的都是硬骨头。&lt;/p&gt;&lt;p&gt;最好的例子就是作为 NoSQL 的开山鼻祖，Google 第一个搞了 NewSQL （Spanner 和 F1）。在后移动时代，业务变得越来越复杂，要求越来越实时，同时对于数据的需求也越来越强。尤其对于一些金融机构来说，一方面产品面临着互联网化，一方面不管是出于监管的要求还是业务本身的需求，ACID 是很难绕开的。更现实的是，大多数传统公司并没有像顶级互联网公司的人才供给，大量历史系统基于 SQL 开发，完全迁移到 NoSQL 上肯定不现实。&lt;/p&gt;&lt;p&gt;在这个背景下，分布式关系型数据库，我认为这是我们这一代人，在开源数据库这个市场上最后一个 missing part，终于慢慢流行起来。这背后的很多细节由于篇幅的原因我就不介绍，推荐阅读 PingCAP TiFlash 技术负责人 maxiaoyu 的一篇文章《&lt;a href=&quot;https://zhuanlan.zhihu.com/p/97085692&quot; class=&quot;internal&quot;&gt;从大数据到数据库&lt;/a&gt;》，对这个话题有很精彩的阐述。&lt;/p&gt;&lt;h2&gt;云基础设施和数据库的进一步整合&lt;/h2&gt;&lt;p&gt;在过去的几十年，数据库开发者都像是在单打独斗，就好像操作系统以下的就完全是黑盒了，这个假设也没错，毕竟软件开发者大多也没有硬件背景。另外如果一个方案过于绑定硬件和底层基础设施，必然很难成为事实标准，而且硬件非常不利于调试和更新，成本过高，这也是我一直对定制一体机不是太感兴趣的原因。但是云的出现，将 IaaS 的基础能力变成了软件可复用的单元，我可以在云上按需地租用算力和服务，这会给数据库开发者在设计系统的时候带来更多的可能性，举几个例子：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Spanner 原生的 TrueTime API 依赖原子钟和 GPS 时钟，如果纯软件实现的话，需要牺牲的东西很多（例如 CockroachDB 的 HLC 和 TiDB 的改进版 Percolator 模型，都是基于软件时钟的事务模型）。但是长期来看，不管是 AWS 还是 GCP 都会提供类似 TrueTime 的高精度时钟服务，这样一来我们就能更好的实现低延迟长距离分布式事务。&lt;/li&gt;&lt;li&gt;可以借助 Fargate + EKS 这种轻量级容器 + Managed K8s 的服务，让我们的数据库在面临突发热点小表读的场景（这个场景几乎是 Shared-Nothing 架构的老大难问题），比如在 TiDB 中通过 Raft Learner 的方式，配合云的 Auto Scaler 快速在新的容器中创建只读副本，而不是仅仅通过 3 副本提供服务；比如动态起 10 个 pod，给热点数据创建 Raft 副本（这是我们将 TiKV 的数据分片设计得那么小的一个重要原因），处理完突发的读流量后再销毁这些容器，变成 3 副本。&lt;/li&gt;&lt;li&gt;冷热数据分离，这个很好理解，将不常用的数据分片，分析型的副本，数据备份放到 S3 上，极大地降低成本。&lt;/li&gt;&lt;li&gt;RDMA/CPU/超算 as a Service，任何云上的硬件层面的改进，只要暴露 API，都是可以给软件开发者带来新的好处。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;例子还有很多，我就不一一列举了。总之我的观点是云服务 API 的能力会像过去的代码标准库一样，是大家可以依赖的东西，虽然现在公有云的 SLA 仍然不够理想，但是长远上看，一定是会越来越完善的。&lt;/p&gt;&lt;p&gt;所以，数据库的未来在哪里？是更加的垂直化还是走向统一？对于这个问题，我同意这个世界不存在银弹，但是我也并不像我的偶像，AWS 的 CTO，Vogels 博士那么悲观，相信未来是一个割裂的世界（AWS 恨不得为了每个细分的场景设计一个数据库）。过度地细分会加大数据在不同系统中流动的成本。解决这个问题有两个关键：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;数据产品应该切分到什么粒度？&lt;/li&gt;&lt;li&gt;用户可不可以不用知道背后发生了什么？&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;第一个问题并没有一个明确的答案，但是我觉得肯定不是越细越好的，而且这个和 Workload 有关，比如如果没有那么大量的数据，直接在 MySQL 或者 PostgreSQL 上跑分析查询其实一点问题也没有，没有必要非去用 Redshift。虽然没有直接的答案，但是我隐约觉得第一个问题和第二个问题是息息相关的，毕竟没有银弹，就像 OLAP 跑在列存储引擎上一定比行存引擎快，但是对用户来说其实可以都是 SQL 的接口。&lt;/p&gt;&lt;p&gt;SQL 是一个非常棒的语言，它只描述了用户的意图，而且完全与实现无关，对于数据库来说，其实可以在 SQL 层的后面来进行切分，在 TiDB 中，我们引入 TiFlash 就是一个很好的例子。动机很简单：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;用户其实并不是数据库专家，你不能指望用户能 100% 在恰当的时间使用恰当的数据库，并且用对。&lt;/li&gt;&lt;li&gt;数据之间的同步在一个系统之下才能尽量保持更多的信息，例如，TiFlash 能保持 TiDB 中事务的 MVCC 版本，TiFlash 的数据同步粒度可以小到 Raft Log 的级别。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;另外一些新的功能仍然可以以 SQL 的接口对外提供，例如全文检索，用 SQL 其实也可以简洁的表达。这里我就不一一展开了。&lt;/p&gt;&lt;p&gt;我其实坚信系统一定是朝着更智能、更易用的方向发展的，现在都 21 世纪了，你是希望每天拿着一个 Nokia 再背着一个相机，还是直接一部手机搞定？&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文是「分布式系统前沿技术」专题文章，目前该专题在持续更新中，欢迎大家保持关注。&lt;/b&gt;&lt;/p&gt;&lt;p class=&quot;ztext-empty-paragraph&quot;&gt;&lt;br/&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/theme/48&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/theme/48&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-26-99587904</guid>
<pubDate>Thu, 26 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Binlog 源码阅读系列文章（七）Drainer server 介绍</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-25-99254953.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/99254953&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fde3792abf37018552c11b49b253f703_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄佳豪&lt;/p&gt;&lt;p&gt;前面文章介绍了 Pump server，接下来我们来介绍 Drainer server 的实现，Drainer server 的主要作用是从各个 Pump server 获取 binlog，按 commit timestamp 归并排序后解析 binlog 同步到不同的目标系统，对应的源码主要集中在 TiDB Binlog 仓库的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/tree/v3.0.7/drainer&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;drainer/&lt;/a&gt; 目录下。&lt;/p&gt;&lt;h2&gt;启动 Drainer Server&lt;/h2&gt;&lt;p&gt;Drainer server 的启动逻辑主要实现在两个函数中：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/server.go%23L88&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;NewServer&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/server.go%23L250&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;(*Server).Start()&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;&lt;code&gt;NewServer&lt;/code&gt; 根据传入的配置项创建 Server 实例，初始化 Server 运行所需的字段。其中重要字段的说明如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;metrics: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/pkg/util/p8s.go%23L36&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MetricClient&lt;/a&gt;，用于定时向 Prometheus Pushgateway 推送 drainer 运行中的各项参数指标。&lt;/li&gt;&lt;li&gt;cp: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/checkpoint/checkpoint.go%23L29&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;checkpoint&lt;/a&gt;，用于保存 drainer 已经成功输出到目标系统的 binlog 的 commit timestamp。drainer 在重启时会从 checkpoint 记录的 commit timestamp 开始同步 binlog。&lt;/li&gt;&lt;li&gt;collector: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/collector.go%23L50&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;collector&lt;/a&gt;，用于收集全部 binlog 数据并按照 commit timestamp 递增的顺序进行排序。同时 collector 也负责实时维护 pump 集群的状态信息。&lt;/li&gt;&lt;li&gt;syncer: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/syncer.go%23L39&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;syncer&lt;/a&gt;，用于将排好序的 binlog 输出到目标系统 (MySQL，Kafka…) ，同时更新同步成功的 binlog 的 commit timestamp 到 checkpoint。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Server 初始化以后，就可以用 &lt;code&gt;(*Server).Start&lt;/code&gt; 启动服务，启动的逻辑包含：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;初始化 &lt;code&gt;heartbeat&lt;/code&gt; 协程定时上报心跳信息到 etcd （内嵌在 PD 中）。&lt;/li&gt;&lt;li&gt;调用 &lt;code&gt;collector.Start()&lt;/code&gt; 驱动 &lt;code&gt;Collector&lt;/code&gt; 处理单元。&lt;/li&gt;&lt;li&gt;调用 &lt;code&gt;syncer.Start()&lt;/code&gt; 驱动 &lt;code&gt;Syncer&lt;/code&gt; 处理单元。&lt;br/&gt;errc := s.heartbeat(s.ctx) go func() {     for err := range errc {         log.Error(&amp;#34;send heart failed&amp;#34;, zap.Error(err))     } }()  s.tg.GoNoPanic(&amp;#34;collect&amp;#34;, func() {     defer func() { go s.Close() }()     s.collector.Start(s.ctx) })  if s.metrics != nil {     s.tg.GoNoPanic(&amp;#34;metrics&amp;#34;, func() {&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;后续的章节中，我们会详细介绍 Checkpoint、Collector 与 Syncer。&lt;/p&gt;&lt;h2&gt;Checkpoint&lt;/h2&gt;&lt;p&gt;Checkpoint 代码在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/tree/v3.0.7/drainer/checkpoint&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;/drainer/checkpoint&lt;/a&gt; 下。&lt;/p&gt;&lt;p&gt;首先看下 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/checkpoint/checkpoint.go%23L29&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;接口定义&lt;/a&gt;：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// When syncer restarts, we should reload meta info to guarantee continuous transmission.
type CheckPoint interface {
    // Load loads checkpoint information.
    Load() error

    // Save saves checkpoint information.
    Save(int64, int64) error

    // TS get the saved commit ts.
    TS() int64

    // Close closes the CheckPoint and release resources after closed other methods should not be called again.
    Close() error
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;drainer 支持把 checkpoint 保存到不同类型的存储介质中，目前支持 mysql 和 file 两种类型，例如 mysql 类型的实现代码在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/checkpoint/mysql.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;mysql.go&lt;/a&gt; 。如果用户没有指定 checkpoit 的存储类型，drainer 会根据目标系统的类型自动选择对应的 checkpoint 存储类型。&lt;/p&gt;&lt;p&gt;当目标系统是 mysql/tidb，drainer 默认会保存 checkpoint 到 &lt;code&gt;tidb_binlog.checkpoint&lt;/code&gt; 表中：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;mysql&amp;gt; select * from tidb_binlog.checkpoint;
+---------------------+---------------------------------------------+
| clusterID           | checkPoint                                  |
+---------------------+---------------------------------------------+
| 6766844929645682862 | {&amp;#34;commitTS&amp;#34;:413015447777050625,&amp;#34;ts-map&amp;#34;:{}} |
+---------------------+---------------------------------------------+
1 row in set (0.00 sec)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;commitTS 表示这个时间戳之前的数据都已经同步到目标系统了。ts-map 是用来做 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/stable/reference/tools/sync-diff-inspector/tidb-diff/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 主从集群的数据校验&lt;/a&gt; 而保存的上下游 snapshot 对应关系的时间戳。&lt;/p&gt;&lt;p&gt;下面看看 MysqlCheckpoint 主要方法的实现。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// Load implements CheckPoint.Load interface
func (sp *MysqlCheckPoint) Load() error {
    sp.Lock()
    defer sp.Unlock()

    if sp.closed {
        return errors.Trace(ErrCheckPointClosed)
    }

    defer func() {
        if sp.CommitTS == 0 {
            sp.CommitTS = sp.initialCommitTS
        }
    }()

    var str string
    selectSQL := genSelectSQL(sp)
    err := sp.db.QueryRow(selectSQL).Scan(&amp;amp;str)
    switch {
    case err == sql.ErrNoRows:
        sp.CommitTS = sp.initialCommitTS
        return nil
    case err != nil:
        return errors.Annotatef(err, &amp;#34;QueryRow failed, sql: %s&amp;#34;, selectSQL)
    }

    if err := json.Unmarshal([]byte(str), sp); err != nil {
        return errors.Trace(err)
    }

    return nil
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Load 方法从数据库中读取 checkpoint 信息。需要注意的是，如果 drainer 读取不到对应的 checkpoint，会使用 drainer 配置的 &lt;code&gt;initial-commit-ts&lt;/code&gt; 做为启动的开始同步点。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// Save implements checkpoint.Save interface
func (sp *MysqlCheckPoint) Save(ts, slaveTS int64) error {
    sp.Lock()
    defer sp.Unlock()

    if sp.closed {
        return errors.Trace(ErrCheckPointClosed)
    }

    sp.CommitTS = ts

    if slaveTS &amp;gt; 0 {
        sp.TsMap[&amp;#34;master-ts&amp;#34;] = ts
        sp.TsMap[&amp;#34;slave-ts&amp;#34;] = slaveTS
    }

    b, err := json.Marshal(sp)
    if err != nil {
        return errors.Annotate(err, &amp;#34;json marshal failed&amp;#34;)
    }

    sql := genReplaceSQL(sp, string(b))
    _, err = sp.db.Exec(sql)
    if err != nil {
        return errors.Annotatef(err, &amp;#34;query sql failed: %s&amp;#34;, sql)
    }

    return nil
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Save 方法构造对应 SQL 将 checkpoint 写入到目标数据库中。&lt;/p&gt;&lt;h2&gt;Collector&lt;/h2&gt;&lt;p&gt;Collector 负责获取全部 binlog 信息后，按序传给 Syncer 处理单元。我们先看下 Start 方法：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// Start run a loop of collecting binlog from pumps online
func (c *Collector) Start(ctx context.Context) {
    var wg sync.WaitGroup
    wg.Add(1)
    go func() {
        c.publishBinlogs(ctx)
        wg.Done()
    }()

    c.keepUpdatingStatus(ctx, c.updateStatus)

    for _, p := range c.pumps {
        p.Close()
    }
    if err := c.reg.Close(); err != nil {
        log.Error(err.Error())
    }
    c.merger.Close()

    wg.Wait()
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里只需要关注 publishBinlogs 和 keepUpdatingStatus 两个方法。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;func (c *Collector) publishBinlogs(ctx context.Context) {
    defer log.Info(&amp;#34;publishBinlogs quit&amp;#34;)

    for {
        select {
        case &amp;lt;-ctx.Done():
            return
        case mergeItem, ok := &amp;lt;-c.merger.Output():
            if !ok {
                return
            }
            item := mergeItem.(*binlogItem)
            if err := c.syncBinlog(item); err != nil {
                c.reportErr(ctx, err)
                return
            }
        }
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;publishBinlogs 调用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/merge.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;merger&lt;/a&gt; 模块从所有 pump 读取 binlog，并且按照 binlog 的 commit timestamp 进行归并排序，最后通过调用 &lt;code&gt;syncBinlog&lt;/code&gt; 输出 binlog 到  Syncer 处理单元。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;func (c *Collector) keepUpdatingStatus(ctx context.Context, fUpdate func(context.Context) error) {
    // add all the pump to merger
    c.merger.Stop()
    fUpdate(ctx)
    c.merger.Continue()

    // update status when had pump notify or reach wait time
    for {
        select {
        case &amp;lt;-ctx.Done():
            return
        case nr := &amp;lt;-c.notifyChan:
            nr.err = fUpdate(ctx)
            nr.wg.Done()
        case &amp;lt;-time.After(c.interval):
            if err := fUpdate(ctx); err != nil {
                log.Error(&amp;#34;Failed to update collector status&amp;#34;, zap.Error(err))
            }
        case err := &amp;lt;-c.errCh:
            log.Error(&amp;#34;collector meets error&amp;#34;, zap.Error(err))
            return
        }
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;keepUpdatingStatus 通过下面两种方式从 etcd 获取 pump 集群的最新状态：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;定时器定时触发。&lt;/li&gt;&lt;li&gt;notifyChan 触发。这是一个必须要提一下的处理逻辑：当一个 pump 需要加入 pump c 集群的时候，该 pump 会在启动时通知所有在线的 drainer，只有全部 drainer 都被通知都成功后，pump 方可对外提供服务。 这个设计的目的是，防止对应的 pump 的 binlog 数据没有及时加入 drainer 的排序过程，从而导致 binlog 数据同步缺失。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;Syncer&lt;/h2&gt;&lt;p&gt;Syncer 代码位于 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/syncer.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;drainer/syncer.go&lt;/a&gt;，是用来处理数据同步的关键模块。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;type Syncer struct {
    schema *Schema
    cp     checkpoint.CheckPoint
    cfg    *SyncerConfig
    input  chan *binlogItem
    filter *filter.Filter
    // last time we successfully sync binlog item to downstream
    lastSyncTime time.Time
    dsyncer      dsync.Syncer
    shutdown     chan struct{}
    closed       chan struct{}
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 Syncer 的结构定义中，我们关注下面三个对象：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;dsyncer 是真正同步数据到不同目标系统的执行器实现，我们会在后续章节具体介绍，接口定义如下：&lt;br/&gt;// Syncer sync binlog item to downstream type Syncer interface {     // Sync the binlog item to downstream  Sync(item *Item) error // will be close if Close normally or meet error, call Error() to check it  Successes() &amp;lt;-chan *Item // Return not nil if fail to sync data to downstream or nil if closed normally  Error() &amp;lt;-chan error // Close the Syncer, no more item can be added by `Sync`  Close() error }&lt;/li&gt;&lt;li&gt;schema 维护了当前同步位置点的全部 schema 信息，可以根据 ddl binlog 变更对应的 schema 信息。&lt;/li&gt;&lt;li&gt;filter 负责对需要同步的 binlog 进行过滤。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Syncer 运行入口在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.7/drainer/syncer.go%23L260&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;run&lt;/a&gt; 方法，主要逻辑包含：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;依次处理 Collector 处理单元推送过来的 binlog 数据。&lt;/li&gt;&lt;li&gt;如果是 DDL binlog，则更新维护的 schema 信息。&lt;/li&gt;&lt;li&gt;利用 filter 过滤不需要同步到下游的数据。&lt;/li&gt;&lt;li&gt;调用 drainer/sync/Syncer.Sync()  异步地将数据同步到目标系统。&lt;/li&gt;&lt;li&gt;处理数据同步结果返回。&lt;br/&gt;a. 通过 Succsses() 感知已经成功同步到下游的 binlog 数据，保存其对应 commit timestamp 信息到 checkpoint。&lt;br/&gt;b. 通过 Error() 感知同步过程出现的错误，drainer 清理环境退出进程。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;本文介绍了 Drainer server 的主体结构，后续文章会具体介绍其如何同步数据到不同下游。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-binlog-source-code-reading-7/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog 源码阅读系列文章（七）Drainer server 介绍 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多 TiDB Binlog 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23TiDB-Binlog-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Blog-cns | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-25-99254953</guid>
<pubDate>Wed, 25 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>效率 10x！打造运维 TiDB 的瑞士军刀</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-20-98525255.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/98525255&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6280884e26b9b44a469ed89b0462a91e_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者介绍：陈霜，做个人吧队成员，PingCAP TiDB 研发工程师，做个人吧队参加了 TiDB Hackathon 2019，其项目「Manage many as one with SQL」获得了三等奖。&lt;/blockquote&gt;&lt;p&gt;极致的易用性一直是 PingCAP 追寻的目标。在这之前，TiDB 通过兼容 MySQL，将分布式的复杂度隐藏在 TiDB 之后，将用户从复杂的分库分表方案中解脱出来，使用户可以像使用单机数据库一样使用 TiDB。&lt;/p&gt;&lt;p&gt;不过兼容 MySQL 只是易用性的第一步，这一步主要提升了开发人员的体验。但是对于 DBA 来说，运维一个分布式系统的难度还是不低。那么分布式数据库的运维目前到底面临哪些难题？大致有以下几个方面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;各组件状态信息分散。&lt;/li&gt;&lt;li&gt;运维操作需要跨节点。&lt;/li&gt;&lt;li&gt;运维脚本重复编写，无法标准化。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 TiDB Hackathon 2019 之后，以上问题我们给出了一个统一的答案：用 SQL 管理整个 TiDB 集群。&lt;/p&gt;&lt;h2&gt;用 SQL 查询集群信息&lt;/h2&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-975b64c3ba03b53ac720a0f5aba87da6_b.gif&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;3360&quot; data-rawheight=&quot;1867&quot; data-thumbnail=&quot;https://pic3.zhimg.com/v2-975b64c3ba03b53ac720a0f5aba87da6_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;3360&quot; data-original=&quot;https://pic3.zhimg.com/v2-975b64c3ba03b53ac720a0f5aba87da6_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-975b64c3ba03b53ac720a0f5aba87da6_b.gif&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;3360&quot; data-rawheight=&quot;1867&quot; data-thumbnail=&quot;https://pic3.zhimg.com/v2-975b64c3ba03b53ac720a0f5aba87da6_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;3360&quot; data-original=&quot;https://pic3.zhimg.com/v2-975b64c3ba03b53ac720a0f5aba87da6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-975b64c3ba03b53ac720a0f5aba87da6_b.gif&quot;/&gt;&lt;figcaption&gt;图 1 查询集群系统信息&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在上面的示例中，我们通过 SQL 获得集群以下信息：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;所有节点的拓扑信息，版本信息。&lt;/li&gt;&lt;li&gt;所有节点的配置信息。&lt;/li&gt;&lt;li&gt;所有节点当前正在处理的请求，即 processlist。&lt;/li&gt;&lt;li&gt;所有节点的慢查询信息。&lt;/li&gt;&lt;li&gt;所有节点的服务器硬件信息。&lt;/li&gt;&lt;li&gt;所有节点当前的负载（Load）信息。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;提供更多的系统内存表&lt;/h3&gt;&lt;p&gt;在此之前，TiDB 提供系统信息的系统表较少。本次 Hackathon 的项目也添加了更多的系统表用于通过 SQL 获取更多信息：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;节点的硬件信息：CPU，memory，网卡，磁盘等硬件信息。&lt;/li&gt;&lt;li&gt;节点的负载信息：CPU / Memory load，网络 / 磁盘流量信息。&lt;/li&gt;&lt;li&gt;节点的配置信息。&lt;/li&gt;&lt;li&gt;节点的监控信息：如 QPS，KV Duration，TSO duration 等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;每个系统表都有一个对应的 cluster_ 前缀的集群系统表视图。这里不一一展示了。&lt;/p&gt;&lt;h2&gt;用 SQL 动态更改集群的配置&lt;/h2&gt;&lt;p&gt;下面演示用 SQL 动态修改集群所有节点的配置:&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e9e254ca81455d7e4184e0fb154dcd21_b.gif&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2510&quot; data-rawheight=&quot;1596&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-e9e254ca81455d7e4184e0fb154dcd21_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;2510&quot; data-original=&quot;https://pic2.zhimg.com/v2-e9e254ca81455d7e4184e0fb154dcd21_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e9e254ca81455d7e4184e0fb154dcd21_b.gif&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2510&quot; data-rawheight=&quot;1596&quot; data-thumbnail=&quot;https://pic2.zhimg.com/v2-e9e254ca81455d7e4184e0fb154dcd21_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;2510&quot; data-original=&quot;https://pic2.zhimg.com/v2-e9e254ca81455d7e4184e0fb154dcd21_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-e9e254ca81455d7e4184e0fb154dcd21_b.gif&quot;/&gt;&lt;figcaption&gt;图 2 动态修改集群配置&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;根据上面的动图可以看出，我们能通过 Update 语句修改 CLUSTER_CONFIG 系统表，即可完成集群配置的变更，而不需要再通过复杂的运维命令。当然，通过这个语句，也能指定只修改某个节点的某项配置。&lt;/p&gt;&lt;h2&gt;用 SQL 完成故障诊断&lt;/h2&gt;&lt;p&gt;当 TiDB 集群出现故障后，以前 DBA 可能会根据现象以及故障排除手册逐个排查。依靠我们添加完成上述的基础能力，这些完全是可以自动化的，比如一些 TiDB 中常见的故障排查场景：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;找出系统写入变慢的原因，例如节点发生宕机等等。&lt;/li&gt;&lt;li&gt;找出集群中慢查询的全链路日志行为。&lt;/li&gt;&lt;li&gt;找出磁盘容量或者内存监控异常的节点。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在本次 Hackathon 之后，通过一些简单的 SQL 语句组合，就可以得到上述的结果。这些大家千呼万唤的功能，都在一点点变成现实。&lt;/p&gt;&lt;blockquote&gt;TiDB 作为一个成熟的分布式数据库，通过一个易用且统一的方式进行集群的运维、管理以及监控非常重要。随着内部组件越来越多，监控及诊断工具的碎片化，再加上本身 TiDB 天然的分布式属性，让这个问题更加棘手。对于数据库来说，通过 SQL 来做这件事情是最天然的，也是最具有扩展性的，目前 TiDB 也正在往这个方向努力，这个项目是一个非常好的例子。&lt;br/&gt;———— 黄东旭（PingCAP | CTO）&lt;br/&gt;对于 DBA 来说，一切从简，效率为先，SQL 当仁不让是 DBA 最有力的、最能提高效率的利器。而分布式数据库有别于传统数据库，其组件居多交互复杂，性能诊断难、问题定位时间长等问题着实让 DBA 头疼。但 TiDB 作为新一代 NewSQL 数据库，一直秉承极致易用性的理念，贴切 DBA ，回归 SQL 实现整个分布式数据库集群管理，无异于节省 DBA 大量精力与时间，完美契合 DBA 。&lt;br/&gt;———— 金文涛（PingCAP | DBA ）&lt;/blockquote&gt;&lt;h2&gt;项目设计细节&lt;/h2&gt;&lt;p&gt;在这个项目之前，查询 TiDB 的系统表时，用户一定会遇到的问题是：某些系统表只包含了当前 TiDB 节点的数据，而不是所有节点的数据。比如 PROCESSLIST（当前执行语句），SLOW_QUERY（慢查询内存表）等。&lt;/p&gt;&lt;p&gt;要实现读取所有节点的系统信息和修改所有节点的配置，首先要打通 TiDB 节点之间的数据交互。先看下目前 TiDB 集群的架构，如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0041dcc66dec239f112590783ca250ac_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;554&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-0041dcc66dec239f112590783ca250ac_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0041dcc66dec239f112590783ca250ac_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;554&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-0041dcc66dec239f112590783ca250ac_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-0041dcc66dec239f112590783ca250ac_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 原来 TiDB 集群系统架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;可以发现，现有结构下，其他的组件是用 RPC 来通信的，那么自然 TiDB 也应该用 RPC 通信，理由如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 用 HTTP API 读其他 TiDB 节点的信息的可扩展性相对会差一些，RPC 框架的可塑性更高，可以承接未来 TiDB 节点间数据交互的需求。&lt;/li&gt;&lt;li&gt;计算下推到各个 TiDB 节点。某些信息的数据量可能很大，比如慢查询内存表，如果没有算子下推来完成数据的过滤，会存在很多多余的网络开销，这一点原理跟 TiDB 下推算子到 TiKV 是一样的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;所以最后我们选择在 TiDB 新增一个 RPC 服务。架构如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8637b2d05f74a0553c3b1bcb8be016b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;570&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8637b2d05f74a0553c3b1bcb8be016b7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8637b2d05f74a0553c3b1bcb8be016b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;570&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8637b2d05f74a0553c3b1bcb8be016b7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8637b2d05f74a0553c3b1bcb8be016b7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 现在 TiDB 集群系统架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;TiDB 新增的 GPC 服务和 HTTP 服务共用 10080 端口，没有新增端口。通过以下 Explain 结果可以看出来，以下语句成功下推到了各个 TiDB。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3f40bf4e9c60111b5f673f723d02ea41_b.png&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;150&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-3f40bf4e9c60111b5f673f723d02ea41_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3f40bf4e9c60111b5f673f723d02ea41_b.png&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;150&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-3f40bf4e9c60111b5f673f723d02ea41_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-3f40bf4e9c60111b5f673f723d02ea41_b.png&quot;/&gt;&lt;figcaption&gt;图 5 计算下推到其他 TiDB 节点执行&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;写在最后&lt;/h2&gt;&lt;p&gt;以上功能由于时间原因并未能完全在 Hackathon 期间完成，不过其目前已经作为一个 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/orgs/pingcap/projects/3&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;社区孵化项目&lt;/a&gt;。相信将会很快跟大家见面。&lt;/p&gt;&lt;p&gt;也欢迎感兴趣的社区小伙伴们加入我们社区的工作组：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/community/blob/master/working-groups/wg-sql-diagnostics.md&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;SQL 诊断工作组&lt;/a&gt;，一起进一步完善并落地这个项目，提高 TiDB 的易用性。&lt;/p&gt;&lt;p&gt;令人振奋的是，这一切只是一个开始。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/manage-many-as-one-with-sql/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;效率 10x！打造运维 TiDB 的瑞士军刀 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-20-98525255</guid>
<pubDate>Fri, 20 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>直击备份恢复的痛点：基于 TiDB Binlog 的快速时间点恢复</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-19-98335426.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/98335426&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b5db286b61ed8a82213524d55931e9ab_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者介绍：吕磊，Better 队成员、美团点评高级 DBA，Better 队参加了 TiDB Hackathon 2019，其项目「基于 TiDB Binlog 的 Fast-PITR」获得了最佳贡献奖。&lt;/blockquote&gt;&lt;p&gt;维护过数据库的同学应该都能体会，数据备份对于数据库来说可以说至关重要，尤其是关键业务。TiDB 原生的备份恢复方案已经在多家客户得到稳定运行的验证，但是对于业务量巨大的系统存在如下几个痛点:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;集群中数据量很大的情况下，很难频繁做全量备份。&lt;/li&gt;&lt;li&gt;传统 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/stable/reference/tidb-binlog/overview/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog&lt;/a&gt; 原样吐出 binlog 增量备份会消耗大量的磁盘空间，并且重放大量 binlog 需要较长时间。&lt;/li&gt;&lt;li&gt;binlog 本身是有向前依赖关系的，任何一个时间点的 binlog 丢失，都会导致后面的数据无法自动恢复。&lt;/li&gt;&lt;li&gt;调大 TiDB gc_life_time 保存更多版本的快照数据，一方面保存时间不能无限长，另一方面过多的版本会影响性能且占用集群空间。&lt;/li&gt;&lt;/ol&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-75a099d8e6142eb425bfcdc65293dcf8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;447&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-75a099d8e6142eb425bfcdc65293dcf8_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-75a099d8e6142eb425bfcdc65293dcf8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;447&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-75a099d8e6142eb425bfcdc65293dcf8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-75a099d8e6142eb425bfcdc65293dcf8_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1 原生 binlog 备份恢复&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们在线上使用 TiDB 已经超过 2 年，从 1.0 RC 版本到 1.0 正式版、2.0、2.1 以及现在的 3.0，我们能感受到 TiDB 的飞速进步和性能提升，但备份恢复的这些痛点，是我们 TiDB 在关键业务中推广的一个掣肘因素。于是，我们选择了这个题目: 基于 TiDB Binlog 的 Fast-PITR (Fast point in time recovery)，即基于 TiDB Binlog 的快速时间点恢复，实现了基于 TiDB Binlog 的逐级 merge，以最小的代价实现快速 PITR，解决了现有 TiDB 原生备份恢复方案的一些痛点问题。&lt;/p&gt;&lt;h2&gt;方案介绍&lt;/h2&gt;&lt;p&gt;1.根据互联网行业特征和 2/8 原则，每天真正会被更新的数据只有 20% 而且是频繁更新。我们也统计了线上万亿级别 DML 中 CUD 真实占比为 15:20:2，其中 update 超过了 50%。row 模式的 binlog 中我们只记录前镜像和最终镜像，可以得到一份非常轻量的“差异备份”，如图所示:&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7a4eec1ffbeed2b5791b15eeddf676dd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;368&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-7a4eec1ffbeed2b5791b15eeddf676dd_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7a4eec1ffbeed2b5791b15eeddf676dd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;368&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-7a4eec1ffbeed2b5791b15eeddf676dd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-7a4eec1ffbeed2b5791b15eeddf676dd_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2 binlog merge 原则&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;2.我们将 binlog 按照时间分段，举例说，每天的 binlog 为一个分段，每段按照上面的原则进行 merge，这段 binlog 合并后成为一个备份集，备份集是一些独立的文件。由于每一个备份集在 merge 阶段已经去掉了冲突，所以一方面对体积进行了压缩，另一方面可以以行级并发回放，提高回放速度，结合 full backup 快速恢复到目标时间点，完成 PITR 功能。而且，这种合并的另一个好处是，生成的备份集与原生 binlog file 可以形成互备关系，备份集能够通过原生 binlog file 重复生成。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9f4cf35b5a9a55470256e0376ea0142a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;456&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-9f4cf35b5a9a55470256e0376ea0142a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9f4cf35b5a9a55470256e0376ea0142a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;456&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-9f4cf35b5a9a55470256e0376ea0142a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-9f4cf35b5a9a55470256e0376ea0142a_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 binlog 并行回放&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;binlog 分段方式可以灵活定义起点和终点:&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;-start-datetime string
      recovery from start-datetime, empty string means starting from the beginning of the first file
-start-tso int
      similar to start-datetime but in pd-server tso format
-stop-datetime string
      recovery end in stop-datetime, empty string means never end.
-stop-tso int
      similar to stop-datetime, but in pd-server tso format&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;3.在此基础上，我们做了些优化:&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bcfb780c4a05f0501199259f864e3201_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;462&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-bcfb780c4a05f0501199259f864e3201_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bcfb780c4a05f0501199259f864e3201_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;462&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-bcfb780c4a05f0501199259f864e3201_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-bcfb780c4a05f0501199259f864e3201_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 优化后&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;备份集的格式与 TiDB Binlog 相同，所以，备份集之间可以根据需要再次合并，形成新的备份集，加速整个恢复流程。&lt;/p&gt;&lt;h2&gt;实现方式&lt;/h2&gt;&lt;h3&gt;Map-Reduce 模型&lt;/h3&gt;&lt;p&gt;由于需要将同一 key（主键或者唯一索引键）的所有变更合并到一条 Event 中，需要在内存中维护这个 key 所在行的最新合并数据。如果 binlog 中包含大量不同的 key 的变更，则会占用大量的内存。因此设计了 Map-Reduce 模型来对 binlog 数据进行处理：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-30009ebbeb06fa6e12ac4cb453cc3927_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;329&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-30009ebbeb06fa6e12ac4cb453cc3927_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-30009ebbeb06fa6e12ac4cb453cc3927_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;329&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-30009ebbeb06fa6e12ac4cb453cc3927_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-30009ebbeb06fa6e12ac4cb453cc3927_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5 binlog 合并方式&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Mapping 阶段：读取 Binlog file，通过 PITR 工具将文件按库名 + 表名输出，再根据 Key hash 成不同的小文件存储，这样同一行数据的变更都保存在同一文件下，且方便 Reduce 阶段的处理。&lt;/li&gt;&lt;li&gt;Reducing 阶段：并发将小文件按照规则合并，去重，生成备份集文件。&lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-979d4fb0d0c24d573f807e9e764970d4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;856&quot; data-rawheight=&quot;454&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;856&quot; data-original=&quot;https://pic1.zhimg.com/v2-979d4fb0d0c24d573f807e9e764970d4_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-979d4fb0d0c24d573f807e9e764970d4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;856&quot; data-rawheight=&quot;454&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;856&quot; data-original=&quot;https://pic1.zhimg.com/v2-979d4fb0d0c24d573f807e9e764970d4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-979d4fb0d0c24d573f807e9e764970d4_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;配合官方的 reparo 工具，将备份集并行回放到下游库。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;DDL 的处理&lt;/h3&gt;&lt;p&gt;Drainer 输出的 binlog 文件中只包含了各个列的数据，缺乏必要的表结构信息（PK/UK），因此需要获取初始的表结构信息，并且在处理到 DDL binlog 数据时更新表结构信息。DDL 的处理主要实现在 DDL Handle 结构中：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e10ccb1784cbcf1b615f391b1ad677c0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;537&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-e10ccb1784cbcf1b615f391b1ad677c0_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e10ccb1784cbcf1b615f391b1ad677c0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;537&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-e10ccb1784cbcf1b615f391b1ad677c0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-e10ccb1784cbcf1b615f391b1ad677c0_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6 DDL 处理&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;首先通过配置 TiDB 的 Restful API 获取 TiKV 中保存的历史 DDL 信息，通过这些历史 DDL 获取 binlog 处理时的初始表结构信息，然后在处理到 DDL binlog 时更新表结构信息。&lt;/p&gt;&lt;p&gt;由于 DDL 的种类比较多，且语法比较复杂，无法在短时间内完成一个完善的 DDL 处理模块，因此使用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//links.jianshu.com/go%3Fto%3Dhttps%253A%252F%252Fgithub.com%252FWangXiangUSTC%252Ftidb-lite&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb-lite&lt;/a&gt; 将 mocktikv 模式的 TiDB 内置到程序中，将 DDL 执行到该 TiDB，再重新获取表结构信息。&lt;/p&gt;&lt;h2&gt;方案总结&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;恢复速度快：merge 掉了中间状态，不但减少了不必要的回放操作，且实现了行级并发。&lt;/li&gt;&lt;li&gt;节约磁盘空间：测试结果表明，我们的 binlog 压缩率可以达到 30% 左右。&lt;/li&gt;&lt;li&gt;完成度高：程序可以流畅的运行，并进行了现场演示。&lt;/li&gt;&lt;li&gt;表级恢复：由于备份集是按照表存储的，所以可以随时根据需求灵活恢复单表。&lt;/li&gt;&lt;li&gt;兼容性高：方案设计初期就考虑了组件的兼容性，PITR 工具可以兼容大部分的 TiDB 的生态工具。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;方案展望&lt;/h2&gt;&lt;p&gt;Hackathon 比赛时间只有两天，时间紧任务重，我们实现了上面的功能外，还有一些没来得及实现的功能。&lt;/p&gt;&lt;h3&gt;增量与全量的合并&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7b8225c8c6dceb51e6b074b365cbd46b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;470&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-7b8225c8c6dceb51e6b074b365cbd46b_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7b8225c8c6dceb51e6b074b365cbd46b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;470&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-7b8225c8c6dceb51e6b074b365cbd46b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-7b8225c8c6dceb51e6b074b365cbd46b_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7 方案展望&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;增量备份集，逻辑上是一些 insert+update+delete 语句。&lt;/p&gt;&lt;p&gt;全量备份集，是由 mydumper 生成的 create schema+insert 语句。&lt;/p&gt;&lt;p&gt;我们可以将增量备份中的 insert 语句前置到全量备份集中，全量备份集配合 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/stable/reference/tools/tidb-lightning/overview/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Lightning 工具&lt;/a&gt; 急速导入到下游 TiKV 集群，Lightning 恢复速度是逻辑恢复的 5 - 10 倍 ，再加上一份更轻量的增量备份集 (update+delete) 直接实现 PITR 功能。&lt;/p&gt;&lt;h3&gt;DDL 预处理&lt;/h3&gt;&lt;p&gt;PIRT 工具实际上是一个 binlog 的 merge 过程，处理一段 binlog 期间，为了保证数据的一致性，理论上如果遇到 DDL 变更，merge 过程就要主动断掉，生成备份集，再从这个断点继续 merge 工作，因此会生成两个备份集，影响 binlog 的压缩率。&lt;/p&gt;&lt;p&gt;为了加速恢复速度，我们可以将 DDL 做一些预处理，比如发现一段 binlog 中包含某个表的 Drop table 操作，那么完全可以将 Drop table 前置，在程序一开始就忽略掉这个表的 binlog 不做处理，通过这些“前置”或“后置”的预处理，来提高备份和恢复的效率。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3c84ad594f57274d89fb17fdc0d76917_b.png&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;156&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-3c84ad594f57274d89fb17fdc0d76917_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3c84ad594f57274d89fb17fdc0d76917_b.png&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;156&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-3c84ad594f57274d89fb17fdc0d76917_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-3c84ad594f57274d89fb17fdc0d76917_b.png&quot;/&gt;&lt;figcaption&gt;图 8 DDL 预处理&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;结语&lt;/h2&gt;&lt;p&gt;我们是在坤坤（李坤）的热心撮合下组建了 Better 战队，成员包括黄潇、高海涛、我，以及 PingCAP 的王相同学。感谢几位大佬不离不弃带我飞，最终拿到了最佳贡献奖。比赛过程惊险刺激（差点翻车），比赛快结束的时候才调通代码，强烈建议以后参加 Hackathon 的同学们一定要抓紧时间，尽早完成作品。参赛的短短两天让我们学到很多，收获很多，见到非常多优秀的选手和炫酷的作品，我们还有很长的路要走，希望这个项目能继续维护下去，期待明年的 Hackathon 能见到更多优秀的团队和作品。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/fast-pitr-based-on-binlog/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;直击备份恢复的痛点：基于 TiDB Binlog 的快速时间点恢复 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-19-98335426</guid>
<pubDate>Thu, 19 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 源码解析系列文章（十六）TiKV Coprocessor Executor 源码解析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-12-96906129.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/96906129&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-88d254931476b5e8cedbb60207b79334_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：邓力铭&lt;/p&gt;&lt;p&gt;在前两篇文章 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/cdC7f9N9C88MJ_syNUg21g&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章（十四）Coprocessor 概览&lt;/a&gt;、&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/UYcny9G5snh-MoMFm2qxsw&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章（十五）表达式计算框架中&lt;/a&gt;，讲到了 TiDB 为了最大化利用分布式计算能力，会尽量将 Selection 算子、Aggregation 算子等算子下推到 TiKV 节点上，以及下推的表达式是如何在 TiKV 上做计算的。本文将在前两篇文章的基础上，介绍下推算子的执行流程并分析下推算子的部分实现细节，加深大家对 TiKV Coprocessor 的理解。&lt;/p&gt;&lt;h2&gt;什么是下推算子&lt;/h2&gt;&lt;p&gt;以下边的 &lt;code&gt;SQL&lt;/code&gt; 为例子：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;select  *  from students where age &amp;gt;  21  limit  2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;TiDB 在解析完这条 &lt;code&gt;SQL&lt;/code&gt; 语句之后，会开始制定执行计划。在这个语句中， TiDB 会向 TiKV 下推一个可以用有向无环图（DAG）来描述的查询请求：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2aab23543ed86a6d8b6c487ae71fbe15_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;818&quot; data-rawheight=&quot;279&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;818&quot; data-original=&quot;https://pic2.zhimg.com/v2-2aab23543ed86a6d8b6c487ae71fbe15_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2aab23543ed86a6d8b6c487ae71fbe15_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;818&quot; data-rawheight=&quot;279&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;818&quot; data-original=&quot;https://pic2.zhimg.com/v2-2aab23543ed86a6d8b6c487ae71fbe15_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-2aab23543ed86a6d8b6c487ae71fbe15_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;以上的 &lt;code&gt;DAG&lt;/code&gt; 是一个由一系列算子组成的有向无环图，算子在 TiKV 中称为 &lt;code&gt;Executor&lt;/code&gt; 。整个 &lt;code&gt;DAG&lt;/code&gt; 描述了查询计划在 TiKV 的执行过程。在上边的例子中，一条查询 &lt;code&gt;SQL&lt;/code&gt; 被翻译成了三个执行步骤：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;扫表&lt;/li&gt;&lt;li&gt;选择过滤&lt;/li&gt;&lt;li&gt;取若干行&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;有了基本概念后，下面我们简单介绍一下这样的查询计划在 TiKV 内部的一个执行流程。&lt;/p&gt;&lt;h2&gt;下推算子如何执行&lt;/h2&gt;&lt;h3&gt;绕不开的火山&lt;/h3&gt;&lt;p&gt;TiKV 执行器是基于 Volcano Model （火山模型），一种经典的基于行的流式迭代模型。现在主流的关系型数据库都采用了这种模型，例如 Oracle，MySQL 等。&lt;/p&gt;&lt;p&gt;我们可以把每个算子看成一个迭代器。每次调用它的 &lt;code&gt;next()&lt;/code&gt; 方法，我们就可以获得一行，然后向上返回。而每个算子都把下层算子看成一张表，返回哪些行，返回怎么样的行由算子本身决定。举个例子：&lt;/p&gt;&lt;p&gt;假设我们现在对一张没有主键，没有索引的表 &lt;code&gt;[1]&lt;/code&gt; ，执行一次全表扫描操作：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;select * from t where a &amp;gt; 2 limit 2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;表 &lt;code&gt;[1]&lt;/code&gt;：&lt;/p&gt;&lt;p&gt;a&lt;code&gt;(int)&lt;/code&gt;b&lt;code&gt;(int)&lt;/code&gt;3112522314&lt;/p&gt;&lt;p&gt;那么我们就可以得到这样的一个执行计划：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-648d673829c8078ea48bd8eee0e5331c_b.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;487&quot; data-rawheight=&quot;559&quot; data-thumbnail=&quot;https://pic1.zhimg.com/v2-648d673829c8078ea48bd8eee0e5331c_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;487&quot; data-original=&quot;https://pic1.zhimg.com/v2-648d673829c8078ea48bd8eee0e5331c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-648d673829c8078ea48bd8eee0e5331c_b.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;487&quot; data-rawheight=&quot;559&quot; data-thumbnail=&quot;https://pic1.zhimg.com/v2-648d673829c8078ea48bd8eee0e5331c_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;487&quot; data-original=&quot;https://pic1.zhimg.com/v2-648d673829c8078ea48bd8eee0e5331c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-648d673829c8078ea48bd8eee0e5331c_b.gif&quot;/&gt;&lt;/figure&gt;&lt;p&gt;每个算子都实现了一个 &lt;code&gt;Executor&lt;/code&gt; 的 &lt;code&gt;trait&lt;/code&gt;， 所以每个算子都可以调用 &lt;code&gt;next()&lt;/code&gt; 来向上返回一行。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub trait Executor: Send {
    fn next(&amp;amp;mut self) -&amp;gt; Result&amp;lt;Option&amp;lt;Row&amp;gt;&amp;gt;;
    // ...
}
当以上的请求被解析之后，我们会在 ExecutorRunner 里边不断的调用最上层算子的 next() 方法， 直到其无法再返回行。
pub fn handle_request(&amp;amp;mut self) -&amp;gt; Result&amp;lt;SelectResponse&amp;gt; {
    loop {
        match self.executor.next()? {
            Some(row) =&amp;gt; {
                // Do some aggregation.
            },
            None =&amp;gt; {
                // ...
                return result;
            }
        }
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;大概的逻辑就是：&lt;code&gt;Runner&lt;/code&gt; 调用 &lt;code&gt;Limit&lt;/code&gt; 算子的 &lt;code&gt;next()&lt;/code&gt; 方法，然后这个时候 &lt;code&gt;Limit&lt;/code&gt; 实现的 &lt;code&gt;next()&lt;/code&gt; 方法会去调用下一层算子 &lt;code&gt;Selection&lt;/code&gt; 的 &lt;code&gt;next()&lt;/code&gt; 方法要一行上来做聚合，直到达到预设的阀值，在例子中也就是两行，接着 &lt;code&gt;Selection&lt;/code&gt; 实现的 &lt;code&gt;next()&lt;/code&gt; 又会去调用下一层算子的 &lt;code&gt;next()&lt;/code&gt; 方法， 也就是 &lt;code&gt;TableScan&lt;/code&gt;， &lt;code&gt;TableScan&lt;/code&gt; 的 &lt;code&gt;next()&lt;/code&gt; 实现是根据请求中的 &lt;code&gt;KeyRange&lt;/code&gt;， 向下边的 &lt;code&gt;MVCC&lt;/code&gt; 要上一行，然后返回给上层算子, 也就是第一行 &lt;code&gt;(3, 1)&lt;/code&gt;，&lt;code&gt;Selection&lt;/code&gt; 收到行后根据 &lt;code&gt;where&lt;/code&gt; 字句中的表达式的值做判断，如果满足条件向上返回一行， 否则继续问下层算子要一行，此时 &lt;code&gt;a == 3 &amp;gt; 2&lt;/code&gt;, 满足条件向上返回， &lt;code&gt;Limit&lt;/code&gt; 接收到一行则判断当前收到的行数时候满两行，但是现在只收到一行，所以继续问下层算子要一行。接下来 &lt;code&gt;TableScan&lt;/code&gt; 返回 &lt;code&gt;(1,2), Selection&lt;/code&gt; 发现不满足条件，继续问 &lt;code&gt;TableScan&lt;/code&gt; 要一行也就是 &lt;code&gt;(5,2), Selection&lt;/code&gt; 发现这行满足条件，然后返回这一行，&lt;code&gt;Limit&lt;/code&gt; 接收到一行，然后在下一次调用其 &lt;code&gt;next()&lt;/code&gt; 方法时，发现接收到的行数已经满两行，此时返回 &lt;code&gt;None&lt;/code&gt;， &lt;code&gt;Runner&lt;/code&gt; 会开始对结果开始聚合，然会返回一个响应结果。&lt;/p&gt;&lt;h3&gt;引入向量化的查询引擎&lt;/h3&gt;&lt;p&gt;当前 TiKV 引入了向量化的执行引擎，所谓的向量化，就是在 &lt;code&gt;Executor&lt;/code&gt; 间传递的不再是单单的一行，而是多行，比如 &lt;code&gt;TableScan&lt;/code&gt; 在底层 &lt;code&gt;MVCC Snapshot&lt;/code&gt; 中扫上来的不再是一行，而是说多行。自然的，在算子执行计算任务的时候，计算的单元也不再是一个标量，而是一个向量。举个例子，当遇到一个表达式：&lt;code&gt;a + b&lt;/code&gt; 的时候， 我们不是计算一行里边 &lt;code&gt;a&lt;/code&gt; 列和 &lt;code&gt;b&lt;/code&gt; 列两个标量相加的结果，而是计算 &lt;code&gt;a&lt;/code&gt; 列和 &lt;code&gt;b&lt;/code&gt; 列两列相加的结果。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f80e2243e24af3bb84f7bd46ff202204_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;916&quot; data-rawheight=&quot;583&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;916&quot; data-original=&quot;https://pic1.zhimg.com/v2-f80e2243e24af3bb84f7bd46ff202204_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-f80e2243e24af3bb84f7bd46ff202204_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;916&quot; data-rawheight=&quot;583&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;916&quot; data-original=&quot;https://pic1.zhimg.com/v2-f80e2243e24af3bb84f7bd46ff202204_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-f80e2243e24af3bb84f7bd46ff202204_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;为什么要引入向量化模型呢，原因有以下几点：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于每行我们至少得调用 1 次 &lt;code&gt;next()&lt;/code&gt; 方法，如果 &lt;code&gt;DAG&lt;/code&gt; 的最大深度很深，为了获取一行我们需要调用更多次的 &lt;code&gt;next()&lt;/code&gt; 方法，所以在传统的迭代模型中，虚函数调用的开销非常大。如果一次 &lt;code&gt;next()&lt;/code&gt; 方法就返回多行，这样平均下来每次 &lt;code&gt;next()&lt;/code&gt; 方法就可以返回多行，而不是至多一行。&lt;/li&gt;&lt;li&gt;由于迭代的开销非常大，整个执行的循环无法被 &lt;code&gt;loop-pipelining&lt;/code&gt; 优化，使得整个循环流水线被卡死，IPC 大大下降。返回多行之后，每个算子内部可以采用开销较小的循环，更好利用 &lt;code&gt;loop-pipelining&lt;/code&gt; 优化。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;当然向量化模型也会带来一些问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;原先最上层算子按需向下层算子拿上一行，而现在拿上多行，内存开销自然会增加。&lt;/li&gt;&lt;li&gt;计算模型发生变化，原来基于标量计算的表达式框架需要重构 （详见上篇文章）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;但是这样并不影响向量化查询带来的显著的性能提升，下边是引入向量化模型后一个基准测试结果：（需要注意的是，Coprocessor 计算还只是 TPC-H 中的其中一部分，所以计算任务比重很大程度上决定了开不开向量化带来的提升比例）。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3877d6e8bb3b713b5ad3789769faf7e2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;497&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-3877d6e8bb3b713b5ad3789769faf7e2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3877d6e8bb3b713b5ad3789769faf7e2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;497&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-3877d6e8bb3b713b5ad3789769faf7e2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-3877d6e8bb3b713b5ad3789769faf7e2_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;引入向量化模型后，原先的 &lt;code&gt;Execturor&lt;/code&gt; trait 就变成了 &lt;code&gt;BatchExecutor&lt;/code&gt;， 对应的 &lt;code&gt;next()&lt;/code&gt; 方法就成了 &lt;code&gt;next_batch()&lt;/code&gt;。 自然的 &lt;code&gt;next_batch&lt;/code&gt; 不再返回一个行，而是一个 &lt;code&gt;BatchExecuteResult&lt;/code&gt;，上边记录了扫上来的一张表 &lt;code&gt;physical_columns&lt;/code&gt;，以及子表中哪些行应当被保留的 &lt;code&gt;logical_rows&lt;/code&gt; 和一个 &lt;code&gt;is_drain&lt;/code&gt; 用来表示下层算子是否已经没有数据可以返回。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub trait BatchExecutor: Send {

    /// 获取表的 `schema`
    fn schema(&amp;amp;self) -&amp;gt; &amp;amp;[FieldType];

    // 向下层算子要回一张表
    fn next_batch(&amp;amp;mut self, scan_rows: usize) -&amp;gt; BatchExecuteResult;

    // ...
}

pub struct BatchExecuteResult {
    // 本轮循环 `TableScan` 扫上来的数据
    pub physical_columns: LazyBatchColumnVec,

    /// 记录 `physical_columns` 中有效的行的下标
    pub logical_rows: Vec&amp;lt;usize&amp;gt;,

    // ...

    // 表示下层算子是否已经没有数据可以返回
    pub is_drained: Result&amp;lt;bool&amp;gt;,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在接下来的文章中，我们将简单介绍一下几种典型算子的实现细节，旨在让大家更加熟悉各个算子的工作原理。&lt;/p&gt;&lt;h2&gt;典型算子的实现&lt;/h2&gt;&lt;h3&gt;&lt;code&gt;BatchTableScanExecutor&lt;/code&gt; 的实现&lt;/h3&gt;&lt;p&gt;首先我们先明确一下 &lt;code&gt;BatchTableScanExecutor&lt;/code&gt; 的功能，&lt;code&gt;TableScan&lt;/code&gt; 实现的 &lt;code&gt;next_batch()&lt;/code&gt; 每被调用一次，它就会从底层的实现了 &lt;code&gt;Storage trait&lt;/code&gt; 的存储层中扫上指定的行数，也就是 &lt;code&gt;scan_rows&lt;/code&gt; 行。但是由于我们在计算的时候是采用向量化的计算模型，计算都是基于列进行的，所以我们会对扫上来的行进行一次行列转换，将表从行存格式转换成列存格式。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bb0e84cbb640b86a5adbc4e82eecd821_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;743&quot; data-rawheight=&quot;504&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;743&quot; data-original=&quot;https://pic2.zhimg.com/v2-bb0e84cbb640b86a5adbc4e82eecd821_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-bb0e84cbb640b86a5adbc4e82eecd821_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;743&quot; data-rawheight=&quot;504&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;743&quot; data-original=&quot;https://pic2.zhimg.com/v2-bb0e84cbb640b86a5adbc4e82eecd821_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-bb0e84cbb640b86a5adbc4e82eecd821_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;接下来我们看看 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/96c3f978f655148b1703a520cb9b2e9001dd256d/components/tidb_query/src/batch/interface.rs%23L19&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;BatchTableScanExecutor&lt;/a&gt;&lt;/code&gt; 现在的定义：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct BatchTableScanExecutor&amp;lt;S: Storage&amp;gt;(ScanExecutor&amp;lt;S, TableScanExecutorImpl&amp;gt;);&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从结构体的定义中我们可以看出，&lt;code&gt;BatchTableScanExecutor&lt;/code&gt; 依赖于 &lt;code&gt;ScanExecutor&lt;/code&gt;，而这个 &lt;code&gt;ScanExecutor&lt;/code&gt; 依赖于一个实现 &lt;code&gt;Storage&lt;/code&gt; 的类型和具体 &lt;code&gt;TableScanExecutorImpl&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;其中 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/96c3f978f655148b1703a520cb9b2e9001dd256d/components/tidb_query/src/batch/executors/util/scan_executor.rs%23L38&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ScanExecutor&lt;/a&gt;&lt;/code&gt; 是一个通用的结构体，其作用是为了抽象出扫表和扫索引两种操作，这两种操作都需要依赖一个 &lt;code&gt;Storage&lt;/code&gt; 而区别他们具体行为的是一个实现了 &lt;code&gt;ScanExecutorImpl&lt;/code&gt; 的结构体，在上边的定义中就是：&lt;code&gt;TableScanExecutorImpl&lt;/code&gt;。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct ScanExecutor&amp;lt;S: Storage, I: ScanExecutorImpl&amp;gt; {
    /// 具体的扫表/扫索引实现。
    imp: I,

    /// 给定一个 `KeyRange`，扫上一行或者多行。
    scanner: RangesScanner&amp;lt;S&amp;gt;,

    // 标记是否已经扫完了所有的行。
    is_ended: bool,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;BatchTableScanExecutor&lt;/code&gt; 中我们需要重点关注的是其实现的 &lt;code&gt;BatchExecutor&lt;/code&gt;, 其中最为关键的就是 &lt;code&gt;next_batch()&lt;/code&gt;，然而其依赖于内部 &lt;code&gt;ScanExecutor&lt;/code&gt; 的 &lt;code&gt;BatchExecutor&lt;/code&gt; 实现，也就是：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn next_batch(&amp;amp;mut self, scan_rows: usize) -&amp;gt; BatchExecuteResult {

        // 创建一个列数组
        let mut logical_columns = self.imp.build_column_vec(scan_rows);
        
        // 扫上 `scan_rows` 行， 然后按列填充到创建好的列数组中。
        let is_drained = self.fill_column_vec(scan_rows, &amp;amp;mut logical_columns);

        // 创建一个 `logical_rows`, 表示当前表中所有行有效。后边可能根据 `Selection` 的结果修改这个 `logical_rows`。
        let logical_rows = (0..logical_columns.rows_len()).collect();

        // 判断是否扫完传入的 `KeyRange`
        match &amp;amp;is_drained {
            // Note: `self.is_ended` is only used for assertion purpose.
            Err(_) | Ok(true) =&amp;gt; self.is_ended = true,
            Ok(false) =&amp;gt; {}
        };

        // 返回 `BatchExecuteResult`
        BatchExecuteResult {
            // ...
        }
    }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;值得注意的是上边 &lt;code&gt;fill_column_vec&lt;/code&gt; 的实现, 它大概的逻辑就是每次问 &lt;code&gt;self.scanner&lt;/code&gt; 要上一个 &lt;code&gt;Key-Value&lt;/code&gt; 对, 然后扔给 &lt;code&gt;self.imp.process_kv_pair&lt;/code&gt; 处理，在扫表的实现中就是将 &lt;code&gt;value&lt;/code&gt; 看成是一个行的 &lt;code&gt;datum&lt;/code&gt; 编码，然后将每列的数据解出来然后放到建好的列数组里边去。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn fill_column_vec(
        &amp;amp;mut self,
        scan_rows: usize,
        columns: &amp;amp;mut LazyBatchColumnVec,
    ) -&amp;gt; Result&amp;lt;bool&amp;gt; {
        assert!(scan_rows &amp;gt; 0);

        for _ in 0..scan_rows {
            let some_row = self.scanner.next()?;
            if let Some((key, value)) = some_row {
                // 将扫上来的一行放入 `columns` 中
                self.imp.process_kv_pair(&amp;amp;key, &amp;amp;value, columns)?;
            } else {
                // 没有 `KeyRange` 可供扫描，已经完成扫表。
                return Ok(true);
            }
        }

        // 表示下层数据还没有扫完。
        Ok(false)
    }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;值得注意的是，现在表中的数据都是未经解码的生数据，所谓的生数据就是还不能直接参与到表达式计算的数据，这里采用的是一种 lazy decoding 的策略，只有要参与计算的时候，我们才会解码特定的列，而不是将数据扫上来就开始解码数据，将其变成能够直接参与计算的结构。&lt;/b&gt;&lt;/p&gt;&lt;h3&gt;&lt;code&gt;BatchSelectionExecutor&lt;/code&gt; 的实现&lt;/h3&gt;&lt;p&gt;接下来要介绍的是 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/96c3f978f655148b1703a520cb9b2e9001dd256d/components/tidb_query/src/batch/executors/selection_executor.rs%23L17&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;BatchSelectionExecutor&lt;/a&gt;&lt;/code&gt; 的实现，我们首先来看看定义：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct BatchSelectionExecutor&amp;lt;Src: BatchExecutor&amp;gt; {
    // ...
    
    // 数据源
    src: Src,

    // 条件表达式
    conditions: Vec&amp;lt;RpnExpression&amp;gt;,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;首先， &lt;code&gt;BatchSelectionExecutor&lt;/code&gt; 需要依赖一个 &lt;code&gt;Src&lt;/code&gt;，一个 &lt;code&gt;BatchExecutor&lt;/code&gt; 来提供数据的来源，然后是一组条件表达式，当 &lt;code&gt;BatchSelectionExecutor&lt;/code&gt; 在执行的时候会对表达式进行求值，然后根据求出的值对下层数据拉上来的行做过滤聚合，然后返回过滤出的行。&lt;/p&gt;&lt;p&gt;观察 &lt;code&gt;BatchSelectionExecutor&lt;/code&gt; 实现的 &lt;code&gt;BatchExecutor&lt;/code&gt; 可以发现，其中的 &lt;code&gt;next_batch()&lt;/code&gt; 方法依赖于 &lt;code&gt;handle_src_result()&lt;/code&gt;：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;#[inline]
    fn next_batch(&amp;amp;mut self, scan_rows: usize) -&amp;gt; BatchExecuteResult {
        // 从下层算子那会一块数据开始过滤
        let mut src_result = self.src.next_batch(scan_rows);

        // 根据表达式的值，过滤出对应的行。
        if let Err(e) = self.handle_src_result(&amp;amp;mut src_result) {
            src_result.is_drained = src_result.is_drained.and(Err(e));
            src_result.logical_rows.clear();
        } else {
            // ... 
        }

        src_result&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;通过观察 &lt;code&gt;handle_src_result&lt;/code&gt; 的实现，我们可以发现，它会遍历所有表达式，对其求值，表达式的值可能是一个标量，也可能是一个向量，但是我们完全是可以把标量看成是每行都一样的向量，然后根据每行的值，将其转换成 &lt;code&gt;bool&lt;/code&gt;，如果该行的值为 &lt;code&gt;true&lt;/code&gt;，则在 &lt;code&gt;logical_rows&lt;/code&gt; 中保留他的下标。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn handle_src_result(&amp;amp;mut self, src_result: &amp;amp;mut BatchExecuteResult) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let mut src_logical_rows_copy = Vec::with_capacity(src_result.logical_rows.len());
        let mut condition_index = 0;
        while condition_index &amp;lt; self.conditions.len() &amp;amp;&amp;amp; !src_result.logical_rows.is_empty() {
            // 拷贝一份下层算子的 `logical_rows`，用做计算表达式。
            src_logical_rows_copy.clear();
            src_logical_rows_copy.extend_from_slice(&amp;amp;src_result.logical_rows);

            // 计算表达式的值，然后根据表达式的值去更新下层算子的 `logical_rows`。
            match self.conditions[condition_index].eval(
                &amp;amp;mut self.context,
                self.src.schema(),
                &amp;amp;mut src_result.physical_columns,
                &amp;amp;src_logical_rows_copy,
                // 表达式产生的结果如果是一列的话, 这里表示表达式应该输出的行数
                src_logical_rows_copy.len(),
            )? {
                RpnStackNode::Scalar { value, .. } =&amp;gt; {
                    // 如果表达式是一个标量，根据转换成 `bool` 的值确定是否保留该列。
                    update_logical_rows_by_scalar_value(
                        &amp;amp;mut src_result.logical_rows,
                        &amp;amp;mut self.context,
                        value,
                    )?;
                }
                RpnStackNode::Vector { value, .. } =&amp;gt; {
                    // 根据每行的结果，确定是否保留那行。
                    update_logical_rows_by_vector_value(
                    &amp;amp;mut src_result.logical_rows,
                    &amp;amp;mut self.context,
                    eval_result,
                    eval_result_logical_rows,
                    )?;
                }
            }

            condition_index += 1;
        }

        Ok(())
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3&gt;&lt;code&gt;BatchFastHashAggregationExecutor&lt;/code&gt; 的实现&lt;/h3&gt;&lt;p&gt;聚合算子的种类有很多种，包括：&lt;/p&gt;&lt;p&gt;&lt;code&gt;SimpleAggregation&lt;/code&gt; (没有 &lt;code&gt;group by&lt;/code&gt; 字句，只有聚合函数)&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;=&amp;gt; &lt;code&gt;select count(*) from t where a &amp;gt; 1&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;FastHashAggregation&lt;/code&gt; (只有一个 &lt;code&gt;group by&lt;/code&gt; column)&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;=&amp;gt; &lt;code&gt;select count(*) from t group by a&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;SlowHashAggregation&lt;/code&gt; (多个 &lt;code&gt;groub by&lt;/code&gt; columns, 或者表达式值不是 &lt;code&gt;Hashable&lt;/code&gt; 的)&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;=&amp;gt; &lt;code&gt;select sum(*) from t group by a, b&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;StreamAggregation&lt;/code&gt; 这种聚合算子假设输入已经按照 &lt;code&gt;group by&lt;/code&gt; columns 排好序。&lt;/p&gt;&lt;p&gt;我们这里挑出一个比较具有代表性的算子：&lt;code&gt;BatchFastHashAggregationExecutor&lt;/code&gt; 来进行分析。&lt;/p&gt;&lt;p&gt;首先要明确一下 &lt;code&gt;BatchFastHashAggregationExecutor&lt;/code&gt; 大致的执行过程，首先我们会根据 &lt;code&gt;group by&lt;/code&gt; column 里边的值给下层算子返回的表进行分组，比如：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;select count(*) from t group by a&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-39c46a99e107639c311e6e2cce953021_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;725&quot; data-rawheight=&quot;479&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;725&quot; data-original=&quot;https://pic2.zhimg.com/v2-39c46a99e107639c311e6e2cce953021_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-39c46a99e107639c311e6e2cce953021_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;725&quot; data-rawheight=&quot;479&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;725&quot; data-original=&quot;https://pic2.zhimg.com/v2-39c46a99e107639c311e6e2cce953021_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-39c46a99e107639c311e6e2cce953021_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;然后，我们会遍历每个组，然后针对每个组求出每个聚合函数的值，在这里就是：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-20e346377b4bd8fe66031f7423510512_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;602&quot; data-original=&quot;https://pic3.zhimg.com/v2-20e346377b4bd8fe66031f7423510512_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-20e346377b4bd8fe66031f7423510512_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;602&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;602&quot; data-original=&quot;https://pic3.zhimg.com/v2-20e346377b4bd8fe66031f7423510512_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-20e346377b4bd8fe66031f7423510512_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;接下来就涉及到两个重要的细节：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;聚合函数如何求值。&lt;/li&gt;&lt;li&gt;如何根据 &lt;code&gt;group_by column&lt;/code&gt; 对行进行分组并聚合。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;后续几节我们着重介绍一下这两个细节是如何实现的。&lt;/p&gt;&lt;h3&gt;聚合函数&lt;/h3&gt;&lt;p&gt;每个聚合函数都会实现一个 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/96c3f978f655148b1703a520cb9b2e9001dd256d/components/tidb_query/src/aggr_fn/mod.rs%23L35&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;AggrFunction&lt;/a&gt;&lt;/code&gt; 这个 trait：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub trait AggrFunction: std::fmt::Debug + Send + &amp;#39;static {
    /// The display name of the function.
    fn name(&amp;amp;self) -&amp;gt; &amp;amp;&amp;#39;static str;

    /// Creates a new state instance. Different states aggregate independently.
    fn create_state(&amp;amp;self) -&amp;gt; Box&amp;lt;dyn AggrFunctionState&amp;gt;;
}

// NOTE: AggrFunctionState 是 AggrFunctionStateUpdatePartial 的 super trait
pub trait AggrFunctionState:
    std::fmt::Debug
    + Send
    + &amp;#39;static
    + AggrFunctionStateUpdatePartial&amp;lt;Int&amp;gt;
    + AggrFunctionStateUpdatePartial&amp;lt;Real&amp;gt;
    + AggrFunctionStateUpdatePartial&amp;lt;Decimal&amp;gt;
    + AggrFunctionStateUpdatePartial&amp;lt;Bytes&amp;gt;
    + AggrFunctionStateUpdatePartial&amp;lt;DateTime&amp;gt;
    + AggrFunctionStateUpdatePartial&amp;lt;Duration&amp;gt;
    + AggrFunctionStateUpdatePartial&amp;lt;Json&amp;gt;
{
    fn push_result(&amp;amp;self, ctx: &amp;amp;mut EvalContext, target: &amp;amp;mut [VectorValue]) -&amp;gt; Result&amp;lt;()&amp;gt;;
}
pub trait AggrFunctionStateUpdatePartial&amp;lt;T: Evaluable&amp;gt; {
    fn update(&amp;amp;mut self, ctx: &amp;amp;mut EvalContext, value: &amp;amp;Option&amp;lt;T&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt;;

    fn update_repeat(
        &amp;amp;mut self,
        ctx: &amp;amp;mut EvalContext,
        value: &amp;amp;Option&amp;lt;T&amp;gt;,
        repeat_times: usize,
    ) -&amp;gt; Result&amp;lt;()&amp;gt;;

    fn update_vector(
        &amp;amp;mut self,
        ctx: &amp;amp;mut EvalContext,
        physical_values: &amp;amp;[Option&amp;lt;T&amp;gt;],
        logical_rows: &amp;amp;[usize],
    ) -&amp;gt; Result&amp;lt;()&amp;gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;聚合函数的求值过程分为三个步骤：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;创建并初始化状态，这一过程一般是由调用者调用：&lt;code&gt;create_state&lt;/code&gt; 实现的。&lt;/li&gt;&lt;li&gt;然后在不断遍历行/向量的过程中，我们会将行的内容传入 &lt;code&gt;update/update_repeat/update_vector&lt;/code&gt; 函数(具体调用那种取决于不同的聚合函数实现)，更新内部的状态，比如遇到一个非空行，&lt;code&gt;COUNT()&lt;/code&gt; 就会给自己内部计数器+1。&lt;/li&gt;&lt;li&gt;当遍历结束之后，聚合函数就会将自己的状态通过 push_result(), 写入到一个列数组里边，这里之所以是列数组是因为聚合函数可能有多个输出列，比如 AVG()，在分布式的场景，我们需要返回两列：&lt;code&gt;SUM&lt;/code&gt; 和 &lt;code&gt;COUNT&lt;/code&gt;。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;这个 &lt;code&gt;trait&lt;/code&gt; 可以通过 &lt;code&gt;#[derive(AggrFuntion)]&lt;/code&gt; 自动推导出实现，并且可以通过过程宏 &lt;code&gt;#[aggr_funtion(state = FooState::new())]&lt;/code&gt; 来指定 &lt;code&gt;create_state&lt;/code&gt; 创建出来的 &lt;code&gt;State&lt;/code&gt; 类型。举个例子，&lt;code&gt;COUNT&lt;/code&gt; 的实现：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;/// The COUNT aggregate function.
#[derive(Debug, AggrFunction)]
#[aggr_function(state = AggrFnStateCount::new())]
pub struct AggrFnCount;

/// The state of the COUNT aggregate function.
#[derive(Debug)]
pub struct AggrFnStateCount {
    count: usize,
}

impl AggrFnStateCount {
    pub fn new() -&amp;gt; Self {
        Self { count: 0 }
    }
}

impl AggrFunctionStateUpdatePartial for AggrFnStateCount { /* .. */ }
impl AggrFunctionState for AggrFnStateCount { /* .. */ }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个时候，调用 &lt;code&gt;create_state()&lt;/code&gt; 的时候就会将内部状态 Box 起来然后返回。&lt;/p&gt;&lt;h3&gt;如何根据 &lt;code&gt;group by&lt;/code&gt; column 分组并聚合&lt;/h3&gt;&lt;p&gt;&lt;code&gt;BatchFastHashAggregationExecutor&lt;/code&gt; 内部会有一个 &lt;code&gt;Groups&lt;/code&gt; 的结构，其核心是一个 &lt;code&gt;HashTable&lt;/code&gt;，根据 &lt;code&gt;group by&lt;/code&gt; 表达式具体的类型作为 &lt;code&gt;key&lt;/code&gt; 的类型，而 &lt;code&gt;value&lt;/code&gt; 的值则是一个 &lt;code&gt;AggrFunctionState&lt;/code&gt; 数组中该组对应的聚合函数状态集合的开始下标。举个例子：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-58cb3d5d6e4ce20f68169eafd34332eb_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;691&quot; data-rawheight=&quot;449&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;691&quot; data-original=&quot;https://pic4.zhimg.com/v2-58cb3d5d6e4ce20f68169eafd34332eb_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-58cb3d5d6e4ce20f68169eafd34332eb_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;691&quot; data-rawheight=&quot;449&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;691&quot; data-original=&quot;https://pic4.zhimg.com/v2-58cb3d5d6e4ce20f68169eafd34332eb_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-58cb3d5d6e4ce20f68169eafd34332eb_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;Hash&lt;/code&gt; 值一样的行会被分配到同一个组中，每组会有若干个状态，聚合的过程其实就是根据每行的 &lt;code&gt;group by&lt;/code&gt; column 找到其对应的分组 (HashTable::get)，然后对组内的每一个状态，根据该行的内容进行更新。最后遍历每个组，将他们的状态写入到列数组即可。&lt;/p&gt;&lt;h3&gt;将两个过程结合起来&lt;/h3&gt;&lt;p&gt;上边两节讨论了聚合函数如何计算，如何分组以及如何对每个组做聚合的基本过程。现在我们通过代码，来探讨一下其中的具体细节。&lt;/p&gt;&lt;p&gt;先来看看 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/96c3f978f655148b1703a520cb9b2e9001dd256d/components/tidb_query/src/batch/executors/fast_hash_aggr_executor.rs%23L34&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;BatchFastHashAggregationExecutor&lt;/a&gt;&lt;/code&gt; 的定义:&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct BatchFastHashAggregationExecutor&amp;lt;Src: BatchExecutor&amp;gt;(
    AggregationExecutor&amp;lt;Src, FastHashAggregationImpl&amp;gt;,
);&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们发现，这个和 &lt;code&gt;BatchTableScanExecutor&lt;/code&gt; 的定义十分相似，区别每个聚合算子行为的是 &lt;code&gt;AggregationExecutor&lt;/code&gt; 里边实现了 &lt;code&gt;AggregationExecutorImpl&lt;/code&gt; trait 的一个结构体。 我们也可以看看这个 trait 提供了哪些方法。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct AggregationExecutor&amp;lt;Src: BatchExecutor, I: AggregationExecutorImpl&amp;lt;Src&amp;gt;&amp;gt; {
    imp: I,
    is_ended: bool,
    entities: Entities&amp;lt;Src&amp;gt;,
}

pub trait AggregationExecutorImpl&amp;lt;Src: BatchExecutor&amp;gt;: Send {
    // 根据 `group by` columns 和 聚合函数初始化 `entities` 中的 `schema`
    fn prepare_entities(&amp;amp;mut self, entities: &amp;amp;mut Entities&amp;lt;Src&amp;gt;);

    // 根据下层算子扫上来的数据做聚合和分组
    fn process_batch_input(
        &amp;amp;mut self,
        entities: &amp;amp;mut Entities&amp;lt;Src&amp;gt;,
        input_physical_columns: LazyBatchColumnVec,
        input_logical_rows: &amp;amp;[usize],
    ) -&amp;gt; Result&amp;lt;()&amp;gt;;

    // 将每个聚合函数的状态更新到列数组中，即写入聚合结果
    // 这里返回的是 `group by` column，在分布式场景如果不把 `group by` column 返回，`TiDB` 没有办法根据分组做二次聚合。
    fn iterate_available_groups(
        &amp;amp;mut self,
        entities: &amp;amp;mut Entities&amp;lt;Src&amp;gt;,
        src_is_drained: bool,
        iteratee: impl FnMut(&amp;amp;mut Entities&amp;lt;Src&amp;gt;, &amp;amp;[Box&amp;lt;dyn AggrFunctionState&amp;gt;]) -&amp;gt; Result&amp;lt;()&amp;gt;,
    ) -&amp;gt; Result&amp;lt;Vec&amp;lt;LazyBatchColumn&amp;gt;&amp;gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上边代码中的 &lt;code&gt;Entities&lt;/code&gt; 是记录源算子已经聚合函数元信息的一个结构体：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct Entities&amp;lt;Src: BatchExecutor&amp;gt; {
    pub src: Src,
    
    // ...

    // 聚合后产生的 `schmea`， 包含 `group_by` columns
    pub schema: Vec&amp;lt;FieldType&amp;gt;,

    /// 聚合函数的集合
    pub each_aggr_fn: Vec&amp;lt;Box&amp;lt;dyn AggrFunction&amp;gt;&amp;gt;,

    /// 每个聚合函数输出的列大小，`COUNT` 是 1，`AVG` 是 2
    pub each_aggr_cardinality: Vec&amp;lt;usize&amp;gt;,

    /// 聚合函数里边的表达式
    pub each_aggr_exprs: Vec&amp;lt;RpnExpression&amp;gt;,

    // 每个聚合表达式输出的类型的集合
    pub all_result_column_types: Vec&amp;lt;EvalType&amp;gt;,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;首先，为了观察到 &lt;code&gt;BatchFastHashAggregationExecutor&lt;/code&gt; 我们需要追踪他的 &lt;code&gt;next_batch()&lt;/code&gt; 的实现，在这里也就是： &lt;code&gt;AggregationExecutor::handle_next_batch&lt;/code&gt;：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn handle_next_batch(&amp;amp;mut self) -&amp;gt; Result&amp;lt;(Option&amp;lt;LazyBatchColumnVec&amp;gt;, bool)&amp;gt; {
        // 从下层算子取回一个 `batch`
        let src_result = self
            .entities
            .src
            .next_batch(crate::batch:🏃:BATCH_MAX_SIZE);

        self.entities.context.warnings = src_result.warnings;

        let src_is_drained = src_result.is_drained?;

        // 如果下层返回的数据不为空，将根据每行的结果分组并聚合
        if !src_result.logical_rows.is_empty() {
            self.imp.process_batch_input(
                &amp;amp;mut self.entities,
                src_result.physical_columns,
                &amp;amp;src_result.logical_rows,
            )?;
        }

        // 在 `FastHashAggr` 中，只有下层算子没有办法再返回数据的时候，才能认为聚合已经完成，
        // 否则我们返回一个空数据给上层算子，等待下一次 `next_batch` 被调用。
        let result = if src_is_drained {
            Some(self.aggregate_partial_results(src_is_drained)?)
        } else {
            None
        };
        Ok((result, src_is_drained))
    }&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;具体到 &lt;code&gt;FastHashAggr&lt;/code&gt; 中，&lt;code&gt;process_batch_input&lt;/code&gt; 就是分组并更新每组的状态。&lt;code&gt;aggregate_partial_results&lt;/code&gt; 就是写入最终的状态到列数组中。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;本文简略的介绍了 TiKV 查询引擎的实现原理和几个简单算子的实现，如果大家对其他算子也感兴趣的话，可以到 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/tree/983c626b069f2a2314d0a47009ca74033b346069/components/tidb_query/src/batch/executors&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tikv/components/tidb_query/src/batch/executors&lt;/a&gt; 下边找到对应的实现，本文中出现的代码都经过一定删减，欢迎大家阅读 TiKV 的源码获取更多的细节。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tikv-source-code-reading-16/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章（十六）TiKV Coprocessor Executor 源码解析 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多 TiKV 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Blog-cns | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-12-12-96906129</guid>
<pubDate>Thu, 12 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>拥抱 Elasticsearch：给 TiDB 插上全文检索的翅膀</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-12-10-96514042.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/96514042&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b874880ffe6069ffd21d8c00e1cd46fb_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;推荐下知乎的晓光老师的文章。TiDB 的 AP 形态其实一直都还在不断补完的路上，这里少不了社区各路神仙的各种神奇贡献。这里推荐下知乎大神孙晓光老师在 TiDB Hackathon 2019 获奖作品 TiSearch 的介绍。&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;孙晓光，知乎技术平台负责人，与薛宁（@Inke）、黄梦龙（@PingCAP）、冯博（@知乎）组队参加了 TiDB Hackathon 2019，他们的项目 TiSearch 获得了 CTO 特别奖。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;“搜索”是大家在使用各种 APP 中非常重要的一个行为，对于知乎这样以海量优质内容为特色的产品来说，借助搜索帮助用户准确、快速地触达想要寻找的内容更是至关重要。而“全文检索”则是隐藏在简单的搜索框背后不可或缺的一项基本能力。&lt;br/&gt;&lt;br/&gt;当前我们正逐步将越来越多的业务数据向 TiDB 迁移，目前在 TiDB 上我们只能使用 SQL Like 对内容进行简单的检索。但即便不考虑性能问题，SQL Like 仍然无法实现一些在搜索场景下常见的信息检索需求，例如下图所示的几种场景，单纯使用 Like 会导致查询到有歧义的结果或满足搜索条件的结果无法返回。&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3f79cd814896271be7784cb561953565_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;471&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-3f79cd814896271be7784cb561953565_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3f79cd814896271be7784cb561953565_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;471&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-3f79cd814896271be7784cb561953565_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-3f79cd814896271be7784cb561953565_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;当前 TiDB 全文检索能力的缺失，使得我们依旧需要使用传统的方式将数据同步到搜索引擎，在过程中需要根据业务特点做大量繁琐的数据流水线工作维护业务数据的全文索引。为了减少这样的重复劳动，在今年 &lt;u&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247490046%26idx%3D1%26sn%3D962bb8aa4619c3815fcc561ed96331d7%26chksm%3Deb163e94dc61b7826b7e73a057f4c9823261c1a79005104dd41dbd6ef4276c01bd6e41a69d14%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Hackathon&lt;/a&gt;&lt;/u&gt; 中我们尝试为 TiDB 引入“全文检索”功能，为存储在 TiDB 中的文本数据提供随时随地搜索的能力。以下是最终的效果展示：&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4446cea979a0fe1eb26efe31ee1c8aeb_b.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;639&quot; data-rawheight=&quot;380&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-4446cea979a0fe1eb26efe31ee1c8aeb_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;639&quot; data-original=&quot;https://pic4.zhimg.com/v2-4446cea979a0fe1eb26efe31ee1c8aeb_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4446cea979a0fe1eb26efe31ee1c8aeb_b.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;639&quot; data-rawheight=&quot;380&quot; data-thumbnail=&quot;https://pic4.zhimg.com/v2-4446cea979a0fe1eb26efe31ee1c8aeb_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;639&quot; data-original=&quot;https://pic4.zhimg.com/v2-4446cea979a0fe1eb26efe31ee1c8aeb_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4446cea979a0fe1eb26efe31ee1c8aeb_b.gif&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;b&gt;方案设计&lt;/b&gt;&lt;br/&gt;要在短短一天的 Hackathon 时间内让 TiDB 中支持全文检索，难度还是非常大的，于是在最开始的时候，我们就选择了一条非常稳妥的设计方案 - 采用整合 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.elastic.co/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Elasticsearch&lt;/a&gt;（后续简称 ES） 的方式为 TiDB 扩展全文检索能力。&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7c21a68c4e684244ee7e1ac743653262_b.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;639&quot; data-rawheight=&quot;310&quot; data-thumbnail=&quot;https://pic3.zhimg.com/v2-7c21a68c4e684244ee7e1ac743653262_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;639&quot; data-original=&quot;https://pic3.zhimg.com/v2-7c21a68c4e684244ee7e1ac743653262_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7c21a68c4e684244ee7e1ac743653262_b.gif&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;639&quot; data-rawheight=&quot;310&quot; data-thumbnail=&quot;https://pic3.zhimg.com/v2-7c21a68c4e684244ee7e1ac743653262_b.jpg&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;639&quot; data-original=&quot;https://pic3.zhimg.com/v2-7c21a68c4e684244ee7e1ac743653262_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-7c21a68c4e684244ee7e1ac743653262_b.gif&quot;/&gt;&lt;/figure&gt;&lt;p&gt;为什么选择 ES？一方面我们可以充分利用 ES 成熟的生态直接获得中文分词和 query 理解能力。另外生态融合所带来的强强联合效应，也符合 TiDB 崇尚社区合作的价值观。考虑到工作量，对于全文索引的数据同步方案我们没有采用 TiKV &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/issues/2475&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Raft Learner&lt;/a&gt; 机制，也没有使用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog&lt;/a&gt; 的方式进行同步，而是采用了最保守的双写机制直接在 TiDB 的写入流程中增加了全文索引更新的流程。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-20928a5a4d50deff0a204f376439c78a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;602&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;938&quot; data-original=&quot;https://pic3.zhimg.com/v2-20928a5a4d50deff0a204f376439c78a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-20928a5a4d50deff0a204f376439c78a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;602&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;938&quot; data-original=&quot;https://pic3.zhimg.com/v2-20928a5a4d50deff0a204f376439c78a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-20928a5a4d50deff0a204f376439c78a_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;架构如上图所示，TiDB 作为 ES 和 TiKV 之间的桥梁，所有同 ES 的交互操作都嵌入在 TiDB 内部直接完成。在 TiDB 内部，我们将表额外增加了支持 FULLTEXT 索引的元数据记录，并且在 ES 上面创建了对应的索引和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.elastic.co/cn/blog/found-elasticsearch-mapping-introduction&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Mapping&lt;/a&gt;，对于 FULLTEXT 索引中的每一个文本列，我们都将它添加到 Mapping 中并指定好需要的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.elastic.co/cn/blog/found-text-analysis-part-1&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Analyzer&lt;/a&gt;，这样就可以在索引上对这些文本列进行全文检索了。在 ES 的索引的帮助下，我们只需要在写入数据或者对数据进行更新的时候在 ES 的索引上进行对应的更新操作，就保持 TiDB 和 ES 数据的同步。而对于查询，现在流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;TiDB 解析用户发送的 Query。&lt;/li&gt;&lt;li&gt;如果发现该 Query 带有全文检索的 hint，TiDB 则会将请求发给 ES，使用 ES 索引查询到记录主键。&lt;/li&gt;&lt;li&gt;TiDB 拿到所有记录主键之后，在 TiDB 内部获取实际的数据，完成最终的数据读取。&lt;/li&gt;&lt;li&gt;TiDB 将结果返回给用户。&lt;br/&gt;&lt;br/&gt;&lt;b&gt;未来规划&lt;/b&gt;&lt;br/&gt;Hackathon 短短的 24 小时，让我们验证了整合 TiDB 和 ES 的可能性，当然，我们不会满足于这套双写的方案。未来我们会参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//medium.com/%40PingCAP/delivering-real-time-analytics-and-true-htap-by-combining-columnstore-and-rowstore-1e006d3c3ef5&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiFlash&lt;/a&gt;，基于 Raft Learner 实时将数据变更同步给 ES，将 TiDB 打造成一个真正的能支持实时全文检索的 HTAP 数据库，如下图所示：&lt;/li&gt;&lt;/ol&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-eb3067e81b08d71bcad8cb3189329eb3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;602&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;938&quot; data-original=&quot;https://pic4.zhimg.com/v2-eb3067e81b08d71bcad8cb3189329eb3_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-eb3067e81b08d71bcad8cb3189329eb3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;938&quot; data-rawheight=&quot;602&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;938&quot; data-original=&quot;https://pic4.zhimg.com/v2-eb3067e81b08d71bcad8cb3189329eb3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-eb3067e81b08d71bcad8cb3189329eb3_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;使用 Raft Learner，对于写流程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 会直接将数据写给底层的 TiKV。&lt;/li&gt;&lt;li&gt;TiKV 会通过 Raft 协议将写入数据同步到 ES Learner 节点，通过该 Learner 节点写入到 ES。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于读流程：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 解析到用户发过来的 Query 带有全文检索的 hint。&lt;/li&gt;&lt;li&gt;TiDB 将请求发给 ES Learner 节点。&lt;/li&gt;&lt;li&gt;ES Learner 节点首先通过 Raft 协议来确保节点上面有了最新的数据，并且最新的数据已经写入到 ES。&lt;/li&gt;&lt;li&gt;ES Learner 节点通过 ES 的索引读取到对应的记录主键，返回给 TiDB。&lt;/li&gt;&lt;li&gt;TiDB 使用记录主键获取到完整的数据，并返回给客户端。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;可以看到，相比于之前让 TiDB 双写到 ES 和 TiKV 的方案，在写入上面，TiDB 并不需要跟 ES 进行交互，而在读取方面，通过 Raft 协议，TiDB 也能保证从 ES 读取到最新的数据，保证了数据的一致性。当然，要实现上面的功能，我们也需要更多的帮助，我们希望能够跟社区小伙伴一起，一起完成这个非常酷的特性。&lt;br/&gt;&lt;br/&gt;&lt;b&gt;写在最后&lt;/b&gt;&lt;br/&gt;得益于个人在知乎搜索团队的短暂经历，对搜索的价值和业务接入搜索的工作量有过很直观的感受。在越来越多的数据存在于 TiDB 的时代，随时可以对业务数据的某些字段进行全文检索的价值很大。这个价值不但体现在能够实现以往 SQL 难以做好的一些事情，更大的意义是将全文检索的能力以接近 free 的方式提供给业务方，给用户搭建起一座连接关系型数据库与搜索引擎的桥梁，做到随时写入，随时搜索。如果你也有这方面的想法，欢迎邮件联系我（sunxiaoguang@zhihu.com）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>Xiaoyu Ma</author>
<guid isPermaLink="false">2019-12-10-96514042</guid>
<pubDate>Tue, 10 Dec 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
