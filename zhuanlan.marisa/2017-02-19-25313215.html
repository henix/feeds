<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>(翻译) 从编程语言的角度看深度学习</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/25313215">原文</a></p>
<div class="title-image"><img src="https://pic1.zhimg.com/v2-73da84b6dab5197a3a87faaa69ffab4c_r.png" alt=""></div><p><a href="https://zhuanlan.zhihu.com/p/24941985" data-editable="true" data-title="原文传送门">原文传送门</a></p><p>题图与内容无关，而且是很早的图了</p><p>看到翻译二字很多人会认为是 En -&gt; Ch ，其实本文是 Scala -&gt; Java &amp;&amp; Scala -&gt; Kotlin 。</p><p>说白了就是把原文的代码翻译了一下，方便那些不会 Scala 的同学们。。。呃。。。</p><p>由于 Jawa 没有 Lambda 对象这概念，我就使用实现单方法接口的匿名内部类的方式来代替 Lambda 对象了。</p><p>下文是学姐写的，也就是说下文中的“我写的”“我做的”这种话都是指学姐啦。</p><h3>正文</h3><p>想必大家都听说过<a href="https://en.wikipedia.org/wiki/Artificial_neural_network" data-editable="true" data-title="Artificial Neural Network">Artificial Neural Network</a>（如果没有，快去看）， 但是具体来说，什么是反向传播呢？ 反传背后的 Intuition 又是啥？我会试着从编程语言的角度，说明深度学习的各种东西，并且指出从编程语言看待深度学习的潜在好处。 同时，我也会介绍一下我从这角度写的深度学习框架 <a href="https://github.com/ThoughtWorksInc/DeepDarkFantasy" data-editable="true" data-title="DeepDarkFantasy">DeepDarkFantasy</a> ，以展示编程语言里面的各种东西可以如何应用上神经网络框架。</p><p>什么是神经网络？</p><p>一言蔽之，一个神经网络就是一个带有一定未知参数的程序！这些未知参数，是一个神经网络的权重。正向传播，就是运行这个程序的过程。深度学习，就是寻找这些参数。</p><p>比如说，假设我们有如下的神经网络（并假设用的激活函数为Relu（为了方便描述，这是个很小的网络））：</p><p>输入0 —–输出0</p><p>输入1 ——-/\</p><p>这个神经网络，如果转为 Java 代码，就是</p><br><code lang="java">public static Function&lt;Double, Function&lt;Double, Function&lt;Double, Double&gt;&gt;&gt; nn(final Double in0) {
	return (final Double in1) -&gt; (final Double w0) -&gt; (final Double w1) -&gt; Math.max(in0 * w0, in1 * w1);
}
</code><p>Java 的 Lambda 对象调用只能这么写：</p><br><code lang="java">public static void main(String[] args) {
	nn(0.1).apply(0.1).apply(0.1).apply(0.1);
}
</code><p>如果转为 Kotlin 代码，就是</p><br><code lang="kotlin">fun nn(in0: Double) = { in1: Double -&gt; { w0: Double -&gt; { w1: Double -&gt; Math.max(in0 * w0, in1 * w1) } } }
</code><p>调用和原文中的 Scala 一样所以不写了。</p><h3>如何训练神经网络？</h3><p>怎么找出最优的未知参数呢？</p><p>首先，什么叫‘最优’的参数呢？我们可以引入一个函数：指标。 指标接受权重，用这些权重运行神经网络，最后得出一个分数。最优的未知参数最小化/最大化这个指标。这个指标叫做 Loss Function。</p><p>Java 并没有类型别名。所以有：</p><br><code lang="kotlin">typealias Weight = Double
//假设只有一个Weight，为Double
typealias LossFunction = (Weight) -&gt; Double
//LossFunction是一切接受权重，返回Double的函数
</code><p>很常见的 Loss Function，有 Mean Square Error：</p><p>对于一个多维度输出跟预期输出的每一维度，平方(该维度输出-该维度预期输出)，并把结果累加，最后除以维数。如果只有一个维度，这就等于平方(输出-预期输出)。很明显，这是一个scala函数。</p><br><code lang="kotlin">fun MSE(l: List&lt;Weight&gt;) = { r: List&lt;Weight&gt; -&gt; l.zip(r).map { p -&gt; p.first * p.second }.sum() / l.size }
</code><p>这并不是上面定义的 LossFunction 形式。但是，如果我们有一个确定的数据集（ List[(Input, Output)] ），就可以写一个 wrapper ： 输入权重，对于给定的数据集，对每一个数据点（一个输入，跟一个输出），用给定的权重，跟该数据点的输入，运行神经网络，把结果跟该数据点的输出相减取绝对值，最后累加起来。</p><p>可以对 LossFunction 做转换，比如可以 Scale 之：</p><br><code lang="kotlin">fun Scale(d: Weight) = { lf: LossFunction -&gt; { w: Weight -&gt; d * lf(w) } }
</code><p>梯度下降/梯度上升之间的转换，就是 Scale -1 ：</p><p>更多的转换：也可以把两个 LossFunction 加起来：</p><br><code lang="kotlin">fun Plus(l: LossFunction) = { r: LossFunction -&gt; { w: Weight -&gt; l(w) * r(w) } }
</code><p>也有其他的 LossFunction ，比如 L1 L2 Regularization ，主要用于防止过大权重：</p><br><code lang="kotlin">fun L1() = { a: Weight -&gt; Math.abs(a) }
fun L2() = { a: Weight -&gt; a * a }
</code><p>如果想对一个已有的 LossFunction 加上 L2 ，很简单：我们先把 L2 scale 一个超参数，以调节 Regularization 的程度，然后跟原 LossFunction 相加。</p><p>回到正文，要最少化某个 LossFunction ，最常见的办法是，</p><ol><li>找一些随机值作为初始参数</li><li>算出在这些参数上， Loss 的导数（在多个权重下，是个 gradient ）</li><li>如果导数为正，就降低权重，如果导数为负，就增加权重（在多个权重下，为往 gradient 的反方向 update ）</li><li>循环往复 1,2 ，直到你喜欢为止。</li></ol><p>这叫梯度下降（按着某个梯度的方向降低权重），是深度学习里面很多优化算法的简化版。</p><p>反向传播，就是找出导数，然后更新的流程。</p><p>导数从那里来？</p><p>按照自动化程度排序：</p><ul><li>手动写出来（这是 PHD 存在的意义（划掉））</li><li>手动写出神经网络层的求导，然后一层层的组合起来。Caffe 就是这样的：在里面，模块就是一层神经网络，比如说有 Convolution 层（用于 CNN ），有 FullyConnected （叫 InnerProduct ）层，这些层都要框架程序员手写，但是其他人可以调用他们组合出自己的神经网络</li><li>提供一个 DSL ，并用这个 DSL 实现神经网络层，或者其他样式的神经网络架构。如果 DSL 中一切都可以求导，实现出来的神经网络层就是可以求导的。这时候，深度学习框架就变成了一个带求导功能的编程语言（因为我们不跟 Caffe /手写比较，我们以后就称一切深度学习框架为 DSL ，并且称神经网络为程序）。 Tensorflow 就是这样的</li></ul><p>细心的同学可以发现，这三者都可以视为同一件东西：我们定义一个 DSL ，手动写出 DSL 的 primitive 的求导算法，然后用这个DSL定义自己需要的东西。</p><p>在 Caffe-like 的情况下，这个 DSL 的基础操作就是层，并且可以把层组合起来（不算很灵活，不过聊胜于无），更极端的情况（全手动）下，这个 DSL 的基础操作只有一个（整个算法），并且没有任何组合方法。这样看，其实一切都是基础操作粒度大小之争（粗粒度细粒度之争）。越细的粒度，就越灵活，（不考虑性能）实现框架也就越简单。</p><p>当然，并不是越细粒度越好-现阶段，由于优化不够好，手动写得越多，效率越高-有多少人工，就有多少智能。</p><p>同时，有三种导数的表示，根据灵活性排序（ again ，不是越灵活越好，因为效率问题）：</p><ol><li>对于一个 DSL 中的程序，返回一个求导函数（不在 DSL ）中。 Caffe</li><li>对于一个 DSL 中的程序，返回一个 DSL 中的程序（这样可以后接优化，或者再次求导）。 Theano 。 DeepDarkFantasy。</li><li>DSL中有一个函数：求导。 StalinGrad 。</li></ol><p>DeepDarkFantasy 有什么特点？</p><p>尽管深度学习框架是程序语言，但是他们支持的操作并不多：有基础的四则运算，有一定的条件控制/循环，有的有 scan/变量（见 tensorflow 白皮书），然后就没啥了。 DDF 中则加入了各种编程语言的操作，在上面的基础下加入了递归，跳转（ Continuation），异常，IO，等等。</p><p>跟其他 AD 语言比起来， DDF 的特点是，是一个 Typed Extensible Embedded DSL - 这 DSL 造出的 AST 是 scala 中强类型的 term ，并且这个 DSL 可以很简单的在 scala 中扩展。同时， DDF 是 Typed 的-我们可以给出任意 type 的导数类型，也可以对任意term求导（并且有正确的type）。我们也同时（未完成）给出了一个算法是正确的证明。</p><p>DDF 的原理是？</p><p>对于任意一个term，如果要找出他的导数形式，我们只需要把所有Double换成对应的二元数。但是，转换完以后，跟一般的，得出一个函数的二元数实现不同，得出的依然是一个AST-换句话说，AD其实可以是Symbolic的。这是DDF AD的原理。</p><p>换句话说，DDF抛弃了‘导数’这个概念。在DDF中，对一个东西求导以后，不会得出他的导数，只会得出该term跟该term的导数的一个混合物。打个比方：</p><p>Either[Double, (Double, Double)] =&gt; Either[Double, Double]并没有一个所谓的‘导数’。</p><p>但是可以把导数插进上面的类型，得出</p><p>Either[(Double, Double), ((Double, Double), (Double, Double))] =&gt; Either[(Double, Double), (Double, Double)]</p><p>这是上面类型的term，但是所有Double跟Double operation都转换成二元数的term，的类型。</p><p>注：的确可以给函数，Sum Type，找出单独的类型，早期DDF也是这样做的，但是我不喜欢，放弃了。</p><p>至于如何做 Typed Extensible EDSL ，可以看<a href="https://www.cs.cornell.edu/info/projects/nuprl/PRLSeminar/PRLSeminar2011/Chung-chiehShan-FinallyTaglessPartiallyEevaluated.pdf" data-editable="true" data-title="Finally Tagless">Finally Tagless</a></p><p>至于如何表示 Lambda Abstraction ，可以看 <a href="https://zhuanlan.zhihu.com/p/22231273" data-editable="true" data-title="Compiling Combinator">Compiling Combinator</a></p><p>形式化定义：</p><p>可能这些东西都太玄乎，大家都没理解，于是我就给出一个缩小版的 DDF，DDF-min，并更严谨地定义 DDF-min ，希望能帮助学过点 Type Theory 的人理解：</p><p>DDF-min 基于 Call By Value Simply Typed Lambda Calculus，带有 Real，Sum Type, Product Type, Recursion（using Y schema）</p><p>有 with_grad_t函数，可以traverse type structure，然后把所有遇到的 Real 转换成 Real * Real。</p><p>还有 with_grad函数，可以 traverse AST，然后把类型转换成 with_grad_t</p><p>然后有个 logical relation，对于函数外的东西，都是 trivial 的定义，或者简单的 recurse 进去。</p><p>对于A -&gt; B，除了普通的‘对所有符合 logical relation 的 A，application 满足 logical relation’外，还有：如果 A -&gt; B = Real -&gt; Real，这个函数的 with_grad加点wrapper就是这个函数的Denotational Semantic的导数函数。</p><p>另：这根<a href="https://github.com/MarisaKirisame/DDFADC" data-editable="true" data-title="MarisaKirisame/DDFADC">MarisaKirisame/DDFADC</a>中描述的有一定出入。</p><p>Forward Mode AD 会不会有性能问题？</p><p>如果对AD很熟的朋友，肯定会指出一个问题：如果有N个Double输入，Forward Mode AD 就要运行 N 次。对于有着很多参数的神经网络来说，这无法忍受！</p><p>解决办法是，我们对 Dual Number 做一次 Generalization：Dual Number 并不一定是(Double, Double)，也可以是(Double, (Double, Double))。用后者，可以运行一次，算出两个导数。</p><p>比如说，给定x, y, z，并且想知道(x+y)*z对于x, z的导，</p><p>可以写出</p><br><code lang="text">((x, (1, 0)) + (y, (0, 0))) * (z, (0, 1)) = 

(x + y,(1, 0)) * (z, (0, 1)) = 

((x + y) * z, (z, x + y))
</code><p>在这里面， pair 的第0项就是表达式的值， pair 的第1项就是另一个 pair ，其中第 0,1，项分别是表达式对于x，y的导。</p><p>或者，可以用(Double, Double =&gt; Double[1000])（注：Double[1000]不是真正的 scala 代码）代替(Double, Double[1000])-这样，当整个 term 要乘以一个 literal 的时候，并不需要进入整个 Array 去算，只需要 update 该 Double 则可-这就是反向传播。</p><p>在实现中，这通过引入一个 Typeclass ， Gradient （满足的有 Unit, (Double, Double),(Double =&gt; Double[1000])等），（并限制 Gradient 一定要满足 field 的一个 variation（其实本质上还是一个 field ，只不过为了提速）），并用之于 Dual Number 之上（第二个参数不再是 Double ，而是该 Gradient ）。然后， AD 的四则运算就可以利用 Field 的操作写出。</p><p>这有什么用？</p><p>我们希望能做到<a href="https://colah.github.io/posts/2015-09-NN-Types-FP/" data-editable="true" data-title="Neural Networks, Types, and Functional Programming">Neural Networks, Types, and Functional Programming</a>里面给的例子</p><p>DDF 可以很简单的给出有递归/循环的函数的高阶导。这点 tensorflow 就不支持（ <a href="https://github.com/tensorflow/tensorflow/issues/675" data-editable="true" data-title="Gradients of non-scalars (higher rank Jacobians) · Issue #675 · tensorflow/tensorflow">Gradients of non-scalars (higher rank Jacobians) · Issue #675 · tensorflow/tensorflow</a>）。</p><p>除了写神经网络以外，我们也希望可以写任意普通的算法，程序（但是带有未知变量），然后用 DDF 自动求导，以找出最优的这些变量。</p><p>能不能给个例子？<a href="https://github.com/ThoughtWorksInc/DeepDarkFantasy/blob/master/doc/poly.md" data-editable="true" data-title="这是一个用梯度下降解 x*x+2x+3=27">这是一个用梯度下降解 x*x+2x+3=27</a> 的例子。</p><h2>没了</h2><p>翻译感言：垃圾 Jawa</p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
