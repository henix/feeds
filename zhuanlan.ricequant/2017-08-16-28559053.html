<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>PonderDNC正逆传播简析</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/28559053">原文</a></p>
<div class="title-image"><img src="https://pic3.zhimg.com/v2-d45e7058c663977201c458c390d00c3c_r.jpg" alt=""></div><p>一个标准RNN模型的正逆传播路径如下图，其中黑色表示正向传播，即根据输入信息推断输出的过程。红色表示误差逆向传播，也就是使用BPTT优化器改善权重参数的过程。</p><img src="https://pic1.zhimg.com/v2-b2095a858251939d83f3bc9dabcd5fb3_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>一个ACT包裹的RNN模型推断过程信息传播路径如下图</p><img src="https://pic2.zhimg.com/v2-fdbae0501271c2085d7d0da6c9d716c8_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>这里假设ACT包裹的RNN模型（简称PonderRNN）在时刻 <equation>t_2</equation> 循环计算了三次，将这个计算路径图展开，如下图</p><img src="https://pic1.zhimg.com/v2-ac78f886041bf5974fbc3dcd70468e76_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>PonderRNN在时刻 <equation>t_2</equation> 循环计算3次形成3个隐藏状态，然后这3个隐藏状态使用p（参考Graves论文Adaptive Computation Time for Recurrent Neural Networks ）合成标准RNN在时刻 <equation>t_2</equation> 的隐藏状态和输出。将图3修改一下，更清晰可见，这个在时刻 <equation>t</equation>  自适应的循环计算次数的架构本质就是可以自动调节计算循环次数（或者是层数）的残差网络架构。图中红色表示误差逆向传播。</p><img src="https://pic4.zhimg.com/v2-856865bd9f9fb6490a1d1914121ba330_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p><br></p><img src="https://pic3.zhimg.com/v2-3a4eae619a25561f23f689d6fb770259_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>这就引出了本文要讨论的问题，也就是PonderDNC模型计算过程中外存储器循环变量access state是否也要使用残差网络加权合成的access state状态传递给下一个时刻。比如在时刻 <equation>t_2</equation> 模型PonderDNC循环计算3次，则传递给 <equation>t_3</equation> 时刻的access state具体是PonderDNC第三次循环计算产生的access state还是三次循环计算产生的三个access state加权值。如下图，DNC在时间序列上的面的信息传播有三个模块</p><p><br></p><img src="https://pic3.zhimg.com/v2-56fe72ad32eeb64ca326b097e2c93b37_r.png" data-rawwidth="755" data-rawheight="264"><p>分别是表示短期记忆的控制器（如LSTM）产生的隐藏状态矩阵control state（control state就是上面RNN的隐藏状态）以及表示外存储器记忆矩阵的access state 和 表示外存储器读取记忆的read_vector，图中实曲线。这其中control state已经在上文进行阐述，并且read_vector也可用同以理论或者假想解释或者理解。比较复杂的是access state这个传递模块，access state模块包含表示记忆矩阵memory以及表示读写头控制5个循环张量。</p><p>首先介绍一种简单的处理方式，截断传播路径（实现见上篇博客代码链接），也就是我们将时刻 <equation>t_2</equation> 循环三次的DNC模型处理为一个重复输入循环计算的过程，可以认为是一个重复 <equation>t_2</equation> 时刻三次的过程并依照时间顺序串联的过程。如下图，截断残差网络的传播，即没有halting(p)加权处理。</p><img src="https://pic3.zhimg.com/v2-859d21ef58b6cd2e1cd60f7409b725be_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>也就是外存储器（记忆矩阵）完全按照时间顺序传递</p><img src="https://pic2.zhimg.com/v2-5d3c4ad16b24fd065241eda6b553f5d0_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>第二种方法是依旧使用这种自适应残差网络计算架构处理access state。为了更好的阐述，这里引入一个虚拟的计算单元SuperDNC，在每个时间步 <equation>t</equation> 无论PonderDNC计算单元循环计算多少次，循环计算产生的access state 与halting的加权和总是等于SuperDNC在时间步 <equation>t</equation> 计算一次产生的access state，也就是在时刻 <equation>t</equation> SuperDNC可以完全等效的替代PonderDNC的多次循环计算。通过引入这个虚拟计算单元之后，PonderDNC的计算效果就等效替代为一个标准RNN变种计算产生的效果。如假设PonderDNC每个时间步 <equation>t</equation> 循环计算3次，一共计算是3t次，则SuperDNC只计算t次。如下图，这个虚拟计算单元输入输出值均可以拆解为PonderDNC计算单元多次循环计算输入输出的加权。</p><img src="https://pic3.zhimg.com/v2-04fd5ba79161df81a3cd849a0769b301_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>为了更好的理解这个过程，对SuperDNC的access state简单拆解，显式表达SuperDNC的access state与PonderDNC循环计算产生的多个access state加权和相等的关系。</p><p>首先拆解最核心的记忆矩阵传递问题。SuperDNC在时刻 <equation>t_2</equation> 接受 <equation>M_1</equation> 计算一次产生 <equation>M_2</equation> 。其中 <equation>M_2</equation> 等同于PonderDNC在 <equation>t_2</equation> 循环计算三次产生三个记忆矩阵 <equation>M_{21}, M_{22}, M_{23}</equation> 的加权和，即 <equation>M_2 = p_1 * M_{21} + p_2 * M_{22} + p_3 * M_{23}</equation> 。如下图，假设memory是一个5行一列的外存储矩阵，则在合成的外存储矩阵中，如果在 <equation>t_2</equation> 时刻循环计算的步骤中没有改变，则合成记忆矩阵依然不会改变，如果改变了取值，则合成矩阵为 <equation>t_2</equation> 时刻改变量的加权值。虽然这里比较费解，但是并不矛盾，因为memory严格对应控制器也就是RNN（LSTM）的隐藏状态，如隐藏状态是加权值，则可以拆解memory形成加权值。可以假想，PonderDNC在时刻 <equation>t_2</equation> 写了三个外存储记忆矩阵，然后再根据思考结果将这个记忆矩阵合成了一个新记忆矩阵传递到下一个时刻开始。</p><img src="https://pic2.zhimg.com/v2-19d21f91ef7088b92c529b1f1b02b2d4_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>关于外存储器（记忆矩阵）的拆解显示，虚拟计算单元的记忆矩阵可以拆解为DNC在时刻 <equation>t</equation> 多次循环计算输出的加权。</p><p>然而需要注意access state 模块不同于control state模块，该模块传递张量均为不可训练变量。所以这里的加权并不会影响access state的取值，这里时刻 <equation>t</equation> 的输出使用循环计算产生的多个输出的加权替代第一种方法取循环计算最后一个access state的方法并不会直接影响access state取值。这里主要影响的是控制器(如LSTM)权重参数的误差梯度计算，因为在标准DNC模型的梯度优化过程，梯度传播包括记忆矩阵和读写头控制等外存储器模块。注：DNC计算单元中只有控制器(LSTM)的权重参数可训练。</p><p>access state 包含5个模块，分别是记忆矩阵memory， 读头控制read_weighting， 写头控制 write_weighting，记忆矩阵位置相对使用程度usage，记忆矩阵位置时间写入顺序链表link，以及生成link的表示记忆矩阵每个存储位置写入程度的precedence_weighting（最后一次写入时间距离选择的远近程度值）。</p><p>由于读写头控制是严格对应相应的memory，这里可以将 <equation>t_2</equation> 时刻SuperDNC从外存储器读取向量的过程拆解为PonderDNC三次循环读向量 <equation>r_{21}, r_{22}, r_{23}</equation> 的加权。如下图的读头控制读取外记忆矩阵过程，外记忆矩阵、读头控制、读头向量循环计算产生产生输出加权就合成了SuperDNC在 <equation>t_2</equation> 的输出。</p><img src="https://pic2.zhimg.com/v2-d351dddecb7c6192872cf916c86778be_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>同样的写头控制也可以压缩为一个加权写头控制，其中 <equation>e</equation>  和 <equation>v</equation>  表示对记忆矩阵的擦除向量和写入向量，这两个值均有控制器（LSTM）在循环计算中即时生成。</p><img src="https://pic4.zhimg.com/v2-caca2c7c89dcd743ae0c425ae86a37d9_r.jpg" data-rawwidth="5120" data-rawheight="3840"><p>对usage 、link和precedence_weighting在PonderDNC循环计算输出的加权值同样也可以假想为SuperDNC的输出，或者SuperDNC的access state传递模块中的usage 、link和precedence_weighting也可以拆解为PonderDNC循环计算输出的加权。 这是因为表示记忆矩阵各个位置相对使用程度和先后写入顺序的usage和link在PonderDNC多次循环的加权值可以看作或者是加权等同于SuperDNC的usage和link。</p><p>注：可以假想SuperDNC是一个控制器比LSTM更强大而且具有很多个读写头的超级DNC计算单元。</p><p><a href="https://www.ricequant.com/community/topic/3856//5">代码链接</a></p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
