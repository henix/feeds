<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>贝叶斯优化（Bayesian Optimization）在量化交易参数优化中的应用</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/82013165">原文</a></p>
<p>贝叶斯优化（Bayesian Optimization）是一种离散优化的方法，它主要适用于以下场景：</p><ul><li>目标函数计算量大</li><li>目标函数无法求导</li><li>参数不多于20个</li><li>参数取值范围类似网格</li></ul><p>一般机器学习参数优化会用到，比如决策树的参数一般有：</p><ul><li>学习速度；</li><li>最大深度；</li><li>树的棵树；</li><li>叶子最小样本数目；</li></ul><p>等等，这些都是作为超参数来优化的。当然，超参数这个词是码农取的，我不明白为啥要起这个词。</p><p>传统的优化有一本圣经叫Numerical Optimization，有第二版，里面介绍非常详细，可惜都是针对连续优化的，对这类离散优化不大合适。那些梯度下降、随机梯度下降也不能用，因为目标函数木有梯度。</p><p>一般来说，解决这类离散优化问题的方法颇为“民科”，名字叫什么遗传算法、蚁群优化，看名字就觉得怪怪的。看一个东西靠不靠谱，就看它取名的词根是不是一些常用的词。</p><p>比如这个“贝叶斯优化”，其实我不看内容，就看这个名字，就知道它靠谱了。为什么呢？因为这两个词都是大词，一般大词不能随便给人用，现在给它用了，自然是靠谱的。</p><p>为什么说是大词呢？就是频率比较高的词，比如我随便找几个词跟他们组合，都是有意义的。</p><p>比如我们听过linear programming, nonlinear programming，那么programming放进去呢？比如bayesian programming，其实也是存在的，人家还写了书：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-6ae2690676342b69470d70827147bd3e_b.jpg" data-caption="" data-size="normal" data-rawwidth="997" data-rawheight="439" class="origin_image zh-lightbox-thumb" width="997" data-original="https://pic3.zhimg.com/v2-6ae2690676342b69470d70827147bd3e_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-6ae2690676342b69470d70827147bd3e_b.jpg" data-caption="" data-size="normal" data-rawwidth="997" data-rawheight="439" class="origin_image zh-lightbox-thumb lazy" width="997" data-original="https://pic3.zhimg.com/v2-6ae2690676342b69470d70827147bd3e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-6ae2690676342b69470d70827147bd3e_b.jpg"/></figure><p>像“蚁群优化”，蚁群这个词就不能随便组了。正因为它太民科，太另类，好词不给他用，只能自己取一些稀奇古怪的词，一看名字就知道不靠谱了。</p><p>像Bayesian Statistics也是存在的，所以能用上Bayesian这样的高频词，以及Optimization这样的超高频词，它们组合起来的东西，肯定是好东西，这是基本的科学素养和直觉。</p><p>然后这玩意怎么用来优化策略参数呢？其实很简单，其实python/R都有相应的工具包，但R的包不叫Bayesian Optimization，叫DiceOptim，Deep Inside Computer Experiments的意思，名字比较怪；大概是学术界的派系之间的政治斗争，我就不用你这个词，虽然内容都一样，在学术界这种情况司空见惯，就不说了。</p><p>python是码农开发的工具，并没有参合统计跟优化界之间的纷争，在GPyOpt包里面就直接用BayesianOptimization了。R里面叫Sequential EI maximization and model re-estimation, with a number of iterations fixed in advance by the userestimation。为啥要起这么长的名字？就是可以避开使用Bayesian Optimization这个词。</p><p>可能这个开发者跟Bayesian Optimization这个名字的发明者有仇吧。比如自己开发的东西，名字没取好，结果别人改了个名字，后人只记住后面的名字，这就悲剧了。</p><p>其实这玩意还挺有用的，很多优化过程自动完成。经典论文是这篇：</p><p>Efficient Global Optimization of Expensive Black-Box Functions</p><p>Donald R. Jones，Matthias Schonlau， William J. Welch</p><a href="https://link.zhihu.com/?target=https%3A//link.springer.com/journal/10898" data-draft-node="block" data-draft-type="link-card" data-image="https://pic4.zhimg.com/v2-c5d01c07e2fa75bfb97b568069766333_120x160.jpg" data-image-width="153" data-image-height="233" class=" wrap external" target="_blank" rel="nofollow noreferrer">Journal of Global Optimization</a><p>这个期刊其实我没听过，可能专门研究优化的人知道吧。网上评价“杂志级别还可以，但是相对来说，比较冷门，关注人数偏少，有些可能是国内不太熟悉，但该杂志在国际仍然有相当知晓度的。因为缺少中国人投稿，稿源可能未必丰富，发表有可能有很大的机会哦。”</p><p>貌似有人弄了并行的算法：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.05149" class=" wrap external" target="_blank" rel="nofollow noreferrer">Parallel Bayesian Global Optimization of Expensive Functions</a>貌似有人弄了并行的算法：</p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.05149" data-draft-node="block" data-draft-type="link-card" class=" wrap external" target="_blank" rel="nofollow noreferrer">Parallel Bayesian Global Optimization of Expensive Functions</a><p>从量化交易的角度看，会调参就行，那些数学推导比较复杂，看了头晕，就算了。</p><p>当然了，从牛逼的角度排序，最牛逼的是证明定理推导公式的，其次是实现开发这些的，最次是调用这些包调参的，貌似量化是调参为主，处于技术层面的低端，不过也没必要重复造轮子嘛。最牛逼的公司可能也会自己造轮子，但绝大多数的人都没到那份上，就洗洗睡吧。</p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
