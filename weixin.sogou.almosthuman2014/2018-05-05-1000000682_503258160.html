<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CVPR 2018 | 密歇根大学&amp;谷歌提出TAL-Net：将Faster R-CNN泛化至视频动作定位中</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1526347052&amp;src=3&amp;ver=1&amp;signature=AUasDgOUyZWhqTjLi2jJaNXqFXz7umuLaP131fYdrTnZFkz97R8QiAy57DobHMZXrBx2TGlEe8CQONWzSm7lmbncJTaAQ3KQxkp166G**T5sOO3-PfUGiW28CzWjytxjj12cpq62*RZFwNh5fqkL69TJdCgXV*iClFFz0tnJwM4=">原文</a></p>
<div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        <div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    CVPR 2018 | 密歇根大学&amp;谷歌提出TAL-Net：将Faster R-CNN泛化至视频动作定位中                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-05-05</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;caret-color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="max-width: 100%;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">Yu-Wei Chao等</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：Geek AI、路<br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(136, 136, 136);">近日，密歇根大学和谷歌研究院的一项研究提出了时序动作定位网络 TAL-Net，该网络将之前常用于图像目标检测的 Faster R-CNN 网络应用于视频时序动作定位中。在 THUMOS'14 检测基准上，TAL-Net 在动作提名（action proposal）和定位上都取得了目前最好的性能，并且在 ActivityNet 数据集上取得了具有竞争力的性能。目前，该论文已被 CVPR 2018 大会接收。</span></p></blockquote><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">对人类动作的视觉理解是构建辅助人工智能系统所需的核心能力。在传统的研究中，这个问题通常在动作分类的范畴内被研究 [46, 37, 30]，其目标是对一个按照时序剪辑的视频片段进行强制选择（forced-choice）分类，分类为若干动作类型中的一类。尽管人们在此领域的研究取得了累累硕果，但这种分类的设定是不现实的，因为现实世界中的视频通常是没有剪辑过的，而且我们感兴趣的行为通常也内嵌在与其不相关的活动背景中。最近的研究关注点已经逐渐向未剪辑视频中的时序动作定位转移 [24, 32, 47]，其任务不仅仅是识别动作的类别，还需要检测每个动作实例的开始和结束时间。时序动作定位的改进可以推动大量重要课题的发展，从提取体育运动视频中的精彩片段这样的即时应用，到更高级的任务，如自动视频字幕。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">时序动作定位，和目标检测一样，都属于视觉检测问题的范畴。然而，目标检测旨在生成物体在二维图像中的空间边界框，时序动作定位则是要在一维的帧序列中生成时序片段。因此，许多动作定位的方法从目标检测技术的进展中得到启发。一个成功的例子是：基于区域的检测器的使用 [18, 17, 33]。这些方法首先从完整的图像中生成一个与类别无关的候选区域的集合，然后遍历这些候选区域，对其进行目标分类。要想检测动作，我们可以遵循这一范式，先从整个视频中生成候选片段，然后对每个候选片段进行分类。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在基于区域的检测器中，Faster R-CNN [33] 由于其在公开的对比基准上极具竞争力的检测精度，被广泛应用于目标检测。Faster R-CNN 的核心思想是利用深度神经网络（DNN）的巨大容量推动候选区域生成和目标检测这两个过程。考虑到它在图像目标检测方面的成功，将 Faster R-CNN 用到视频时序动作定位也引起了研究者极大的兴趣。然而，这种领域的转变也带来了一系列挑战。本论文作者回顾了 Faster R-CNN 在动作定位领域存在的问题，并重新设计了网络架构，来具体地解决问题。研究者重点关注以下几个方面：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">1. 如何处理动作持续时间的巨大差异？与图像中物体的大小相比，动作的时间范围差别很大——从零点几秒到几分钟不等。但是，Faster R-CNN 根据共享的特征表示对不同规模的候选片段（即 anchor）进行评估，由于特征的时间范围（即感受野）和 anchor 跨度在对齐时存在偏差，因此 Faster R-CNN 可能无法捕获相关的信息。研究者提出使用 multi-tower 网络和扩张时序卷积（dilated temporal convolution）来执行此类对齐工作。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">2. 如何利用时序上下文（temporal context）？动作实例之前和之后的时刻包含用于定位和分类的关键信息（可能比目标检测中的空间上下文更重要）。直接简单地将 Faster R-CNN 应用于时序动作定位可能无法利用时序上下文。研究者提出通过扩展生成候选片段和动作分类的感受野来显性地对时序上下文进行编码。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">3. 如何最好地融合多流特征？当前最优的动作分类结果主要是通过融合 RGB 和基于光流的特征得到的。然而，探索将这样的特征融合应用到 Faster R-CNN 上的研究还十分有限。研究者提出了一个晚融合（late fusion，在分类结果上融合）方案，并且通过实验证明了它相对于常见的早融合（early fusion，在特征上融合）的优势。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">本研究的贡献有两方面：（1）介绍了时序动作定位网络（TAL-Net），一种基于 Faster R-CNN 的视频动作定位新方法；（2）在 THUMOS』14 检测基准 [22] 中，本研究提出的模型在动作提名（action proposal）和定位上都取得了目前最好的性能，并且在 ActivityNet 数据集 [5] 上取得了具有竞争力的性能。</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.3236784938450398" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8PqYht7hfBnsoZYDianOgj4Al6XUePZmqQcuBuePtdPQia3aGYVZsdd6jEZOCq70wZuQAzSh1wbnqg/640?wx_fmt=png" data-type="png" data-w="1381" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 1：用于图像目标检测的 Faster R-CNN 架构 [33]（左图）和用于视频时序动作定位的 Faster R-CNN 架构 [15, 9, 16, 51]（右图）的对比。时序动作定位可以被看作是目标检测任务的一维版本。</em></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">Faster R-CNN</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Faster R-CNN 最初的提出是为了解决目标检测问题 [33]，在给定一个输入图像时，Faster R-CNN 的目标是输出一组检测边界框，每一个边界框都带有一个目标类别标签。整个流程包括两个阶段：生成候选区域和分类。首先，输入图像经过二维卷积处理生成一个二维特征图。另一个二维卷积（即候选区域网络，Region Proposal Network）用于生成一组稀疏的类别无关的候选区域，这是通过对一组大小不同的、以特征图上的每个像素点为中心的锚点框进行分类来实现的。这些候选区域的边界也通过回归进行调整。之后，对于每个候选区域，区域内的特征首先被池化为一个固定大小的特征图（即 RoI 池化）。接着，DNN 分类器使用池化之后的特征计算目标类别的概率，同时为每个目标类别的检测边界进行回归。图 1（左）展示了完整的流程。该框架通常通过交替进行第一阶段和第二阶段的训练来完成训练工作 [33]。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Faster R-CNN 很自然地被拓展到时序动作定位领域 [15, 9, 51]。回想一下，目标检测的目的是检测二维空间区域。而在时序动作定位中，目标则是检测一维的时序片段，每个片段都以一个开始时间和一个结束时间来表示。时序动作定位因此可以被看作是目标检测的一维版本。图 1（右）展示了一个典型的 Faster RCNN 时序动作定位流程。与目标检测类似，它包含两个阶段。首先，给定一组帧序列，我们通常通过二维或者三维卷积网络提取出一个一维特征图。之后，将该特征图传输给一维卷积网络（指候选片段网络，Segment Proposal Network），在每个时间点上对一组大小不同的 anchor 片段进行分类，并且对边界进行回归。这将返回一组稀疏的类别无关的候选片段。接着，对于每个候选片段，我们计算动作类别的概率，并进一步对片段边界进行回归（修正）。在这一步，首先使用一维的 RoI 池化层（也称「SoI 池化」），接着使用 DNN 分类器来实现。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">TAL-Net</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">TAL-Net 遵循了 Faster R-CNN 的检测模式，并用于时序动作定位（图 1 右），但有三种新的架构变化。</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.308843537414966" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8PqYht7hfBnsoZYDianOgj4KicS0Vkv5u2Sibj4ADmDLHSibTKuClyfgicOryXQ4rOk7McA73wjuJnJ3w/640?wx_fmt=png" data-type="png" data-w="1470" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 2：左图：在时序动作定位中，不同规模的 anchor 共享感受野的局限性。右图：本研究提出的的候选片段网络的 multi-tower 架构。每个 anchor 大小都有一个具备对齐后的感受野的相关网络。</em></span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.6115942028985507" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8PqYht7hfBnsoZYDianOgj4oz17QZqMy3wkWqWG772Mcy3hPIubib3T5aocgJvDNfhD5PcIrdTnOGA/640?wx_fmt=png" data-type="png" data-w="690" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 3：使用扩张时序卷积控制感受野的大小 s。</em></span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.5947521865889213" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8PqYht7hfBnsoZYDianOgj4gBqV3qzh2h2gBicgicoFhcr8huJ0nqTfcPIwrruF6IJTpjwV1JDBulOg/640?wx_fmt=png" data-type="png" data-w="686" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 4：在生成候选片段中纳入上下文特征。</em></span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="1.0116788321167882" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8PqYht7hfBnsoZYDianOgj4v4slrM7c9iaek16rWAohg3wo4czTVibyELf8SmkGibcicRESdvoK2ARtBw/640?wx_fmt=png" data-type="png" data-w="685" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 5：不纳入上下文特征的候选片段分类（上图）[17, 33]，纳入上下文特征后的候选片段分类（下图）。</em></span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.8296943231441049" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8PqYht7hfBnsoZYDianOgj4dxxcqzOB5yCWypAJkPcZfLwJm1JiaGtAc1kz5icicV8P4mxu7eeG8PhfQ/640?wx_fmt=png" data-type="png" data-w="687" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 6：双流 Faster RCNN 框架的晚融合方案。</em></span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.9929971988795518" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8PqYht7hfBnsoZYDianOgj4O3IgVjtB6ibFuFyO3vRFwRkI6L63NspoBAwxeI77c38Y99fMKow94VQ/640?wx_fmt=png" data-type="png" data-w="714" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>表 5：在 THUMOS'14 上的动作定位 mAP（%）。</em></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">论文：</span></strong><span style="font-size: 15px;">Rethinking the Faster R-CNN Architecture for Temporal Action Localization</span></p><p><br></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.16594960049170251" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8PqYht7hfBnsoZYDianOgj4bJJybSeuNM5s2OJjCSH7pz7aDU6BslkXnZCYTc1qbE5ic5ZDr0EUiaibg/640?wx_fmt=png" data-type="png" data-w="1627" style=""></p><p><br></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1804.07667</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">摘要：</span></strong><span style="font-size: 15px;">我们提出了 TAL-Net，一种用于视频时序动作定位的改进方法，它受到了 Faster R-CNN 目标检测框架的启发。TAL-Net 解决了现有方法存在的三个关键问题：（1）我们使用一个可适应动作持续时间剧烈变化的 multi-scale 架构来提高感受野的对齐程度；（2）通过适当扩展感受野，我们更好地利用动作的时序上下文，用于生成候选片段和动作分类；（3）我们显性地考虑了多流特征融合，并证明了动作晚融合的重要性。我们在 THUMOS'14 检测基准上取得了动作提名和定位目前最好的性能，并且在 ActivityNet 数据集上取得了很有竞争力的性能。 <img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);text-align: justify;white-space: normal;caret-color: rgb(62, 62, 62);font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></span></p><p><br></p><p><br></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);caret-color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;caret-color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;caret-color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;caret-color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="1986718093" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                        
            <ul id="js_hotspot_area" class="article_extend_area"></ul>


            
                        <div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>

        
        <div class="rich_media_area_extra">

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_friend_cmt_area" style="display:none">
              
              
              
            </div>

                        <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_area" style="display:none">
            </div>
                    </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
