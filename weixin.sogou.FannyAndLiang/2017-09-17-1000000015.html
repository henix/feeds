<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>AI学习笔记：[2]神经网络向量化和成本函数</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1550529246&amp;src=3&amp;ver=1&amp;signature=EcsW-XtZZhKzNhYY-L4UxnIkGBwVsxJPmHIWikPwhKu9KjTBUcUz3YdLfNfOi8PjsI1HuV*vF9VXMeWltRstYTjvZCuqXl-D1qco*RVLlS3kmAVpwo4ZYaaknQ5pm5S50XjH1siuVtsVQwD2l1bhwkXPrB6RBMQFSTf09k4C8m8=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    AI学习笔记：[2]神经网络向量化和成本函数                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2017-09-17</em>

                                        <em class="rich_media_meta rich_media_meta_text">Liang</em>
                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">ImFanny</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">ImFanny</span>

                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">ImFanny</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value"></span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">Hello, I'm Fanny. Nice to meet you.</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <blockquote style="box-sizing: border-box;padding: 20px;margin-bottom: 25px;font-size: 16px;border-left-width: 6px;border-left-color: rgb(180, 180, 180);background-color: rgb(247, 247, 247);line-height: 30px;color: rgb(47, 47, 47);"><p style="margin-bottom: 25px;max-width: 100%;min-height: 1em;color: rgb(47, 47, 47);font-size: 16px;white-space: normal;line-height: 1.7;word-break: break-word;box-sizing: border-box !important;word-wrap: break-word !important;">写在前面：</p><p style="margin-bottom: 25px;max-width: 100%;min-height: 1em;color: rgb(47, 47, 47);font-size: 16px;white-space: normal;line-height: 1.7;word-break: break-word;box-sizing: border-box !important;word-wrap: break-word !important;">文中涉及到很多上标、下标的显示，但遗憾的是微信后台的编辑器并不支持，想获得更好的阅读体验，请查看http://www.jianshu.com/p/22a872f52c7f</p><p style="max-width: 100%;min-height: 1em;color: rgb(47, 47, 47);font-size: 16px;white-space: normal;line-height: 1.7;word-break: break-word;box-sizing: border-box !important;word-wrap: break-word !important;">上一篇请戳：<a href="http://mp.weixin.qq.com/s?__biz=MzUxMjAxMzU4Mw==&amp;mid=2247483701&amp;idx=1&amp;sn=7398966903c6487f7a95951c770c5472&amp;chksm=f96ba555ce1c2c43b2f6aaa8a04bb7dd417aefb267d4f345dd04e2efff3ce43cdf300be6418e&amp;scene=21#wechat_redirect" target="_blank">《AI学习笔记：[1]神经元与神经网络》</a><br></p><p style="max-width: 100%;min-height: 1em;color: rgb(47, 47, 47);font-size: 16px;white-space: normal;line-height: 1.7;word-break: break-word;box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="max-width: 100%;min-height: 1em;color: rgb(47, 47, 47);font-size: 16px;white-space: normal;line-height: 1.7;word-break: break-word;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(217, 33, 66);font-size: 16px;background-color: rgb(247, 247, 247);">如果你喜欢本文，请关注本账号，预计会以一周一篇的频率进行更新~</span></p></blockquote><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">继续对吴恩达的<span style="color: rgb(0, 128, 255);">《深度学习工程师》</span>做笔记总结。</p><h2 style="box-sizing: border-box;"><span style="font-size: 24px;"><strong>向量化</strong></span></h2><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">上一篇笔记中，已经通过数学公式归纳出<span style="box-sizing: border-box;font-weight: 700;">一个样本</span>输入到神经网络中，并计算其输出<span style="box-sizing: border-box;font-weight: 700;">递归公式</span>，这一篇继续学习如何将 m 个样本整合到一个矩阵中，也就是吴恩达课程中的 <span style="box-sizing: border-box;font-weight: 700;">向量化</span> 的内容。</p><blockquote style="box-sizing: border-box;padding: 20px;margin-bottom: 25px;font-size: 16px;border-left-width: 6px;border-left-color: rgb(180, 180, 180);background-color: rgb(247, 247, 247);line-height: 30px;color: rgb(47, 47, 47);"><p style="box-sizing: border-box;line-height: 1.7;word-break: break-word !important;"><span style="box-sizing: border-box;font-weight: 700;">为什么要向量化？</span><br style="box-sizing: border-box;">主要目的是为了提升计算速度。向量化编程（或称矢量化编程）最早出现在Matlab编程语言中，因为Matlab或者Python都是解析执行的，使用for循环的效率较低，通过使用向量化编程来用编译后的向量化计算库来替代for循环的解析执行，并加上对并行计算的特性的利用，计算性能会大大提高。</p></blockquote><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">按照吴恩达的套路，一定会举个例子，以下面这个网络为例：</p><p><img class="" data-ratio="0.4815217391304348" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvGia6sktE6lMgzwk45V5tHcIovIq5Ec2hYQriaTFbvoc4ALicWmUicbo4Aw/0?wx_fmt=png" data-type="png" data-w="920" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="text-align: center;">图1：4层神经网络</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">按照之前给出的公式：</p><p><img class="" data-ratio="0.12751677852348994" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvOtnxnnVGxILYsj1bB2URiczKTCSHhiaXicf9agRPRBPSb8pxGY5Ysiav4A/0?wx_fmt=png" data-type="png" data-w="149" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p><img class="" data-ratio="0.2079207920792079" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvicR06Tny9gMxbC5a0Mp2KTuL5RwW3nBCicok6RDMg28oHZ7kLNF7PkYg/0?wx_fmt=png" data-type="png" data-w="101" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">这里稍微有点变化，用 g<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>(z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>) 代替了 σ(z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>)，因为每一个隐层的激活函数可能都不一样。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">对于一个样本：</p><p><img class="" data-ratio="0.7127659574468085" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvbOTcex1uwk1J27ILT5jFtNLPmRt1mNrlPhJIiaWQegjaH5NMgnTnDSQ/0?wx_fmt=png" data-type="png" data-w="94" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">对于 m 个样本，将 (3, 1) 的列向量组合成 (3, m) 的矩阵：</p><p><img class="" data-ratio="0.17357512953367876" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvxpVlE6EUt0IyS1Abu9eySFZqTdu0cd1MZpQH62KXvCibuuA4EAPbqfg/0?wx_fmt=png" data-type="png" data-w="386" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">这里的 x<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(i)</span> 表示第 i 个样本。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">用 Z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span> 来表示对 m 个样本的第一层的加权求和：</p><p><img class="" data-ratio="0.0963855421686747" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvicqtsp0kB01F9dvKzGt8PHrBZG4qPKiaPlHtfFPrYYRzo2qvqU1iaiaz3w/0?wx_fmt=png" data-type="png" data-w="249" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">其中的 z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1](i)</span> 是 (3, 1) 的列向量，因此 Z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span> 是一个(3, m) 的矩阵。将其中每一列展开：</p><p><img class="" data-ratio="0.041666666666666664" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvNupI5OScmkekj802ibe3OmFq8oGl6jQWyic9icfP1OqM3p6X7dOeuOZIw/0?wx_fmt=png" data-type="png" data-w="576" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">这里的 w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1](i)</span> 以及 b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1](i)</span> 是一样的，因为对于 m 个样本来讲，神经元的参数是一致的。所以，可以简化为：</p><p><img class="" data-ratio="0.05139186295503212" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBv1wjIzOupSibFAqxXzlEDVsD6Ws1ibCFrzHpicrJIQMrs0r0ekwTvwou9w/0?wx_fmt=png" data-type="png" data-w="467" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p><img class="" data-ratio="0.0729483282674772" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBv7hJY0lNBsxsGA2cUAorY5QTGWSPsEqmrG5mxGYtMt7rmbCctFicaCIg/0?wx_fmt=png" data-type="png" data-w="329" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p><img class="" data-ratio="0.12418300653594772" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvmIOE4y7pcmD0uwIuBmtTl9G8uUjpKytjIrUq4Bb83l1JiabxJFb81wQ/0?wx_fmt=png" data-type="png" data-w="153" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">以此类推：</p><p><img class="" data-ratio="0.11949685534591195" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvOnWV5G4ib8VuuLqkk8NcXvQS1k0qHtaI6Sn3ZIYf2XqSibVqSGMaI1pg/0?wx_fmt=png" data-type="png" data-w="159" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p><img class="" data-ratio="0.1926605504587156" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvvlickCLKVhdxiaxEtH1yescaibicbbHwB6w3npw0nX2icH5TtzBQRUMB4fg/0?wx_fmt=png" data-type="png" data-w="109" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">这就是 <span style="box-sizing: border-box;font-weight: 700;">所有样本</span> 输入 <span style="box-sizing: border-box;font-weight: 700;">神经网络</span> 的递归公式了。</p><h2 style="box-sizing: border-box;"><span style="font-size: 24px;"><strong>矩阵的维度</strong></span></h2><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">前文中，我们说矩阵的维度时用 3x5 来表达，现在我们用 (3, 5) 来表达，在Python中这样的数据结构是 <span style="box-sizing: border-box;font-weight: 700;">Tuple</span> 类型。在NumPy中<code style="box-sizing: border-box;">numpy.array</code>对象的<code style="box-sizing: border-box;">shape</code>属性返回的就是就是这样一个结构：<br></p><pre class="hljs python" style="box-sizing: border-box;overflow: auto;"><code class="python" style="box-sizing: border-box;"><span class="hljs-keyword" style="box-sizing: border-box;color: rgb(133, 153, 0);">import</span> numpy <span class="hljs-keyword" style="box-sizing: border-box;color: rgb(133, 153, 0);">as</span> np;

ary = <span class="hljs-number" style="box-sizing: border-box;color: rgb(42, 161, 152);">10</span> * np.random.rand(<span class="hljs-number" style="box-sizing: border-box;color: rgb(42, 161, 152);">4</span>,<span class="hljs-number" style="box-sizing: border-box;color: rgb(42, 161, 152);">3</span>)
print(ary.shape) <span class="hljs-comment" style="box-sizing: border-box;color: rgb(147, 161, 161);"># 输出：(4, 3)</span></code></pre><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">以<em style="box-sizing: border-box;">图1：4层神经网络</em>为例，回顾一下各种表达：</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>L，表示神经网络的层数，这里 L=4。</p></li><li><p>n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>，表示神经网络第 l 层的神经元个数，在这个例子中，n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>=4, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[2]</span>=5, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[3]</span>=3, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[4]</span>=1。同时，也可以用 n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[0]</span>=3 表示输入层的特征（feature）数。</p></li><li><p>a<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>表示第l层的所有神经元的激活状态（也就是输出），a<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>=g<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>(z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>)，同时 a<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[0]</span> = x 是输入层，ŷ = a<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[L]</span> 表示最终的输出。</p></li><li><p>w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>，表示第 l 层的每个输入的权重。</p></li><li><p>b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>，表示第 l 层每个神经元的内部强度。</p></li></ol><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">以 <span style="box-sizing: border-box;font-weight: 700;">一个样本x输入</span> 为例下面来计算下 z, w, b的维度，以第一层 z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span> = w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>x + b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span> 为例：</p><table width="620"><thead style="box-sizing: border-box;"><tr style="box-sizing: border-box;"><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;"><br></th><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;">z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span></th><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;">w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span></th><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;">x</th><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;">b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span></th></tr></thead><tbody style="box-sizing: border-box;"><tr style="box-sizing: border-box;"><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">维度</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(4,1)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(4,3)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(3,1)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(4,1)</td></tr><tr style="box-sizing: border-box;background-color: rgba(181, 181, 181, 0.098);"><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">归纳</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, 1)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[0]</span>)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[0]</span>, 1)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, 1)</td></tr></tbody></table><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">以此类推，得出:</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度为 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l-1]</span>)，dw<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>的维度也一样。</p></li><li><p>b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度为 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>, 1)，db<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>的维度也一样。</p></li><li><p>z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度为 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>, 1)，dz<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>的维度也一样。</p></li><li><p>a<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度为 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>, 1)，da<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>的维度也一样。</p></li></ol><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">这里也新出现了两个变量，后面在梯度下降法中会用到。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">以<em style="box-sizing: border-box;">m个样本</em>的输入为例，向量化后公式为：Z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>=w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>X<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l-1]</span> + b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>，其中， w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 和 b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度与向量化之前一样（向量化一节的推导过程可以清楚的表明这一点）。</p><table width="620"><thead style="box-sizing: border-box;"><tr style="box-sizing: border-box;"><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;"><br></th><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;">Z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span></th><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;">w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span></th><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;">X</th><th style="box-sizing: border-box;padding: 8px;border-top-width: 1px;border-top-color: rgb(221, 221, 221);line-height: 20px;vertical-align: middle;">b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span></th></tr></thead><tbody style="box-sizing: border-box;"><tr style="box-sizing: border-box;"><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">维度</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(4,m)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(4,3)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(3,m)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(4,m)</td></tr><tr style="box-sizing: border-box;background-color: rgba(181, 181, 181, 0.098);"><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">归纳</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, m)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[0]</span>)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[0]</span>, m)</td><td style="box-sizing: border-box;padding: 8px;line-height: 20px;vertical-align: middle;">(n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, m)</td></tr></tbody></table><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>Z<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度为 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>, m)，dZ<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>的维度也一样。</p></li><li><p>A<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 的维度为 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>, m)，dA<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>的维度也一样。</p></li></ol><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">这里需要的注意的是，虽然上b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 是一个 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, m) 的矩阵，但实际上，是 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, 1) 的矩阵水平扩展到 m 列的，每一列都一样。而且在Python中，因为Python的Broadcasting功能，在计算“矩阵+向量”的时候，Python会将向量自动水平扩展到和矩阵同样的维度，所以吴恩达的课程中仍然将 b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 认为是 (n<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[1]</span>, 1) 的向量。</p><h2 style="box-sizing: border-box;"><strong><span style="font-size: 24px;">成本函数<br></span></strong></h2><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">我们先回顾下单个神经元的 Logistic Regresion ，公式是这样的：</p><p><img class="" data-ratio="0.1652892561983471" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvBsticp6dWibeMNEx1TeKpIAGTfGgdIGAlwbn6vz4VEiaHakAtFdYOYEEg/0?wx_fmt=png" data-type="png" data-w="121" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p><img class="" data-ratio="0.3333333333333333" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvNFeJL1cdXEzAtIqWPGticgWibEkGumoFhP3ZuiashoHZibhenYtysxVS8A/0?wx_fmt=png" data-type="png" data-w="117" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);"><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">如果我们希望让我们的机器学习结果能更准确，也就是希望对于样本{(x<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(1)</span>, y<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(1)</span>), (x<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(2)</span>, y<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(2)</span>),...,(x<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(m)</span>, y<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(m)</span>)}来说，要找到一组w和b的值，使得根据x<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(i)</span>计算出来的结果 ŷ<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(i)</span> 能尽可能接近 y<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">(i)</span>。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">可是，“尽可能接近”这种说法有点说不清、摸不着，所以我们就把问题转化一下，引出损失函数的概念。</p><h4 style="box-sizing: border-box;"><strong><span style="font-size: 18px;">Loss function</span></strong></h4><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">对于一个样本(x, y)，输出为 ŷ，一般情况下，我们可以误差的平方来定义 <span style="box-sizing: border-box;font-weight: 700;">损失函数（Loss function 或者 Error function）</span>，我印象中大学物理就是用这种方法，形式如下：</p><p><img class="" data-ratio="0.24503311258278146" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvhnfibibCuvZVN8GpfX7qxQJfWoboCjuSgZCWMfHMoEzcbiacYrZv2PmMA/0?wx_fmt=png" data-type="png" data-w="151" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">吴恩达说，在 Logsitic Regression 中一般不这么做，因为当求解参数的时候优化问题会变成 <span style="box-sizing: border-box;font-weight: 700;">非凸</span> 的，会导致有很多局部最优解，进而导致 <span style="box-sizing: border-box;font-weight: 700;">梯度下降法</span> 无法找到全局最优解。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">在 Logistic Regression 中，一般使用这样一个损失函数：</p><p><img class="" data-ratio="0.05714285714285714" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvzoj4LhwN3YJo8g3herlUoU6wlXSbXjk2Hz3Ecfz8GabvuWeCN5ia7wA/0?wx_fmt=png" data-type="png" data-w="315" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">在这个公式中，当ŷ越接近于y时，这个函数的值就越小。吴恩达并没有给出严谨的数学证明，只是简单地代入两个极限值0和1来举例（还记得σ(z)的最大值上限和最小值下限分别就是1和0吗？）：</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>如果 y=1，则 L(ŷ, y)=-log(ŷ)，只有当ŷ趋近于最大值1时，-log(ŷ) 才达到最小</p></li><li><p>如果 y=0，则 L(ŷ, y)=-log(1-ŷ)，只有当 ŷ 趋近于最小值0时，-log(1-ŷ) 才达到最小</p></li></ol><h4 style="box-sizing: border-box;"><br></h4><h4 style="box-sizing: border-box;"><span style="font-size: 18px;"><strong>Cost function<br></strong></span></h4><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">上面的 <span style="box-sizing: border-box;font-weight: 700;">损失函数（Lost function）</span> 是针对一个样本的，对于 m 个样本，我们定义 <span style="box-sizing: border-box;font-weight: 700;">成本函数（Cost function）</span> 为：</p><p><img class="" data-ratio="0.24056603773584906" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvD4el676ENRpfSPQaAmYvHZFhW2e0stzxJeAoaSVjqzcxg6kYjDDBfw/0?wx_fmt=png" data-type="png" data-w="212" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p><img class="" data-ratio="0.12142857142857143" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLcHU1fETGdZzTicyvc4ggaBvh60cS61VCjgo8ibgZbQgJPCPvib4xUmqiaGsuHnY3BiaYvmcYwI4xRvlLg/0?wx_fmt=png" data-type="png" data-w="420" style="box-sizing: border-box;border-width: 0px;border-style: initial;border-color: initial;vertical-align: middle;cursor: zoom-in;transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br></p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">特别注意的是，前面的 <span style="box-sizing: border-box;font-weight: 700;">Loss function</span> 的参数是 y 和 ŷ，但是 <span style="box-sizing: border-box;font-weight: 700;">Cost function</span> 的参数是 w 和 b，也就是说，我们会通过一些方法来求解 w 和 b 使得全局的 <span style="box-sizing: border-box;font-weight: 700;">Cost function</span>J(w,b) 最小。而这个“方法”就是<span style="box-sizing: border-box;font-weight: 700;">梯度下降法</span>。</p><h2 style="box-sizing: border-box;"><span style="font-size: 24px;"><strong>小结</strong></span></h2><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">这一部分我总结了向量化、矩阵的维度以及<span style="box-sizing: border-box;font-weight: 700;">Cost function</span>。我们的学习脉络是：一个神经元到一层神经网络，再到多层神经网络，再到多样本输入的神经网络，这个过程梳理清楚了，学习就容易了。</p><p style="box-sizing: border-box;margin-bottom: 25px;color: rgb(47, 47, 47);">下一步要学习和总结 <span style="box-sizing: border-box;font-weight: 700;">计算图</span>，以及如何通过 <span style="box-sizing: border-box;font-weight: 700;">梯度下降法</span> 求解 w<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span> 和 b<span style="box-sizing: border-box;font-size: 12px;line-height: 0;vertical-align: baseline;top: -0.5em;">[l]</span>，会涉及到一些 <span style="box-sizing: border-box;font-weight: 700;">导数</span>、<span style="box-sizing: border-box;font-weight: 700;">偏导数</span>。本文中已经逐渐开始学习到 <span style="box-sizing: border-box;font-weight: 700;">numpy</span> 的用法了，如果不了解的话，对吴恩达课程讲解的公式会不容易理解，下一篇也会总结。</p>
                </div>
                <script nonce="1738552420" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##">赞赏</a>
                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
</body>
</html>
