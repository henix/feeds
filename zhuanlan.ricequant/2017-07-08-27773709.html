<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>浅析至强RNN可微分神经计算机(DNC)</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/27773709">原文</a></p>
<div class="title-image"><img src="https://pic3.zhimg.com/v2-a522152c896b6eb9103def1cdc052621_r.jpg" alt=""></div><p>CNN和RNN神经网络模型在广泛的模式识别任务中表现良好。其中RNN可以以通过将“word states”转换为记忆向量（隐藏状态）处理诸如翻译，手写生成和语音识别之类的序列建模任务。但是在实践中RNN的主要变种LSTM处理长距离依赖序列需要大量的计算资源，且效果欠佳。为了解决这个问题，多种具有外存储机制的神经网络被设计出来，目前最具知名度的是Deepmind 的<a href="https://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html">Differentiable Neural Computer (DNC)</a>。</p><p>通过把可训练的神经网络控制器和可读写的外部存储器进行结合，可微分神经计算机（Differentiable Neural Computer，DNC）这种混合学习型神经网络，既能像神经网络那样进行算法和参数的学习，又能像计算机那样处理复杂数据信息流。在发表于Nature的论文中，DNC是一种具有外存储器（不可训练）的特殊的循环神经网络。在每时间步 t 由可训练的控制器基于t-1 时刻的信息流与外存储器交换信息流之后线性组合两部分的预测信息决定最终输出预测。</p><p><br></p><img src="https://pic2.zhimg.com/v2-a9db48d20a301f308132824769558e11_r.png" data-rawwidth="731"><p>DNC使用向量Vector来存储记忆。存储器矩阵Memory matrix的每行对应于不同的记忆。控制器通过使用接口向量Interface parameters控制一个写头控制和多个读头控制（每个读头都是由两种寻址机制线性组合而成，读头数量在结构设计中未有约束）与外存储记忆交互。记忆矩阵<equation>M \in \mathbb{R}^{N \times W}</equation>一行向量表示一组记忆，N行表示记忆矩阵最多可以保有多少组记忆。在每个时间步dnc接受上一时刻读头信息流与此时刻外部输入信息流组成广义dnc外部输入信息流（也就是传统LSTM对应每步外部输入inputs），经过处理发出隐藏状态，隐藏状态生成输出向量和接口向量。接口向量控制读写头控制通过读写机制与外存储矩阵交互，生成此时刻的写信息，并更新矩阵获得此时刻的读信息。读信息与输出向量线性组合生成此时刻最终输出向量。</p><p>外存储器记忆矩阵更新</p><p><equation>M_t = M_{t-1} \circ (E-\mathbf{w}_t^w \mathbf{e}_t^\intercal) + \mathbf{w}_t^w \mathbf{v}_t^\intercal\\ r_t^i = M_t^T w_t^{r,i}</equation></p><p>其中<equation>E \in \mathbb{R}^{N \times W}</equation>为全1矩阵；<equation>w \in \mathbb{R}^N</equation>为写头是归一化的分布权重；<equation>\mathbf{e} \in \mathbb{R}^W</equation>为擦除向量，取值局限于[0,1]之间；<equation>\mathbf{v} \in \mathbb{R}^W</equation>为写入记忆向量也就是此时刻新的记忆信息；注意读写头控制变量为记忆矩阵行与行之间的相对强度，而不是具体的记忆信息向量。从左向右，先擦除后写入。在此时刻记忆矩阵更新之后，读头提取此时刻记忆矩阵读头信息流<equation>r_t </equation>该信息流线性组合此时刻最终输出并且作为下一时刻输入外部输入使用。</p><p><br></p><img src="https://pic1.zhimg.com/v2-1ac47a8c88a63ffc6d2b3a98c5974be6_r.png" data-rawwidth="1188"><p><br></p><p>比较直观的看发表在Nature上的DNC相当于对LSTM添加一个外存储器，提高LSTM的记忆遗忘的问题。这篇论文里面构建了一个非常流畅的可微分的外记忆读写系统。如下图，在时间步t，control接收input vector <equation>x_t</equation>和上一个时间步记忆读取向量（读头<equation>r_t</equation>）以及control(LSTM的隐藏状态矩阵)进行计算处理之后，发出两个向量(ouput vector <equation>v_t</equation> &amp; interface vector<equation>\xi_t</equation>)。其中<equation>v_t</equation>与读头<equation>r_t</equation>线性组合为最终输出向量<equation>y_t</equation>。<equation>\xi_t</equation>通过寻址机制Memory Addressing 与记忆矩阵<equation>M</equation>
交互生成输出读头记忆<equation>r_t </equation>。 
</p><p><br></p><img src="https://pic3.zhimg.com/v2-56fe72ad32eeb64ca326b097e2c93b37_r.png" data-rawwidth="755"><p><br></p><p>control 通过发出接口向量控制access中write weighting 和read weightings的更新来控制记忆矩阵并得到读头读取的外部记忆向量。写头控制write weighting通过基于余弦相似性的内容寻址和基于写入频率和时间的使用情况usage确定记忆矩阵行位置的使用情况（根据先后写入和读取频率的程度信息决定被新记忆覆盖的顺序）。读头控制read weighting 通过基于余弦相似性的内容寻址和记忆矩阵行之间相对写入顺序更新读头控制。如下图，每时间步t，control接受上一时刻t-1读头记忆向量和此时刻外部输入作为时刻t的控制器输入信息流，在计算之后发出interface vector 控制外存储器更新写头，由写头控制擦除和写入更新记忆过程（图中绿色操作）。之后新的读头基于新记忆矩阵得到时刻t的读取记忆向量。时刻t得到读取记忆向量同控制器输出向量线性变换映射为时刻t的输出，并且读取记忆向量作为新的短期记忆传递向下一时刻t+1。DNC最终输出结果由时刻t得到读取记忆向量同控制器输出向量线性组合得到，类似于人脑的长期记忆和短期记忆Search of Associative Memory 模型，也就是每时间步的输出由短期记忆和长期记忆线性组合生成。</p><p><equation>\mathbf{y}_t = \mathbf{u_t} + W_r [\mathbf{r}_t^1; \ldots ;\mathbf{r}_t^R]</equation></p><p><a href="https://www.ricequant.com/community/topic/3641/">代码实现</a></p><p>引用：</p><p><a href="https://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html">Hybrid computing using a neural network with dynamic external memory</a></p><p><a href="https://github.com/Mostafa-Samir/DNC-tensorflow">Mostafa-Samir/DNC-tensorflow</a></p><h2>附录：</h2><h2>Controller</h2><p>论文使用一个标准的LSTM模型。注意将工业界用的的LSTM公式组去掉batch_size，并设置hidden_num=1即为如下公式组书写形式。
</p><p><equation>i_t^l = \sigma(W_i^l[\chi_t;h_{t-1}^l;h_t^{l-1}] + b_i^l)\\ f_t^l = \sigma(W_f^l[\chi_t;h_{t-1}^l;h_t^{l-1}] + b_f^l)\\ s_t^l = f_t^l s_{t-1}^l + i_t^l tanh (W_s^l[\chi_t;h_{t-1}^l;h_t^{l-1}] + b_s^l) \\ o_t^l = \sigma(W_o^l[\chi_t;h_{t-1}^l;h_t^{l-1}] + b_o^l)\\ h_t^l = o_t^ltanh(s_t^l)\\ \chi_t = [x_t;r_{t-1}^1;...;r_{t-1}^l]</equation></p><code lang="python">"""
inputs: [batch_size, in_width]
* read_vectors_prev: [batch_size, memory_W * read_head_num]
hidden_prev: [batch_size, hidden_num]
hidden_lower: [batch_size, hidden_num]
state_prev: [batch_size, hidden_num]

weights_input: [in_width, hidden_num]
weights_read_vectors_prev: [memory_W * read_head_num, hidden_num]
weights_hidden_prev: [hidden_num, hidden_num]
weights_hidden_lower: [hidden_num, hidden_num]
biaes: [hidden_num]    

input_gate: [batch_size, hidden_num]
forget_gate: [batch_size, hidden_num]
state: [batch_size. hidden_num]
output_gate: [batch_size, hidden_num]
hidden: [batch_size, hidden_num]
"""                   

input_gate = tf.nn.sigmoid(tf.matmul(inputs, weights['input_gate_inputs']) +                 tf.matmul(read_vectors_prev, weights['input_gate_read_vectors_prev']) + tf.matmul(hidden_prev, weights['input_gate_hidden_prev']) + tf.matmul(hidden_lower, weights['input_gate_hidden_lower']) + weights['input_gate_bias'])        

forget_gate = tf.nn.sigmoid(tf.matmul(inputs, weights['forget_gate_inputs']) + tf.matmul(read_vectors_prev, weights['forget_gate_read_vectors_prev']) + tf.matmul(hidden_prev, weights['forget_gate_hidden_prev']) + tf.matmul(hidden_lower, weights['forget_gate_hidden_lower']) + weights['forget_gate_bias'])        

state = tf.multiply(forget_gate, state_prev) + tf.multiply(input_gate, tf.tanh(tf.matmul(inputs, weights['state_inputs']) + tf.matmul(read_vectors_prev, weights['state_read_vectors_prev']) + tf.matmul(hidden_prev, weights['state_hidden_prev']) + tf.matmul(hidden_lower, weights['state_hidden_lower']) + weights['state_bias']))

output_gate = tf.nn.sigmoid(tf.matmul(inputs, weights['output_gate_inputs']) + tf.matmul(read_vectors_prev, weights['output_gate_read_vectors_prev']) + tf.matmul(hidden_prev, weights['output_gate_hidden_prev']) + tf.matmul(hidden_lower, weights['output_gate_hidden_lower']) + weights['output_gate_bias'])  

hidden = tf.multiply(output_gate, state)
</code><p>多层LSTM之间信息流动方向如下图：</p><img src="https://pic4.zhimg.com/v2-6e5cea714840afa6b3cec68b7a560023_r.jpg" data-rawwidth="5248"><p><br></p><code lang="python"># input layer
hidden_L1, state_L1 = self.element(inputs, read_vectors_prev, hidden_L1_prev, hidden_L1_lower, state_L1_prev, weights_L1)

# primary thinking layer
hidden_L2_lower = hidden_L1
hidden_L2, state_L2 = self.element(inputs, read_vectors_prev, hidden_L2_prev, hidden_L2_lower, state_L2_prev, weights_L2)

# advanced thinking layer
hidden_L3_lower = hidden_L2
hidden_L3, state_L3 = self.element(inputs, read_vectors_prev, hidden_L3_prev, hidden_L3_lower, state_L3_prev, weights_L3)

# output layer
hidden_L4_lower = hidden_L3
hidden_L4, state_L4 = self.element(inputs, read_vectors_prev, hidden_L4_prev, hidden_L4_lower, state_L4_prev, weights_L4)

hiddens = tf.concat([hidden_L1, hidden_L2, hidden_L3, hidden_L4], axis=1)       
output_vector = tf.matmul(hiddens, weight_output)
interface_vector = tf.matmul(hiddens, weight_interface)   
</code><p>每时间步control最终输出的output vector和 interface vector综合所有隐藏状态向量生成。</p><p><equation>\nu_t = W_y[h_t^l;...;h_t^l]\\ \varepsilon_t = W_{\varepsilon}[h_t^l;...;h_t^l]</equation></p><h2>控制外存储器的Interface parameters</h2><p>使用<equation>oneplus</equation>和<equation>softmax</equation>函数约束值范围。通过数值约束函数保证外存储器的读写头寻址机制控制权重被局限在[0,1]的预期范围内。
</p><code lang="python"># 处理控制器发出接口向量
"""
interface_vector: [batch_size, W*R+5R+3W+3]

* read_keys: [batch_size, memory_W * read_head_num] -&gt; [batch_size, memory_W, read_head_num]
read_strengths: [batch_size, read_head_num]
* write_key: [batch_size, memory_W] -&gt; [batch_size, memory_W, write_head_num=1]
write_strengths: [batch_size, write_head_num=1]
erase_vector: [batch_size, memory_W]
write_vector: [batch_size, memory_W]
free_gates: [batch_size, read_head_num]
allocation_gate: [batch_size, 1]
write_gate: [batch_size, 1]
* read_modes: [batch_size, 3 * read_head_num] -&gt; [batch_size, 3, read_head_num]
"""
R,W = self.read_head_num, self.memory_W        
splits = np.cumsum([0,W*R,R,W,1,W,W,R,1,1,3*R])        
tmp_list = [interface_vector[:, splits[i]:splits[i+1]] for i in range(len(splits)-1)]

read_keys = tf.reshape(tmp_list[0], shape=[-1, self.memory_W, self.read_head_num])
read_strengths = 1 + tf.nn.softplus(tf.reshape(tmp_list[1], shape=[-1, self.read_head_num]))
write_key = tf.reshape(tmp_list[2], shape=[-1, self.memory_W, 1])
write_strength = 1 + tf.nn.softplus(tf.reshape(tmp_list[3], shape=[-1,1]))
erase_vector = tf.reshape(tmp_list[4], shape=[-1, self.memory_W])
write_vector = tf.reshape(tmp_list[5], shape=[-1, self.memory_W])
free_gates = tf.nn.sigmoid(tf.reshape(tmp_list[6], shape=[-1, self.read_head_num]))
allocation_gate = tf.nn.sigmoid(tmp_list[7])
write_gate = tf.nn.sigmoid(tmp_list[8])
read_modes = tf.nn.softmax(tf.reshape(tmp_list[9], shape=[-1, 3, self.read_head_num]))
</code><h2>Reading and writing to memory</h2><p>注意在每时间步记忆矩阵先根据写头更新记忆再返回读头新记忆信息流。</p><p>更新写头控制-&gt;更新记忆矩阵-&gt;更新读头控制-&gt;计算获得读头新记忆信息流</p><code lang="python"># Memory addressing -&gt; write weighting       
"""
memory_matrix_prev: [batch_size, memory_N, memory_W]
write_key: [batch_size, memory_W, write_head_num=1]
write_strengths: [batch_size, write_head_num=1]

free_gates: [batch_size, read_head_num]
read_weightings_prev: [batch_size, memory_N, read_head_num]
write_weighting_prev: [batch_size, memory_N]
usage_vector_prev: [batch_size, memory_N]

allocation_gate: [batch_size, 1]
write_gate = tf.nn.sigmoid(tmp_list[8])

write_weighting: [batch_size, memory_N]
usage_vector: [batch_size, memory_N]
"""
write_weighting, usage_vector = \
self._Write_weighting(memory_matrix_prev, 
                      write_key,
                      write_strength,
                      free_gates, 
                      read_weightings_prev, 
                      write_weighting_prev, 
                      usage_vector_prev,
                      allocation_gate, 
                      write_gate)


# reading and writing to memory -&gt; update memory
"""
write_weighting: [batch_size, memory_N]
* write_weighting_fit: [batch_size, memory_N] -&gt; write_weighting: [batch_size, memory_N, 1]
erase_vector: [batch_size, memory_W] -&gt; erase_vector: [batch_size, 1, memory_W]
write_vector: [batch_size, memory_W] -&gt; write_vector: [batch_size, 1, memory_W]

memory_matrix_prev: [batch_size, memory_N, memory_W]
"""
write_weighting_fit = tf.expand_dims(write_weighting, axis=2)
erase_vector = tf.expand_dims(erase_vector, axis=1)
write_vector = tf.expand_dims(write_vector, axis=1)

erase_operation = tf.multiply(memory_matrix_prev, (1-tf.matmul(write_weighting_fit, erase_vector)))
write_operation = tf.matmul(write_weighting_fit, write_vector)
memory_matrix = erase_operation + write_operation        

# memory addressing -&gt; read weightings
"""
memory_matrix: [batch_size, memory_N, memory_W]
read_keys: [batch_size, memory_W , read_head_num]
read_strengths: [batch_size, read_head_num]
precedence_weighting_prev: [batch_size, memory_N]
link_marix_prev: [batch_size, memory_N, memory_N]
read_modes: [batch_size, 3 * read_head_num]
read_weightings: [batch_size, memory_N, read_head_num]
precedence_weighting: [batch_size, memory_N]
link_marix: [batch_size, memory_N, memory_N]

read_vectors: [batch_size, memory_W, read_head_num] 
"""

read_weighting, precedence_weighting, link_matrix = \
self._Read_weighting(memory_matrix, 
                     read_keys, read_strengths, 
                     write_weighting, precedence_weighting_prev, 
                     link_matrix_prev,
                     read_weightings_prev,
                     read_modes)      

read_vectors = tf.matmul(memory_matrix, read_weighting, adjoint_a=True)
</code><h2>Memory addressing</h2><h2>content-based addressing</h2><p>通过余弦可以通过夹角的大小，来判断向量的相似程度。夹角越小，就代表越相似。余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫"余弦相似性"。通过内容相似度定义Memory Matrix 行(N or locations) 归一化的概率分布, 其中余弦相似性使用tf.nn.l2_normalize拆分计算。</p><p><equation>\mathcal{C}(M, \mathbf{k}, \beta)[i] = \frac{exp\{\mathcal{D}(\mathbf{b},M[i,\cdot])\beta\}}{\sum_j exp\{\mathcal{D}(\mathbf{b},M[j,\cdot])\beta\}} \quad \quad \mathrm{where} \quad \quad \mathcal{D}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\lvert \mathbf{u} \rvert \lvert \mathbf{v} \rvert}</equation></p><p><br></p><img src="https://pic1.zhimg.com/v2-b4b45a81ef3c9040866a7e66d6b26c64_r.png" data-rawwidth="574"><p><br></p><code lang="python">"""
memory_matrix: [batch_size, memory_N, memory_W]
lookup_key: [batch_size, memory_W, read_head_num or write_head_num]
key_strength: [batch_size, read_head_num or write_head_num] -&gt; [batch_size, 1, read_head_num or write_head_num] 
cosine_similarity: [batch_size, memory_N, read_head_num or write_head_num]
content_weighting: [batch_size, memory_N, read_head_num or write_head_num]
"""
cosine_similarity = tf.matmul(tf.nn.l2_normalize(memory_matrix, dim=2), \ tf.nn.l2_normalize(lookup_key, dim=1))

content_weighting = tf.nn.softmax(tf.multiply(cosine_similarity, tf.expand_dims(key_strength, axis=1)), dim=1)
</code><h2>Dynamic memory allocation</h2><p>使用可微分的"free list"通过添加和删除链表中的地址来维持可用内存位置列表，允许controller在需要时释放和分配内存。retention_vector 根据所有读头反馈决定memory_matrix 矩阵索引memory_N的不被释放的程度。使用usage_vector 表述Memory 位置（N）的使用程度，usage_vector 取值限于[0,1]，Memory位置写入信息增加usage_vector 值，并且usage_vector 值只能被free_gate 释放内存位置的行为降低。free_list 根据usage_vector的值升序排列，确定新写入Memory位置时候有内容被释放顺序。最终根据写头根据allocation_weighting决定在哪个位置写入新记忆。</p><p>首先根据usage排序计算allocation之后使用tf.TensorArray返回信息流对应的行顺序。</p><p>Note: 在设定初始值的情况下，计算公式得出结果的数值范围自然被局限在<equation>\Delta N</equation>边界条件内。原文中认为这里排序信息似乎对于学习没有贡献，在计算梯度的时候忽略这里由于排序索引造成不连续。
</p><code lang="python"># 动态内存分配，retention_vector 根据所有读头反馈决定memory_matrix 矩阵索引memory_N的不被释放的程度    

"""
* free_gates: [batch_size, read_head_num] -&gt; [batch_size, 1, read_head_num]       
read_weightings_prev: [batch_size, memory_N, read_head_num]
retention_vector: [batch_size, memory_N]

write_weighting_prev: [batch_size, memory_N]
usage_vector_prev: [batch_size, memory_N]
usage_vector: [batch_size, memory_N]

allocation_weighting: [batch_size, memory_N]

Note: * read_weightings_prev: [batch_size, memory_N, read_head_num]
      * write_weighting_prev: [batch_size, memory_N]
"""
free_gates = tf.expand_dims(free_gates, axis=1)
retention_vector = tf.reduce_prod(1 - tf.multiply(read_weightings_prev, free_gates), axis=2)       

usage_vector = tf.multiply((usage_vector_prev + write_weighting_prev - tf.multiply(usage_vector_prev, write_weighting_prev)), retention_vector)

values, indices = tf.nn.top_k(usage_vector, k=self.memory_N)  # 降序排列    
values_sorted = tf.reverse(values, axis=[1]) # 值 升序
indices_sorted = tf.reverse(indices, axis=[1]) # 索引 升序
allocation_weighting = tf.multiply((1-values_sorted), tf.cumprod(values_sorted, axis=1, exclusive=True))
"""
allocation_weighting 值返回原始位置
这里根据论文计算的公式论文里面使用的索引计算方式，也就是按索引顺序的计算得出上面的矩阵，还需要返回正常的顺序。
通过TensorArray操作将allocation_weighting 返回原来的序列，也就是usage_vector 最初对应的序列，也就是memory 矩阵的正常序列，
返回的数值为根据 allocation_weighting 计算的释放程度的值
"""
indices_flat = tf.reshape(tensor= indices_sorted + self.flat_shift, shape=[-1])
flat_array = tf.TensorArray(dtype=tf.float32, size=self.batch_size*self.memory_N)
scatter= flat_array.scatter(indices= indices_flat, value= tf.reshape(allocation_weighting, shape=[-1]))
allocation_sort_undo = scatter.stack()
allocation_weighting = tf.reshape(allocation_sort_undo, shape=[self.batch_size, self.memory_N])     
allocation_weighting = tf.stop_gradient(allocation_weighting)
return allocation_weighting, usage_vector
</code><h2>Temporal memory linkage</h2><p>使用时间链接矩阵L 跟踪Memory写入位置顺序信息也即记忆的新旧。使用矩阵<equation>L (N \times N)</equation> 表示每次写入新记忆之后各记忆位置相对新旧程度。其中precedence weighting控制记忆矩阵各行之间的优先保留或更新的强度。写头根据precedence weighting强度确定记忆矩阵优先写入位置（行选择）。
</p><code lang="python">"""
write_weighting: [batch_size, memory_N]
precedence_weighting_prev: [batch_size, memory_N]
precedence_weighting: [batch_size, memory_N]

* write_weighting: [batch_size, memory_N] -&gt; write_weighting: [batch_size, memory_N, 1]
tmp: [batch_size, memory_N, memory_N]
link_marix_prev: [batch_size, memory_N, memory_N]
left: [batch_size, memory_N, memory_N]
* precedence_weighting_fit: [batch_size, memory_N] -&gt; precedence_weighting: [batch_size, 1, memory_N] 维度匹配
right: [batch_size, memory_N, memory_N]
link_marix: [batch_size, memory_N, memory_N]

read_weightings_prev: [batch_size, memory_N, read_head_num]
forward_weighting: [batch_size, memory_N, read_head_num]
backward_weighting: [batch_size, memory_N, read_head_num]
"""

precedence_weighting = tf.multiply(precedence_weighting_prev, (1 - tf.reduce_sum(write_weighting, axis=1, keep_dims=True))) + write_weighting

write_weighting = tf.expand_dims(write_weighting, axis=2)
tmp = tf.tile(write_weighting, multiples=[1,1,self.memory_N])
left = tf.multiply(link_matrix_prev, (1 - tmp - tf.transpose(tmp,perm=[0,2,1])))
precedence_weighting_fit = tf.expand_dims(precedence_weighting_prev, axis=1) # 维度匹配
right = tf.matmul(write_weighting, precedence_weighting_fit)        
link_matrix = tf.multiply((left+right), self.mask_eye) # 掩码处理对角线消除

forward_weighting = tf.matmul(link_matrix, read_weightings_prev)
backward_weighting = tf.matmul(link_matrix, read_weightings_prev, transpose_a=True)和(https://github.com/Mostafa-Samir/DNC-tensorflow</code>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
