<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>DQN与HS300指数择时</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/29945839">原文</a></p>
<div class="title-image"><img src="https://pic1.zhimg.com/v2-3c956f653177aeb6839017b262c71d08_r.jpg" alt=""></div><p>2018-01-07 更新</p><p>重构了一下DQN期货则时，样本外效果还不错。</p><p>日频交易，股指期货策略，单纯判断方向信号。</p><img src="https://pic1.zhimg.com/v2-c562240759ce30fedeae069b7b0873a4_r.jpg" data-caption="胜率: 0.537190082645" data-size="normal" data-rawwidth="944" data-rawheight="351"><img src="https://pic1.zhimg.com/v2-1b0fe80f9ca86bf1271e0b5e3b13482c_r.jpg" data-caption="胜率: 0.5413223140495868" data-size="normal" data-rawwidth="1152" data-rawheight="432"><p><br></p><p>2017-10-10 更新粘合RQAlpha版本见文后</p><p><br></p><p>假期突击入门增强学习(reinforcement learning)，简单构建了一个简单的运算框架加深对RL的理解。</p><p><br></p><p>指数择时的外部Environment 简单的使用日线HS300指数bar数据，在每交易日使用开盘价响应Agent的行为（沽空、沽多和空仓），使用收盘价+账户现金存余为当日账户总值。</p><p>下图为Agent使用完全探索策略运行5000次，账户总值走势。</p><img src="https://pic1.zhimg.com/v2-c779f2c9bb00640d404f6553c7f5817d_r.jpg" data-caption="" data-size="normal" data-rawwidth="947" data-rawheight="356"><p>Agent使用greedy policy作为行为选择策略与Environment交互的账户总值走势。</p><img src="https://pic2.zhimg.com/v2-386e419379a4786d2e3c350375c6bf86_r.jpg" data-caption="" data-size="normal" data-rawwidth="947" data-rawheight="356"><p><br></p><p>不同于此前博客介绍的RNN用于股票的预测，RL在每个state（或帧）预测的是折扣未来预期收益（此处可以理解为远期标折现）。对比RNN模型，RL模型在对股票市场未来走势的预测方面更加宽松或更鲁棒。RL在 <equation>s_t</equation> 生的预测由未来预期收益由真实奖励 <equation>R_t</equation> 和逼近函数模型估计未来一段时间折扣收益组成。这相比于RNN更加宽松，因为我们不能期望股票市场的模式或者状态与未来股价走势存在严格的函数关系，也就是不应该期望在 <equation>t</equation> 时刻的事件精准的反馈在 <equation>t+k</equation> 时刻的股价上面。此外通常在实际使用RL模型的时候不需要特别精确的收敛，通常只要达到一定的准确程度或一定的收敛范围即可。</p><p>RL在状态state( <equation>s_t</equation> )预测是庞大但数量有限的预期收益折现累加和。</p><p><equation>predict_t= G_t \doteq \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\\=R_t+ \sum_{k=1}^{\infty} \gamma^k R_{t+k+1} </equation> </p><p>RNN在状态 <equation>s_t</equation> 预测的是未来某个节点或者节点很小波动范围内的预期收益。</p><p><equation>predict_t= R_{t+k} + Var(R_{t+k}) </equation> </p><p>然而针对决策过程设计的RL系统，大多数强制假设马尔可夫决策过程(Markov Decision Processes)，当然这在工程应用中具有极好的效果，通常非马尔科夫属性的决策过程也可以通过MDP有效的近似。然而不同于工业应用场景，虚拟交易员(Agent)在A股市场面对的是一个动态变化的环境，不同于前面的工业应用，A股市场Environment 的规则是动态变动的，并且这种变动通常无法从盘面数据预测。此外，RL系统一般可以通过选择action来控制或影响情景发展，如AlphaGo、DQN等在围棋、游戏上面的动作可以切实影响RL决策系统未来接受的state， 但是Agent的行为难以影响A股市场，或无法捕捉这种行为影响。也就是，Agent对action的决策只能影响到未来获得奖励，而无法对交互环境产生影响。</p><p>Agent for game</p><p><equation>action \to reward\ and \ next\ state</equation> </p><p>Agent for A股</p><p><equation>action \to reward</equation> </p><p><br></p><p><a href="https://www.ricequant.com/community/topic/4167/">实现链接</a></p><p><a href="https://github.com/AlphaSmartDog/DeepLearningNotes/tree/master/Note-5%20Agent%20Thinker/ThinkerV2mini">github</a></p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p><p>2017-10-10 更新粘合RQAlpha版本</p><p>设计Environment净值走势</p><img src="https://pic1.zhimg.com/v2-eb989fcd9a5e7644c95e45906632c52d_r.jpg" data-caption="" data-size="normal" data-rawwidth="947" data-rawheight="356"><p>使用RQAlpha的净值走势</p><img src="https://pic4.zhimg.com/v2-5ca623326786dd272913319894c26b49_r.jpg" data-caption="" data-size="normal" data-rawwidth="1061" data-rawheight="359"><p><a href="https://www.ricequant.com/community/topic/4189/%E7%99%BD%E9%93%B6%E6%9C%9F%E8%B4%A7%E6%8B%A9%E6%97%B6%E6%A1%86%E6%9E%B6-rqalpha%E4%B8%8Edqn%E7%9A%84%E7%AE%80%E5%8D%95%E7%B2%98%E5%90%88">RQ链接</a></p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
