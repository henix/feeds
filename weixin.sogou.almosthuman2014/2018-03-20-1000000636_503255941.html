<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>学界 | 新型循环神经网络IndRNN：可构建更长更深的RNN（附GitHub实现）</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1522348890&amp;src=3&amp;ver=1&amp;signature=bWMRPrpJIVNM1Xblkyjetqyc4AWOJBEJI2RoxLZ4HSs6rtt-NV9LsXrWtvY3RUU-ujvjdFks5Rb93pMb7mVCeSFH2JMsEiOmzpMHztc*rBbrNKuA7V*zoVB8xrvJ*iG8kwoYBweR2dziDrxfaBI50bltM*RD-CMHQfUgucwD5wY=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    学界 | 新型循环神经网络IndRNN：可构建更长更深的RNN（附GitHub实现）                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-20</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;"></span></p><section style="white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="font-size: 14px;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="font-size: 16px;padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">作者：Shuai Li等</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">参与：张倩、黄小天</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 14px;text-align: justify;">近日，澳大利亚伍伦贡大学联合电子科技大学提出一种新型的循环神经网络 IndRNN，不仅可以解决传统 RNN 所存在的梯度消失和梯度爆炸问题，还学习长期依赖关系；此外，借助 relu 等非饱和激活函数，训练之后 IndRNN 会变得非常鲁棒，并且通过堆叠多层 IndRNN 还可以构建比现有 RNN 更深的网络。实验结果表明，与传统的 RNN 和 LSTM 相比，使用 IndRNN 可以在各种任务中取得更好的结果。同时本文还给出了 IndRNN 的 TensorFlow 实现，详见文中 GitHub 链接。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="line-height: 1.75em;"><span style="font-size: 14px;">循环神经网络 (RNN) [16] 已在动作识别 [8]、场景标注 [4] 、语言处理 [5] 等序列学习问题中获得广泛应用，并且成果显著。与卷积神经网络 ( CNN ) 等前馈网络相比，RNN 具有循环连接，其中最后的隐藏状态是到下一状态的输入。状态更新可描述如下：</span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.14316239316239315" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5KoDMUOCXibtM3ROdsKxw2mUAb0ia1eCRZ8wZ2NVeMaL61MC5DaD7zCaw/640?wx_fmt=png" data-type="png" data-w="936" style="width: 434px;height: 62px;"></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">其中 </span><img class="" data-copyright="0" data-ratio="0.3333333333333333" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5fLtY8JhY3yc9EUkyF6r1wJeYOJJfUZ9CpDtyd7TibV1jprFz724gfwA/640?wx_fmt=png" data-type="png" data-w="96" style="width: 61px;height: 20px;"> <span style="font-size: 14px;">和</span><img class="" data-copyright="0" data-ratio="0.3125" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5GKEaksXY3jt5oByAAgibZCmBia5r7raytWGrMFtgNK0q9NZnibsNVjeTA/640?wx_fmt=png" data-type="png" data-w="96" style="width: 58px;height: 20px;"><span style="font-size: 14px;">分别为时间步 t 的输入和隐藏状态。</span><img class="" data-copyright="0" data-ratio="0.19718309859154928" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5C4WhAQnPjLhKmiaUKr37ucmhiajFT5ACGPIuCKoFibib1XHohbHfOM82Qg/640?wx_fmt=png" data-type="png" data-w="142" style="width: 78px;height: 19px;"><span style="font-size: 14px;">、</span><img class="" data-copyright="0" data-ratio="0.28125" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5oDVhYv4atAm9odWpibpQZqNcJIuSAJvDKNAQNq7fgSn3Z2q54uyjAXQ/640?wx_fmt=png" data-type="png" data-w="128" style="width: 77px;height: 22px;"> <span style="font-size: 14px;">和</span><img class="" data-copyright="0" data-ratio="0.34782608695652173" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5BeHvcyibyOI0TxprL93HKiawIpvX3PjYibg6O4kVW4gGeRU1BdKUcnO3Q/640?wx_fmt=png" data-type="png" data-w="92" style="width: 61px;height: 21px;"> <span style="font-size: 14px;">分别为当前输入的权重、循环输入以及神经元偏差，σ 是神经元的逐元素激活函数，N 是该 RNN 层中神经元的数目。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">由于循环权重矩阵不断相乘，RNN 的训练面临着梯度消失和梯度爆炸的问题。长短期记忆 ( LSTM ) [ 10，17 ] 和门控循环单元 ( GRU ) [5] 等若干 RNN 模型可用来解决这些梯度问题。然而，在这些变体中使用双曲正切和 Sigmoid 函数作为激活函数会导致网络层的梯度衰减。因此，构建和训练基于 RNN 的深度 LSTM 或 GRU 其实存在困难。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">相比之下，使用 relu 等非饱和激活函数的现有 CNN 可以堆栈到非常深的网络中 (例如，使用基本卷积层可以堆叠到 20 层以上；使用残差连接可以到 100 层以上 [12])，并且仍然在接受高效的训练。虽然在若干研究 [44, 36] 中已经尝试把残差连接用于 LSTM 模型，但情况并没有明显改善 (上述使用双曲正切和 sigmoid 函数的 LSTM 的梯度衰减是主要原因)。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">此外，现有的 RNN 模型在 ( 1 ) 中使用相同的 </span><img class="" data-copyright="0" data-ratio="0.1328125" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5CDoiblVoNSEDhHs3prBr8rA9rzSsIJVNXSeoZYibF67aCXIePAOjyv7Q/640?wx_fmt=png" data-type="png" data-w="256" style="width: 166px;height: 22px;"><span style="font-size: 14px;">，其中的循环连接连通所有神经元。这使得解释和理解已训练的神经元 (如每个神经元响应哪种模式) 的作用变得困难，因为单个神经元 [18] 的输出的简单可视化很难在不考虑其它神经元的情况下确定一个神经元的功能。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">本文提出了一种新型循环神经网络——独立循环神经网络（IndRNN）。在 IndRNN 中，循环输入用 Hadamard 乘积处理为</span><img class="" data-copyright="0" data-ratio="0.09382151029748284" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5pEENys5yN9MakgEhDG49Tic9GRuEnibaRYcF6MHYqFvaLabWGyuheuZw/640?wx_fmt=png" data-type="png" data-w="874" style="width: 227px;height: 21px;"><span style="font-size: 14px;">。与传统 RNN 相比，它有许多优点，其中包括：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;"></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">通过调节基于时间的梯度反向传播，可以有效地解决梯度消失和爆炸问题。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">利用 IndRNN 可以保留长期记忆，处理长序列。实验表明，IndRNN 可以很好地处理 5000 步以上的序列，而 LSTM 能够处理的序列还不到 1000 步。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">IndRNN 可以很好地利用 relu 等非饱和函数作为激活函数，并且训练之后非常鲁棒。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">IndRNN 可以实现高效的多层堆叠以增加网络的深度，尤其是在层上具有残差连接的情况下。语言建模实验给出了一个 21 层 IndRNN 的实例。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">由于各层神经元相互独立，就很容易解释每层 IndRNN 神经元的行为。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">实验表明，IndRNN 在加法问题、序贯 MNIST 分类、语言建模和动作识别等方面的性能明显优于传统的 RNN 和 LSTM 模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">3. 独立循环神经网络（IndRNN）</span></strong></p><p style="line-height: 1.75em;"><span style="font-size: 14px;">本文提出了一种独立循环神经网络 ( IndRNN )，具体描述如下：</span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.13986013986013987" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW52yYianHOPsPEq6gajWfcDnhL3sEGqRhJ48icicSiavM0RKDUoJPrEarSNA/640?wx_fmt=png" data-type="png" data-w="715" style="width: 408px;height: 57px;"></p><p style="line-height: 1.75em;"><span style="font-size: 14px;">其中循环权重 u 是向量，</span><img class="" data-copyright="0" data-ratio="1.0714285714285714" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5tzNWlicqP9CkgZLy6v7coxpchclrJs0pRKdR3CiaEGdeibicEPdODPiaNiaA/640?wx_fmt=png" data-type="png" data-w="28" style="width: 19px;height: 20px;"><span style="font-size: 14px;">表示 Hadamard 乘积。每一层的每个神经元各自独立，神经元之间的连接可以通过堆叠两层或更多层的 IndRNNs 来实现（见下文）。对于第 n 个神经元，隐藏状态 h_n,t 可以通过下式得出：</span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.12584573748308525" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5SHdxHwZM8Q2tFcia6A7VgnqBd172iaxZbADAibibv07rahSryQ3wkxsSCg/640?wx_fmt=png" data-type="png" data-w="739" style="width: 460px;height: 58px;"></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">其中 w_n 和 u_n 分别是输入权重和循环权重的第 n 行。每个神经元仅在前一时间步从输入和它自己的隐藏状态中接收信息。也就是说，IndRNN 中的每个神经元独立地处理一种类型的时空模型。传统上，RNN 被视为时间上的、共享参数的多层感知器。与传统的 RNN 不同的是，本文提出的 IndRNN 神经网络为循环神经网络提供了一个新视角，即随着时间的推移 (即通过 u ) 独立地聚集空间模式 (即通过 w )。不同神经元之间的相关性可以通过两层或多层的堆叠来加以利用。在这种情况下，下一层的每个神经元处理上一层所有神经元的输出。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">4. 多层 IndRNN</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">如上所述，同一 IndRNN 层中的神经元彼此独立，时间上的跨通道信息通过多层 IndRNN 进行探索。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">IndRNN 基本架构如图 1(a) 所示，其中「weight」和「Recurrent+ ReLU」表示以 relu 作为激活函数的每个步骤的输入处理和循环处理。通过堆叠此基本架构，可以构建深度 IndRNN 网络。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">基于 [13] 中残差层的「预激活」类型的残差 IndRNN 实例见图 1(b)。在每个时间步，梯度都可以从恒等映射直接传播到其他层。由于 IndRNN 解决了随时间累积的梯度爆炸和消失的问题，所以梯度可以在不同的时间步上有效地传播。因此，网络可以更深更长。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="1.1417218543046357" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5mqEWS26uOvRycAWRRFff9MET1jPwnDNl2NLVff2JQCFtKVWv7mztbw/640?wx_fmt=png" data-type="png" data-w="755" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 1：( a ) 为 IndRNN 基本架构图解；( b ) 为残差 IndRNN 架构图解。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">5. 实验</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="0.8264540337711069" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5u3j8Yyw1engv47bJG3tcQb5jvBefv1ATOGFGias17Ilx9WMmALiaPVjQ/640?wx_fmt=png" data-type="png" data-w="1066" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 2：不同序列长度情况下解决相加问题的结果。所有图的图例相同，因此仅在 ( a ) 中示出。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="0.5120671563483735" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5XGcEpE0r2F7KKZYquXHrHv2xAKBUDnBWGT5UrEWdlDiaWNp03yfOg9A/640?wx_fmt=png" data-type="png" data-w="953" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 1：序贯 MNIST 和置换 MNIST(误差率 ( % ) ) 结果。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="0.7505592841163311" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5DnvzCJN7Qv9L3A7LxEf7ibsal4M0J3aUjxia5L4FEsVcfDBWx1QdaPAg/640?wx_fmt=png" data-type="png" data-w="894" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 2：IndRNN 模型的 PTB-c 结果与文献记录结果的对比（基于 BPC）。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="1.0456852791878173" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5hiatvutPXwBRIAJLYCedoiaLHxMb3iaYJv4xicxshdAVicHGF8DnQWr7Q3g/640?wx_fmt=png" data-type="png" data-w="788" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 3：所有基于骨架的方法在 NTU RGB+D 数据集上的结果。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">论文：Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="0.3053783044667274" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicxEuZ91UfMutRlK2nY2vW5Bmiad3RFdpTLdeeIrnMJtU4pvMmS6W0B58ibgTBRqYIvqPhQ8dBbXOmg/640?wx_fmt=png" data-type="png" data-w="1097" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1803.04831</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">摘要：</span></strong><span style="font-size: 14px;">循环神经网络 ( RNN ) 已广泛应用于序列数据的处理。然而，由于众所周知的梯度消失和爆炸问题以及难以保持长期学习的模式，RNN 通常难以训练。长短期记忆 ( LSTM ) 和门控循环单元 ( GRU ) 被用来解决这些问题，但是双曲正切函数和 sigmoid 函数的使用会导致层上梯度衰减。因此，构建可有效训练的深度网络颇具挑战性。此外，每层 RNN 中的所有神经元都连接在一起，它们的运行状况很难解释。针对这些问题，本文提出了一种新的循环神经网络——独立循环神经网络 ( IndRNN )，即同一层的神经元相互独立，跨层连接。我们指出，IndRNN 可以通过简单的调节避免梯度爆炸和消失问题，同时允许网络学习长期依赖关系。此外，IndRNN 可以使用 relu 等非饱和激活函数，训练之后可变得非常鲁棒。通过堆叠多层 IndRNN 可以构建比现有 RNN 更深的网络。实验结果表明，本文中的 IndRNN 能够处理很长的序列 (超过 5000 个时间步)，可以用来构建很深的网络 (实验中使用了 21 层)，并且经过训练还可以更加鲁棒。与传统的 RNN 和 LSTM 相比，使用 IndRNN 可以在各种任务中取得更好的结果。</span></p><p style="text-align: center;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 16px;"><strong>GitHub实现</strong></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">上文是 IndRNN 具体的论文简介，论文作者其实还提供了该循环架构的 TensorFlow 实现代码和试验结果。我们发现架构代码中有非常详尽的参数解释，因此各位读者可参考 ind_rnn_cell.py 文件详细了解 IndRNN 的基本架构。此外，作者表示该实现使用 Python 3.4 和 TensorFlow 1.5 完成，所以我们可以在该环境或更新的版本测试。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(123, 12, 0);">项目地址：https://github.com/batzner/indrnn</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">1. 用法</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">将 ind_rnn_cell.py 文件复制到你的项目目录中，如下展示了 IndRNN 单元的简单调用过程：</span></p><pre style="box-sizing: border-box;margin-top: 0px;margin-bottom: 0px;padding: 0px;font-size: 16px;color: rgb(62, 62, 62);line-height: inherit;font-variant-ligatures: normal;orphans: 2;widows: 2;background-color: rgb(255, 255, 255);"><code class="python language-python hljs" style="box-sizing: border-box;margin-right: 2px;margin-left: 2px;padding: 0.5em;font-size: 14px;color: rgb(169, 183, 198);line-height: 18px;border-top-left-radius: 0px;border-top-right-radius: 0px;border-bottom-right-radius: 0px;border-bottom-left-radius: 0px;background-color: rgb(40, 43, 46);font-family: Consolas, Inconsolata, Courier, monospace;display: block;overflow-x: auto;letter-spacing: 0px;word-wrap: normal !important;word-break: normal !important;overflow-y: auto !important;background-position: initial initial;background-repeat: initial initial;"><span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">from</span> ind_rnn_cell <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">import</span> IndRNNCell<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><span class="hljs-comment" style="box-sizing: border-box;font-size: inherit;color: rgb(128, 128, 128);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"># Regulate each neuron's recurrent weight as recommended in the paper</span><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">recurrent_max = pow(<span class="hljs-number" style="box-sizing: border-box;font-size: inherit;color: rgb(174, 135, 250);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">2</span>, <span class="hljs-number" style="box-sizing: border-box;font-size: inherit;color: rgb(174, 135, 250);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">1</span> / TIME_STEPS)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">cell = MultiRNNCell([IndRNNCell(<span class="hljs-number" style="box-sizing: border-box;font-size: inherit;color: rgb(174, 135, 250);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">128</span>, recurrent_max_abs=recurrent_max),<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">                     IndRNNCell(<span class="hljs-number" style="box-sizing: border-box;font-size: inherit;color: rgb(174, 135, 250);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">128</span>, recurrent_max_abs=recurrent_max)])<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">output, state = tf.nn.dynamic_rnn(cell, input_data, dtype=tf.float32)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">...</code></pre><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">2. 原论文中提到的实验</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">有关本文中重构「相加问题」的相关脚本，请参见示例 examples/addition_rnn.py。更多实验（如 Sequential MNIST）将在今后几天进行更新与展示。</span><img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOd6kYzKSNx7GpKhf1OMhgW30B8WEsyibXYuvBogNHE5TQTpUQGLsWmeQ/640?wx_fmt=png" data-type="png" data-w="73" width="51px" style="font-size: 16px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 51px !important;"></p><p style="white-space: normal;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="white-space: normal;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="margin-bottom: 20px;white-space: normal;max-width: 100%;min-height: 1em;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="701561280" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                <p class="media_tool_meta tips_global meta_primary article_modify_tag">修改于<span id="js_modify_time"></span></p>
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                <p class="media_tool_meta tips_global meta_primary article_modify_tag">修改于<span id="js_modify_time"></span></p>
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
