<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>学界 | 神奇！只有遗忘门的LSTM性能优于标准LSTM</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1525644618&amp;src=3&amp;ver=1&amp;signature=w-8opdlfaEju7wnjiKlYb3nD23Ag*5KjMl9qMqzEe4cnJB5xuej0CPz8Uy8fliNI0OPGobmwGFs-pu-48CWP5Hemw3K0-yM2SCRU6G6ULYckm5*rumaelZ9FQx2BxrB7ENaY8jpU1Ak8rheGLCGKcsTx6dWkTuZUr7yNBaEMsoM=">原文</a></p>
<div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        <div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    学界 | 神奇！只有遗忘门的LSTM性能优于标准LSTM                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-27</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="max-width: 100%;font-size: 14px;text-align: justify;font-family: -webkit-standard;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;text-align: justify;font-family: -webkit-standard;box-sizing: border-box !important;word-wrap: break-word !important;">Jos van der Westhuizen、Joan Lasenby</span></strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：<span style="font-family: -webkit-standard;">Pedro、路</span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">本论文研究 LSTM 只有遗忘门的话会怎样，并提出了 JANET，实验表明该模型的性能优于标准 LSTM。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><strong>1.介绍</strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">优秀的工程师确保其设计是实用的。目前我们已经知道解决序列分析问题最好的方式是长短期记忆（LSTM）循环神经网络，接下来我们需要设计一个满足资源受限的现实世界应用的实现。鉴于使用两个门的门控循环单元（Cho 等，2014）的成功，第一种设计更硬件高效的 LSTM 的方法可能是消除冗余门（redundant gate）。因为我们要寻求比 GRU 更高效的模型，所以只有单门 LSTM 模型值得我们研究。为了说明为什么这个单门应该是遗忘门，让我们从 LSTM 的起源讲起。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在那个训练循环神经网络（RNN）十分困难的年代，Hochreiter 和 Schmidhuber（1997）认为在 RNN 中使用单一权重（边）来控制是否接受记忆单元的输入或输出带来了冲突性更新（梯度）。本质上来讲，每一步中长短期误差（long and short-range error）作用于相同的权重，且如果使用 sigmoid 激活函数的话，梯度消失的速度要比权重增加速度快。之后他们提出长短期记忆（LSTM）单元循环神经网络，具备乘法输入门和输出门。这些门可以通过「保护」单元免受不相关信息（其他单元的输入或输出）影响，从而缓解冲突性更新问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">LSTM 的第一个版本只有两个门：Gers 等人（2000）首先发现如果没有使记忆单元遗忘信息的机制，那么它们可能会无限增长，最终导致网络崩溃。为解决这个问题，他们为这个 LSTM 架构加上了另一个乘法门，即遗忘门，完成了我们今天看到的 LSTM 版本。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">鉴于遗忘门最新发现的重要性，那么设想 LSTM 仅使用一个遗忘门，输入和输出门是否必要呢？本研究将探索单独使用遗忘门的优势。在五个任务中，仅使用遗忘门的模型提供了比使用全部三个 LSTM 门的模型更好的解决方案。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><strong>3 JUST ANOTHER NETWORK</strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们提出了一个简单的 LSTM 变体，其只有一个遗忘门。它是 Just Another NETwork，因此我们将其命名为 JANET。我们从标准 LSTM（Lipton 等，2015）开始，其中符号具备标准含义，定义如下</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p><img class="" data-copyright="0" data-ratio="0.2686366689053056" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPANzFAA7OBciaP05QMV1QqzjPtkB4IWgItibsLov9dqO3mic89qOo6t6fzA/640?wx_fmt=png" data-type="png" data-w="1489" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了将上述内容转换成 JANET 架构，我们删除了输入和输出门。将信息的累积和删除关联起来似乎是明智的，因此我们将输入和遗忘调制结合起来，就像 Greff et al. (2015) 论文中所做的那样，而这与 leaky unit 实现 (Jaeger, 2002, §8.1) 类似。此外，h_t 的 tanh 激活函数使梯度在反向传播期间出现收缩，这可能加剧梯度消失问题。权重 U∗ 可容纳 [-1,1] 区间外的值，因此我们可移除这个不必要且可能带来问题的 tanh 非线性函数。得出的 JANET 结果如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.18175018698578907" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAiclzGa5Y99fhbXxib6MIqjzDxj5tIiahwlF2bAZ5QnL64swOViaHp1QfjA/640?wx_fmt=png" data-type="png" data-w="1337" style=""></p><p style="text-align: center;"><strong>4 实验与结果</strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.3548387096774194" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAKoAyv68fib7EaLv9yIZkX4mfAWubRNrXQMExVZMbWwMAzME61ib9yicsQ/640?wx_fmt=png" data-type="png" data-w="961" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">表 1：不同循环神经网络架构的准确率 [%]。图中展示了 10 次独立运行得到的平均值和标准差。我们实验中的最佳准确率结果以及引用论文中的最佳结果以粗体显示。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">令人惊讶的是，结果表明 JANET 比标准 LSTM 的准确率更高。此外，JANET 是在所有分析数据集上表现最佳的模型之一。因此，通过简化 LSTM，我们不仅节省了计算成本，还提高了测试集上的准确率！</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.6199376947040498" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPA2I7bogU4R1iaIqy49chjeTOvayIFwwxPNIiaUUOTSRIIOTNuiaJIlR8zg/640?wx_fmt=png" data-type="png" data-w="963" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 1：在 MNIST 和 pMNIST 上训练的 LSTM 的测试准确率。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.5893385982230998" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAYqMD8v394xmo5jQicaOLJJe3sP64sfwSLQZDX1YCSn6RWicDS22G8Mbw/640?wx_fmt=png" data-type="png" data-w="1013" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 2：JANET 和 LSTM 在 MNIST 上训练时的测试集准确率对比。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.6094552929085303" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAVjhFciayFOQgvY51gDE1gk9ZfiaHkhtFkPgJftRiayWfudbpjN9Cbnpow/640?wx_fmt=png" data-type="png" data-w="973" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 3：不同层大小的 JANET 和 LSTM 在 pMNIST 数据集上的准确率（％）。</span></em></span></p><p><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><strong>论文：THE UNREASONABLE EFFECTIVENESS OF THE FORGET GATE</strong></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.2893539581437671" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPA0jSTvUgUTl9niadV1lJFJ3YlgSSMRwM1gRoBaZ8a57ibAwXR51ZY0WZQ/640?wx_fmt=png" data-type="png" data-w="1099"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(123, 12, 0);font-size: 15px;">论文链接：https://arxiv.org/abs/1804.04849</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">摘要：鉴于门控循环单元（GRU）的成功，一个很自然的问题是长短期记忆（LSTM）网络中的所有门是否是必要的。之前的研究表明，遗忘门是 LSTM 中最重要的门之一。这里我们发现，一个只有遗忘门且带有 chrono-initialized 偏置项的 LSTM 版本不仅能节省计算成本，而且在多个基准数据集上的性能优于标准 LSTM，能与一些当下最好的模型竞争。我们提出的网络 JANET，在 MNIST 和 pMNIST 数据集上分别达到了 99% 和 92.5% 的准确率，优于标准 LSTM 98.5% 和 91% 的准确率。<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);text-align: justify;white-space: normal;font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p>
                </div>
                <script nonce="2121268281" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                        
            <ul id="js_hotspot_area" class="article_extend_area"></ul>


            
                        <div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>

        
        <div class="rich_media_area_extra">

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_friend_cmt_area" style="display:none">
              
              
              
            </div>

                        <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_area" style="display:none">
            </div>
                    </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div><div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    学界 | 神奇！只有遗忘门的LSTM性能优于标准LSTM                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-27</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="max-width: 100%;font-size: 14px;text-align: justify;font-family: -webkit-standard;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;text-align: justify;font-family: -webkit-standard;box-sizing: border-box !important;word-wrap: break-word !important;">Jos van der Westhuizen、Joan Lasenby</span></strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：<span style="font-family: -webkit-standard;">Pedro、路</span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">本论文研究 LSTM 只有遗忘门的话会怎样，并提出了 JANET，实验表明该模型的性能优于标准 LSTM。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><strong>1.介绍</strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">优秀的工程师确保其设计是实用的。目前我们已经知道解决序列分析问题最好的方式是长短期记忆（LSTM）循环神经网络，接下来我们需要设计一个满足资源受限的现实世界应用的实现。鉴于使用两个门的门控循环单元（Cho 等，2014）的成功，第一种设计更硬件高效的 LSTM 的方法可能是消除冗余门（redundant gate）。因为我们要寻求比 GRU 更高效的模型，所以只有单门 LSTM 模型值得我们研究。为了说明为什么这个单门应该是遗忘门，让我们从 LSTM 的起源讲起。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在那个训练循环神经网络（RNN）十分困难的年代，Hochreiter 和 Schmidhuber（1997）认为在 RNN 中使用单一权重（边）来控制是否接受记忆单元的输入或输出带来了冲突性更新（梯度）。本质上来讲，每一步中长短期误差（long and short-range error）作用于相同的权重，且如果使用 sigmoid 激活函数的话，梯度消失的速度要比权重增加速度快。之后他们提出长短期记忆（LSTM）单元循环神经网络，具备乘法输入门和输出门。这些门可以通过「保护」单元免受不相关信息（其他单元的输入或输出）影响，从而缓解冲突性更新问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">LSTM 的第一个版本只有两个门：Gers 等人（2000）首先发现如果没有使记忆单元遗忘信息的机制，那么它们可能会无限增长，最终导致网络崩溃。为解决这个问题，他们为这个 LSTM 架构加上了另一个乘法门，即遗忘门，完成了我们今天看到的 LSTM 版本。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">鉴于遗忘门最新发现的重要性，那么设想 LSTM 仅使用一个遗忘门，输入和输出门是否必要呢？本研究将探索单独使用遗忘门的优势。在五个任务中，仅使用遗忘门的模型提供了比使用全部三个 LSTM 门的模型更好的解决方案。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><strong>3 JUST ANOTHER NETWORK</strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们提出了一个简单的 LSTM 变体，其只有一个遗忘门。它是 Just Another NETwork，因此我们将其命名为 JANET。我们从标准 LSTM（Lipton 等，2015）开始，其中符号具备标准含义，定义如下</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p><img class="" data-copyright="0" data-ratio="0.2686366689053056" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPANzFAA7OBciaP05QMV1QqzjPtkB4IWgItibsLov9dqO3mic89qOo6t6fzA/640?wx_fmt=png" data-type="png" data-w="1489" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了将上述内容转换成 JANET 架构，我们删除了输入和输出门。将信息的累积和删除关联起来似乎是明智的，因此我们将输入和遗忘调制结合起来，就像 Greff et al. (2015) 论文中所做的那样，而这与 leaky unit 实现 (Jaeger, 2002, §8.1) 类似。此外，h_t 的 tanh 激活函数使梯度在反向传播期间出现收缩，这可能加剧梯度消失问题。权重 U∗ 可容纳 [-1,1] 区间外的值，因此我们可移除这个不必要且可能带来问题的 tanh 非线性函数。得出的 JANET 结果如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.18175018698578907" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAiclzGa5Y99fhbXxib6MIqjzDxj5tIiahwlF2bAZ5QnL64swOViaHp1QfjA/640?wx_fmt=png" data-type="png" data-w="1337" style=""></p><p style="text-align: center;"><strong>4 实验与结果</strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.3548387096774194" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAKoAyv68fib7EaLv9yIZkX4mfAWubRNrXQMExVZMbWwMAzME61ib9yicsQ/640?wx_fmt=png" data-type="png" data-w="961" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">表 1：不同循环神经网络架构的准确率 [%]。图中展示了 10 次独立运行得到的平均值和标准差。我们实验中的最佳准确率结果以及引用论文中的最佳结果以粗体显示。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">令人惊讶的是，结果表明 JANET 比标准 LSTM 的准确率更高。此外，JANET 是在所有分析数据集上表现最佳的模型之一。因此，通过简化 LSTM，我们不仅节省了计算成本，还提高了测试集上的准确率！</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.6199376947040498" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPA2I7bogU4R1iaIqy49chjeTOvayIFwwxPNIiaUUOTSRIIOTNuiaJIlR8zg/640?wx_fmt=png" data-type="png" data-w="963" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 1：在 MNIST 和 pMNIST 上训练的 LSTM 的测试准确率。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.5893385982230998" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAYqMD8v394xmo5jQicaOLJJe3sP64sfwSLQZDX1YCSn6RWicDS22G8Mbw/640?wx_fmt=png" data-type="png" data-w="1013" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 2：JANET 和 LSTM 在 MNIST 上训练时的测试集准确率对比。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.6094552929085303" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAVjhFciayFOQgvY51gDE1gk9ZfiaHkhtFkPgJftRiayWfudbpjN9Cbnpow/640?wx_fmt=png" data-type="png" data-w="973" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 3：不同层大小的 JANET 和 LSTM 在 pMNIST 数据集上的准确率（％）。</span></em></span></p><p><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><strong>论文：THE UNREASONABLE EFFECTIVENESS OF THE FORGET GATE</strong></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.2893539581437671" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPA0jSTvUgUTl9niadV1lJFJ3YlgSSMRwM1gRoBaZ8a57ibAwXR51ZY0WZQ/640?wx_fmt=png" data-type="png" data-w="1099"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="color: rgb(123, 12, 0);font-size: 15px;">论文链接：https://arxiv.org/abs/1804.04849</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">摘要：鉴于门控循环单元（GRU）的成功，一个很自然的问题是长短期记忆（LSTM）网络中的所有门是否是必要的。之前的研究表明，遗忘门是 LSTM 中最重要的门之一。这里我们发现，一个只有遗忘门且带有 chrono-initialized 偏置项的 LSTM 版本不仅能节省计算成本，而且在多个基准数据集上的性能优于标准 LSTM，能与一些当下最好的模型竞争。我们提出的网络 JANET，在 MNIST 和 pMNIST 数据集上分别达到了 99% 和 92.5% 的准确率，优于标准 LSTM 98.5% 和 91% 的准确率。<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);text-align: justify;white-space: normal;font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p>
                </div>
                <script nonce="2121268281" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><ul id="js_hotspot_area" class="article_extend_area"></ul><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
