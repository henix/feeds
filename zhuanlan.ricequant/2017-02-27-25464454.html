<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>PonderRNN对HS300指数预测</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/25464454">原文</a></p>
<div class="title-image"><img src="https://pic3.zhimg.com/v2-4c42d7fc5749dbeba7ce85d1a110b0c7_r.jpg" alt=""></div><p>前几天看博客看到Alex Graves设计的ACT (Adaptive Computation Time Graves, 2016)通过构建包裹函数实现RNN模型在时间序列t可以进行n迭代计算，取权重累加和作为时间序列在时刻t的输出，其核心逻辑为通过拉长RNN的在时间序列上的长度增强RNN模型的复杂性，提高非线性表达。</p><p>由于RNN使用tanh激活函数，超过5层的RNN计算成本很大，近两年的论文认为拉长RNN的时间序列长度可以提高RNN模型的效果，通过对一个时刻的RNN反复迭代计算并输出权重和作为时间序列在时刻t的输出，可以变相拉长RNN的长度。ACT模型通过使用时间损失函数来压迫模型学习在有限时间（有限的迭代步长）学习判断输出。</p><p>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p><p>2017-08-09更新</p><p><a href="https://www.ricequant.com/community/topic/2644//2">代码实现</a></p><p>自适应计算时间算法是一种允许神经网络常态性的学习在输入和输出之间需要执行多少计算步骤的算法，根据具体问题的复杂度动态调整模型使用的计算量，该网络结构主要针对如下问题设计：</p><ul><li>RNN的计算时间由给定问题的序列长度和设计者指定，无法自行变动适应具体问题</li><li>具有高度复杂变化的序列数据处理</li><li>研究表明，在条件受限无法加深RNN模型深度的情况下，拉成RNN时间轴长度也能够提升RNN精度</li></ul><p><br></p><img src="https://pic2.zhimg.com/v2-39b6d7d4a6a2a000daf210d4b5faaba1_r.png" data-rawwidth="916" data-rawheight="526"><p>上图显示了ACT模型通过嵌套包裹RNN单元来实现自适应计算时间的调整的操作方法，两条虚线之间的时间展开对应传统RNN模型的时刻i的一步计算，相当于传统RNN模型在时刻i也就是序列位置i只进行一次计算，而ACT在时刻i或者序列位置i可以自行调整计算次数提升模型泛化效果。</p><p>标准RNN迭代公式如下：</p><p><equation>s_t = S(s_{t-1}, x_t)\\ y_t = W_y s_t + b_y</equation></p><p>ACT修改RNN迭代公式为：<br></p><p><equation>s_t^n = \begin{cases} S(s_{t-1}, x_t^1) \;\;\; if \;\; n = 1 \\ S(s_t^{n-1},x_t^n) \;\;\; otherwise\\ \end{cases}</equation></p><p>即在时刻t进行n次迭代计算，并使用halting probability unit决定每个输入步的更新量：<br></p><p><equation>p_t^n =\begin{cases} R(t) \;\;\; if \;\; n=N(t)\\ h_t^n \;\;\; otherwise \end{cases}\\ </equation></p><p><equation>h_t^n = \sigma(W_hs_t^n + b_h)\\</equation></p><p>并设置最大迭代限制N防止网络在某步进程中无限制迭代计算,<br></p><p><equation>N(t) = \min { n':\sum_{n=1}^{n'}} h_t^n &gt;=1-\epsilon\\ R(t) = 1- \sum_{n=1}^{N(t)-1}h_t^n\\</equation></p><p>最终在时刻t的更新量为：<br></p><p><equation>s_t = \sum _{n=1}^{N(t)}p_t^n s_t^n\\ y_t = \sum_{n=1}^{N(t)}p_t^ny_t^n\\</equation></p><p>通过使用包含思考代价的复合损失函数<equation>\hat L(x,y) = L(x,y) + \tau P(x)</equation>,该模型被设计为鼓励神经网络快速进行判断而不是一味的追求精度消耗大量计算资源。<br><equation>P(x) = \sum _{t=1}^T p_t \\</equation></p><p><equation> p_t = N(t) + R(t)</equation></p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
