<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>AI学习笔记：[1]神经元与神经网络</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1554780790&amp;src=3&amp;ver=1&amp;signature=Xp3SH0gv6cVRkak8MiDWFqZ6OJM3-m36CFmhvhJWQZuVv8vHm6scAO417OcspkqFBBj7gQ*bu1Ujkrly4ZjpZ1ESUBloN6ZqBLH8*jueknXd-fnxE3EQOM1ErkqZ5BAIwVY1xQQDoWk4m0RHanObcWyDW6hLVC5vvyeqN9tCa5A=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    AI学习笔记：[1]神经元与神经网络                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2017-09-10</em>

                                        <em class="rich_media_meta rich_media_meta_text">特邀作者：Liang</em>
                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">ImFanny</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">ImFanny</span>

                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">ImFanny</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value"></span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">Hello, I'm Fanny. Nice to meet you.</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <blockquote style=" padding: 20px; margin-bottom: 25px; border-left-width: 6px; border-left-color: rgb(180, 180, 180); background-color: whitesmoke; word-break: break-word; font-size: 16px; line-height: 30px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; "><p style="margin-bottom: 25px; line-height: 1.7; word-break: break-word;">写在前面：</p><p style="margin-bottom: 25px; line-height: 1.7; word-break: break-word;">1、文中提及的阅读材料和图书，都有链接附带，到“阅读原文”中查看本文的简书版本可以一一访问。</p><p style="margin-bottom: 25px; line-height: 1.7; word-break: break-word;">2、文中涉及到一些上标、下标的显示，但遗憾的是微信后台的编辑器并不支持，想获得更好的阅读体验，请查看“阅读原文”。</p><p style="line-height: 1.7; word-break: break-word;"><span style="color: rgb(217, 33, 66);">如果你喜欢本文，请关注本账号，预计会以一周一篇的频率进行更新~</span><span style="color: rgb(0, 128, 255);"><br></span></p></blockquote><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">学习吴恩达在网易云课堂上的<span style="color: rgb(0, 128, 255);">《深度学习工程师》</span>课程有一周了，差不多学完了4周的课程（只是理论学习，而非动手实践）。</p><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><strong><span style="font-size: 20px;">1. 学习材料</span></strong></h2><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>在看吴恩达的课程前，我找了不少资料，比如：<br></p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p><span style="color: rgb(0, 128, 255);">Deep Learning（深度学习）学习笔记整理系列</span></p></li><li><p><span style="color: rgb(0, 128, 255);">关于深度学习，看这一篇就够了</span></p></li><li><p><span style="color: rgb(0, 128, 255);">深度学习如何入门</span></p></li><li><p><span style="color: rgb(0, 128, 255);">台湾李宏毅教授《1天搞懂深度学习》</span></p></li><li><p><span style="color: rgb(0, 128, 255);">DeepLearningBook读书笔记</span></p></li><li><p><span style="color: rgb(0, 128, 255);">Tensorflow中文社区</span></p></li><li><p><span style="color: rgb(0, 128, 255);">神经网络入门</span></p></li><li><p><span style="color: rgb(0, 128, 255);">Andrew Ng机器学习入门学习笔记</span></p></li><li><p><span style="color: rgb(0, 128, 255);">超智能体-分享最通俗易懂的深度学习教程</span></p></li><li><p><span style="color: rgb(0, 128, 255);">无痛的机器学习</span></p></li></ol><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">需要的话还可以列出更多，但是这些只会让你越看越糊涂，什么是Sigmoid、LSTM、RNN、CNN，然后再加上一堆推导公式，让人直接晕菜。相比之下，吴恩达的课程则是从基本的理论开始，更单纯清晰，并辅于简单的Numpy的方法调用，很容易理解。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">有时候学习太着急了反而不容易学好，因此还是<span style="box-sizing: border-box; font-weight: 700;">推荐踏踏实实将这个课程视频看完</span>。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">当然课余资料仍然是不可避免的，我看了如下一些资料值得推荐：</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>书籍<span style="color: rgb(0, 128, 255);"> 《神经网络与深度学习》</span>：其中关于神经网络的历史、感知机、以及基于Neuroph这个Java神经网络框架的演示非常棒（我目前只阅读到这部分）</p></li><li><p>博客<span style="color: rgb(0, 128, 255);"> 《神经网络入门》</span>：形象讲解了神经元的工作原理、如何通过数学来进行建模</p></li><li><p>知乎专栏<span style="color: rgb(0, 128, 255);"># 无痛的机器学习第一季</span>：其中对于梯度下降算法等的解释非常详细，其他的内容我尚未阅读。</p></li></ol><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></h2><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><strong><span style="font-size: 20px;">2. 吴恩达课程大纲</span></strong></h2><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>吴恩达的课程分为5个部分：<br></p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>神经网络和深度学习</p></li><li><p>改善深层神经网络：超参数调试、正则化以及优化</p></li><li><p>结构化机器学习项目</p></li><li><p>卷积神经网络</p></li><li><p>序列模型</p></li></ol><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>其中第一部分“神经网络和深度学习”又分为4周的课程：<br></p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>深度学习概论：深度神经网络可以干什么</p></li><li><p>神经网络基础：logistic回归、梯度下降法、计算图、向量化、以及python的numpy和Jupyter的使用</p></li><li><p>浅层神经网络：神经网络表示、激活函数、激活函数的导数、神经网络的梯度下降法</p></li><li><p>深层神经网络：深层神经网络的表示、前向和后向传播、超参数</p></li></ol><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>我总结成了以下思维导图，其中红色部分是本文涉及到的。</p><p><img class="" data-ratio="0.31072210065645517" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVZ9nTkzlsusPHEQe9bvUAhfloPRdFUSadZicv0IudbxXYbiaqscjPBXmA/0?wx_fmt=png" data-type="png" data-w="914" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">下面，进入正文。</p><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><strong><span style="font-size: 20px;">3. 神经元</span></strong></h2><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>要理解什么是神经网络，需要从什么是神经元以及如何对神经元进行抽象建模进行了解，神经元也就是神经细胞。</p><p><img class="" data-ratio="0.47338709677419355" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVDFiaxYEnuflWIRWmg3tfl8kLABD0V5icQTd8hZOSXibzrTsoONsj7Ydxg/0?wx_fmt=png" data-type="png" data-w="1240" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out; width: 100%; height: auto;"><br style="box-sizing: border-box;"></p><p><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">上图是生物课上学习到过的神经元，比较有意思的是如下两个组成部分：</p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>细胞核周边冒出来的分支称为“树突”，用于接收刺激信号</p></li><li><p>中间长长的丝称为“轴突”，其末端也有一些分支，和其他神经元的“树突”连在一起，作用是将信号传递给其他神经细胞</p></li></ol><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>人类的中枢神经系统中约有1000亿个神经元，每个神经元大概与1万个其他神经元相连，要用电脑去模拟人类的神经系统即便是大数据时代也是非常困难的。甚至有人将人类的大脑比喻成宇宙，因为宇宙中的恒星的数量可能和大脑中的神经元是差不多量级的，因此说每个人都顶着一个宇宙。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">神经元的工作模式非常简单，接受来自“树突”的信号，来决定是否要“激发”，并且将状态通过“轴突”传递给下一个神经元。可以用简单的数据模型描述如下。</p><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><strong><span style="font-size: 20px;">4.神经元的数据模型</span></strong></h2><p><strong><span style="font-size: 20px;"><br></span></strong></p><p><img class="" data-ratio="0.4096774193548387" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVbE5FNk3qVkXFBzUabMicfVxiaDiaKJRkbXxXJibyMDVXVapdUwMKE3wgAQ/0?wx_fmt=png" data-type="png" data-w="1240" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out; width: 100%; height: auto;"><br style="box-sizing: border-box;"></p><p style="text-align: center;"><span style="color: rgb(136, 136, 136);">神经元建模</span></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">在吴恩达的课程中，输入被描述成</p><p style="text-align: center;"><img class="" data-ratio="0.7127659574468085" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVHnCGqHLicPtzDBMjZs95D40sibKgrDQOxTtFdbOhMG1UT66wrMKgAlSg/0?wx_fmt=gif" data-type="gif" data-w="94" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">信号处理函数分为两部分（<span style="color: rgb(217, 33, 66);"><strong>这两个部分很重要，是后续所有探讨的基石，一定要先搞懂</strong></span>）：<br></p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p><strong>权重加权</strong>：每个输入信号x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">1</span>, x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">2</span>, x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">3</span>，对应的权重分别为w<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">1</span>, w<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">2</span>, w<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">3</span>，然后加上内部强度（用 b 表示）</p></li><li><p><strong>激活函数</strong>：用 a=σ(z) 表示</p></li></ol><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">整个公式表达为：</p><p style="text-align: center;"><img class="" data-ratio="0.07727272727272727" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVZKicw8U7FfbV43zErVZicHBocDD1BlytQ0NEe14AFngBnM8C5mOvK1mQ/0?wx_fmt=gif" data-type="gif" data-w="220" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br style=" box-sizing: border-box; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; "></p><p style="text-align: center;"><img class="" data-ratio="0.26865671641791045" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVmHk3qSFNaTC1LMbtV4xtk58XLL8Slhz9ZpJbn9kBJM10XVzRna9cgQ/0?wx_fmt=gif" data-type="gif" data-w="67" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">使用向量表示，则为：</p><p style="text-align: center;"><img class="" data-ratio="0.18556701030927836" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzViaW1j9PE3rictsrSXqbDKnDdfdRq5GqZ41JkF26icGHKicREiaaOibMrZibCA/0?wx_fmt=gif" data-type="gif" data-w="97" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p><br style=" box-sizing: border-box; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; "></p><p style="text-align: center;"><img class="" data-ratio="0.26865671641791045" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVmHk3qSFNaTC1LMbtV4xtk58XLL8Slhz9ZpJbn9kBJM10XVzRna9cgQ/0?wx_fmt=gif" data-type="gif" data-w="67" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">其中：</p><p style="text-align: center;"><img class="" data-ratio="0.67" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVvHag500wiaPbPGVAIMJgIfCdVS7MsPDo3aMiaFgtKjhjGZ4HAPJRMibdQ/0?wx_fmt=gif" data-type="gif" data-w="100" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">如果你看过我的上一篇文章<span style="color: rgb(0, 128, 255);">《AI学习笔记：[0]什么是矩阵》</span>，你就会发现那篇文章最后一部分提到了这个公式。这里再重新叙述一次：w和x都是 3x1 的列向量，其中w转置后为 1x3 的行向量，因此与x相乘后为标量（实数），然后和 b 相加就得到标量 z。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">z被代入到激活函数 a=σ(z) 得到神经元的输出，这里的 a 表示神经元的激活状态。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">注意到这里的σ(z)还没有具体展开。 σ(z)被称为<span style="box-sizing: border-box; font-weight: 700;">激活函数</span>，具体有很多种，在整个神经网络中不同层的神经元可以使用不同的激活函数，以下将列举几种。</p><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><strong><span style="font-size: 20px;">5. 激活函数<br><br></span></strong></h2><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">σ(z)激活函数有好几种，吴恩达讲的比较多的是这四种：</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><span style="box-sizing: border-box; font-weight: 700;">Sigmoid函数</span>（知名度最高，但其实只是激活函数的其中一种）</p><p style="text-align: center;"><img class="" data-ratio="0.42391304347826086" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVoWnIiaJCwyobNjzYN46z3wnP06jHKfz5ZibmOn0mTdBd2ca9vGdlVqmQ/0?wx_fmt=gif" data-type="gif" data-w="92" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><span style="box-sizing: border-box; font-weight: 700;"><br></span></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><span style="box-sizing: border-box; font-weight: 700;">Tanh函数</span></p><p style="text-align: center;"><img class="" data-ratio="0.3939393939393939" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVZibgIwslMD0pAP7UhlOVHl8uNhGRpia9VvZ4rRVSY4oMD60zZaVgicRicQ/0?wx_fmt=gif" data-type="gif" data-w="99" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><span style="box-sizing: border-box; font-weight: 700;"><br></span></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><span style="box-sizing: border-box; font-weight: 700;">ReLu函数</span></p><p style="text-align: center;"><img class="" data-ratio="0.16666666666666666" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVSYfJ2rCSdHk6YkTa1aMtWGo4gLiavheiaBJIQkhya5icic5tnswBwicLmKw/0?wx_fmt=gif" data-type="gif" data-w="108" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><span style="box-sizing: border-box; font-weight: 700;"><br></span></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><span style="box-sizing: border-box; font-weight: 700;">Leaky ReLu函数</span></p><p style="text-align: center;"><img class="" data-ratio="0.12857142857142856" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVI1ia2rHAWRSia7xgiba1AIbM1bZbQ4iaZic3IBQVOQ7ovDKekkwEy95g2jQ/0?wx_fmt=gif" data-type="gif" data-w="140" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">这四个函数的图形分别为：</p><p><img class="" data-ratio="0.34919354838709676" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVwoicu1I95kpiaGl6NHK19ibOcL8M7WhRJVpHGCnBdkJPPbnsicjnzN4WPQ/0?wx_fmt=png" data-type="png" data-w="1240" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out; width: 100%; height: auto;"><br style="box-sizing: border-box;"></p><p><br></p><p><span style=" background-color: rgb(255, 255, 255); color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ">吴恩达在课程中说：<br></span><br></p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>几乎所有的情况下 Tanh 都要比 Sigmoid 要好</p></li><li><p>默认用 ReLu 就很好，可以参考下 ReLu(Rectified Linear Units)激活函数</p></li></ol><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>至于为什么是这样，应该需要阅读好多论文才能懂，暂且先记住吧。<br></p><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><strong><span style="font-size: 20px;">6. 神经网络</span></strong></h2><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>整个神经网络分为：输入层，隐藏层，输出层。一般我们说L层神经网络，指的是有L个隐层，输入层和输出层都不计算在内的。</p><p><img class="" data-ratio="0.6701612903225806" src="http://mmbiz.qpic.cn/mmbiz_jpg/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVbia9aD6GhJy0xMKKFmJJYz8ZsNJMzewhdibO3rjbDWgFg271hpZz8Wng/0?wx_fmt=jpeg" data-type="jpeg" data-w="1240" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out; width: 100%; height: auto;"><br style="box-sizing: border-box;"></p><p style="text-align: center;"><span style="color: rgb(136, 136, 136);">图：两层神经网络</span></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">在吴恩达的课程中，神经网络分为 <span style="box-sizing: border-box; font-weight: 700;">浅层神经网络</span> 和 <span style="box-sizing: border-box; font-weight: 700;">深层神经网络</span>，下图是 <em style="box-sizing: border-box;">神经网络和深度学习Week4</em> 中的图。</p><p style="white-space: normal;"><img class="" data-ratio="0.5447839831401475" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVicY9SLwTMOrJiasGib1cs4R96IcZ0l7Z8dvUJ5h2eZOlyRkDpkTmcUUjw/0?wx_fmt=png" data-type="png" data-w="949" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="white-space: normal; text-align: center;"><span style="color: rgb(136, 136, 136);">图：神经网络的分类</span><br></p><p style="white-space: normal;"><br></p><ol style="margin-left: 22px;" class=" list-paddingleft-2"><li><p>一个神经元被称为 <em style="box-sizing: border-box;">逻辑斯蒂回归（logistic regression）</em></p></li><li><p>隐层（hidden layer）较少的被称为 <em style="box-sizing: border-box;">浅层</em>，而隐层较多的（比如这个图中的5 hidden layer）被称为 <em style="box-sizing: border-box;">深层</em></p></li></ol><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>基本上是层次越深越好，但是带来的计算成本都会增加，有时候不知道个该用多少的时候，就从logistic回归开始，一层一层增加。</p><p><br></p><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><strong><span style="font-size: 20px;">7. 神经网络的表达<br><br></span></strong></h2><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">在 <em style="box-sizing: border-box;">4.神经元的数据模型</em> 中已经清楚地用公式表达了<span style="box-sizing: border-box; font-weight: 700;">一个样本</span>输入<span style="box-sizing: border-box; font-weight: 700;">一个神经元</span>后的输出。下面看<span style="box-sizing: border-box; font-weight: 700;">一个样本</span>输<span style="box-sizing: border-box; font-weight: 700;">第一个隐层(1 hidden layer)</span>如何表达。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">以“神经网络的分类”图中的“1 hidden layer”为例，输入样本有3个特征x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">1</span>, x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">2</span>, x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">3</span></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">输入量表达为 x=(x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">1</span>, x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">2</span>, x<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">3</span>)，对于一个神经元而言w=(w<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">1</span>, w<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">2</span>, w<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">3</span>)，对于第一个有3个神经元的隐层而言：</p><p style="text-align: center;"><img class="" data-ratio="0.6666666666666666" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVMCr9z1YIvRTCCSf01LbJ0jwTJqr18NRwiaibFibS5k2fl3SmuAcuak0mA/0?wx_fmt=gif" data-type="gif" data-w="117" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><span style=" color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ;  ; ">其中，w</span><span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ;  ; ">n</span><span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; top: -0.5em; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ;  ; ">[l]</span><span style=" color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ;  ; "> 右上方括号中的l（字母L的小写，代表layer）表示第l个隐层，下标n表示第l层中的第n个神经元。w</span><span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ;  ; ">n</span><span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; top: -0.5em; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ;  ; ">[l]T</span><span style=" color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ;  ; "> 在本例中是1x3的行向量，因为有第一层有三个神经元，因此w是 3x3 的矩阵。</span></p><p style="text-align: center;"><img class="" data-ratio="0.8125" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVEeSRevQzia5BibAB30zKhqK6GIET2icgrfKTnC9SCLWGfAXiaADPnnTiblA/0?wx_fmt=gif" data-type="gif" data-w="96" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">这里，b<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; bottom: -0.25em; ">n</span><span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; top: -0.5em; ">[l]</span>表示第l层的第n个节点的内部强度，是一个实数，因此这里的 b 是一个 3x1 的列向量。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">所以，对第一层神经元而言，第一步计算z：</p><p style="text-align: center;"><img class="" data-ratio="0.2047244094488189" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzV1ERNQblWcSGlvicwEMZky8iaPKibxahcXFAdlWiaDelTp7gO4GLJhBJBpQ/0?wx_fmt=gif" data-type="gif" data-w="381" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">简化一下，就是</p><p style="text-align: center;"><img class="" data-ratio="0.14728682170542637" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVkdlnP9uvOlVVLfia1tDhcIDgricLic7YDuetF9XibownR9dZJNMbic6X5icg/0?wx_fmt=gif" data-type="gif" data-w="129" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">第二步计算a：</p><p style="text-align: center;"><img class="" data-ratio="0.22105263157894736" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVr5ibu4wwLj6G1mZTShn9zYCcU1chDey4joeljD9O8GibB7fDpBYib66Hw/0?wx_fmt=gif" data-type="gif" data-w="95" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">抽象一下，第 l-1 层输出到第 l 层的公式就是：</p><p style="text-align: center;"><img class="" data-ratio="0.12751677852348994" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVJ7s0ias56O8jeJyerTMbNpv4QGpEC6Tmo12TRC9toHOvukBcoubeolQ/0?wx_fmt=gif" data-type="gif" data-w="149" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style="text-align: center;"><img class="" data-ratio="0.23333333333333334" src="http://mmbiz.qpic.cn/mmbiz_png/EWXKCU2hmLeVEtAAMicibLtpicNNUZ05WzVic5AaRbbTSxWG8NQOvwj6pOL6fE2p1FnZbO19QwV5prbZGQjnhIdreg/0?wx_fmt=gif" data-type="gif" data-w="90" style="box-sizing: border-box; border-width: 0px; border-style: initial; border-color: initial; vertical-align: middle; cursor: zoom-in; transition: all 0.25s ease-in-out;"><br style="box-sizing: border-box;"></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">注意这里，用 a<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; top: -0.5em; ">[l-1]</span> 替代了 x，因为，实际上a<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; top: -0.5em; ">[0]</span>就是输入层x。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">到此为止，已经通过数学公式归纳出<span style="box-sizing: border-box; font-weight: 700;">一个样本</span>输入到神经网络中并计算其输出的<span style="box-sizing: border-box; font-weight: 700;">递归公式</span>。仔细分析还是挺简单的，基本上只涉及到简单的线性代数知识。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; ">实际上以上就是吴恩达课程中的 <span style="box-sizing: border-box; font-weight: 700;">向量化</span> 的主要内容了。至于如何将 m 个样本整合到一个矩阵中，就请听下回分解了，本篇内容已经很多了，连我自己都感觉有点烧脑。</p><h2 style=" box-sizing: border-box; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><strong><span style="font-size: 20px;">8. 下一步</span></strong></h2><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br>下一步要学习和总结 <span style="box-sizing: border-box; font-weight: 700;">计算图</span> 和 <span style="box-sizing: border-box; font-weight: 700;">损失函数</span>，以及如何通过 <span style="box-sizing: border-box; font-weight: 700;">梯度下降法</span> 求解 w<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; top: -0.5em; ">[l]</span> 和 b<span style=" box-sizing: border-box; font-size: 12px; line-height: 0;  vertical-align: baseline; top: -0.5em; ">[l]</span>，还会涉及到一些 <span style="box-sizing: border-box; font-weight: 700;">导数</span>、<span style="box-sizing: border-box; font-weight: 700;">偏导数</span>和 <span style="box-sizing: border-box; font-weight: 700;">Numpy</span> 的内容。</p><p style=" box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47); ; ; ; ; ; ; ; ; ; ; ; ; ; ; "><br></p><p style="box-sizing: border-box; margin-bottom: 25px; color: rgb(47, 47, 47);"><em style="box-sizing: border-box;">如果你喜欢本文，请关注我的账号，预计会以一周一篇的频率进行更新~</em></p>
                </div>
                <script nonce="1289159029" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##">赞赏</a>
                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                                            <a class="media_tool_meta meta_primary" id="js_view_source" href="##">阅读原文</a>
                                <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                                <a class="media_tool_meta meta_primary" href="http://www.jianshu.com/p/65eb2fce0e9e" target="_blank">阅读原文</a>
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
