<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>无需预训练分类器，清华&amp;旷视提出专用于目标检测的骨干网络DetNet</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1524937872&amp;src=3&amp;ver=1&amp;signature=-PTYiQF7dGzGvF1Q2StkpItgtTvRVka08PGYkqNaQpJiSeySgnEvnYfikBv0z83ptewWtHFMTLvi-ByHgCjuXu0Kdi6AkVEZ9ZMi-nyrpPAFD7MZchT7is2IW2cXwUapMcDw*e2UbpzoiylJGKYInqnyKZ*msft7yMJQdHOy6lc=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    无需预训练分类器，清华&amp;旷视提出专用于目标检测的骨干网络DetNet                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-19</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="white-space: normal;max-width: 100%;caret-color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;color: rgb(62, 62, 62);font-size: 16px;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自arXiv</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color:#888888;"><span style="font-size: 12.000000953674316px;"><strong>作者：Zeming Li、Chao Peng、Gang Yu、Xiangyu Zhang、Yangdong Deng、Jian Sun</strong></span></span></p><p style="color: rgb(62, 62, 62);font-size: 16px;max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);font-size: 12.000000953674316px;"><strong>参与：路雪、刘晓坤</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="white-space: normal;"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">基于当前用预训练分类器开发目标检测器的方法的固有缺陷，来自清华大学和旷视的研究者提出了专用于目标检测的骨干网络 DetNet。DetNet 可在保持高分辨率特征图和大感受野的同时，高效地执行目标检测任务，并可以自然地扩展到实例分割任务上。在 MSCOCO 数据集的目标检测和实例分割任务上，DetNet 都取得了当前最佳的结果。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">目标检测是计算机视觉中最基础的任务之一。由于深度卷积神经网络（CNN）的快速发展，目标检测的性能也随着显著提升。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">近期的基于 CNN 的目标检测器可以被分类为 1 阶段检测器（例如 YOLO、SSD 和 RetinaNet），以及 2 阶段检测器（例如 R-CNN、R-FCN、FPN）。它们都是基于在 ImageNet 分类任务上预训练的骨干网络。然而，图像分类和目标检测问题之间有一个显著的区别，后者不仅仅需要识别目标实例的类别，还需要对边界框进行空间定位。具体来说，使用分类骨干网络对于目标检测任务有两个问题：（1）近期的检测器如 FPN，包含额外的阶段以在不同尺度上进行目标检测；（2）传统的骨干网络基于大的下采样因子可以生成更高的感受野，这对于视觉分类很有帮助。然而，这却牺牲了空间分辨率，从而使网络难以准确地定位大型目标和识别小型目标。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一个设计良好的检测骨干应该解决以上所有问题。在此论文中，研究者提出了 DetNet，这是专门来做目标检测的全新骨干。更特别的是，因为不同的物体尺度，DetNet 具体来说，由于不同的目标尺度，DetNet 包含了额外的阶段，在其它目标检测器中的作用类似于 FPN。和传统的利用在 ImageNet 分类任务上预训练的模型不同，即使包含了额外的阶段，DetNet 也能保持特征的空间分辨率。然而，由于计算和内存开销，高分辨率的特征图给建立深度神经网络带来了更大的挑战。为了保持 DetNet 的效率，研究者部署了一个低复杂度的扩张瓶颈结构。通过整合这些改进，DetNet 不仅保持了高分辨率的特征图，还保持了大的感受野，两者对目标检测任务都很重要。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本研究的贡献如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本文首次分析了传统的将 ImageNet 预训练模型微调来开发目标检测器的固有缺陷。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本研究通过保持空间分辨率和扩大感受野，提出了一种新型的专为目标检测任务而设计的骨干网络 DetNet。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">作者利用基于低复杂度的 DetNet59 骨干网路，在 MSCOCO 目标检测和实例分割追踪任务上取得了当前最佳结果。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.30760233918128654" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0FygkuFicU2JHZicB6sxJxc9UX6k6zkGmm3OSLm6azxq5ns4emLJ4TG5Xkw/640?wx_fmt=png" data-type="png" data-w="855" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 1：FPN（特征金字塔网络）中使用的不同骨干网络的对比。（A）FPN 结合传统骨干网络；（B）传统图像分类网络；（C）本文提出的 DetNet 骨干网络，其拥有更高的空间分辨率，和 FPN 有完全相同的各阶段。由于图像尺寸限制，图中没有显示阶段 1 的特征图（步幅=2）。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">3.2 DetNet 设计</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这部分介绍 DetNet 的结构细节。研究者使用 ResNet-50 作为基线模型，其作为骨干网络广泛用于大量目标检测器中。为了公平地与 ResNet-50 进行对比，研究者使 DetNet 的阶段 1、2、3、4 与原始 ResNet-50 的阶段保持一致。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">创建高效的目标检测骨干网络存在两项挑战：保持深度神经网络的空间分辨率需要耗费大量时间和内存；降低下采样因子等于减少有效的接受野，这对很多视觉任务都是有害的，如图像分类和语义分割任务。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">DetNet 经过仔细设计以解决这两项挑战。具体来说，DetNet 遵循 ResNet 的 4 个阶段。区别从第 5 个阶段开始，用于图像分类的 DetNet 图示详见图 2D。下面我们来看从 ResNet50 扩展而来的 DetNet59 实现细节。类似地，DetNet 可以使用深度层（正如 ResNet101）轻松扩展。DetNet59 的设计细节如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">研究者引入了额外的阶段，例如 P6，其在骨干网络中的作用与 FPN 中一样。同时，研究者固定空间分辨率为 16x 下采样，即使在第 4 阶段之后也是如此。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">由于第 4 阶段后的空间分辨率是固定的，为了引入新的阶段，研究者在每个阶段刚开始时使用了扩张 [29,30,31] 瓶颈和 1x1 的卷积投影（图 2B），并发现图 2B 中的模型对于多阶段检测器（如 FPN）非常重要。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">研究者使用扩张瓶颈作为基础网络模块，以高效扩大感受野。由于扩张卷积仍然消耗大量时间，阶段 5 和阶段 6 保持与阶段 4 相同的通道（瓶颈模块有 256 个输入通道）。这与传统的骨干网络设计不同，后者会在后面的阶段中将通道数量扩展为之前的 2 倍。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">将 DetNet 和任意具备／不具备特征金字塔的检测器整合到一起是很容易的。在不损害代表性的前提下，研究者采用检测器 FPN 作为基线网络，来验证 DetNet 的效用。由于 DetNet 只改变了 FPN 的骨干网络，因此研究者不改变 FPN 的其他结构（除了骨干网络）。由于在阶段 4 之后并未减少 Resnet-50 的空间分辨率大小，因此只需按照自上而下的路径将所有阶段的输出相加即可。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">4 实验</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.6993006993006993" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0Fyzky4St94iblwh2MNt3ic4TBNrPQPW9Y7C5gG1ze6tgCQthsQ2K5yJdNQ/640?wx_fmt=png" data-type="png" data-w="715" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 2：DetNet（D）和基于 DetNet 的 FPN（E）的细节结构。（A,B）展示了 DetNet 中使用的不同的瓶颈模块。（C）展示了原始瓶颈模块。在阶段 4 之前，DetNet 和 ResNet 的设计是相同的，而在阶段 4 之后将保持空间分辨率（例如阶段 5 和 6）。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.2474074074074074" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0FyQzMNgUS21WHVTebt6dHbEwuBGCvVLZ2WF0wicmWFaj2stYQvuPA1x6g/640?wx_fmt=png" data-type="png" data-w="675" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">表 1：FPN 结合不同的骨干网络得到的结果。包括在 ImageNet 分类任务上的标准 top-1 误差。FLOPs 是指计算复杂度。还展示了 FPN 在 COCO 数据集上的结果以探索这些骨干网络对目标检测的有效性。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.4115755627009646" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0FysRjkBQGUC7HLa3rKcT5JV4SW5OoWhhnsscGuwAKKOJmGUic56x4kTRA/640?wx_fmt=png" data-type="png" data-w="622" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">表 3：FPN 在不同 IoU 阈值和不同边框尺度上的平均精度（AP）的对比。AP50 是一个有效的评估分类能力的指标。AP85 的评估需要对边框预测的准确定位。因此它可以验证本文方法的回归能力。上表还展示了不同尺度的 AP 以捕捉骨干网络中高分辨率特征图的影响。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.2668269230769231" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0FyFSIMNu8lMsMiahUQmfo8zk6PqQzaEYQvDBMksrNLutsJtOjibysnA8CQ/640?wx_fmt=png" data-type="png" data-w="832" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 3：DetNet-59-NoProj 的细节结构，它使用了图 1A 中的模块以分离阶段 6（而原始的 DetNet-59 使用图 1B 中的模块来分离阶段 6）。设计 DetNet-59-NoProj 的目的是验证包含一个新型的语义阶段用于目标检测（正如 FPN）的重要性。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.6477272727272727" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0Fyb8apOCkXGvibG3icrLNKyWJEAibib3CRjfm9aXm3F465r8zonDCTjxXw0Q/640?wx_fmt=png" data-type="png" data-w="792" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 4：基于 DetNet-59 的 FPN 检测器结果。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.26807980049875313" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0FypYPyMHt2qRCrgYQtk7qw185jyXk5aCSVWmqicgXERoLia3iaQPPyM6ZQQ/640?wx_fmt=png" data-type="png" data-w="802" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">表 7：在 MSCOCO 数据集上，本文的方法与其他顶尖方法目标检测结果的对比，基于简单、有效的骨干 DetNet-59，该模型超越了先前所有的顶尖方法。值得注意的是，DetNet-59 在更少 FLOPs 情况下就得到了更好的结果。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.20979899497487436" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0Fy2JazNzAjXmLTcwibXd9zEnIhnoLUs71bqdKvOE5L7SQWYcsiapBbQOnA/640?wx_fmt=png" data-type="png" data-w="796" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">表 8：在 MSCOCO 数据集上，本文的方法与其他顶尖方法做实例分割的结果对比。得益于 DetNet-59，在实例分割任务上 DetNet 取得了新纪录。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.515527950310559" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0FyxkF1eNwy3JNkwQibQJmAjlGZhfXI1qa2iaOIKCQzdl8hdFQf8Evtx4bw/640?wx_fmt=png" data-type="png" data-w="805" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">图 5：基于 DetNet-59 的 Mask R-CNN 的实例分割结果展示。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">论文：DetNet: A Backbone network for Object Detection</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.39420654911838793" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8kvbFO2qbs7Apc1Ydbf0Fya17KEcp6vxWuIJrdEyzhKTwmdLec9yTwvNKo61DictdCYCbuqABg1QQ/640?wx_fmt=png" data-type="png" data-w="794" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/pdf/1804.06215.pdf</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">摘要：</span></strong><span style="font-size: 14px;">无论是当前 YOLO、SSD、RetinaNet 这样的一阶段方法，还是 Faster R-CNN、R-FCN 和 FPN 这样的二阶段检测器，这些基于 CNN 的目标检测器通常都尝试直接从 ImageNet 预训练模型进行微调。而很少有研究探讨用骨干特征提取器专门做目标检测。更重要的是，图像分类和目标检测任务间有多个区别：(i)FPN 和 RetinaNet 这样的目标检测器通常要比图像分类任务有更多阶段，从而处理多尺度的物体。(ii) 目标检测不只需要识别物体样例的类别，也需要空间定位其位置。大的下采样因子带来大的有效感受野，这对图像分类有好处，却会折损目标定位的能力。因为图像分类和目标检测间的差距，我们在此论文中提出了 DetNet，这是一种专门为目标检测设计的全新骨干网络。此外，在更深层中维持高空间分辨率的同时，DetNet 还包含与传统图像分类骨干网络不同的额外阶段。基于我们提出的 DetNet（4.8G FLOPs）骨干，在 MSCOCO 数据集基准上取得了目标检测和示例分割的当前最佳结果。复现代码将在近期发布。<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" width="48px" style="font-size: 14px;text-align: justify;white-space: normal;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;"></span></p><p><br></p><p><br></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="2003019942" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
