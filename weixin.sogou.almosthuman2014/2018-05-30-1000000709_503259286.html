<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CVPR 2018 | 腾讯优图提出SRN-DeblurNet：高效高质量去除复杂图像模糊</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1527695441&amp;src=3&amp;ver=1&amp;signature=sz35w1tHE*ocopopMwKokTgGvVsDFfdKVgscyaotBzTiG2zy-DGDjTW7qG3v1DHYqfzmEJ-7IVfIsCCuZwC-m-Sh0fHUSd06tng3srq36GhiVhRjuXmntj2Q77Dg7ecLzhY2b4Wo5B8bmF*waxRkvylXqzDPUwLLoMeg8i4KXjo=">原文</a></p>
<div id="js_top_ad_area" class="top_banner"></div><div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        
            <div id="img-content">

                
                <h2 class="rich_media_title" id="activity-name">
                    
                    <script nonce="1914108181" type="text/javascript">
                        if(/(iPhone|iPad|iPod|iOS)/i.test(navigator.userAgent)){
                            document.write("<span class='rich_media_title_ios'>CVPR 2018 | 腾讯优图提出SRN-DeblurNet：高效高质量去除复杂图像模糊");
                        }else{
                            document.write("CVPR 2018 | 腾讯优图提出SRN-DeblurNet：高效高质量去除复杂图像模糊");
                        }
                    </script>

                                                                                </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                                                <span class="rich_media_meta rich_media_meta_nickname" id="profileBt">
                      <a href="javascript:void(0);">
                        机器之心                      </a>
                      <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                          <div class="profile_inner">
                              <strong class="profile_nickname">机器之心</strong>
                              <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                              <p class="profile_meta">
                              <label class="profile_meta_label">微信号</label>
                              <span class="profile_meta_value">almosthuman2014</span>
                              </p>

                              <p class="profile_meta">
                              <label class="profile_meta_label">功能介绍</label>
                              <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                              </p>
                              
                          </div>
                          <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                              <i class="profile_arrow arrow_out"></i>
                              <i class="profile_arrow arrow_in"></i>
                          </span>
                      </div>
                    </span>


                    <em id="publish_time" class="rich_media_meta rich_media_meta_text"></em>





                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " lang='="en"' id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;caret-color: rgb(51, 51, 51);"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="color: rgb(51, 51, 51);font-size: 17px;margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);font-size: 16px;box-sizing: border-box !important;word-wrap: break-word !important;">选自arXiv<span style="max-width: 100%;font-size: 15px;text-align: justify;"></span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="color: rgb(51, 51, 51);font-size: 17px;max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：Xin Tao、Hongyun Gao、Xiaoyong Shen、Jue Wang、Jiaya Jia</strong></span></p><p style="color: rgb(51, 51, 51);font-size: 17px;max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><strong>参与：Panda</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;caret-color: rgb(51, 51, 51);color: rgb(51, 51, 51);"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 15px;">因为手抖或焦点选择等问题，相机拍摄的图像中常常存在模糊状况。消除图像模糊，呈现图像细节是计算机视觉领域内的一个重要研究主题。香港中文大学、腾讯优图实验室和旷视科技的研究者合作提出的 SRN-DeblurNet 能更高效地实现比之前最佳方法更好的结果。该论文已被将在当地时间 6 月 18-22 日于美国犹他州盐湖城举办的 CVPR 2018 接收。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">图像去模糊一直以来都是计算机视觉和图像处理领域内的一个重要问题。给定一张因运动或失焦而模糊（由相机摇晃、目标快速移动或对焦不准而造成）的图像，去模糊的目的是将其恢复成有清晰的边缘结构和丰富真实的细节的图像。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">单图像去模糊在数学上是一个高度病态（ill-posed）问题。传统方法是通过对模糊的原理进行简化和建模（比如均匀模糊/非均匀模糊/考虑深度的模糊），并使用不同的自然图像先验 [1, 3, 6, 14, 26, 37, 38] 来约束解空间。这些方法大多数都涉及到大量的（有时是试验式的）参数调整和成本高昂的计算。此外，简化后的模糊模型往往有碍它们在真实拍摄样本上的表现。在真实世界中，模糊比建模的情况要复杂很多，甚至还涉及到相机内部的图像处理过程。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">也有研究者为去模糊提出了基于学习的方法。早期的方法 [28, 32, 35] 是借助外部训练数据，用一组可学习的参数替代传统框架中的一些模块或步骤。更近期的工作则开始使用端到端的可训练网络来进行图像 [25] 和视频 [18,31] 去模糊。其中，Nah et al.[25] 使用一种多尺度卷积神经网络（CNN）达到了当前最佳水平。他们的方法从非常小尺度的模糊图像开始，然后逐渐恢复更高分辨率的清晰图像，直到达到完整分辨率。这一框架遵循传统方法中的多尺度机制，其中「由粗到精」流程在处理大的模糊核时很常见 [6]。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在本论文中，我们探索了一种用于多尺度图像去模糊的更有效的网络结构。我们提出了一种新的尺度循环网络（SRN：scale-recurrent network），它讨论和解决了基于 CNN 的去模糊系统中两个重要的一般性问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><span style="font-size: 16px;"><strong>尺度训练结构</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在现有的多尺度方法中，求解器及其每个尺度的参数通常是一样的。直观上看，这是一种自然的选择，因为在每个尺度上，我们的目标都是求解同样的问题。还可以发现，每个尺度上使用不同的参数可能会引入不稳定性并带来非限制性解空间的额外问题。另一个问题是输入图像可能会有不同的分辨率和运动尺度。如果允许每个尺度上都进行参数调节，那么这个解可能会在特定图像分辨率或运动尺度上过拟合。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">基于同样的原因，我们相信这个方案也应该被应用于基于 CNN 的方法。但是，近期的级联网络 [4, 25] 仍然为每个尺度使用了独立的参数。在本研究中，我们提出在不同尺度上共享网络权重，从而显著降低训练复杂度以及引入明显的稳定性优势。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这种做法有两种好处。首先，这能显著减少可训练参数的数量。即使用同样数目的训练数据，在共享权重的循环利用下的效果也像是有多倍数据来学习参数，这实际上相当于在尺度上进行的数据增强。其次，我们提出的结构可以利用到循环模块，其状态传递能隐含地获取各个尺度上的有用信息并帮助图像恢复。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><span style="font-size: 15px;"><strong>编码器-解码器 ResBlock 网络</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">编码器-解码器结构在多种计算机视觉任务上有效应用 [23, 31, 33, 39]，我们探索了将其应用于图像去模糊任务的有效方法。在本论文中，我们将表明直接应用已有的编码器-解码器结构不能得到最优结果。相对而言，我们的编码器-解码器 ResBlock 网络会放大各种 CNN 结构的优势并实现训练的可行性。同时，这还会产生非常大的感受野，这对运动模糊很大的图像的去模糊至关重要。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们的实验表明，使用循环结构并结合上述优势，我们的端到端深度图像去模糊框架可以极大地提升训练效率（大约 [25] 的四分之一的训练时间就能实现近似的恢复效果）。我们只使用了不到三分之一的可训练参数以及远远更少的测试时间。除了训练效率，我们的方法在定量和定性比较上都能得到比已有方法更高质量的结果，如图 1 所示。我们将这个框架称为尺度循环网络（SRN）。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.7560693641618497" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibFSZURMSxLtR5eymDzbbsFPhooibicuKGwOSAPzqu4K3ubkB62dJKT8vERKibT3UwzajyR5icPUXpgMg/640?wx_fmt=png" data-type="png" data-w="865" style=""></p><p style="text-align: left;line-height: 1.75em;"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 1：一个真实拍摄的示例。（a）输入的模糊图像，（b）Sun et al. [32] 的结果，（c）Nah et al. [25] 的结果，（d）我们的结果</span></em></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.35953757225433525" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibFSZURMSxLtR5eymDzbbsFdo1x2ia56GB3nrRvpDpsV9qC1icqPkTueSzqaXc34WlyY1LI2cLRyR6w/640?wx_fmt=png" data-type="png" data-w="865" style=""></p><p style="text-align: left;line-height: 1.75em;"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 2：用于图像处理的不同 CNN。（a）U-Net [27] 或编码器-解码器网络 [24]，（b）多尺度 [25] 或级联细化网络 [4]，（c）扩张卷积网络 [5]，（d）我们提出的尺度循环网络（SRN）。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><span style="font-size: 15px;"><strong>网络架构</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们将我们提出的网络的整体架构称为 SRN-DeblurNet，如图 3 所示。其以在不同尺度上从输入图像下采样的一个模糊图像序列为输入，然后得到一组对应的锐利图像。在全分辨率下的锐利图像即为最终输出。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.5317919075144508" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibFSZURMSxLtR5eymDzbbsF0ia0XwDYrSPSSYpxVapTVAw8E5XyJPcJYrVJvicaiaG4e4W7oECX3CWpQ/640?wx_fmt=png" data-type="png" data-w="865" style=""></p><p style="text-align: left;line-height: 1.75em;"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 3：我们提议的 SRN-DeblurNet 框架</span></em></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;"><span style="font-size: 15px;"><strong>实验</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们的实验是在一台 PC 上执行的，其配置有英特尔 Xeon E5 CPU 和一块英伟达 Titan X GPU。我们在 TensorFlow 平台 [11] 上实现了我们的框架。我们全面评估了多种网络结构，以验证不同的结构对于效果的影响。为了公平起见，除非另有说明，所有实验都是在同一数据集上，使用同样的训练配置完成的。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.37372013651877134" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibFSZURMSxLtR5eymDzbbsFwZWnRJjniaqxDuic4m9XrULm0RicGzKEMsUkCxTdCGTg0jqXBueI0Tpgg/640?wx_fmt=png" data-type="png" data-w="586" style=""></p><p style="text-align: left;line-height: 1.75em;"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">表 1：基准模型的定量结果</span></em></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 15px;"><em><br></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.28390596745027125" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibFSZURMSxLtR5eymDzbbsFTXXoImbib8iaYdsNpa0smbEMDIjjwwIvgzAGlehz3NlVQowVvtymLd4w/640?wx_fmt=png" data-type="png" data-w="553" style=""></p><p style="text-align: left;line-height: 1.75em;"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">表 2：在测试数据集上的定量结果（PSNR/SSIM）</span></em></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><span style="color: rgb(136, 136, 136);font-size: 15px;"><em><img class="" data-copyright="0" data-ratio="1.2057803468208093" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibFSZURMSxLtR5eymDzbbsFt15qvUJibHxoR0JGMY7BQDzz0Mb9xfU89Z9yXpNPsyPfzfzbiaQHibQlQ/640?wx_fmt=png" data-type="png" data-w="865" style=""></em></span></p><p style="text-align: left;line-height: 1.75em;"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">图 5：在测试数据集上的视觉比较。从上到下：输入、Whyte et al. [34]、Sun et al. [32]、Nah et al. [25] 和我们的方法。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><strong>论文：用于深度图像去模糊的尺度循环网络（Scale-recurrent Network for Deep Image Deblurring）</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.21271676300578035" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibFSZURMSxLtR5eymDzbbsFEA4eDGApZXR9vf73IX967iabNpFic5GI8lxwOrFLyY8t4aa1zQtib7FSQ/640?wx_fmt=png" data-type="png" data-w="865" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(123, 12, 0);font-size: 15px;">论文地址： http://www.cse.cuhk.edu.hk/leojia/papers/scaledeblur_cvpr18.pdf</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(123, 12, 0);font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">摘要：在单图像去模糊中，「粗糙到精细」方案（即以金字塔的形式在不同分辨率上逐步恢复锐利图像）在传统的基于优化的方法和近期的基于神经网络的方法中都非常成功。在本论文中，我们研究了这一策略并提出了一种用于去模糊任务的尺度循环网络（SRN-DeblurNet）。相比于 [25] 中很多近期的基于学习的方法，它的网络结构更简单，参数数量更少，训练更容易。我们在带有复杂运动的大规模去模糊数据集上评估了我们的方法。结果表明，在定量和定性比较上，我们的方法能得到比之前最佳结果更高质量的结果。</span><span style="font-size: 14px;"><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(51, 51, 51);" width="48px"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;letter-spacing: 0.544px;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;letter-spacing: 0.544px;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：<strong style="max-width: 100%;color: rgb(62, 62, 62);font-size: 18px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">content</span></strong>@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;letter-spacing: 0.544px;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 15px;box-sizing: border-box !important;word-wrap: break-word !important;"></span></p>
                </div>
                <script nonce="1914108181" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg_new/winwx3de35e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc reward_area_primary" id="js_preview_reward_author" style="display:none;">
                    <div class="reward-avatar" style="display: none;" id="js_preview_reward_author_avatar">
                        <img src="" alt="" id="js_preview_reward_author_head">
                    </div>
                    <div class="reward-author" id="js_preview_reward_author_name"></div>
                    <p class="reward_tips js_preview_reward_author_wording" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_author_link" href="##"><span id="js_preview_reward_link_text">赞赏</span></a>
                    </p>
                </div>
                
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                                        
                                

                        
            <ul id="js_hotspot_area" class="article_extend_area"></ul>


            
                        <div class="rich_media_tool" id="js_toobar3">

                                            <div id="js_read_area3" class="media_tool_meta tips_global_primary meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_extra meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">

                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>


        
        <div class="rich_media_area_extra">
            
            <div id="js_share_appmsg">
            </div>

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_container" style="display:none">
              

              
              <div class="discuss_mod" id="js_friend_cmt_area" style="display:none">
                
                
                
              </div>

                            <div class="discuss_mod" id="js_cmt_area" style="display:none">
              </div>
                          </div>
        </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
