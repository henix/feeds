<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CVPR2018 | 让AI识别语义空间关系：斯坦福大学李飞飞组提出「参考关系模型」</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1524021895&amp;src=3&amp;ver=1&amp;signature=wdy3flB*-DlyTfhpwIEParpzgVf0X6kTwNbQiv*LoZQlWcoMvLhIEHkecfu1vmvgyuftqoRynME4QGfE1TdMdJYxJob8C7XlZm91GU7HPKqhdPrLPhqV2X-eIrLf8HC-MKT3fZmZtLPVipG1oBOC2gmlkOGAtGa81tzkcP5OTRA=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    CVPR2018 | 让AI识别语义空间关系：斯坦福大学李飞飞组提出「参考关系模型」                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-08</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;caret-color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自arXiv</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">Ranjay Krishna 等</span></strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">参与：</span></strong><span style="font-size: 12px;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">张倩、路雪</span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p><br></p><blockquote><p style="line-height: 1.75em;"><span style="font-size: 14px;text-align: justify;color: rgb(136, 136, 136);">图像不仅仅是一组目标集合，同时每个图像还代表一个相互关联的关系网。在本文中，李飞飞等人提出了利用「参考关系」明确区分同类实体的任务。实验结果表明，该模型不仅在 CLEVR、VRD 和 Visual Genome 三个数据集上均优于现有方法，并且是可解释的，甚至能发现完全没见过的类别。</span></p></blockquote><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">日常用语中的参考式表达可以帮助我们识别和定位周围的实体。例如，我们可以用「踢球的人」和「守门的人」将两个人区分开（图 1）。在这两个例子中，我们通过两人与其他实体的关系来明确他们的具体身份 [24]。其中一个人在踢球，而另一个人在守门。我们的最终目标是建立计算模型，以明确其他词汇与哪些实体相关 [ 36 ]。</span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.6282051282051282" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicfslNJ30z69JYcAHVujfe1ePlqiaJYFvia1tlgwTwUicQxkyalL0W9nbicUnBr1HRH88MoaG6xtVrL7g/640?wx_fmt=png" data-type="png" data-w="702" style="white-space: normal;"></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 1：参考关系可以通过利用同一类别中的物体与其他实体之间的相对关系来明确区分这些物体。给定&lt;person- kicking - ball&gt;这种关系之后，我们需要模型通过理解谓词「踢」来正确识别图像中的哪个人在踢球。</em></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了实现这种交互，我们引入了参考关系（referring relationships），即在给定关系的情况下，模型应该清楚场景中的哪些实体在该关系中用作参考。从形式上讲，该任务需要输入带有&lt;subject - predicate - object&gt;关系的图像，并输出主体和客体的定位。例如，我们可以将上面的示例表示为&lt;person -kicking - ball&gt;和&lt;person - guarding - goal&gt;（图 1）。以前的研究工作已经尝试在参考式表达理解的背景下明确区分同一类别的各个实体 [ 29，25，43，44，12 ]。它们的任务需要输入自然语言，例如「a person guarding the goal」，从而产生需要自然语言和计算机视觉组件的评估。精确地指出这些模型所产生的错误是来自语言还是来自视觉模块可能有点困难。我们的任务是参考表达的一种特殊运用，通过连接结构化关系输入减少对语言建模的需要。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">参考关系在前期任务的核心保留并改善算法难题。在客体定位文献中，一些实体 (如斑马和人) 差别非常明显，很容易被区分开来，而另一些客体（如玻璃和球）则较难区分 [ 30 ]。造成这些困难的原因包括某些成分尺寸小、不易区分。这种难度上的差异转化为参考关系任务。为了应对这一挑战，我们提出这样一种思路：如果我们知道另一个实体在哪里，那检测一个实体就会变得更容易。换句话说，我们可以借助踢球的人为条件来发现球，反之亦然。我们通过展开模型及通过谓词定义的运算符在主客体之间迭代传递消息来训练这种循环依赖关系。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然而，对这个谓词运算符建模并不简单，这就引出了我们的第二个挑战。传统上，以前的视觉关系论文已经能为每个谓词训练了一个基于外观的模型 [21, 24, 27]。不幸的是，谓词语义的急剧变化（取决于所涉及的实体）增加了学习谓词模型的难度。例如，谓词 carrying 的语义在以下两种关系之间可能有很大差异：&lt;person - carrying - phone&gt; 和 &lt;truck - carrying -hay&gt;。受心理学移动焦点理论 [ 19，37 ] 的启发，我们通过使用谓词作为从一个实体到另一个实体的视觉焦点转移操作来绕过这一挑战。当一个移位操作学习将焦点从主体转移到客体时，逆谓词移位以相似的方式将焦点从客体转移回主体。经过多次迭代，我们将主体和客体之间的这些不对称焦点转移实施为每个谓词 [ 39，10 ] 的不同类型的消息操作。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">总而言之，我们介绍了参考关系这一任务，它的结构化关系输入使得我们可以评估识别图片中同一类别实体的能力。我们在包含视觉关系的三个视觉数据集（CLEVR [13], VRD [24] 和 Visual Genome [18]）上评估我们的模型。这些数据集中的 33 %、60.3 % 和 61 % 的关系涉及不明确的实体，即相同类别中的多个实例的实体。我们扩展我们的模型以使用属于场景图的关系来执行焦点扫视 [ 38 ]。最后，我们证明了在没有主体或客体的情况下，这一新模型仍然可以明确各个实体，同时还可以辨别来自以前从未见过的新类别实体。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们的模型使用带有 TensorFlow 后端的 Keras 进行编写。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">模型地址：https://github.com/StanfordVL/ReferringRelationships。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p><img class="" data-copyright="0" data-ratio="0.31710615280594995" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicfslNJ30z69JYcAHVujfe1xJLZhpEGdtyFpAyEUwzviaUcAicBicCxwzOm1eRVsCibhgvoEQloGLicnWw/640?wx_fmt=png" data-type="png" data-w="1479" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 2：参考关系的推理过程始于图像特征提取，然后使用这些图像特征独立地生成主体和客体的初始标注。接下来，使用这些估计值将谓词的焦点从主体转移到我们期望客体的位置。在细化客体的新评估时，我们通过关注偏移区域来修改图像特</em></span><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>征。同时，我们学习从初始客体到主体的逆向变换。我们通过两个谓词移位模块以迭代的方式在主客体之间传递消息，以最终定位这两个实体。</em></span></p><p><br></p><p><span style="color: rgb(136, 136, 136);font-size: 12px;"><em><img class="" data-copyright="0" data-ratio="0.23636363636363636" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicfslNJ30z69JYcAHVujfe1PNhNYs4gUeestxJM9npa3XujW9h0mNqaOAHup7ksGNAniaTugaCD4fg/640?wx_fmt=png" data-type="png" data-w="1430" style=""></em></span></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>表 1：CLEVR[ 13 ]、VRD [ 24 ] 和 Visual Genome[ 18 ] 上对参考关系的测试结果。我们分别报告了主体和客体位置的 Mean IoU 和 KL 散度。</em></span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.8309659090909091" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicfslNJ30z69JYcAHVujfe1wulwSRmf4lTLBLkibqMFnIv5yGRAz04J1FDLia0ux8Is7gMBIU1ZmLiaQ/640?wx_fmt=png" data-type="png" data-w="704" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 3：( a ) 相对于图像中间的主体，谓词 left 在使用关系查找对象时将焦点转移到右边。相反，当用客体来寻找主体时，逆谓词 left 会把焦点转移到左边。我们在附录中将所有 70 个 VRD、6 个 CLEVR 和 70 个 Visual Genome 谓词和逆谓词移位进行可视化。( b ) 我们还发现，在查看用于学习这些变化的数据集时，这些变化是直观的。例如，我们发现骑行通常对应于主体在客体的下方。</em></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><span style="color: rgb(136, 136, 136);font-size: 12px;"><em><img class="" data-copyright="0" data-ratio="0.7305502846299811" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicfslNJ30z69JYcAHVujfe1mnTKE6kmDNibYbiauye56n1QuoKCxDcfhN1PDrUibCtKib2Mibo2KYWwNMQ/640?wx_fmt=png" data-type="png" data-w="1054" style=""></em></span></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 4：焦点如何从 CLEVR 和 Visual Genome 数据集进行多次迭代转移的可视化示例。在第一次迭代中，模型仅接收关于它试图查找的实体信息，因此试图定位这些类别的所有实例。在后面的迭代中，我们看到谓词转移了焦点，它允许我们的模型明确区分同一类别中的不同实例。</em></span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.2046070460704607" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicfslNJ30z69JYcAHVujfe1FcLTibxkcQEqKuVXabsJGib5VicrVNmeNM5VJSgrPic8nvbpbDSNXaHheg/640?wx_fmt=png" data-type="png" data-w="1476" style=""></p><p style="text-align: left;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 5：我们可以将新模型分解为焦点和移位模块，并将其堆叠起来，以覆盖场景图的节点。本图展示了如何使用我们的模型从一个节点（电话）开始根据关系遍历场景图，以连接它们并定位短语 &lt;phone on the person next to another person wearing a jacket&gt; 中的所有实体。第二个示例涉及 &lt;hat worn by person to the right of another person above the table&gt; 中的实体。</em></span></p><p><br></p><p style="text-align: left;line-height: 1.75em;"><strong><span style="font-size: 14px;">论文：Referring Relationships</span></strong></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.2755102040816326" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicfslNJ30z69JYcAHVujfe1ufxXiaQyY1Uj8LlwN8H0LosARe9aKuLVDj95ic3WekLeMYia9Znsvlsicw/640?wx_fmt=png" data-type="png" data-w="882" style=""></p><p><br></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1803.10362</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">摘要：</span></strong><span style="font-size: 14px;">图像不仅仅是一组目标集合，同时每个图像还代表一个相互关联的关系网。实体之间的这些关系承载着语义功能，帮助观察者区分一个实体中的实例。例如，一张足球比赛的图片中可能不止一人，但每个人都处在不同的关系中：其中一人在踢球，另一人则在防守。在本文中，我们提出了利用这些「参考关系」明确区分同类实体的任务。我们引入了一个迭代模型，利用该模型区分参考关系中的两个实体，二者互为条件。我们通过谓词建模来描述以上关系中实体之间的循环条件，这些谓词将实体连接为从一个实体到另一个实体的焦点移位。实验结果表明，该模型不仅在 CLEVR、VRD 和 Visual Genome 三个数据集上均优于现有方法，而且能作为可解释神经网络的一个实例。此外，它还能产生可视的有意义的谓词移位。最后，我们提出，通过将谓词建模为注意转移，我们甚至可以区分模型没见过的类别中的实体，从而使我们的模型发现完全没见过的类别。</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></p><p style="margin-bottom: 20px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="1339209507" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
