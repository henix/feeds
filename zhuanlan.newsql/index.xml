<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Tue, 30 Jul 2019 18:03:14 +0800</lastBuildDate>
<item>
<title>TiKV 源码解析系列文章（十一）Storage - 事务控制层</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-29-75708576.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/75708576&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-1d12480f0214931303d33ba3ad7a94f2_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：张金鹏&lt;/p&gt;&lt;h2&gt;背景知识&lt;/h2&gt;&lt;p&gt;TiKV 是一个强一致的支持事务的分布式 KV 存储。TiKV 通过 raft 来保证多副本之间的强一致，事务这块 TiKV 参考了 Google 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//ai.google/research/pubs/pub36726&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Percolator 事务模型&lt;/a&gt;，并进行了一些优化。&lt;/p&gt;&lt;p&gt;当 TiKV 的 Service 层收到请求之后，会根据请求的类型把这些请求转发到不同的模块进行处理。对于从 TiDB 下推的读请求，比如 sum，avg 操作，会转发到 Coprocessor 模块进行处理，对于 KV 请求会直接转发到 Storage 进行处理。&lt;/p&gt;&lt;p&gt;KV 操作根据功能可以被划分为 Raw KV 操作以及 Txn KV 操作两大类。Raw KV 操作包括 raw put、raw get、raw delete、raw batch get、raw batch put、raw batch delete、raw scan 等普通 KV 操作。 Txn KV 操作是为了实现事务机制而设计的一系列操作，如 prewrite 和 commit 分别对应于 2PC 中的 prepare 和 commit 阶段的操作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文将为大家介绍 TiKV 源码中的 Storage 模块，它位于 Service 与底层 KV 存储引擎之间，主要负责事务的并发控制。TiKV 端事务相关的实现都在 Storage 模块中。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;源码解析&lt;/h2&gt;&lt;p&gt;接下来我们将从 Engine、Latches、Scheduler 和 MVCC 等几个方面来讲解 Storage 相关的源码。&lt;/p&gt;&lt;h3&gt;1. Engine trait&lt;/h3&gt;&lt;p&gt;TiKV 把底层 KV 存储引擎抽象成一个 Engine trait（trait 类似其他语言的 interface），定义见 &lt;code&gt;storage/kv/mod.rs&lt;/code&gt;。Engint trait 主要提供了读和写两个接口，分别为 &lt;code&gt;async_snapshot&lt;/code&gt; 和 &lt;code&gt;async_write&lt;/code&gt;。调用者把要写的内容交给 &lt;code&gt;async_write&lt;/code&gt;，&lt;code&gt;async_write&lt;/code&gt; 通过回调的方式告诉调用者写操作成功完成了或者遇到错误了。同样的，&lt;code&gt;async_snapshot&lt;/code&gt;通过回调的方式把数据库的快照返回给调用者，供调用者读，或者把遇到的错误返回给调用者。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub trait Engine: Send + Clone + &amp;#39;static {
    type Snap: Snapshot;
    fn async_write(&amp;amp;self, ctx: &amp;amp;Contect, batch: Vec&amp;lt;Modify&amp;gt;, callback: Callback&amp;lt;()&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt;;
    fn async_snapshot(&amp;amp;self, ctx: &amp;amp;Context, callback: Callback&amp;lt;Self::Snap&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;只要实现了以上两个接口，都可以作为 TiKV 的底层 KV 存储引擎。在 3.0 版本中，TiKV 支持了三种不同的 KV 存储引擎，包括单机 RocksDB 引擎、内存 B 树引擎和 RaftKV 引擎，分别位于 &lt;code&gt;storage/kv&lt;/code&gt; 文件夹下面的 &lt;code&gt;rocksdb_engine.rs&lt;/code&gt;、&lt;code&gt;btree_engine.rs&lt;/code&gt; 和 &lt;code&gt;raftkv.rs&lt;/code&gt;。其中单机 RocksDB 引擎和内存红黑树引擎主要用于单元测试和分层 benchmark，TiKV 真正使用的是 RaftKV 引擎。当调用 RaftKV 的 &lt;code&gt;async_write&lt;/code&gt; 进行写入操作时，如果 &lt;code&gt;async_write&lt;/code&gt; 通过回调方式成功返回了，说明写入操作已经通过 raft 复制给了大多数副本，并且在 leader 节点（调用者所在 TiKV）完成写入了，后续 leader 节点上的读就能够看到之前写入的内容。&lt;/p&gt;&lt;h3&gt;2. Raw KV 执行流程&lt;/h3&gt;&lt;p&gt;Raw KV 系列接口是绕过事务直接操纵底层数据的接口，没有事务控制，比较简单，所以在介绍更复杂的事务 KV 的执行流程前，我们先介绍 Raw KV 的执行流程。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Raw put&lt;/b&gt;&lt;/p&gt;&lt;p&gt;raw put 操作不需要 Storage 模块做额外的工作，直接把要写的内容通过 engine 的 &lt;code&gt;async_write&lt;/code&gt; 接口发送给底层的 KV 存储引擎就好了。调用堆栈为 &lt;code&gt;service/kv.rs: raw_put&lt;/code&gt; -&amp;gt; &lt;code&gt;storage/mod.rs: async_raw_put&lt;/code&gt;。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;impl&amp;lt;E: Engine&amp;gt; Storage&amp;lt;E&amp;gt; {
    pub fn async_raw_put(
        &amp;amp;self,
        ctx: Context,
        cf: String,
        key: Vec&amp;lt;u8&amp;gt;,
        value: Vec&amp;lt;u8&amp;gt;,
        callback: Callback&amp;lt;()&amp;gt;,
    ) -&amp;gt; Result&amp;lt;()&amp;gt; {
        // Omit some limit checks about key and value here...
        self.engine.async_write(
            &amp;amp;ctx,
            vec![Modify::Put(
                Self::rawkv_cf(&amp;amp;cf),
                Key::from_encoded(key),
                value,
            )],
            Box::new(|(_, res)| callback(res.map_err(Error::from))),
        )?;
        Ok(())
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;Raw get&lt;/b&gt;&lt;/p&gt;&lt;p&gt;同样的，raw get 只需要调用 engine 的 &lt;code&gt;async_snapshot&lt;/code&gt; 拿到数据库快照，然后直接读取就可以了。当然对于 RaftKV 引擎，&lt;code&gt;async_snapshot&lt;/code&gt; 在返回数据库快照之前会做一些检查工作，比如会检查当前访问的副本是否是 leader（3.0.0 版本只支持从 leader 进行读操作，follower read 目前仍然在开发中），另外也会检查请求中携带的 region 版本信息是否足够新。&lt;/p&gt;&lt;h3&gt;3. Latches&lt;/h3&gt;&lt;p&gt;在事务模式下，为了防止多个请求同时对同一个 key 进行写操作，请求在写这个 key 之前必须先获取这个 key 的内存锁。为了和事务中的锁进行区分，我们称这个内存锁为 latch，对应的是 &lt;code&gt;storage/txn/latch.rs&lt;/code&gt; 文件中的 Latch 结构体。每个 Latch 内部包含一个等待队列，没有拿到 latch 的请求按先后顺序插入到等待队列中，队首的请求被认为拿到了该 latch。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;#[derive(Clone)]
struct Latch {
    pub waiting: VecDeque&amp;lt;u64&amp;gt;,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Latches 是一个包含多个 Latch 的结构体，内部包含一个固定长度的 Vector，Vector 的每个 slot 对应一个 Latch。默认配置下 Latches 内部 Vector 的长度为 2048000。每个 TiKV 有且仅有一个 Latches 实例，位于 &lt;code&gt;Storage.Scheduler&lt;/code&gt; 中。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct Latches {
    slots: Vec&amp;lt;Mutex&amp;lt;Latch&amp;gt;&amp;gt;,
    size: usize,
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Latches 的 &lt;code&gt;gen_lock&lt;/code&gt; 接口用于计算写入请求执行前所需要获取的所有 latch。&lt;code&gt;gen_lock&lt;/code&gt; 通过计算所有 key 的 hash，然后用这些 hash 对 Vector 的长度进行取模得到多个 slots，对这些 slots 经过排序去重得到该命令需要的所有 latch。这个过程中的排序是为了保证获取 latch 的顺序性防止出现死锁情况。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;impl Latches {
    pub fn gen_lock&amp;lt;H: Hash&amp;gt;(&amp;amp;self, keys: &amp;amp;[H]) -&amp;gt; Lock {
        // prevent from deadlock, so we sort and deduplicate the index.
        let mut slots: Vec&amp;lt;usize&amp;gt; = keys.iter().map(|x|
        self.calc_slot(x)).collect();
        slots.sort();
        slots.dedup();
        Lock::new(slots)
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3&gt;4. Storage 和事务调度器 Scheduler&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Storage&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Storage 定义在 &lt;code&gt;storage/mod.rs&lt;/code&gt; 文件中，下面我们介绍下 Storage 几个重要的成员：&lt;/p&gt;&lt;p&gt;&lt;code&gt;engine&lt;/code&gt;：代表的是底层的 KV 存储引擎。&lt;/p&gt;&lt;p&gt;&lt;code&gt;sched&lt;/code&gt;：事务调度器，负责并发事务请求的调度工作。&lt;/p&gt;&lt;p&gt;&lt;code&gt;read_pool&lt;/code&gt;：读取线程池，所有只读 KV 请求，包括事务的非事务的，如 raw get、txn kv get 等最终都会在这个线程池内执行。由于只读请求不需要获取 latches，所以为其分配一个独立的线程池直接执行，而不是与非只读事务共用事务调度器。&lt;/p&gt;&lt;p&gt;&lt;code&gt;gc_worker&lt;/code&gt;：从 3.0 版本开始，TiKV 支持分布式 GC，每个 TiKV 有一个 &lt;code&gt;gc_worker&lt;/code&gt; 线程负责定期从 PD 更新 safepoint，然后进行 GC 工作。&lt;/p&gt;&lt;p&gt;&lt;code&gt;pessimistic_txn_enabled&lt;/code&gt;： 另外 3.0 版本也支持悲观事务，&lt;code&gt;pessimistic_txn_enabled&lt;/code&gt; 为 true 表示 TiKV 以支持悲观事务的模式启动，关于悲观事务后续会有一篇源码阅读文章专门介绍，这里我们先跳过。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct Storage&amp;lt;E: Engine&amp;gt; {
    engine: E,
    sched: Scheduler&amp;lt;E&amp;gt;,
    read_pool: ReadPool,
    gc_worker: GCWorker&amp;lt;E&amp;gt;,
    pessimistic_txn_enabled: bool,
    // Other fields...
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;对于只读请求，包括 txn get 和 txn scan，Storage 调用 engine 的 &lt;code&gt;async_snapshot&lt;/code&gt; 获取数据库快照之后交给 &lt;code&gt;read_pool&lt;/code&gt;线程池进行处理。写入请求，包括 prewrite、commit、rollback 等，直接交给 Scheduler 进行处理。Scheduler 的定义在 &lt;code&gt;storage/txn/scheduler.rs&lt;/code&gt; 中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Scheduler&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub struct Scheduler&amp;lt;E: Engine&amp;gt; {
    engine: Option&amp;lt;E&amp;gt;,
    inner: Arc&amp;lt;SchedulerInner&amp;gt;,
}

struct SchedulerInner {
    id_alloc, AtomicU64,
    task_contexts: Vec&amp;lt;Mutex&amp;lt;HashMap&amp;lt;u64, TaskContext&amp;gt;&amp;gt;&amp;gt;,
    lathes: Latches,
    sched_pending_write_threshold: usize,
    worker_pool: SchedPool,
    high_priority_pool: SchedPool,
    // Some other fields...
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;接下来简单介绍下 Scheduler 几个重要的成员：&lt;/p&gt;&lt;p&gt;&lt;code&gt;id_alloc&lt;/code&gt;：到达 Scheduler 的请求都会被分配一个唯一的 command id。&lt;/p&gt;&lt;p&gt;&lt;code&gt;latches&lt;/code&gt;：写请求到达 Scheduler 之后会尝试获取所需要的 latch，如果暂时获取不到所需要的 latch，其对应的 command id 会被插入到 latch 的 waiting list 里，当前面的请求执行结束后会唤醒 waiting list 里的请求继续执行，这部分逻辑我们将会在下一节 prewrite 请求在 scheduler 中的执行流程中介绍。&lt;/p&gt;&lt;p&gt;&lt;code&gt;task_contexts&lt;/code&gt;：用于存储 Scheduler 中所有请求的上下文，比如暂时未能获取所需 latch 的请求都会被暂存在 &lt;code&gt;task_contexts&lt;/code&gt; 中。&lt;/p&gt;&lt;p&gt;&lt;code&gt;sched_pending_write_threshold&lt;/code&gt;：用于统计 Scheduler 内所有写入请求的写入流量，可以通过该指标对 Scheduler 的写入操作进行流控。&lt;/p&gt;&lt;p&gt;&lt;code&gt;worker_pool&lt;/code&gt;，&lt;code&gt;high_priority_pool&lt;/code&gt;：两个线程池，写请求在调用 engine 的 async_write 之前需要进行事务约束的检验工作，这些工作都是在这个两个线程池中执行的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;prewrite 请求在 Scheduler 中的执行流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面我们以 prewrite 请求为例子来讲解下写请求在 Scheduler 中是如何处理的：&lt;/p&gt;&lt;p&gt;1）Scheduler 收到 prewrite 请求的时候首先会进行流控判断，如果 Scheduler 里的请求过多，会直接返回 &lt;code&gt;SchedTooBusy&lt;/code&gt;错误，提示等一会再发送，否则进入下一步。&lt;/p&gt;&lt;p&gt;2）接着会尝试获取所需要的 latch，如果获取 latch 成功那么直接进入下一步。如果获取 latch 失败，说明有其他请求占住了 latch，这种情况说明其他请求可能也正在对相同的 key 进行操作，那么当前 prewrite 请求会被暂时挂起来，请求的上下文会暂存在 Scheduler 的 &lt;code&gt;task_contexts&lt;/code&gt; 里面。当前面的请求执行结束之后会将该 prewrite 请求重新唤醒继续执行。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;impl&amp;lt;E: Engine&amp;gt; Scheduler&amp;lt;E&amp;gt; {
    fn try_to_wake_up(&amp;amp;self, cid: u64) {
        if self.inner.acquire_lock(cid) {
            self.get_snapshot(cid);
        }
    }
    fn release_lock(&amp;amp;self, lock: &amp;amp;Lock, cid: u64) {
        let wakeup_list = self.inner.latches.release(lock, cid);
        for wcid in wakeup_list {
            self.try_to_wake_up(wcid);
        }
    }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;3）获取 latch 成功之后会调用 Scheduler 的 &lt;code&gt;get_snapshot&lt;/code&gt; 接口从 engine 获取数据库的快照。&lt;code&gt;get_snapshot&lt;/code&gt; 内部实际上就是调用 engine 的 &lt;code&gt;async_snapshot&lt;/code&gt; 接口。然后把 prewrite 请求以及刚刚获取到的数据库快照交给 &lt;code&gt;worker_pool&lt;/code&gt; 进行处理。如果该 prewrite 请求优先级字段是 &lt;code&gt;high&lt;/code&gt; 就会被分发到 &lt;code&gt;high_priority_pool&lt;/code&gt; 进行处理。&lt;code&gt;high_priority_pool&lt;/code&gt; 是为了那些高优先级请求而设计的，比如 TiDB 系统内部的一些请求要求 TiKV 快速返回，不能由于 &lt;code&gt;worker_pool&lt;/code&gt; 繁忙而被卡住。需要注意的是，目前 &lt;code&gt;high_priority_pool&lt;/code&gt; 与 &lt;code&gt;worker_pool&lt;/code&gt; 仅仅是语义上不同的两个线程池，它们内部具有相同的操作系统调度优先级。&lt;/p&gt;&lt;p&gt;4）&lt;code&gt;worker_pool&lt;/code&gt; 收到 prewrite 请求之后，主要工作是从拿到的数据库快照里确认当前 prewrite 请求是否能够执行，比如是否已经有更大 ts 的事务已经对数据进行了修改，具体的细节可以参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//ai.google/research/pubs/pub36726&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Percolator 论文&lt;/a&gt;，或者参考我们的官方博客 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-transaction-model/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiKV 事务模型概览》&lt;/a&gt;。当判断 prewrite 是可以执行的，会调用 engine 的 &lt;code&gt;async_write&lt;/code&gt; 接口执行真正的写入操作。这部分的具体的代码见 &lt;code&gt;storage/txn/process.rs&lt;/code&gt; 中的 &lt;code&gt;process_write_impl&lt;/code&gt; 函数。&lt;/p&gt;&lt;p&gt;5）当 &lt;code&gt;async_write&lt;/code&gt; 执行成功或失败之后，会调用 Scheduler 的 &lt;code&gt;release_lock&lt;/code&gt; 函数来释放 latch 并且唤醒等待在这些 latch 上的请求继续执行。&lt;/p&gt;&lt;h3&gt;5. MVCC&lt;/h3&gt;&lt;p&gt;TiKV MVCC 相关的代码位于 &lt;code&gt;storage/mvcc&lt;/code&gt; 文件夹下，强烈建议大家在阅读这部分代码之前先阅读 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//ai.google/research/pubs/pub36726&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Percolator 论文&lt;/a&gt;，或者我们的官方博客 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-transaction-model/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiKV 事务模型概览》&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;MVCC 下面有两个比较关键的结构体，分别为 &lt;code&gt;MvccReader&lt;/code&gt; 和 &lt;code&gt;MvccTxn&lt;/code&gt;。&lt;code&gt;MvccReader&lt;/code&gt; 位于 &lt;code&gt;storage/mvcc/reader/reader.rs&lt;/code&gt; 文件中，它主要提供读功能，将多版本的处理细节隐藏在内部。比如 &lt;code&gt;MvccReader&lt;/code&gt; 的 &lt;code&gt;get&lt;/code&gt; 接口，传入需要读的 key 以及 ts，返回这个 ts 可以看到的版本或者返回 &lt;code&gt;key is lock&lt;/code&gt; 错误等。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;impl&amp;lt;S: Snapshot&amp;gt; MvccReader&amp;lt;S&amp;gt; {
    pub fn get(&amp;amp;mut self, key: &amp;amp;Key, mut ts: u64) -&amp;gt; Result&amp;lt;Option&amp;lt;Value&amp;gt;&amp;gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;MvccTxn&lt;/code&gt; 位于 &lt;code&gt;storage/mvcc/txn.rs&lt;/code&gt; 文件中，它主要提供写之前的事务约束检验功能，上一节 prewrite 请求的处理流程中第四步就是通过调用 &lt;code&gt;MvccTxn&lt;/code&gt; 的 prewrite 接口来进行的事务约束检验。&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;TiKV 端事务相关的实现都位于 Storage 模块中，该文带大家简单概览了下这部分几个关键的点，想了解更多细节的读者可以自行阅读这部分的源码（code talks XD）。另外从 3.0 版本开始，TiDB 和 TiKV 支持悲观事务，TiKV 端对应的代码主要位于 &lt;code&gt;storage/lock_manager&lt;/code&gt; 以及上面提到的 MVCC 模块中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读&lt;/b&gt;：&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tikv-soucre-code-reading-11/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章（十一）Storage - 事务控制层 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多 TiKV 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Blog-cns | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-29-75708576</guid>
<pubDate>Mon, 29 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>文稿分享记录：TiDB Operator 设计与实现</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-23-74897388.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/74897388&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2d703c5defe9201ab3017cfe917a6c32_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;刚刚在 DockOne 微信群做了一次文稿分享，由于临到开始前才知道分享形式是纯文稿的，所以只列了一下提纲，没有 markdown 格式的正文，就先发到专栏里（方便贴图）明天再整理到 &lt;a href=&quot;https://link.zhihu.com/?target=http%3A//aleiwu.com/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;aleiwu.com&lt;/a&gt; 上。&lt;br/&gt;全文无改动，原汁原味还原分享现场🤣（什么鬼）&lt;/blockquote&gt;&lt;h2&gt;正文&lt;/h2&gt;&lt;p&gt;大家好，我是 PingCAP 的 Cloud 工程师吴叶磊，目前在做 TiDB Operator 相关的开发工作，很高兴今天能跟大家分享一下 TiDB Operator 这个项目背后的一些东西。(自己整理自己的分享感觉好奇怪啊。。。虽然只是复制粘贴）&lt;/p&gt;&lt;p&gt;首先要说的当然是我们为什么要做 TiDB Operator，这得从 TiDB 本身的架构开始说起。下面是 TiDB 的架构图：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1198&quot; data-rawheight=&quot;742&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1198&quot; data-original=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1198&quot; data-rawheight=&quot;742&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1198&quot; data-original=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-4baf6db8e1c8e1b4ca66419711418382_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;其中，TiKV 是一套分布式的 Key-Value 存储引擎，它是整个数据库的存储层，在 TiKV 中，数据被分为一个个 Region，而一个 Region 就对应一个 Raft Group，使用 Raft 协议做 Log Replication 来保证数据的强一致性。那么自然容易想到，我们只要水平增加 TiKV 节点数，再把数据切成更多的 Region 均匀分布在这些节点上，就能实现存储层的水平扩展。&lt;/p&gt;&lt;p&gt;TiDB 则是计算执行层，负责 SQL 的解析和查询计划优化，真正执行 SQL 时则通过 TiKV 提供的 API 来访问数据。&lt;/p&gt;&lt;p&gt;最后是 PD，PD 是集群的“大脑”，它一方面是是集群的 metadata server，TiKV 中的数据分布情况都会通过心跳上报给 PD 存储起来；另一方面又承担集群数据调度的任务，我们前面说 TiKV 要把数据拆成更多的 Region 均匀分布到节点上，什么时候拆、怎么拆、拆完分配到哪些节点上这些事情就都是 PD 通过调度算法来决定的。从直观上，PD 其实有点像 Kubernetes 里的 Control Plane。&lt;/p&gt;&lt;p&gt;这么一套架构的优势是分层清晰，指责明确，每一层都可以独立地做功能扩展和规模上的水平伸缩。但是这对于运维管理来说，是一个巨大的挑战，再加上 TiDB 本身的一些比较复杂的分布式共识算法和事务算法，可以说是把整个 TiDB 的运维入门门槛拉得相当高。另一方面，传统的基于虚拟机的部署方式也不能很好地发挥 TiDB 水平伸缩和故障自动转移的潜力。所以其实我们在内部很早就在尝试使用 Kubernetes 来编排管理 TiDB 集群，甚至在这个开源的 TiDB Operator 之前，我们还有一版废弃掉的 TiDB Operator。最后的事实也确实证明我们一直以来的选择和投入是正确的，相信大家听了后面的分析，也会认同这一点。&lt;/p&gt;&lt;p&gt;接下来我们正式进入 TiDB Operator 的解读。其实 Operator 模式在 Kubernetes 社区已经不新鲜了，现在大部分流行的有状态应用都有自己的 Operator。但回顾一下 Operator 的一些概念仍然非常必要。&lt;/p&gt;&lt;p&gt;我们知道 Kubernetes 里两个很重要的概念就是声明式 API 和控制循环。所有的 API 对象都是对用户意图的记录，再由控制器去 watch 这些意图，对比实际状态，执行调谐（reconcile）操作来驱动集群达成用户意图。Kubernetes 本身有很多的内置 API 对象，比如 ReplicaSet 表达我们需要一个应用有几个实例，DaemonSet 表达我们希望在部分被选中的节点上每个节点运行且只运行一个实例。那我们该怎么向 Kubernetes 表达 “我需要一个 TiDB 集群呢“？答案就是定义一个用于描述 TiDB 集群的对象，在 Kubernetes 中，目前有两种方式可以定义一个新对象，一是 CustomResourceDefinition（CRD）、二是 Aggregation ApiServer（AA），其中 CRD 是相对简单也是目前应用比较广的方法。TiDB Operator 就用 CRD 定义了一个 ”TidbCluster” 对象。&lt;/p&gt;&lt;p&gt;有了对象还没完，这个对象现在谁都还不认识它呢。这时候就是自定义控制器出场的时候了，我们的自定义控制器叫 tidb-controller-manager，它会 watch TidbCluster 对象和其它一些相关对象，并且按照我们编写的逻辑做调谐来驱动真实的 TiDB 集群向我们定义的终态转移。&lt;/p&gt;&lt;p&gt;CRD 加上控制器就是典型的 Operator 模式了。当然这还没完，很多逻辑控制器也是无能为力的，比如 Pod 的调度逻辑。这一块为了实现 TiDB 容器的自定义调度策略，我们编写了 Scheduler Extender。还有一些验证逻辑，比如某些特殊情况下，我们要阻止集群的变更，这样的逻辑就用 Admission Webhook 来实现。而在用户侧，我们则开发了 kubectl plugin 来做 TiDB 的一些特定操作。所以大家就可以知道，TiDB Operator 其实不止于 Operator，我们的核心理念是利用 Kubernetes 大量的扩展点，为 Kubernetes 全面注入 TiDB 的领域知识，把 Kubernetes 打造成 TiDB 的一个最佳底座。&lt;/p&gt;&lt;p&gt;这样做有两大好处(划重点）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;一是在 Kubernetes 基于控制循环的自运维模式下，我们可以把 TiDB 的运维门槛降到最低，让入门用户也能轻松搞定水平伸缩和故障转移这些高级玩法；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;二是我们基于 Kubernetes 的 Restful API 提供了一套标准的集群管理 API，用户可以拿着这个 API 把 TiDB 集成到自己的工具链或 PaaS 平台中，真正赋能用户去把 TiDB 玩好玩精。&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面说了一些比较玄乎的、方法论上的东西，可能大家都觉得脚快够不着地了 。下面我们就讲一些技术干货，用一些功能场景来解析 TiDB Operator 的实现机制，更为重要的是，我们认为这里面的一些套路对于在 Kubernetes 上管理有状态应用是通用的，可能能给大家带来一些启发。&lt;/p&gt;&lt;p&gt;第一是 TiDB Operator 该怎么去构建一个 TiDB 集群。我们尝试过直接操作 Pod，最后的结论是工作量太大了，k8s 自己的控制器里处理了大量的 corner case，并且有大量的单测和 e2e 测试来保障正确性，我们要自己再去实现一遍成本很高。因此我们最后的选型是 TiDB Operator 分别为 PD、TiKV、TiDB 创建一个 StatefulSet，再去管理这些 StatefulSet 来实现优雅升级和故障转移等功能。大家也可以看到有很多社区的 Operator 都是这么做的，而且部分没有这么做的 Operator 已经开始反思了，比如 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/elastic/cloud-on-k8s/issues/1173&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/elastic/clou&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;d-on-k8s/issues/1173&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;。确实按照我们的经验，管理有状态应用的 Operator 往往做到后来发现是需要自己实现 statefulset 80%的功能的，选择直接管理裸 Pod 就有点吃力不讨好了 。🤔&lt;/p&gt;&lt;p&gt;第二个是 Local PV，大部分存储型应用对磁盘性能是相当敏感的，因此 Local PV 是一个必选项。好在现在 Kubernetes 的 Local PV 支持已经比较成熟了，也有 local-pv-provisioner 来辅助创建 Local PV。但一个很让人头大的问题是用了 Local PV 之后，Pod 就和特定节点绑死了，节点故障后要调度到其它机器必须手动删除 PVC，这其实不是编排层能解决的问题，因为本地磁盘相比于背后通常会有三副本的网络存储本身就是不可靠的，使用本地磁盘的应用必须得在应用层做数据冗余。当然，TiDB 的存储层 TiKV 本身就是多副本高可用的，这种情况下我们采取的策略是不管旧的 Pod，直接创建新 Pod 来做故障转移，利用 TiKV 本身的数据调度把数据在新 Pod 上补齐。&lt;/p&gt;&lt;p&gt;接下来就是故障转移怎么做的问题。我们知道 StatefulSet 提供的语义保证是相同名字的 Pod 集群中同时最多只有一个，也就是假如发生了节点宕机，StatefulSet 是不会帮助我们做故障转移的，因为这时候 Kubernetes 并不知道是节点宕机还是网络分区，也就是它无法确定节点上的 Pod 还在不在跑。我们假设挂掉的 Pod 叫 tikv-0，那这时候 k8s 再创建一个 tikv-0 就脑裂了。当然了，在公有云上不会有这个问题，因为公有云上 Node Controller 会通过公有云 API 检查节点是不是真的消失了，假如是的话就会移除节点，那 k8s 就知道 tikv-0 不可能再运行，可以做故障转移了。可惜一难接一难😂，故障转移之后 Pod 又会碰到找不到 Local PV 的问题而 Pending🤣……&lt;/p&gt;&lt;p&gt;我们最终的解决方案是在 Tide cluster 对象的 status 中记录当前挂掉的 Pod，这个挂掉是指一方面 k8s 认为 Pod 挂了，另一方面，TiDB 集群，也就是 PD 也认为这个实例挂了，这个非常重要，因为我们实际场景中就遇到过因为单边网络问题 apiserver 认为节点掉线而其实正常运行的，这时候 PD 就救了我们一命。我们在控制循环中专门同步这些状态，一旦两面都确认某个 Pod 以及 Pod 中的实例挂了，我们就在 status 里记录下来。而另一个扩缩容控制循环会检查这个 status，假如有挂掉的实例，就给 StatefulSet 的副本数+1，实现故障转移。示意图如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1126&quot; data-rawheight=&quot;492&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1126&quot; data-original=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1126&quot; data-rawheight=&quot;492&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1126&quot; data-original=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-7badeb6addf2614d20ab4687868e4262_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;大家可以看到，控制器里是结合了 k8s 的信息和 PD 的信息去更新这个 failureStore 字段。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1242&quot; data-rawheight=&quot;590&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1242&quot; data-original=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1242&quot; data-rawheight=&quot;590&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1242&quot; data-original=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-f2452dd2a881e9be8ca0b5d35fcaaca7_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;当我们确认 k8s 层面和业务层面（PD）都认为实例恢复正常后，我们就会从 failureStore 中删除对应实例，自动把实例数降下来，当然对于 PD 我们是这么做的，对于 TiKV，我们把删除 failureStore 这一步交给了用户，避免节点迁移次数过多，数据迁移太频繁影响集群性能。从这个 case 我们可以看到，在自定义控制器里糅合业务状态（来自业务，比如 TiDB 的 PD）与基础设施状态（来自 k8s）是重要且必要的。&lt;/p&gt;&lt;p&gt;第三个想说的是优雅升级，以 TiKV 为例，优雅升级就是在升级前主动逐出待升级实例上的所有 Raft Group 的 Leader，避免出现请求失败。大家可能会说这个用 preStopHook 可以做，但 preStopHook 的超时时间是一个比较难确定的东西，而且也不够灵活，我们最后选择是在 Controller 中实现，示意如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;656&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1238&quot; data-original=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1238&quot; data-rawheight=&quot;656&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1238&quot; data-original=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-3cae228a8c4095275d78230dbc307b88_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;大家可以看到，我们其实用 StatefulSet 的 partition 字段来控制哪些序号可以被升级到新版本，哪些要呆在旧版本。升级开始时，partition = 节点数-1&lt;b&gt;（这里分享时写错了，见后文 QA）&lt;/b&gt;，也就是所有的 Pod 都不升级，然后呢，我们会去判断下一个待升级的 Pod 上是否存在 Leader，假如存在就进行逐出，逐出之后就 return 了，因为控制循环会不断进入，所以我们就会不断检查目标 Pod 上的 leader 是否逐出完了，一旦逐出完毕，就会往下走，将 partition - 1，让 k8s 把目标 Pod 升级到新版本，这样不断循环，确保每个节点在升级前都已经清干净了 leader，做到业务完全无损。后续呢，我们希望把这个功能放到 ValidatingAdmissionWebhook 上来实现，这样呢，可以做到功能与 controller 完全正交，大大提升可维护性，具体的方案我在个人博客里也有记录 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//aleiwu.com/post/tidb-opeartor-webhook/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;aleiwu.com/post/tidb-op&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;eartor-webhook/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; (不是广告（才怪 🤪&lt;/p&gt;&lt;p&gt;我们在控制器里还有很多这样和 TiDB 本身的架构和特性深度集成的功能设计，所以大家可以看到，做一个 Operator 的前提条件是要对你要运维的系统架构做到了若指掌，甚至对源码也要有所了解。&lt;/p&gt;&lt;p&gt;说了很多的 tidb-controller-manager，最后说一下 tidb-scheduler，tidb-scheduler 其实是利用 k8s 本身的调度器扩展机制开发的，我们把 kube-scheduler 和 tidb-scheduler 打到了一个 pod 里，并且整个注册为 ”tidb-scheduler“，这样所有标记了使用该 schduler 的 pod 就能走到我们所定制的调度逻辑。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1818&quot; data-rawheight=&quot;786&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1818&quot; data-original=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1818&quot; data-rawheight=&quot;786&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1818&quot; data-original=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-ad261d417077a8043d7b4c567b3ee49d_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;这里讲一个调度策略作为例子，PD 的高可用调度，PD 里内嵌了一个 etcd，所以它是一个基于 quorum 的共识系统，需要 majority 也就是超过一半的节点存活来保证可用性，我们的调度目标就是不在一台机器上部署超过半数的 PD 节点。你可能认为用 inter-pod anti-affinity 也能实现这个需求，但其实不是这样的。&lt;/p&gt;&lt;p&gt;anti-affinity 有两种，soft 和 hard，对于 soft 的反亲和性，当无法满足反亲和时，Pod 仍会被调度到同一个节点上，而 hard 则禁止这种情况出现。我们举个一个看看反亲和性为什么不能完美满足 quorum based 的系统调度需求：&lt;/p&gt;&lt;p&gt;我们假设现在有 3 个 node，5 个 pd 实例，那么下面这样的排布是能接受的：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1056&quot; data-rawheight=&quot;470&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1056&quot; data-original=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1056&quot; data-rawheight=&quot;470&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1056&quot; data-original=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-65751c7a52f445794fb18b26ebe8baab_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;假如我们使用 hard 的反亲和性，这个拓扑无法接受；&lt;/li&gt;&lt;li&gt;假如我们使用 soft 的 f反亲和性，假设现在其中一个节点挂了，那么 Pod 就会转移到其它节点上，这时候由于 k8s 没有 de-schedule 机制，即使我们恢复了挂掉的节点，集群拓扑也不会转移回来；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;那么 tidb-scheduler 中是怎么做的呢？策略也很简单，对于每个 Node，我们假设 Pod 调度到了目标 Node 上，再计算上面的实例数是否大于一半，假如是的话，就在 filter 阶段剔除这个 Pod。使用了这样的策略之后，大家可以推演一下，上面的拓扑是可以调度出来的，而且当节点挂掉之后，PD 实例会 Pending，不会带来一个存在风险的拓扑结构。&lt;/p&gt;&lt;p&gt;时间有限，只能分享这么多了。最后呢，是用 operator 管理有状态应用的一点点总结：&lt;/p&gt;&lt;p&gt;1.站在巨人的肩膀上，尽量复用 k8s 原生对象；&lt;/p&gt;&lt;p&gt;2.使用 local pv 必须在应用层实现数据冗余；&lt;/p&gt;&lt;p&gt;3.operator 要尽可能多地去结合业务状态，通过 apiserver 推导出的业务状态在大规模集群下未必准确；&lt;/p&gt;&lt;p&gt;4.不要只着眼于自定义控制器，k8s 的扩展点还有很多，善加利用能够大幅降低复杂度；&lt;/p&gt;&lt;p&gt;最后的最后，TiDB Operator &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-operator&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; 也即将在本月 GA 了，还有很多来不及分享的特性等着大家，欢迎大家到时候关注。&lt;/p&gt;&lt;h2&gt;QA&lt;/h2&gt;&lt;p&gt;Q1：升级开始时，partition = 节点数-1，也就是所有的 Pod 都不升级,为啥是partition = 节点数-1？&lt;/p&gt;&lt;p&gt;A：这里要纠错一下，是 pod ordinal 从 0 开始计数，大于或等于 partition 序号的 pod 会被升级 ，所以最大的序号是节点数-1，最开始的 partition 是等于节点数，分享时表达错了（我自己也记错了），抱歉😅；&lt;/p&gt;&lt;p&gt;（这里其实是我在分享时犯错了，虽然看到问题反应过来了，但还是非常尴尬，感觉自己像个沙雕）&lt;/p&gt;&lt;p&gt;Q2：还有就是驱逐leader成功了怎么防止要升级的pod重新被选为leader&lt;/p&gt;&lt;p&gt;A：我们实际上是在 PD 中提交了一个驱逐 leader 的任务，PD 会持续保证驱逐完毕后没有新 leader 进来，直到升级完毕后，由控制器移除这个任务；&lt;/p&gt;&lt;p&gt;Q3：集群规模多大？多少 pod node ?&lt;/p&gt;&lt;p&gt;A：我们在 Kubernetes 上内部测试的规模较大的集群有 100 + TiKV 节点 50+ TiDB 节点，而每位研发都会部署自己的集群进行性能测试或功能测试；&lt;/p&gt;&lt;p&gt;Q4： 请问你们实现精准下线某一个pod 的功能了嘛，因为statefulset是顺序的？如何实现的？可以分享下思路嘛？ &lt;/p&gt;&lt;p&gt;A：这个功能在 1.0 中还没有实现，我们计划在 1.1 中实现这个特性。&lt;/p&gt;&lt;p&gt;Q5：想了解下数据库容器化，推荐使用localpv吗，有没有哪些坑或最佳实践推荐？我们在考虑mysql数据库容器化以及中间件容器化，是选择localpv还是线下自建ceph集群？&lt;/p&gt;&lt;p&gt;A：Local PV 其实不是一个选项，而是一个强制因素，因为网络盘的 IOPS 是达不到在线存储应用的生产环境需求的，或者说不是说线上完全不能用，而是没法支撑对性能要求比较高的场景。MySQL 的运维我相对不是很清楚，假如 MM 能够做到双副本冗余强一致的话，那理论上就能用。大多数中间件比如 Kafka、Cassandra 都有数据冗余，这些使用 local pv 在理论上都是没问题的。&lt;/p&gt;&lt;p&gt;Q6：看你的方案感觉k8s和pd的逻辑结合在一起了，二者之间如何互通？会有代码互相侵入吗？明白了，就好像问题2驱逐问题，pd收到驱逐任务，k8s控制器不断的检查是否驱逐成功，如果成功就开始升级，对吧？&lt;/p&gt;&lt;p&gt;A：这就是自定义控制器的绝佳场景了，k8s 和 pd 本身完全没有交互，是控制循环在同步两边的状态，一方面控制循环会把 PD 记录的集群状态塞到 TidbCluster 对象的 status 里面，另一方面控制循环在将实际状态向期望状态转移时，也会生成一些 PD 的任务和操作子（Opeartor）提交到 PD 中来调谐集群状态。&lt;/p&gt;&lt;h2&gt;最后&lt;/h2&gt;&lt;p&gt;最后当然是招人啦，假如你对我们正在做的事情感兴趣，无论是 Cloud 也好数据库研发也好，都以联系 wuyelei@pingcap.com 投递简历勾搭。&lt;/p&gt;</description>
<author>吴叶磊</author>
<guid isPermaLink="false">2019-07-23-74897388</guid>
<pubDate>Tue, 23 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>DM 源码阅读系列文章（十）测试框架的实现</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-23-74873268.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/74873268&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0e7419982b6c2b2cd4a4b4f88e89c20b_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：杨非&lt;/p&gt;&lt;p&gt;本文为 DM 源码阅读系列文章的第十篇，之前的文章已经详细介绍过 DM 数据同步各组件的实现原理和代码解析，相信大家对 DM 的实现细节已经有了深入的了解。本篇文章将从质量保证的角度来介绍 DM 测试框架的设计和实现，探讨如何通过多维度的测试方法保证 DM 的正确性和稳定性。&lt;/p&gt;&lt;h2&gt;测试体系&lt;/h2&gt;&lt;p&gt;DM 完整的测试体系包括以下四个部分：&lt;/p&gt;&lt;h3&gt;1. 单元测试&lt;/h3&gt;&lt;p&gt;主要用于测试每个 go 模块和具体函数实现的正确性，测试用例编写和测试运行方式依照 go 单元测试的标准，测试代码跟随项目源代码一起发布。具体测试用例编写使用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/check&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pingcap/check&lt;/a&gt; 工具包，该工具包是在 go 原生测试工具基础上进行的扩展，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/check/blob/67f458068fc864dabf17e38d4d337f28430d13ed/run.go%23L98-L131&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;按照 suite 分组进行测试&lt;/a&gt;，提供包括更丰富的检测语法糖、并行测试、序列化测试在内的一些扩展特性。单元测试的设计出发点是白盒测试，测试用例中通过尽可能明确的测试输入得到期望的测试输出。&lt;/p&gt;&lt;h3&gt;2. 集成测试&lt;/h3&gt;&lt;p&gt;用于测试各个组件之间交互的正确性和完整数据同步流程的正确性，完整的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;测试用例集合和测试工具在项目代码的 tests 目录&lt;/a&gt; 发布。集成测试首先自定义了一些 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/_utils&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM 基础测试工具集&lt;/a&gt;，包括启动 DM 组件，生成、导入测试数据，检测同步状态、上下游数据一致性等 bash 脚本，每个测试用例是一个完整的数据同步场景，通过脚本实现数据准备、启动 DM 集群、模拟上游数据输入、特定异常和恢复、数据同步校验等测试流程。集成测试的设计出发点是确定性的模拟测试场景，为了能够确定性的模拟一些特定的同步场景，为此我们还引入了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/failpoint&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;failpoint&lt;/a&gt; 来注入测试、控制测试流程， 以及 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/tracing&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;trace&lt;/a&gt; 机制来更准确地获取程序内存状态、辅助控制测试流程，具体的实现细节会在后文详细介绍。&lt;/p&gt;&lt;h3&gt;3. 破坏性测试&lt;/h3&gt;&lt;p&gt;真实的软件运行环境中会遇到各种各样的问题，包括各类硬件故障、网络延迟和隔离、资源不足等等。DM 在数据同步过程中也同样会遇到这些问题，借助于 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//thenewstack.io/chaos-tools-and-techniques-for-testing-the-tidb-distributed-newsql-database/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PingCAP 内部的自动化混沌测试平台 schrodinger&lt;/a&gt;，我们设计了多个破坏性测试用例，包括在同步过程中随机 kill DM-worker 节点，同步过程中重启部分 DM-worker 节点，分发不兼容 DDL 语句等测试场景。这一类测试的关注点是在各类破坏性操作之后数据同步能否正常恢复以及验证在这些场景下数据一致性的保证，测试用例通常以黑盒的形式去运行，并且长期、反复地进行测试。&lt;/p&gt;&lt;h3&gt;4. 稳定性测试&lt;/h3&gt;&lt;p&gt;目前该类测试运行在 PingCAP 内部的 K8s 集群上，通常每个测试的应用规模会比较大，譬如有一些 100+ 上游实例，300+ 分库分表合并的测试场景，数据负载也会相对较高，目标在于测试大规模 DM 集群在高负载下长期运行的稳定性。该类测试也属于黑盒测试，每个测试用例内会根据任务配置启动上游的 MySQL 集群、DM 集群、下游 TiDB 集群和数据导入集群。上游数据输入工具有多种，包括 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/amyangfei/data-dam&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;随机 DML 生成工具&lt;/a&gt;，schrodinger 测试用例集等。具体的测试 case 和 K8s 部署脚本可以在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/csuzhangxc/dm-k8s&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;dm-K8s 仓库&lt;/a&gt; 找到。&lt;/p&gt;&lt;h3&gt;5. 测试方法对比&lt;/h3&gt;&lt;p&gt;我们通过以下的表格对比不同测试维度在测试体系中发挥的作用和它们之间的互补性。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1220&quot; data-rawheight=&quot;926&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1220&quot; data-original=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1220&quot; data-rawheight=&quot;926&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1220&quot; data-original=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-147a4197ffd090538bcfe391be1b116e_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h2&gt;测试 case 与测试工具的实现 &lt;/h2&gt;&lt;h3&gt;1. 在单元测试中进行 mock&lt;/h3&gt;&lt;p&gt;我们在单元测试运行过程中希望尽量减少外部环境或内部组件的依赖，譬如测试 relay 模块时我们并不希望从上游的 MySQL 拉取 binlog，或者测试到下游的一些数据库读写操作并不希望真正部署一个下游 TiDB，这时候我们就需要对测试 case 进行适当的 mock。在单元测试中针对不同的场景采用了多种 mock 方案。接下来我们选取几种具有代表性的方案进行介绍。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Mock golang interface&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 golang 中只要调用者本身实现了接口的全部方法，就默认实现了该接口，这一特性使得使用接口方法调用的代码具有良好的扩展性，对于测试也提供了天然的 mock 方法。以 worker 内部各 subtask 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/worker/subtask_test.go%23L258&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;任务暂停、恢复的测试用例&lt;/a&gt; 为例，测试过程中会涉及到 dump unit 和 load unit 的运行、出错、暂停和恢复等操作。我们定义 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/worker/subtask_test.go%23L67-L76&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MockUnit&lt;/a&gt; 并且实现了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/unit/unit.go%23L24&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;unit interface&lt;/a&gt; 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/worker/subtask_test.go%23L86-L124&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;全部方法&lt;/a&gt;，就可以在单元测试里模拟任务中 unit 的各类操作。还可以定义 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/worker/subtask_test.go%23L126-L143&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;各类注入函数&lt;/a&gt;，实现控制某些逻辑流程中的出错测试和执行路径控制。&lt;/p&gt;&lt;p&gt;&lt;b&gt;自定义 binlog 生成工具&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在前文已经介绍过 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/dm-source-code-reading-6/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;relay 处理单元从上游读取 binlog 并写入本地文件&lt;/a&gt; 的实现细节，这一过程重度依赖于 MySQL binlog 的处理和解析。为了在单元测试中完善模拟 binlog 数据流，DM 中实现了一个 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/binlog/event&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;binlog 生成工具&lt;/a&gt;，该工具包提供了通用的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/binlog/event/generator.go%23L25&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;generator&lt;/a&gt; 用于连续生成 Event 以及相对底层的生成特定 Event 的接口，支持 MySQL 和 MariaDB 两种数据库的 binlog 协议。generator 提供的生成接口会返回一个 go-mysql 的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/siddontang/go-mysql/blob/7ed1210c02a2867a8d4570f526422af9fcd4246b/replication/event.go%23L25&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;BinlogEvent&lt;/a&gt;&lt;/code&gt; 列表和 binlog 对应的 byte 数组，同时在 generator 中自动更新 binlog 位置信息和 &lt;code&gt;GTID&lt;/code&gt; 信息。类似的，更底层的生成 Event 接口会要求提供数据类型、&lt;code&gt;serverID&lt;/code&gt;、&lt;code&gt;latestPos&lt;/code&gt;、&lt;code&gt;latestGTID&lt;/code&gt; 以及可能需要的库名、表名、SQL 语句等信息，生成的结果是一个 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/binlog/event/common.go%23L28&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDLDMLResult&lt;/a&gt;&lt;/code&gt; 对象。&lt;/p&gt;&lt;p&gt;我们通过测试中的一个 case 来了解如何使用这个工具，以 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L370&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;relay 模块读取到多个 binlog event 写入文件的正确性测试&lt;/a&gt; 这个 case 为例：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=h%3Ccode%3Ettps%3A//g%3C/code%3Ei%3Ccode%3Ethub%3C/code%3E.co%3Ccode%3Em/p%3C/code%3Eingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L371-L387&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;首先配置数据库类型，serverID，GTID 和 XID 相关信息，初始化 relay log 写入目录和文件名&lt;/a&gt;&lt;/li&gt;&lt;li&gt;ref=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L390&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L390&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;初始化 allEvents 数组，用于模拟从上游接收到的 &lt;code&gt;replication.BinlogEvent&lt;/code&gt;；ref=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L391&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L391&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;初始化 allData，&lt;code&gt;allData&lt;/code&gt; 存储 binlog binary 数据，用于后续 relay log 写入的验证；ref=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L392&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L392&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;初始化 generator&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3Ccode%3Es%3A//github.co%3C/code%3Em/ping%3Ccode%3Ecap/dm/blob/7cba6d21d78%3C/code%3Edd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L396&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;通过 generator GenFileHeader 接口生成 replication.BinlogEvent 和 binlog 数据&lt;/a&gt;（对应的 binlog 中包含 &lt;code&gt;FormatDescriptionEvent&lt;/code&gt; 和 &lt;code&gt;PreviousGTIDsEvent&lt;/code&gt;）。生成的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github%3C/code%3E.com/%3Ccode%3Epingcap/d%3C/code%3Em/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L398&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;replication.BinlogEvent 保存到 allEvents&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=http%3Ccode%3Es%3A//git%3C/code%3Ehub.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L399&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;binlog 数据保存到 allData&lt;/a&gt;。&lt;/code&gt;&lt;/li&gt;&lt;li&gt;按照 3 的操作流程分别href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//gi&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;gi&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;code&gt;thub.com/pin&lt;/code&gt;gcap/&lt;code&gt;dm/blo&lt;/code&gt;b/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L402-L421&amp;#34;&amp;gt;生成 CREATE DATABASE，CREATE TABLE 和一条 INSERT 语句对应的 event/binlog 数据并保存&lt;/li&gt;&lt;li&gt;href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;code&gt;blob/7cba6d21d78dd16e9a&lt;/code&gt;b159e9c0300efcbdeb1e4a/relay/writer/file_test.go#L424-L430&amp;#34;&amp;gt;创建 relay.FileWriter，按照顺序读取 3, 4 步骤中保存的 replication.BinlogEvent，向配置的 relay log 文件中写入 relay log&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.%3Ccode%3Ecom/pin%3C/code%3Egcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L432&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;检查 relay log 文件写入的数据长度与 allData 存储的数据长度相同&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.%3Ccode%3Ecom/pin%3C/code%3Egcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/relay/writer/file_test.go%23L435-L438&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;读取 relay log 文件，检查数据内容和 allData 存储的数据内容相同&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;至此我们就结合 binlog 生成工具完成了一个 relay 模块的测试 case。目前 DM 已经在很多 case 中使用 binlog 生成工具模拟生成 binlog，仍然存在的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/syncer/syncer_test.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;少量 case&lt;/a&gt; 依赖上游数据库生成 binlog，我们已经计划借助 binlog 生成工具移除这些外部依赖。&lt;/p&gt;&lt;p&gt;&lt;b&gt;其他 mock 工具&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在验证数据库读写操作逻辑正确性的测试中，使用了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/DATA-DOG/go-sqlmock&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;go-sqlmock&lt;/a&gt; 来 mock sql driver 的行为。&lt;/li&gt;&lt;li&gt;在验证 gRPC 交互逻辑的正确性测试中，使用了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/golang/mock&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;官方提供的 mock 工具&lt;/a&gt;，针对 gRPC 接口生成 mock 文件，在此基础上测试 gRPC 接口和应用逻辑的正确性。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;2. 集成测试的方法和相关工具&lt;/h3&gt;&lt;p&gt;&lt;b&gt;Trace 信息收集&lt;/b&gt;&lt;/p&gt;&lt;p&gt;DM 内部定义了一个简单的信息 trace 收集工具，其设计目标是在 DM 运行过程中，通过增加代码内部的埋点，定期收集系统运行时的各类信息。trace 工具包含一个提供 gRPC 上报信息接口和 HTTP 控制接口的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/tracer&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tracer 服务器&lt;/a&gt; 和提供埋点以及后台收集信息上传功能的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/tracing&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tracing 包&lt;/a&gt;。tracing 模块上传到 tracer 服务器的事件数据通过 &lt;code&gt;protobuf&lt;/code&gt; 进行定义，&lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/proto/tracer_base.proto%23L11-L18&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;BaseEvent&lt;/a&gt;&lt;/code&gt; 定义了最基本的 trace 事件，包含了运行代码文件名、代码行、事件时间戳、事件 ID、事件组 ID 和事件类型，用户自定义的事件需要包含 &lt;code&gt;BaseEvent&lt;/code&gt;。tracing 模块会 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/pkg/tracing/tracer.go%23L129&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;定期向 tracer 服务器同步全局时间戳&lt;/a&gt;，通过这种方式保证多节点不同的 trace 事件会保持大致的时间顺序（注意这里并不是严格的时间序，会依赖于每分钟内本地时钟的准确性，仍然有各种出现乱序的可能）。设计 tracing 模块的主要目的有以下两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对于同一个 DM 组件（DM-master/DM-worker），希望记录一些重要内存信息的数据流历史。例如在 binlog replication 处理单元处理一条 query event 过程中会经历处理 binlog event 、生成 ddl job、执行 job 这三个阶段，我们将这三个处理逻辑抽象为三个事件，三个事件在时间上是有先后关系的，在逻辑上关联了同一个 binlog 的处理流程，在 DM 中记录这三个事件的 trace event 时使用了同一个 &lt;code&gt;traceID&lt;/code&gt;（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/syncer/syncer.go%23L1597&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;处理 binlog event 生成一个新的 traceID&lt;/a&gt;，该 &lt;code&gt;traceID&lt;/code&gt; 记录在 ddl job 中，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/syncer/syncer.go%23L688&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;分发 ddl job 时记录的 trace 事件会复用此 traceID&lt;/a&gt;；&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cb%3Ccode%3Ea6d21d7%3C/code%3E8dd16e9ab159e9c0300efcbdeb1e4a/syncer/syncer.go%23L864&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;在 executor 中最后执行 ddl job 的过程中记录的 trace 事件也会复用此 traceID&lt;/a&gt;），这样就将三个事件关联起来，因为在同一个进程内，他们的时间戳真实反映了时间维度上的顺序关系。&lt;/li&gt;&lt;li&gt;由于 DM 提供了 shard DDL 的机制，多个 DM-worker 之间的数据会存在关联，譬如在进行 shard DDL 的过程中，处于同一个 shard group 内的多个 DM-worker 的 DDL 是关联在一起的。&lt;code&gt;BaseEvent&lt;/code&gt; 定义中的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/proto/tracer_base.proto%23L16&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;groupID&lt;/a&gt;&lt;/code&gt; 字段就是用来解决多进程间 trace 事件关联性的问题，定义具有相同 &lt;code&gt;groupID&lt;/code&gt; 的事件属于同一个事件组，表示它们之间在逻辑上有一定关联性。举一个例子，在 shard DDL 这个场景下，DM-master 协调 shard DDL 时会分别 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/master/server.go%23L1423-L1432&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;向 DDL owner 分发执行 SQL 的请求&lt;/a&gt;，以及 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/dm/master/server.go%23L1457-L1466&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;向非 owner 分发忽略 DDL 的请求&lt;/a&gt;，在这两组请求中携带了相同的 &lt;code&gt;groupID&lt;/code&gt;，binlog replication 分发 ddl job 时会获取到 &lt;code&gt;groupID&lt;/code&gt;，这样就将不同进程间 shard DDL 的执行关联了起来。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们可以利用收集的 trace 信息辅助验证数据同步的正确性。譬如在 href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/tree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/safe_mode&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/t&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ree/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/safe_mode&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;验证 safe_mode 逻辑正确性的测试 中，&lt;a href=&quot;https://link.zhihu.com/?target=http%3Ccode%3Es%3A//githu%3C/code%3Eb.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/safe_mode/run.sh%23L35&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;我们将 DM 启动阶段的 safe_mode 时间调短为 0s&lt;/a&gt;，期望验证对于上游 update 操作产生的 binlog，如果该操作发生时上下游 shard DDL 没有完全同步，那么同步该 binlog 时的 &lt;code&gt;safe_mode&lt;/code&gt; 为 true；反之如果该操作发生时上下游没有进行 shard DDL 或 shard DDL 已经同步，那么 &lt;code&gt;safe_mode&lt;/code&gt; 为 false。通过 trace 机制，可以很容易从 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/_dmctl_tools/check_safe_mode.go%23L42-L55&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tracer server 的接口获取测试过程中的所有事件信息&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/_dmctl_tools/check_safe_mode.go%23L123-L133&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;并且抽取出 update DML，DDL 等对应的 trace event 信息&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=htt%3Ccode%3Eps%3A//gith%3C/code%3Eub.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/_dmctl_tools/check_safe_mode.go%23L167-L180&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;进一步通过这些信息验证 safe_mode 在 shard DDL 同步场景下工作的正确性&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Failpoint 的使用&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在集成测试中，为了对特定的同步流程或者特定的错误中断做确定性测试，我们开发了一个名为 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/failpoint&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;failpoint&lt;/a&gt; 的项目，用来在代码中注入特定的错误。现阶段 DM 集成测试的 case 都是 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/7cba6d21d78dd16e9ab159e9c0300efcbdeb1e4a/tests/safe_mode/run.sh%23L35-L38&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;提前设定环境变量，然后启动 DM 相关进程来控制注入点的生效与否&lt;/a&gt;。目前我们正在探索将 trace 和 failpoint 结合的方案，通过 trace 获取进程内部状态，借助 failpoint 提供的 http 接口动态调整注入点，以实现更智能、更通用的错误注入测试。&lt;/p&gt;&lt;h3&gt;3. 破坏性测试和大规模测试的原理与展望&lt;/h3&gt;&lt;p&gt;&lt;b&gt;破坏性测试中的错误注入&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前破坏性测试的测试 case 并没有对外开源，我们在这里介绍 DM 破坏性测试中所使用的部分故障注入&lt;/p&gt;&lt;ul&gt;&lt;li&gt;使用 &lt;code&gt;kill -9&lt;/code&gt; 强制终止 DM-worker 进程，或者使用 &lt;code&gt;kill&lt;/code&gt; 来优雅地终止进程，然后重新启动&lt;/li&gt;&lt;li&gt;模拟上游写入 TiDB 不兼容的 DDL，通过 &lt;code&gt;sql-skip/sql-replace&lt;/code&gt; 跳过或替换不兼容 DDL 恢复同步的场景&lt;/li&gt;&lt;li&gt;模拟上游发生主从切换时 DM 进行主从切换处理的正确性&lt;/li&gt;&lt;li&gt;模拟下游 TiDB/TiKV 故障不可写入的场景&lt;/li&gt;&lt;li&gt;模拟网络出现丢包或高延迟的场景&lt;/li&gt;&lt;li&gt;在未来 DM 提供高可用支持之后，还会增加更多的高可用相关测试场景，譬如磁盘空间写满、DM-worker 节点宕机自动恢复等&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;大规模测试&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大规模测试中的上游负载复用了很多在 TiDB 中的测试用例，譬如银行转账、大规模 DDL 操作等测试场景。该测试所有 case 均运行在 K8s 中，基于 K8s deployment yaml 部署一系列的 statefuset，通过 &lt;code&gt;configmap&lt;/code&gt; 传递拓扑信息。目前 DM 正在规划实现 DM-operator 以及运行于 K8s 之上的完整解决方案，预期在未来可以更便捷地部署在 K8s 环境上，后续的大规模测试也会基于此继续展开。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;本篇文章详细地介绍了 DM 的测试体系，测试中使用到的工具和一些 case 的实例分析，分析如何通过多维度的测试保证 DM 的正确性、稳定性。然而尽管已经有了如此多的测试，我们仍不能保证 bug free，也不能保证测试 case 对于各类场景和逻辑路径进行了百分之百的覆盖，对于测试方法和测试 case 的完善仍需要不断的探索。&lt;/p&gt;&lt;p&gt;&lt;b&gt;至此 DM 的源码阅读系列就暂时告一段落了，但是 DM 还在不断地发展演化，DM 中长期的规划中有很多激动人心的改动和优化，譬如高可用方案的落地、DM on K8s、实时数据校验、更易用的数据迁移平台等（未来对于 DM 的一些新特性可能会有番外篇）。希望感兴趣的小伙伴可以持续关注 DM 的发展，也欢迎大家提供改进的建议和提&lt;/b&gt; &lt;b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/pulls&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PR&lt;/a&gt;。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/dm-source-code-reading-10/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/blog-cn/dm-&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;source-code-reading-10/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多 DM 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23DM-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Blog-cns | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-23-74873268</guid>
<pubDate>Tue, 23 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>PingCAP 唐刘：如何利用混沌工程打造健壮的分布式系统？</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-22-74717464.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/74717464&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a888a9babe11aed8a2c5f839eb3c8092_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：赵钰莹&lt;/p&gt;&lt;p&gt;&lt;i&gt;本文转载于 InfoQ。&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;原文链接：&lt;/i&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/article/EEKM947YbboGtD_zQuLw&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/article/EEKM94&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;7YbboGtD_zQuLw&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;作为混沌工程的重要推动者，Netflix 在混沌工程手册（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/article/AsN34J2T9QDXB0s-t9JN&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/article/AsN34J&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;2T9QDXB0s-t9JN&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）中谈到，在生产环境进行软件验证的想法通常会被嘲笑。过去，这句话基本都被翻译为“我们在发布之前不打算完善地验证这些代码”。在经典的测试链路中，寻找软件缺陷的普遍信条是离生产环境越远越好。例如，在单元测试中发现缺陷要比在集成测试中发现更好，这里的逻辑是：离生产环境越远，或者是离发布越远的时候，发现的缺陷就越容易被找到根本原因并彻底修复。&lt;br/&gt;对于混沌工程而言，整个链路刚好反过来：在离生产环境越近的地方进行实验越好，理想的实践就是直接在生产环境中执行。对于软件工程师来说，最难的莫过于，系统用户永远不会如预期那样与系统进行交互，混沌工程是解决这一问题的理想方法，可以让开发者了解除代码之外，整个系统其他方面的情况，特别是状态、输入、以及第三方系统导致的难以预见的行为。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;据了解，在 TiDB 的研发初期，PingCAP 就引入了混沌工程，以此保证 TiDB 在各种极端情况下的稳定性。在 ArchSummit 全球架构师峰会（深圳站）2019 大会期间，InfoQ 就混沌工程理念及实践这一话题采访了 PingCAP 首席架构师 &lt;/b&gt;&lt;a class=&quot;member_mention&quot; href=&quot;https://www.zhihu.com/people/df9ec6a48ca50364852daa71b20a6192&quot; data-hash=&quot;df9ec6a48ca50364852daa71b20a6192&quot; data-hovercard=&quot;p$b$df9ec6a48ca50364852daa71b20a6192&quot;&gt;@唐刘&lt;/a&gt;&lt;b&gt; ，以此了解 PingCAP 的实践历程。&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-3f06f4ce9ef11ea8e6f94d5a5dfd60e8_b.jpg&quot;/&gt;&lt;figcaption&gt;唐刘，PingCAP 首席架构师，主要负责分布式 Key-Value 数据库 TiKV 的研发工作，也会折腾下 TiDB 整个产品的测试，工具开发等工作。&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;混沌工程与分布式系统&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;理解是实践的前提之一，唐刘在采访中坦言，混沌工程这个名字比较容易让人困惑，包括其英文“Chaos Engineering”，初次听到这个单词时确实不太好理解。&lt;b&gt;唐刘表示：“最开始，我就是把它当成一种注入测试的方法，后来才发现这其实是一门工程学科，通过实验的方式发现问题并解决问题。”&lt;/b&gt;&lt;/p&gt;&lt;p&gt;其实，混沌工程的理念很早之前就存在，唐刘表示，过去使用的错误注入就可以理解为混沌工程的一种表现方式，只不过 Netflix 将其提炼出来变成了通用准则，只要照着相关方法就能实施混沌工程，而这一技术诞生之际就与分布式系统密切相关，在《Chaos Engineering》一书中是这样表述的：&lt;/p&gt;&lt;blockquote&gt;混沌工程是在分布式系统上进行实验的学科 , 目的是建立对系统抵御生产环境中失控条件的能力以及信心 。&lt;br/&gt;注：分布式系统就是，其中有台你根本不知道的机器故障了，有可能会让你自己的服务也故障。——Leslie Lamport&lt;/blockquote&gt;&lt;p&gt;即使可预见所有在控制范围内系统的状态，也总是会出现意外情况，比如系统依靠的某些外部服务突发宕机，这在系统搬迁上云后尤为明显，云服务也并不总是稳定可靠的。采访中，唐刘解释道，混沌工程主要是解决常规测试不能覆盖的问题。对于分布式系统来说，因为其异常的复杂性，加上错误可能在任何时候、任何地点发生，众多常规测试方法并不能保证系统正确。&lt;/p&gt;&lt;p&gt;当然，在某些场景下，直接在生产环境中进行实践是非常困难且不可用的，比如将干扰直接注入到行驶中的自动驾驶汽车的传感器上，这是比较危险的，但大部分用户应该都不是在操作这类生死攸关的系统。&lt;/p&gt;&lt;p&gt;&lt;b&gt;相比较而言，唐刘认为混沌工程比较适合对数据安全性要求较高的场景。&lt;/b&gt;此外，如果业务对故障容错有所承诺，也需要通过混沌工程验证系统是否可以支持容错。量化到具体指标来看，如果开发人员确定系统会宕机并且清楚宕机之后会造成较大损失，可以通过“支持快速终止实验”和“最小化实验造成的‘爆炸半径’”等方式实施混沌工程。&lt;/p&gt;&lt;p&gt;当执行任何混沌工程实验前，应该先有一个用来立即终止实验的“红色按钮”，更好的方法则是自动化该功能，当其监测到对稳定状态有潜在危害时立即自动终止实验。第二个策略涉及在设计实验时，考虑从实验中获得有意义结论的同时，兼顾最小化实验可能造成的潜在危害。&lt;/p&gt;&lt;p&gt;&lt;b&gt;无论是架构师、开发人员还是测试人员，唐刘都建议关注这一技术，这相当于从另一个视角审视系统，尤其是对开发者而言。唐刘补充道，开发一个优秀的系统并不只是写代码就足够了，测试不应该仅仅依靠测试人员，他一直相信：&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;优秀的开发者一定是优秀的测试人员。&lt;/b&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;PingCAP 混沌工程实践&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;如上文所言，在开始研发 TiDB 时，PingCAP 就决定引入混沌工程，应该算是国内吃螃蟹的团队之一。谈到当初的引入原因，唐刘表示，起初开发分布式数据库时，整个团队很自然就想到需要保证开发的数据库能够让用户放心使用，这就需要进行各种各样的测试。当时，Netflix 开发的 Chaos Monkey 已经非常知名，得益于 Netflix 成功的部署经验，PingCAP 团队想到利用该工具解决稳定性问题。&lt;/p&gt;&lt;p&gt;回顾整个实践历程，唐刘表示大概可以分为三个阶段，第一个阶段是 2017 年之前，那时并没有自动化的概念，所有实验全部需要手动完成，包括申请机器、手动部署等。虽然比较繁琐，但也在系统上线之前发现了不少问题。&lt;/p&gt;&lt;p&gt;第二个阶段是从 2017 年到 2019 年初，PingCAP 基于 K8s 搭建了一套自动化 chaos 框架，叫做 Schrodinger，这套系统极大提升了整体生产力，只需自定义要做的实验，Schrodinger 就能搞定。&lt;/p&gt;&lt;p&gt;第三个阶段则是 2019 年初至今，PingCAP 一直在做 Schrodinger Cloud，Schrodinger 主要是为测试 TiDB，也可用来测试 TiDB 的周边工具，甚至是合作伙伴的业务。面对这些需求，PingCAP 考虑基于 K8s 做一套更加通用的 Chaos 框架，采用 Operator 的方式，任何 Chaos 在 K8s 里面都是 CRD，用户只需要定义好自己的 CRD，Schrodinger 就可以自动完成后续事宜。&lt;/p&gt;&lt;p&gt;&lt;b&gt;在开发混沌工程实验时，唐刘建议可遵循以下原则，将有助于实验设计：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;建立稳定状态的假设；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;多样化现实世界事件；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;在生产环境运行实验；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;持续自动化运行实验；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;最小化“爆炸半径”。&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;614&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;614&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-dd76c78a6c85421662be48143f680da4_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;具体来说，系统稳态可以通过一些指标，比如延迟和 QPS 数据等来定义，当系统指标在测试完成后，无法快速恢复稳态要求，可以认为这个系统是不稳定的；其次，引进多样化的现实变量，比如网卡、磁盘故障等；然后，最为重要的是在生产环境中进行验证，这样做是存在风险的，因此最好提前与协作部门同步；最后，自动化可以让整个过程的效率更高，最小化“爆炸半径”可以避免不必要的损失。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;608&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-383b2924cf827fc0927010056ea66e99_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;举例来说，对于一个三副本的系统而言，可以通过随机杀死 Leader 节点的方式来验证系统是否可以保持稳态。可预想的情况是系统在主节点被杀死后会出现一段抖动，随后恢复正常则证明系统是具备容错能力的，反之，则证明系统存在问题。&lt;/p&gt;&lt;p&gt;在这之中，主要有两个大方向：一是发现错误；二是注入错误。TiDB 主要通过 Metrics（Prometheus 项目）、Log 和 Tracing 三种方式分析系统状态。其中，TiDB 默认不开启 Tracing 方式，因为这会对性能产生一定影响，仅在必要时启动该方法。至于注入错误，应用、存储、网络、CPU 等存在多种故障方式，唐刘在分享中提到了如下部分，供开发者参考：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;761&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;761&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-2b8ff1edae7473ebf99fbecde6ad46a3_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;根据过往实践经验，唐刘建议希望使用混沌工程的开发者可以参考混沌工程主页（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//principlesofchaos.org/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;principlesofchaos.org/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;） 列出的步骤和原则。但是，要想真正实践，还需要做很多工作，包括更好地对系统进行错误注入，更好地发现系统问题，这些其实业界没有通用的解决方案，因此实践起来还是比较麻烦的。采访中，唐刘推荐可以阅读 Netflix 的《Chaos Engineering》一书（中文翻译版：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.infoq.cn/theme/13&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;infoq.cn/theme/13&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;），GitHub 上有一个 awesome-chaos-engineering的 Repo（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/dastergon/awesome-chaos-engineering&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/dastergon/aw&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;esome-chaos-engineering&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）也可以参考。此外，如果整个开发团队本来对测试就不太重视，认为这完全是测试团队的事情，那可能也很难推动混沌工程落地。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;结束语&lt;/b&gt;&lt;/h2&gt;&lt;blockquote&gt;三年前，我很少听到有人谈论混沌工程，现在已经蛮多了。&lt;/blockquote&gt;&lt;p&gt;采访最后， &lt;a class=&quot;member_mention&quot; href=&quot;https://www.zhihu.com/people/df9ec6a48ca50364852daa71b20a6192&quot; data-hash=&quot;df9ec6a48ca50364852daa71b20a6192&quot; data-hovercard=&quot;p$b$df9ec6a48ca50364852daa71b20a6192&quot;&gt;@唐刘&lt;/a&gt; 表示如今的混沌工程在国内已经比较出名，这个概念应该已经进入普及阶段。但据唐刘的了解，国内真正对其应用很好的，并没有特别多公司，大部分仍然处于理清概念，但不知道如何实施的阶段。&lt;b&gt;在这种背景下，企业可能最需要关注的是如何建立起自己的一整套自动化测试平台，实现对系统的自动错误注入，并自动发现系统问题。在此基础上，企业可以根据业务情况考虑深入实践混沌工程。&lt;/b&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-22-74717464</guid>
<pubDate>Mon, 22 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>DM 源码阅读系列文章（九）shard DDL 与 checkpoint 机制的实现</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-18-74205520.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/74205520&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cfe2dd456c96422b7941425773b6833b_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：张学程&lt;/p&gt;&lt;p&gt;本文为 DM 源码阅读系列文章的第九篇，在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/dm-source-code-reading-8/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;上篇文章&lt;/a&gt; 中我们详细介绍了 DM 对 online schema change 方案的同步支持，对 online schema change 同步方案以及实现细节等逻辑进行了分析。&lt;/p&gt;&lt;p&gt;在本篇文章中，我们将对 shard DDL 同步机制以及 checkpoint 机制等进行详细的介绍，内容包括 shard group 的定义、shard DDL 的同步协调处理流程、checkpoint 机制以及与之相关的 safe mode 机制。&lt;/p&gt;&lt;h2&gt;shard DDL 机制的实现&lt;/h2&gt;&lt;p&gt;DM 中通过 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/dm-source-code-reading-7/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;库表路由与列值转换&lt;/a&gt; 功能，实现了对分库分表合并场景下 DML 的同步支持。但当需要同步的各分表存在 DDL 变更时，还需要对 DDL 的同步进行更多额外的处理。有关分表合并时 shard DDL 同步需要处理的问题以及 DM 中的同步支持原理，请先阅读 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Ecosystem Tools 原理解读系列（三）TiDB-DM 架构设计与实现原理&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;shard group&lt;/h3&gt;&lt;p&gt;在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这篇文章&lt;/a&gt; 中，我们介绍了 DM 在处理 shard DDL 同步时引入了两级 shard group 的概念，即用于执行分表合并同步任务的各 DM-worker 组成的 shard group、每个 DM-worker 内需要进行合表同步的各上游分表组成的 shard group。&lt;/p&gt;&lt;p&gt;&lt;b&gt;DM-worker 组成的 shard group&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由 DM-worker 组成的 shard group 是由集群部署拓扑及同步任务配置决定的，即任务配置文件中定义的需要进行合表同步的所有上游 MySQL 实例对应的所有 DM-worker 实例即组成了一个 shard group。为了表示同步过程中的相关动态信息，DM-master 内部引入了两个概念：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/lock.go%23L24&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Lock&lt;/a&gt;：对于每组需要进行合并的表，其中每一条需要进行同步协调的 shard DDL，由一个 Lock 实例进行表示；每个 Lock 实例在有 shard DDL 需要协调同步时被创建、在协调同步完成后被销毁；在 dmctl 中使用 show-ddl-locks 命令查看到的每一个 Lock 信息即对应一个该实例&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/ddl_lock.go%23L91&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;LockKeeper&lt;/a&gt;：维护所有的 Lock 实例信息并提供相关的操作接口&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Lock 中各主要成员变量的作用如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0a92b55c1b60ed26df9a59da855cbc89_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1256&quot; data-rawheight=&quot;686&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1256&quot; data-original=&quot;https://pic2.zhimg.com/v2-0a92b55c1b60ed26df9a59da855cbc89_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0a92b55c1b60ed26df9a59da855cbc89_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1256&quot; data-rawheight=&quot;686&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1256&quot; data-original=&quot;https://pic2.zhimg.com/v2-0a92b55c1b60ed26df9a59da855cbc89_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-0a92b55c1b60ed26df9a59da855cbc89_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;DM-worker 内分表组成的 shard group&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每个 DM-worker 内的 shard group 是由对应上游 MySQL 实例内分表及同步任务配置决定的，即任务配置文件中定义的对应 MySQL 实例内需要进行合并同步到同一个下游目标表的所有分表组成一个 shard group。在 DM-worker 内部，我们维护了下面两个对象：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/sharding_group.go%23L87&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ShardingGroup&lt;/a&gt;：对于每一组需要进行合并的表，由一个 ShardingGroup 实例进行表示；每个 ShardGroup 实例在同步任务启动阶段被创建，在任务停止时被销毁&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/sharding_group.go%23L395&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ShardingGroupKeeper&lt;/a&gt;：维护所有的 ShardingGroup 实例信息并提供相关的操作接口&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;ShardingGroup 中各主要成员变量的作用如下：&lt;/p&gt;&lt;h3&gt;shard DDL 同步流程&lt;/h3&gt;&lt;p&gt;对于两级 shard group，DM 内部在依次完成两个级别的 相应的 shard DDL 同步协调。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;对于 DM-worker 内由各分表组成的 shard group，其 shard DDL 的同步在对应 DM-worker 内部进行协调&lt;/li&gt;&lt;li&gt;对于由各 DM-worker 组成的 shard group，其 shard DDL 的同步由 DM-master 进行协调&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;DM-worker 间 shard DDL 协调流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们基于在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这篇文章&lt;/a&gt; 中展示过的仅包含两个 DM-worker 的 shard DDL 协调流程示例（如下图）来了解 DM 内部的具体实现。&lt;/p&gt;&lt;p class=&quot;ztext-empty-paragraph&quot;&gt;&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6ccd31fea26ecf270f373f9eb9337498_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;447&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-6ccd31fea26ecf270f373f9eb9337498_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-6ccd31fea26ecf270f373f9eb9337498_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;447&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-6ccd31fea26ecf270f373f9eb9337498_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-6ccd31fea26ecf270f373f9eb9337498_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ol&gt;&lt;li&gt;DM-worker-1 将 shard DDL 信息发送给 DM-master&lt;br/&gt;a. 当 DM-worker-1 内部 shard DDL 协调完成时，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1727&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker-1 将对应的 shard DDL 信息保存在 channel 中&lt;/a&gt;供 DM-master 通过 gRPC 获取&lt;br/&gt;b. DM-master 在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1243&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;fetchWorkerDDLInfo&lt;/a&gt; 方法中&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1277&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;以 gRPC streaming 的方式读取到 DM-worker-1 的 shard DDL 信息&lt;/a&gt;&lt;br/&gt;c. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1308&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-master 调用 ShardingGroupKeeper 的 TrySync 方法创建对应的 lock 信息&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/lock.go%23L77&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;并在 lock 中标记已收到 DM-worker-1 的 shard DDL 信息&lt;/a&gt;&lt;/li&gt;&lt;li&gt;DM-master 将 lock 信息发回给 DM-worker-1&lt;br/&gt;a. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1319&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-master 以 gRPC streaming 的方式将 lock 信息发送给 DM-worker-1&lt;/a&gt;&lt;br/&gt;b. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/worker/subtask.go%23L535&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker-1 将来自 DM-master 的 lock 信息保存在内存中&lt;/a&gt;用于在 DM-master 请求 DM-worker 执行/跳过 shard DDL 时进行验证&lt;/li&gt;&lt;li&gt;DM-worker-2 将 shard DDL 信息发送给 DM-master（流程与 step.1 一致）&lt;/li&gt;&lt;li&gt;DM-master 将 lock 信息发回给 DM-worker-2（流程与 step.2 一致）&lt;/li&gt;&lt;li&gt;DM-master 协调 DM-worker-1 向下游同步 shard DDL&lt;br/&gt;a. DM-master 根据 step.1 与 step.3 时收到的 shard DDL 信息&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/lock.go%23L80&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;判定已经收到 shard group 内所有 DM-worker 的 shard DDL 信息&lt;/a&gt;&lt;br/&gt;b. DM-master 在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1360&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;resolveDDLLock&lt;/a&gt; 方法中&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1431&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;向 DM-worker-1 发送向下游同步 shard DDL 的请求&lt;/a&gt;（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1427&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Exec 参数为 true&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;DM-worker-1 向下游同步 shard DDL&lt;br/&gt;a. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1732&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker-1 接收到来自 DM-master 的向下游执行 shard DDL 的请求&lt;/a&gt;&lt;br/&gt;b. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1773&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker-1 构造 DDL job 并添加到 DDL 执行队列中&lt;/a&gt;&lt;br/&gt;c. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L874&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker-1 将 shard DDL 执行结果保存在 channel 中&lt;/a&gt;供 DM-master 通过 gRPC 获取&lt;/li&gt;&lt;li&gt;DM-worker-2 忽略向下游同步 shard DDL&lt;br/&gt;a. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1436&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-master 获取 DM-worker-1 向下游同步 shard DDL 的结果&lt;/a&gt;判断得知 DM-worker-1 同步 shard DDL 成功&lt;br/&gt;b. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1486&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-master 向 DM-worker-2 发送忽略向下游同步 shard DDL 的请求&lt;/a&gt;（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/dm/master/server.go%23L1461&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Exec 参数为 false&lt;/a&gt;）&lt;br/&gt;c. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L843&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker-2 根据 DM-master 请求忽略向下游同步 shard DDL&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;DM-worker 内 shard DDL 同步流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们基于在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;实现原理文章&lt;/a&gt; 中展示过的一个 DM-worker 内仅包含两个分表 &lt;code&gt;（table_1，table_2）&lt;/code&gt; 的 shard DDL（仅一条 DDL）协调处理流程示例来了解 DM 内部的具体实现。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;DM-worker 收到 &lt;code&gt;table_1&lt;/code&gt; 的 DDL&lt;br/&gt;a. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1659&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;根据 DDL 及 binlog event position 等信息更新对应的 shard group&lt;/a&gt;&lt;br/&gt;b. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1675&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;确保 binlog replication 过程已进入 safe mode&lt;/a&gt;（后文介绍 checkpoint 机制时会再介绍 safe mode）&lt;br/&gt;c. href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1683&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/369933f31b/syncer/syncer.go#L1683&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;更新 table_1 的 checkpoint（后文会详细介绍 checkpoint 机制）&lt;/li&gt;&lt;li&gt;DM-worker 继续解析后续的 binlog event&lt;br/&gt;根据 step.1 时返回的更新后的 shard group 信息得知还&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1684&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;未收到 shard group 内所有分表对应的 shard DDL&lt;/a&gt;，不向下游同步 shard DDL 并&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1686&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;继续后续解析&lt;/a&gt;&lt;/li&gt;&lt;li&gt;忽略 &lt;code&gt;table_1&lt;/code&gt; 的 DML 并同步 &lt;code&gt;table_2&lt;/code&gt; 的 DML&lt;br/&gt;href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1331&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/369933f31b/syncer/syncer.go#L1331&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;由于 table_1 已收到 shard DDL 但 shard DDL 自身还未完成同步，ref=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1335&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/369933f31b/syncer/syncer.go#L1335&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;忽略对 table_1 相关 DML 的同步&lt;/li&gt;&lt;li&gt;DM-worker 收到 &lt;code&gt;table_2&lt;/code&gt; 的 DDL（流程与 step.1 一致）&lt;/li&gt;&lt;li&gt;DM-worker 向下游同步 shard DDL&lt;br/&gt;a. 根据 step.4 时返回的更新后的 shard group 信息得知已经收到 shard group 内所有分表对应的 shard DDL&lt;br/&gt;b. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1690&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;尝试让 binlog replication 过程退出 safe mode&lt;/a&gt;&lt;br/&gt;c. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1707&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;将当前 shard DDL 同步完成后 re-sync 时重新同步 step.3 忽略的 DML 所需的相关信息保存在 channel 中&lt;/a&gt;&lt;br/&gt;d. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1716&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;等待已分发的所有 DML 同步完成&lt;/a&gt;（确保等待并发同步的 DML 都同步到下游后再对下游 schema 进行变更）&lt;br/&gt;e. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1727&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;将 shard DDL 相关信息保存在 channel 中以进行 DM-worker 间的同步&lt;/a&gt;（见前文 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/dm-source-code-reading-9/%23dm-worker-%25E9%2597%25B4-shard-ddl-%25E5%258D%258F%25E8%25B0%2583%25E6%25B5%2581%25E7%25A8%258B&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker 间 shard DDL 协调流程&lt;/a&gt;）&lt;br/&gt;f. 待 DM-worker 间协调完成后，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L846&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;向下游同步 shard DDL&lt;/a&gt;&lt;/li&gt;&lt;li&gt;将 binlog 的解析位置重定向回 step.1 对应 DDL 后的 binlog event position 进入 re-sync 阶段&lt;br/&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1074&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;根据 step.5 中保存的信息&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1080&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;将 binlog 的解析位置重定向回 step.1 对应的 DDL 后的 binlog event position&lt;/a&gt;&lt;/li&gt;&lt;li&gt;重新解析 binlog event&lt;/li&gt;&lt;li&gt;对于不同表的 DML 做不同的处理&lt;br/&gt;a. 对于 &lt;code&gt;table_1&lt;/code&gt; 在 step.3 时忽略的 DML，解析后向下游同步&lt;br/&gt;b. href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1310&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/dm/b&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;lob/369933f31b/syncer/syncer.go#L1310&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&amp;#34;&amp;gt;对于 table_2 的 DML，根据 checkpoint 信息忽略向下游同步&lt;/li&gt;&lt;li&gt;解析到达 step.4 时 DDL 对应的 binlog position，re-sync 阶段完成&lt;br/&gt;a. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1296&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;解析 binlog position 到达 step.4 的 DDL&lt;/a&gt;&lt;br/&gt;b. &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1298&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;结束 re-sync 过程&lt;/a&gt;&lt;/li&gt;&lt;li&gt;继续进行后续的 DDL 与 DML 的同步&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;需要注意的是，在上述 step.1 与 step.4 之间，如果有收到 &lt;code&gt;table_1&lt;/code&gt; 的其他 DDL，则对于该 shard group，需要协调同步由一组 shard DDL 组成的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/sharding-meta/shardmeta.go%23L53&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ShardingSequence&lt;/a&gt;。当在 step.9 对其中某一条 shard DDL 同步完成后，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1051&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;如果有更多的未同步的 shard DDL 需要协调处理&lt;/a&gt;，则会&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1057&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;重定向到待处理的下一条 shard DDL 对应的位置重新开始解析 binlog event&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;checkpoint 机制的实现&lt;/h2&gt;&lt;p&gt;DM 中通过 checkpoint 机制来实现同步任务中断后恢复时的续传功能。对于 load 阶段，其 checkpoint 机制的实现在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/dm-source-code-reading-4/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM 源码阅读系列文章（四）dump/load 全量同步的实现&lt;/a&gt; 文章中我们已经进行了介绍，本文不再赘述。在本文中，我们将介绍 binlog replication 增量同步阶段的 checkpoint 机制的实现及与之相关的 safe mode 机制的实现。&lt;/p&gt;&lt;h3&gt;checkpoint 机制&lt;/h3&gt;&lt;p&gt;DM 在 binlog replication 阶段以 binlog event 对应的 position 为 checkpoint，包括两类：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;全局 checkpiont：对应已成功解析并同步到下游的 binlog event 的 position，同步任务中断恢复后将从该位置重新进行解析与同步&lt;/li&gt;&lt;li&gt;每个需要同步 table 的 checkpoint：对应该 table 已成功解析并同步到下游的 binlog event 的 position，主要用于在 re-sync 过程中避免对已同步的数据进行重复同步&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;DM 的 checkpoint 信息保存在下游数据库中，通过 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/checkpoint.go%23L174&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RemoteCheckPoint&lt;/a&gt;&lt;/code&gt; 对象进行读写，其主要成员变量包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/checkpoint.go%23L197&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;globalPoint&lt;/a&gt;&lt;/code&gt;：用于保存全局 checkpoint&lt;/li&gt;&lt;li&gt;&lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/checkpoint.go%23L189&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;points&lt;/a&gt;&lt;/code&gt;：用于保存各 table 的 checkpoint&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;checkpoint 信息在下游数据库中对应的 schema 通过 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/checkpoint.go%23L453&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;createTable&lt;/a&gt;&lt;/code&gt; 方法进行创建，其中各主要字段的含义为：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-304a3fa97bd59f5a36ceb66e6431d3bc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1252&quot; data-rawheight=&quot;422&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1252&quot; data-original=&quot;https://pic1.zhimg.com/v2-304a3fa97bd59f5a36ceb66e6431d3bc_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-304a3fa97bd59f5a36ceb66e6431d3bc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1252&quot; data-rawheight=&quot;422&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1252&quot; data-original=&quot;https://pic1.zhimg.com/v2-304a3fa97bd59f5a36ceb66e6431d3bc_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-304a3fa97bd59f5a36ceb66e6431d3bc_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;对于全局 checkpoint，在以下情况下会更新内存中的信息：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L652&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;收到 XID event 时&lt;/a&gt;（表示一个 DML 事务的结束）&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L696&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDL 向下游同步成功后&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于各 table checkpoint，在以下情况下会更新内存中的信息：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L705&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DML 向下游同步成功后&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L696&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDL 向下游同步成功后&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1683&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;收到 shard DDL 且成功更新了 shard group，但未向下游同步 shard DDL 时&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;对于全局与 table 的 checkpoint，会在以下情况下 flush 到下游数据库中：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L663&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;收到 flush 通知&lt;/a&gt;（如同步任务将暂停或停止时）&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L710&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;已分发的任务成功同步到下游&lt;/a&gt;（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L635&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DDL 同步到下游&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L639&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;超过指定时间阈值 flush&lt;/a&gt;）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;值得注意的是，在 shard DDL 未同步到下游之前，为确保中断恢复后仍能继续整个 shard DDL 的协调过程，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L718&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM 不会将全局 checkpoint 更新为比 shard DDL 起始 position 更大的 position&lt;/a&gt;，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L757&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM 也不会将 shard DDL 协调过程中对应 table 的 checkpoint flush 到下游&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;safe mode 机制&lt;/h3&gt;&lt;p&gt;当同步任务中断恢复后，DM 在 binlog replication 阶段通过 checkpoint 机制保证了重新开始同步的起始点前的数据都已经成功同步到了下游数据库中，即保证了 at-least-once 语义。但由于 flush checkpoint 与同步 DDL、DML 到下游不是在同一个事务中完成的，因此从 checkpoint 开始重新同步时，可能存在部分数据被重复同步的可能，即不能保证 at-most-once 。&lt;/p&gt;&lt;p&gt;在 DM 的 binlog replication 阶段，通过增加 safe mode 机制确保了重复同步数据时的可重入，即：&lt;/p&gt;&lt;ul&gt;&lt;li&gt; href=&amp;#34;https&lt;code&gt;://gith&lt;/code&gt;ub.com/pingcap/dm/blob/369933f31b/syncer/dml.go#L132&amp;#34;&amp;gt;将 INSERT 操作转为 REPLACE 操作&lt;/li&gt;&lt;li&gt; href=&amp;#34;https&lt;code&gt;://git&lt;/code&gt;hub.com/pingcap/dm/blob/369933f31b/syncer/dml.go#L195&amp;#34;&amp;gt;将 UPDATE 操作转为 DELETE 操作和 &lt;code&gt;=&amp;#34;https://github.com/pingcap/dm/blob/369933f31b/syncer/dml.go#L200&amp;#34;&amp;gt;REPLACE 操作&lt;/code&gt;&lt;/li&gt;&lt;li&gt; href=&amp;#34;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//gith&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;gith&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;code&gt;ub.com&lt;/code&gt;/pingcap/dm/blob/369933f31b/syncer/dml.go#L265&amp;#34;&amp;gt;对 DELETE 操作不进行转换仍保持为 DELETE&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;目前，safe mode 会在以下情况时启用：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1023&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;启动或恢复任务时的前 5 分钟&lt;/a&gt;，确保从 checkpoint 位置开始被重复同步的部分数据最终一致&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/syncer.go%23L1675&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker 内进行 shard DDL 同步协调时&lt;/a&gt;（见前文 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/dm-source-code-reading-9/%23dm-worker-%25E5%2586%2585-shard-ddl-%25E5%2590%258C%25E6%25AD%25A5%25E6%25B5%2581%25E7%25A8%258B&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM-worker 内 shard DDL 同步流程&lt;/a&gt;），确保即使 shard DDL 协调过程中异常重启且 5 分钟内无法重复同步完之前已同步数据也能最终一致&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/369933f31b/syncer/mode.go%23L33&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;用户在同步任务配置文件中指定了启用 safe mode&lt;/a&gt;，用于其他需要以 safe mode 同步超 5 分钟的场景&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;本篇文章详细地介绍了 shard DDL 机制与 checkpoint 机制的实现，内容包括了两级 shard group 的定义与 DM-worker 间及 DM-worker 内的 shard DDL 同步协调处理流程、checkpoint 机制及与之相关的 safe mode 机制。下一篇文章中，我们将介绍用于保证 DM 正确性与稳定性的测试框架的实现，敬请期待。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/dm-source-code-reading-9/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/blog-cn/dm-&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;source-code-reading-9/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多 DM 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23DM-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-18-74205520</guid>
<pubDate>Thu, 18 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>我们是如何设计 Rust &amp; 分布式存储教程的？ | Talent Plan 背后的故事</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-17-73950816.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/73950816&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8679d48307d8ac03f45e6bf43469dc43_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：沈泰宁 唐刘&lt;/p&gt;&lt;blockquote&gt;许多人眼中的 PingCAP Talent Plan 可能就是 &lt;a href=&quot;https://link.zhihu.com/?target=http%3A//github.com/pingcap/talent-plan&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tale&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;nt-plan&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; 这个项目，但从内容角度来说并不完整，这个 Repo 只是线上课程的内容，我们还有与其配套的线下课程。&lt;br/&gt;本文将从课程设计的角度和大家聊一聊 PingCAP Talent Plan（TiKV 方向）课程，包括课程设计的逻辑、课程设计中遇到的困难，以及大家在学习过程中常见的问题和解答等。&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;PingCAP Talent Plan 是什么？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 是一个新型的开源分布式关系型数据库，目标是希望在大数据和云时代的新的业务需求下，帮助大家更好地解决数据大规模存储和实时计算的问题。我们听说很多同学跟我们反映他们很想参与到这些项目中去，但遇到了一些问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;编程语言是参与项目的敲门砖。Golang 和 Rust 都比较新，需要一定的学习，是最直接的高门槛。&lt;/li&gt;&lt;li&gt;理论知识是深度参与的基础。从理论到实践有一道鸿沟，并不是这么容易跨越。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了解决上述问题，我们启动了 PingCAP Talent Plan。目前课程主要分为两个方向，面向 SQL 优化器、分布式计算的 &lt;b&gt;TiDB 方向&lt;/b&gt;，和面向大规模、一致性的分布式存储的 &lt;b&gt;TiKV 方向&lt;/b&gt;。课程大体分成线上学习阶段和线下学习阶段，线上课程的主要目标是帮助大家从编程语言开始，学习并掌握 Golang 和 Rust，逐步学习关键的理论知识。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiKV 方向课程内容&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiKV 是 TiDB 的分布式存储层，它本身也是一个高性能、可水平扩展、支持分布式事务的 Key-Value 数据库，目前已经成为了 CNCF 的孵化项目。在课程内容的制定上，我们主要参考了 TiKV 现有的技术栈：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Rust 编程语言。&lt;/b&gt;TiKV 主要使用 Rust 开发，根据 GitHub 统计，99.5% 的代码是 Rust。我们选择 Rust 主要看中了它的内存安全和高性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Raft 一致性算法。&lt;/b&gt;Raft 一致性算法主打可理解性，业界也有成熟的实现，TiKV 使用 Raft 算法同步节点之间的状态。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Percolator 分布式事务算法。&lt;/b&gt;分布式事务是 TiKV 很重要的功能，TiDB 的事务也是基于这个算法改进的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;语言真的是门槛吗？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;坊间传闻 Rust 学习曲线非常陡峭，&lt;i&gt;「rust steep learning curve」&lt;/i&gt; 关键词在 Google 上有超过 70 万条搜索结果。诚然，Rust 有些概念确实比较隐晦，比如 &amp;#39;static lifetime，Sync 和 Send，但它们也仅仅知识隐晦罢了，理解起来并不困难，结合我们的个人经验来看，在实际编写 Rust 代码过程中也很少遇到不明所以的编译问题，Rust 编译器对于大部分的错误都提供了详细的解释，我们只要按照编译器的指导修改代码即可。所以“Rust 学习曲线陡峭”或许是我们的刻板印象导致的。&lt;/p&gt;&lt;p&gt;纸上得来终觉浅，我们一致认为学习一门语言的最佳方法就是「动手去写」。为了让大家改变对 Rust 的印象，也为了让新手能有一个平缓的开始，Brian（TiKV 成员，Rust 语言主要开发者）编写了一套手把手的 Rust 教程——&lt;i&gt;&lt;b&gt;Practical Networked Applications in Rust&lt;/b&gt;&lt;/i&gt;，这个教程的目标是带领大家循序渐进地开发一个 KV 存储服务。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Tools&lt;/b&gt;：Rust 提供了完整周边开发工具，比如包管理工具 - Cargo，代码格式化工具 - rustfmt，代码 linter - clippy，它们将给我们的开发带来极大帮助。&lt;/li&gt;&lt;li&gt;&lt;b&gt;I/O&lt;/b&gt;：在这个章节我们将学习 Rust 中的 I/O 操作，错误处理，比较并使用不同的 collection 类型。除此之外，还将使用 failure 和 serde 这两个库来构建强健的持久化 KV 存储。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Networking&lt;/b&gt;：Rust 标准库提供了完整的网络接口，我们将使用标准库实现 client-server 的持久化 KV 服务，期间将学习 Rust 强大的 trait 系统，logging 和 benchmarking。&lt;/li&gt;&lt;li&gt;&lt;b&gt;Concurrency&lt;/b&gt;：相信写过并发编程的同学一定也写过数据竞争 🙂，在 Rust 中这将不复存在。这个章节我们将深入介绍 Sync 和 Send，体验 Rust 的「Fearless Concurrency」。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;教程的内容足够深入，相信大家在完成后能无障碍地使用 Rust 进行日常开发。语言将不再是门槛问题。&lt;/p&gt;&lt;p&gt;PS：目前课程还在开发中，之后将添加异步章节，教大家使用 Future 进行异步编程。&lt;/p&gt;&lt;p&gt;&lt;b&gt;带你突破两个重要的分布式算法&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大家在阅读 TiKV 源码时，如果不了解 Raft 和 Percolator 算法，就很容易迷失方向，不知如何下手。就算了解这两个算法，实操过程中也有可能出现“知道大体做什么，却不明白为什么要这么做”的情况。因此在 Rust 语言课程完成之后，我们将继续带大家学习这两个重要的分布式算法。&lt;/p&gt;&lt;p&gt;关于 Raft，我们直接采用 MIT 6.824 课程，并将 Golang 的教材移植到了 Rust。6.824 提供丰富的测试，从 leader 选举一路测到线性一致性。在移植教程的时候，我们想尽可能地使用最简单的代码来实现，方便大家理解课程内容。但是移植过程并非一帆风顺，其中最大困难是 goroutine 的代码。在 Golang 可以随时开启 goroutine 执行异步的代码，但是 Rust 没有类似的功能，考虑再三，我们最后决定使用 Future 来写这类代码。&lt;/p&gt;&lt;p&gt;在移植完代码后，我们还验证了代码的正确性，验证 Raft 的测试框架教程的思路也很简单，用一个正确实现了 Raft 的库来跑一遍教程中的考核测试。我们选择了 &lt;a href=&quot;https://link.zhihu.com/?target=http%3A//github.com/pingcap/raft-rs&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/raft&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-rs&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; 这个项目，测试一遍后修复了几个 BUG。不过事实证明只测试一遍是不够的，课程发布后社区小伙伴在学习过程中，又陆陆续续发现并解决了几个 BUG，在这里特别感谢 @wjhuang 和 @NingLin-P 两位同学的贡献。&lt;/p&gt;&lt;p&gt;至于 Percolator，由于没有现成的教学课程，我们决定从零开始设计。好在有一部分可以直接复用 6.824 代码，比如网络框架，它是比较通用的网络测试框架，可以模拟多种常见的网络错误，比如丢包、乱序等。Percolator 的测试也比较丰富，从 TSO 到 lost update 再到 read/write skew 最后到网络错误，都有覆盖。&lt;/p&gt;&lt;p&gt;学完这两个算法后，相信大家能达到“知其然，又知其所以然”的程度，再看 TiKV 代码的时候不会一头雾水。之后我们还将补充更多内容，比如 multi-raft、集群调度等，终极目标是让大家可以实现一个简易版 TiKV。&lt;/p&gt;&lt;p&gt;&lt;b&gt;另外值得一提的是，在线下课程中，我们将着重介绍 TiDB/TiKV 架构，带领大家了解实现原理与细节，还有专门的 Mentor 指导大家深度参与 TiDB/TiKV 开发。&lt;/b&gt;更多的细节已在 &lt;u&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247489103%26idx%3D1%26sn%3D7de7c0ce7733e6d18eb3f0e95fc5e426%26chksm%3Deb163125dc61b83341aa265ad7b908e93800fce82876a9f1475d09fc13bd2901fc383ebb66b1%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这篇文章&lt;/a&gt;&lt;/u&gt; 中披露，在这就不赘述了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;FAQ&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;PingCAP Talent Plan 已经成功举办了两期，期间我们收集了学员的一些问题，在这里挑几个比较有意思的和大家分享一下。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q1：&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//github.com/pingcap/talent-plan&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tale&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;nt-plan&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; 是如何组织内容的？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A1：talent-plan repo 里面有 3 个文件夹，/tidb 存放着 TiDB 相关实验，/rust 存放 Rust 课程，/dss 存放 Raft 和 Percolator。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q2：dss 是什么意思？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A2：好问题，可能我们内部人员都不一定知道。dss 其实是 Distributed Storage System 的缩写。😂&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q3：Raft 测试为什么会卡住？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A3：大部分情况是在 RPC 的 handler 里面写了会阻塞的代码，受限于 Rust 的 Future 机制，我们不能 100% 地模拟 goroutine，后台执行 Future 的线程数量是有限的，如果 RPC 会阻塞，并发上来后就会卡住。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q4：dss 为什么会&lt;/b&gt; #[allow(dead_code,unused)]&lt;b&gt;？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A4：这是因为我们留了一些公开 API 交由学员实现时使用，测试本身不会直接用到，这就导致了 dead code 的误报。这些 allow 我们是要求学员在提交的作业的时候去掉的。有不少学员没有注意这点而被扣分了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Q5：dss 需不需要写注释？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;A5：需要！不写会扣分。我们坚信注释是代码的一部分。&lt;/p&gt;&lt;p&gt;关于 PingCAP Talent Plan 的 TiKV 方向的课程暂时就介绍到这里，最后向 MIT 6.824 的工作人员致谢，感谢他们开源了优秀的 Raft 教材。&lt;/p&gt;&lt;blockquote&gt;&lt;u&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247489103%26idx%3D1%26sn%3D7de7c0ce7733e6d18eb3f0e95fc5e426%26chksm%3Deb163125dc61b83341aa265ad7b908e93800fce82876a9f1475d09fc13bd2901fc383ebb66b1%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PingCAP Talent Plan 第三期线下课程&lt;/a&gt;&lt;/u&gt; 已于昨日开启，我们来到了华中科技大学开始为期一周的集中授课，之后学员们将前往 PingCAP 北京总部在 Mentor 的带领下继续学习 TiDB/TiKV 相关知识并上手实操，在这里预祝大家顺利结业！&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-17-73950816</guid>
<pubDate>Wed, 17 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在小红书从 0 到 200+ 节点的探索和应用</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-12-73262030.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/73262030&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7cedfad69983a016a11bce6570e03733_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者介绍：&lt;br/&gt;张俊骏，小红书数据库与中间件团队负责人&lt;/blockquote&gt;&lt;p&gt;小红书使用 TiDB 历史可以追溯到 2017 年甚至更早，那时在物流、仓库等对新技术比较感兴趣的场景下应用，在 2018 年 5 月之后，我们就开始逐步铺开，延展到其他适合 TiDB 的场景中去。&lt;b&gt;截止目前，小红书使用的 TiDB 节点数在 200+ 个，未来也有更大扩展空间。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文根据近两年 TiDB 在小红书的落地过程，和大家一起探讨一下，小红书在新数据库选型的考虑因素、以及 TiDB 从场景分类的角度是如何考量及逐步推广使用的。具体包括以下内容：&lt;/p&gt;&lt;p&gt;1. 目前小红书数据服务整体架构，以及从数据流角度如何对不同数据库服务进行定义和划分。&lt;/p&gt;&lt;p&gt;2. 从基本功能、数据同步、部署管理、运维、二次开发及优化、安全等多个维度解读小红书在数据库选型的考虑因素及思考。&lt;/p&gt;&lt;p&gt;3. TiDB 的适用场景，以及在小红书如何进行场景选择、如何逐步进行上线规划。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一、小红书数据服务整体架构&lt;/b&gt;&lt;/h2&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-327bbd12cd411a5bf8138b4d71d0cbee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-327bbd12cd411a5bf8138b4d71d0cbee_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-327bbd12cd411a5bf8138b4d71d0cbee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-327bbd12cd411a5bf8138b4d71d0cbee_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-327bbd12cd411a5bf8138b4d71d0cbee_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如图 1 所示，小红书数据服务整体架构最上层是在线应用层（online app），应用层往下肯定会依赖一些离线（offline）或者在线（online）的 database（其实它更多的意义应该算存储，比如 Redis 也被我们理解为 database，所以称之为“数据服务”可能会更好），这些在线数据服务（online database）会有两条线：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通过实时数据流（dataflow）将数据导入到离线数据库（offline database）支撑离线分析以及实时展示的场景，也就是图 1 最下层的展示类服务（presentation）和数仓（data warehouse）。&lt;/li&gt;&lt;li&gt;这些数据还可能会回灌到线上其他 database 上，有些是离线，有些是实时。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;图 1 蓝框中的部分基本上都由我们团队负责。我们首先需要保证在线数据库（online database） 的稳定性、安全性以及性能优化等，其次我们的多种数据库数据同步服务（database to database replication） 有点像阿里提出的 data replication center 这个概念，这部分也基本上由我们团队全权负责。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、小红书数据服务组件选型 RoadMap&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;对于一个新的数据库或数据服务组件选型（如 TiDB），我们该从哪些方面去入手搞清楚它的特性？下面分享一下我们的经验。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 产品的基本功能&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第一步，我们需要考察该数据服务/组件的基本功能，首先，我们要了解它的读写场景，包括点查、批量获取（batch get）、范围扫描（range scan）、过滤查询（filter query）、聚合查询（aggregation）等等。然后我们看看它是否符合响应时间（latency） 以及带宽（bandwidth，即能承接多少并发）的要求。最后我们会关注可扩展性，比如 TiDB 可能最大的特点就是扩展性非常好。这几点是大家都会想到的最基本的要求，这里我就一笔略过。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 数据同步与处理相关解决方案&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第二部分是数据同步与处理相关解决方案。这里我们有以下 4 点考虑：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先考虑这个数据服务组件的数据同步是同构或异构的场景，同构的同步比如 Redis to Redis、MongoDB to MongoDB，异构的同步比如 TiDB 到 Kafka 等等。这个情况基本上大家也会遇到，因为一个数据服务很难同时支持两种或更多的场景，不同的数据服务之间的数据要保持一致，就会产生数据同步的问题。&lt;/li&gt;&lt;li&gt;接下来考察离线导出，比如如果我们依赖 Hive、 Spark 做离线分析，那么可能要放在 HDFS、S3 等对象存储上，就需要离线导出，一般是每天导出一次。&lt;/li&gt;&lt;li&gt;第三点是实时导出，即在实时场景下可能需要导出到消息中间件，比如 Kafka、RocketMQ 等。&lt;/li&gt;&lt;li&gt;第四点是离线导入，导入的场景一般是在离线的引擎计算的结果，作为评估的指标再写入线上的 database 提供数据服务。&lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d72434b8eb0020dcefa97c0fd4ee44bf_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-d72434b8eb0020dcefa97c0fd4ee44bf_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d72434b8eb0020dcefa97c0fd4ee44bf_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-d72434b8eb0020dcefa97c0fd4ee44bf_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-d72434b8eb0020dcefa97c0fd4ee44bf_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;3. 产品的部署及管理&lt;/b&gt;&lt;/p&gt;&lt;p&gt;部署其实非常重要，它涵盖以下 5 个方面。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;第一点是组件管理界面。&lt;/b&gt;当集群多到一定程度时，如果你没有一个很好的管理界面，会连自己用的是什么集群都记不清楚。所以管理界面非常必要，而且最初可能是 1 个集群 1 个管理界面，然后是 100 个集群 1 个管理界面。&lt;/li&gt;&lt;li&gt;&lt;b&gt;第二点是选版本和机型。&lt;/b&gt;在版本选择方面，不同版本提供的功能不一样，同时也要考虑版本升级的成本。在机型的选择方面，无论是自建机房、用云主机，还是使用最近推出来的新概念“Bare-Metal”（裸金属），机型选择都是非常痛苦的事情，但同时机型选择对存储来说至关重要。我们目前绝大多数都是部署在腾讯云和 AWS 上，并且开始慢慢尝试在 Bare-Metal 上的应用。&lt;/li&gt;&lt;li&gt;&lt;b&gt;第三点是监控、报警、日志收集。&lt;/b&gt;我将这个问题分为三个级别：机器级、应用级和业务级。机器级指机器主机上的问题，包括如何做监控、报警、日志收集，虽然这点与该数据服务组件没有太大关系，但是我们仍然需要关注；应用级指该数据服务组件的报警、监控、日志收集具体是怎么做的；业务级指特定的业务有特定的报警需求，例如一个订单表突然有几十万的 QPS 写入，在平时属于异常的情况，这种异常是需要自定义的，甚至需要我们在某些特定位置埋点并输出结论，因为如果不关注这些异常情况，就很可能导致这三件事用三种不同架构，最后部署的集群极其复杂繁琐，三个级别用了三个不同的监控工具，看到三个不同的监控界面，导致运维成本增加。&lt;/li&gt;&lt;li&gt;&lt;b&gt;第四点是跨区/跨云部署。&lt;/b&gt;这一点可能是互联网公司的比较大的需求。在遇到跨区/跨云的部署的时候，需要考察该数据服务组件是否天生支持跨区/跨云。如果不支持，需要再考虑是否需要再启动数据同步。&lt;/li&gt;&lt;li&gt;&lt;b&gt;第五点是考察附属组件，&lt;/b&gt;也就是与该数据服务组件强绑定的其他组件，比如 zk、lb、jmx_exporter 等等，这些组件的部署成本也需要考虑。我们需要减少 OPS 成本，或者说，一个好的整体架构设计能够防止业务疯狂上线时很多意外的出现。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;4. 运维的易用性&lt;/b&gt;&lt;/p&gt;&lt;p&gt;运维包括扩容、缩容、迁移，其中迁移可能要考虑跨区迁移、机型升级迁移等。在使用维护某个组件的时候会产出“XX 组件的运维手册”，这样下次遇到问题的时候，可以先去看看运维手册里它是否是一个已知问题，有没有现成的解决方案。在公司人员变动比较频繁或者业务方直接介入到这个场景的时候，如果没有运维手册，有些项目很难落地。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9568e361d555d5aceb6ef272a9b62962_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-9568e361d555d5aceb6ef272a9b62962_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9568e361d555d5aceb6ef272a9b62962_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-9568e361d555d5aceb6ef272a9b62962_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-9568e361d555d5aceb6ef272a9b62962_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;5. 产品可优化的空间&lt;/b&gt;&lt;/p&gt;&lt;p&gt;优化部分基本上分为配置调优、客户端代码调优、二次开发、三次开发。其中二次开发就是在现有的开源产品上再开发，修复 bug 或者自己实现某些新增功能/工具，未来可能还会贡献给社区；而三次开发则是自己写一个和某些组件类似的东西，直接替换掉。在小红书内部，二次开发是比较主流的，三次开发很少，毕竟从零开始自研一个组件到适应特定业务场景，其实是跟不上我们的业务上线节奏的，所以三次开发至少眼下不适合作为我们主要的攻坚方向。&lt;/p&gt;&lt;p&gt;&lt;b&gt;6. 其他考虑因素&lt;/b&gt;&lt;/p&gt;&lt;p&gt;未来在小红书数据服务组件系统，我们会做很多完善工作，比如安全、审计、服务化、容器化等方面的事情。譬如我们目前在部署一个组件的时候，容器化还没有在讨论范围之内，也就是需要用容器部署就容器部署，需要在虚拟机上部署就在虚拟机上部署，并没有一个明确的结论倾向。当然，我个人认为未来容器化是一个主流趋势。&lt;/p&gt;&lt;p&gt;&lt;b&gt;以上就是小红书的数据服务组件选型的 RoadMap，看起来跟接下来要讲的“TiDB 在小红书多场景下的应用”没有太大的关系，但我认为在做应用之前应该先把上面列举的这些方向思考清楚，这样会对未来落地工作的投入产出比产生非常大的影响，比如我们最近按照上面的方向调研 Tidis 和 TiFlash 的时候速度就快很多。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、TiDB 在小红书多场景下的应用&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;场景 1：展示类业务&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-93a05a95399afc0a9be78f895e21c6a2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-93a05a95399afc0a9be78f895e21c6a2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-93a05a95399afc0a9be78f895e21c6a2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-93a05a95399afc0a9be78f895e21c6a2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-93a05a95399afc0a9be78f895e21c6a2_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;TiDB 在小红书的第一个应用场景是展示类业务，它的 pipeline 如图 4 中红色部分所示，线上一般是 MongoDB 或者 MySQL，通过一条实时数据流（realtime dataflow） 连接 Redis 或者 TiDB，最后实现 presentation 功能。接下来介绍这类场景下的两个具体项目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;项目 1：大促实时看板&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3fa8ab771e479bfb23ea140e996900ee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-3fa8ab771e479bfb23ea140e996900ee_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3fa8ab771e479bfb23ea140e996900ee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-3fa8ab771e479bfb23ea140e996900ee_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-3fa8ab771e479bfb23ea140e996900ee_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;第一个项目是大促实时看板，在去年双十一期间上线，当时我们的节奏也比较快，7、8 月开始调研，11 月就上大促业务。&lt;/p&gt;&lt;p&gt;当时该项目下一共有 8 个实时报表，QPS 写入均值 5K，大促活动开始时 QPS 峰值接近 200K/秒，每过 2s 会有较大的聚合查询 query，聚合结果还需要写入 Redis 再 pop 到 TiDB，集群规模方面只用了 10 个 TiKV 和 3 个 PD。还有一点值得提一下，当时每个节点挂了 3.5T * 4 块的 NVME SSD，但是后来事实证明这个选型是有问题的，因为大促的时候我们人人都在盯着，磁盘坏了会立刻得到解决，所以即使把四块盘做了 raid0，然后上线了，根本无法确定 NVME 盘出问题的概率是多少，后来差不多每个月会出现一两次类似的故障，故障率很高，虽然我相信未来 NVME 会做得更好，但这样高的故障率从设计角度来看，这个选型就未必是最合适的。&lt;/p&gt;&lt;p&gt;在实现上，我们遇到的第一个问题是保证最终一致性的写入。我们做了多线程写入，每个线程写入特定的记录，保证线程之间不会冲突。除此之外，我们还做了一些调优工作，&lt;b&gt;我们发现每一个事务的 batch insert size 设置为 100 时能达到吞吐、延迟综合最优的要求。&lt;/b&gt;最初业务侧设置的 batch size 非常大，后来发现事务之间冲突的概率、响应的时间等等都会出现一些问题，但 batch size 设置为 1，那么并发又会成为一个问题。所以经过了一段时间的调优，最后得到了前面的结论。这个参数大家可以根据需求自己调整，用二分法/折纸法试验就可以得到。&lt;/p&gt;&lt;p&gt;&lt;b&gt;这个项目最终全程写入和查询在大促期间保持稳定，写入时延小于 20ms，查询时延小于 1s&lt;/b&gt;，因为我们需要 2s 做一次查询，这个响应时间是能满足要求的。&lt;/p&gt;&lt;h3&gt;&lt;b&gt;项目 2：实时业务数据展示&lt;/b&gt;&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-668e675eef2bc8590d0fce00b72c1f40_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-668e675eef2bc8590d0fce00b72c1f40_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-668e675eef2bc8590d0fce00b72c1f40_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-668e675eef2bc8590d0fce00b72c1f40_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-668e675eef2bc8590d0fce00b72c1f40_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这个项目背景有两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一，我们业务方有实时分析的需求，需要实时观测线上库写入内容，可能是针对某个用户做一些查询，还可能是一个非常大的 query，比如需要快速看到新上线功能的效果，尤其是在实验以及风控等项目上响应时间要求非常高。&lt;/li&gt;&lt;li&gt;其次需要作为离线 ETL 任务的数据源，同时需要预备改为线上服务。盘算一下业务量，总共支持需要超过一百个 MongoDB 或 MySQL 数据库的实时展示，峰值总读写 QPS 超过 500K，现在的业务需求大概这个量级，未来可能会更高。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们当前考虑是按业务线去拆分集群，部分核心表一式多份。比如用户表可能有多个业务依赖，比如社区业务、订单物流业务等等，但如果按照业务线拆分集群之后，就无法做 Join 了，这也是我们不能接受的，所以对核心表会以一式多份的形式存在。实际使用场景下，大部分都是点查，比如查特定用户、特定订单的线上状态，同时有少量的单表聚合查询和跨表 Join 查询。换句话说，可以认为是一个实时的数据仓库，但又不做复杂 ETL，更多依赖线上真实数据。&lt;/p&gt;&lt;p&gt;我们的设计方案是把 TiDB 作为一个 MySQL/MongoDB 的从库，但对于 MongoDB 来说可能还要做一点同步任务的数据改造工作。现在规模是 10 节点 TiKV + 3 节点 PD 的集群总共有 3 个，后面可能会按需求扩增。&lt;/p&gt;&lt;p&gt;在实践细节上，首先我们会基于 Canal 去做 oplog/binlog 的实时同步。其次，目前我们对加列之外的 DDL 支持得不够好，这部分还需要 DBA 手工介入，但在未来会有一些改进。最后是多租户问题，比如判断某个部门的同事是否有权限访问另一个部门的数据库，这件事在线上会非常头疼，现在在接入层解决这个问题，我们内部有一个叫 venus 的展示平台，将上层全链控制、认证等事情去掉，所以我们就不用关注这件事了，至少眼下不用关注。这个项目已经开始逐步上线，基本上架构已经确定。&lt;/p&gt;&lt;p&gt;&lt;b&gt;场景 2：分析类业务&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b69e82426b3c57bf15d37dbe9453cdc8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-b69e82426b3c57bf15d37dbe9453cdc8_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-b69e82426b3c57bf15d37dbe9453cdc8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-b69e82426b3c57bf15d37dbe9453cdc8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-b69e82426b3c57bf15d37dbe9453cdc8_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;分析类业务的 pipeline 如图 7 所示，最后的 data warehouse 构建在 AWS 上。&lt;/p&gt;&lt;h3&gt;&lt;b&gt;项目 3：分库分表 MySQL ETL&lt;/b&gt;&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bfbf658472f0fdcc53ade07cf3b9f26c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-bfbf658472f0fdcc53ade07cf3b9f26c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-bfbf658472f0fdcc53ade07cf3b9f26c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-bfbf658472f0fdcc53ade07cf3b9f26c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-bfbf658472f0fdcc53ade07cf3b9f26c_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这个场景下的第一个项目是做分库分表的 MySQL ETL。以最大的表为例，上游 10 节点的MySQL，共计 10000 个分表，存量数据 1000 亿条左右，每日增量 10 亿+，QPS 写入均值 3000 条/s，峰值接近 10000 条/s，平台促销活动对这部分影响也不大，甚至反而会降低，因为活动主要是电商部门在做，社区的查询需求反而变少。我们在 TiDB 离线库保留了大约 30 天增量监控数据，全量数据存在 S3 上，每日夜间（白天偶尔）会有基于 sqoop 的抽数任务触发。集群规模方面，目前使用 10 节点 TiKV + 3 节点 PD。&lt;/p&gt;&lt;p&gt;在实践细节方面，首先我们对 MySQL 自增 ID 进行了处理，然后对 sqoop 进行了一些基于 TiDB 的细节上适配，最后调整 TiDB 的 max transaction size 以优化抽取率。&lt;b&gt;除此之外，还有一个值得一提的事情，因为实体数据（用户/笔记/订单数据等）不宜硬删除，但是在 MySQL 关系表做软删除是非常可怕的事情，最后可能会因为数据量太过于庞大造成雪崩。但 TiDB 不一样，我们把线上的硬删除变成了 TiDB 的软删除，这对于数仓来说是非常有价值的事情。&lt;/b&gt;对于每天全量抽数的表来说，无论软硬删除，第二天数仓里的数据总是对的。但是对于大数量的场景，全量抽数代价过高，就会采取增量抽取的方式，即设置一个条件，一般是 update_time 为今天。这时候硬删除就存在问题了：上面的 query 条件无法判断一条记录究竟是被删除了，还是在当天没有被更新。而前面又提到，关系表上是不适合做软删除的。所以我们在做 ETL 的时候，线上做 delete 的操作，我们在 TiDB 上会新增一个 is_deleted 字段，并将其设置为 true。这个时候有一个小细节，删除这个操作的时间戳怎么设置。删除这个操作时的时间戳是跟普通写入的时间戳不一样的。普通的写入，时间戳就是线上库的 update time，但是删除的时候是不会带上线上的 update_time 的，所以因为这条记录被硬删除了，时间戳都找不到了，这时我们只能用收到这条消息的 update_time 去做它的时间戳，这时就会有些小问题，当然这个问题我们还没有完全解决掉，假设大家有类似的需求的话，我们可以私下交流讨论。目前这个项目已经上线，运行稳定。&lt;/p&gt;&lt;p&gt;&lt;b&gt;项目 4：MySQL 归档&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a0c4c6eec24a31c2d3c0b500a561d3b6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-a0c4c6eec24a31c2d3c0b500a561d3b6_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a0c4c6eec24a31c2d3c0b500a561d3b6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-a0c4c6eec24a31c2d3c0b500a561d3b6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-a0c4c6eec24a31c2d3c0b500a561d3b6_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;项目 4 MySQL 归档是基于项目 3 的演进。业务背景方面，以最大的表为例，主要为物流仓储部门的订单及衍生信息，存量非常非常大，每月进行归档到 TiDB 的数据有数十亿，但对 QPS 要求不是很高，与业务方讨论之后暂定，过去一年半的记录存放在 TiDB 供业务方查询，更久远的记录归档到 S3/Cos 上。&lt;/p&gt;&lt;p&gt;项目 4 与项目 3 代码相比处理的场景更复杂一些，因为它之前 MySQL 的分库分表逻辑不像项目 3 那些清晰，集群规模也会相对大一些，目前是 25 个 TiKV 节点 + 3 个 PD 节点，未来可有扩容的需求。实现细节上，项目 4 和项目 3 类似，这里就不赘述了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;场景 3：线上服务&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-385b457fceacd6bdf22300a0ea3ae60e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-385b457fceacd6bdf22300a0ea3ae60e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-385b457fceacd6bdf22300a0ea3ae60e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-385b457fceacd6bdf22300a0ea3ae60e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-385b457fceacd6bdf22300a0ea3ae60e_b.jpg&quot;/&gt;&lt;figcaption&gt;图 10&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;TiDB 接入实时数据写入服务的业务有以下四个考虑：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一点是代码更改成本，这一项成本已经比较低了，因为基本上都是 jdbc 连接，但多多少少会有一些变更。&lt;/li&gt;&lt;li&gt;第二点是数据迁移成本，对于已经上线的业务来说，迁移数据都是一件非常费劲的事情，尤其是我们还要求不停服务进行热迁移的话。&lt;/li&gt;&lt;li&gt;第三点是运维成本，从原本的 MySQL 切换到我们自己维护 TiDB ，其实无形中增加了运维成本，尤其是在挂盘率比较高的场景下。&lt;/li&gt;&lt;li&gt;第四点是技术栈成本，因为有些人对 TiDB 不熟悉，会比较害怕接触和使用，绝大部分人更愿意用自己熟悉的东西，所以需要有一个知识学习的过程，这也是一个成本。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;现在我们已经有一部分线上业务从 Hive 离线导入到 TiDB 做 T+1 级别数据服务，而且我们新上线业务的关系型数据库选型已经开始倾向于 TiDB，主要是因为它的扩展性为我们节省了很大的时间成本，尤其是业务增长比较快的情况下，选择 MySQL 分库分表其实是一件代价极其大的事情。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;我记得之前有同事问了一个问题，说这个场景用别的东西也可以做，为什么一定要用 TiDB 呢？为什么要用牛刀来杀一只鸡呢？我回答他：有种情况是你找不到一只牛来杀，只能先“杀鸡”成功了，未来才有“杀牛”的机会，但是大家不要认为“杀鸡用牛刀”是一件很蠢事情，这可以理解为一个鉴定或者测试的过程。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四、未来 TiDB 在小红书的接入计划&lt;/b&gt;&lt;/h2&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6f472f6a5b7fe362ef1f5bc68c5f95ba_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-6f472f6a5b7fe362ef1f5bc68c5f95ba_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6f472f6a5b7fe362ef1f5bc68c5f95ba_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-6f472f6a5b7fe362ef1f5bc68c5f95ba_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-6f472f6a5b7fe362ef1f5bc68c5f95ba_b.jpg&quot;/&gt;&lt;figcaption&gt;图 11 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;最后分享一下 TiDB 未来在小红书的接入方向。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先在 ETL 方面，TiDB 的事务隔离性对某些场景来说有点高，我们希望能自定义事务隔离需求，比如两个事务有冲突，但我们实际的写入需求只要最终一致性。但是从目前 TiDB 的设计来说，这个需求可能比较困难，但是也不排除将这个事情 raise 起来的可能性。&lt;/li&gt;&lt;li&gt;第二个很重要的事情是跨数据中心的部署，这是我们未来会重点关注的方向，可能最终会得到一个通用的解决方案，目前的规划还不是特别明晰，因为未来业务可能在不同的云会有不同的形态，我们也希望能找到成本相对更低的解决方案。&lt;/li&gt;&lt;li&gt;第三点是自动化运维，目前是往 TiDB + K8s 的方向推动，更好的解决集群部署问题，因为在虚机上部署还是比较痛苦的。&lt;/li&gt;&lt;li&gt;最后，我们已经有同事负责调研 TiFlash、Tidis，但目前还没有线上应用在依赖。同时我们也在做 CK 和 TiFlash 的对比调研，目前 CK 已经在线上提供服务，未来如果 TiFlash 的调研结论是比较优秀的，肯定也会有计划替换。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文根据张俊骏老师在 TiDB TechDay 2019 上海站上的演讲整理。&lt;/p&gt;&lt;p&gt;- END - &lt;/p&gt;&lt;p&gt;&lt;b&gt;更多案例阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-12-73262030</guid>
<pubDate>Fri, 12 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 源码解析系列文章（十）Snapshot 的发送和接收</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-09-72880848.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/72880848&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-72226a1aa2a61516be7404c2f7b95548_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄梦龙&lt;/p&gt;&lt;h2&gt;背景知识&lt;/h2&gt;&lt;p&gt;TiKV 使用 Raft 算法来提供高可用且具有强一致性的存储服务。在 Raft 中，Snapshot 指的是整个 State Machine 数据的一份快照，大体上有以下这几种情况需要用到 Snapshot：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;正常情况下 leader 与 follower/learner 之间是通过 append log 的方式进行同步的，出于空间和效率的考虑，leader 会定期清理过老的 log。假如 follower/learner 出现宕机或者网络隔离，恢复以后可能所缺的 log 已经在 leader 节点被清理掉了，此时只能通过 Snapshot 的方式进行同步。&lt;/li&gt;&lt;li&gt;Raft 加入新的节点的，由于新节点没同步过任何日志，只能通过接收 Snapshot 的方式来同步。实际上这也可以认为是 1 的一种特殊情形。&lt;/li&gt;&lt;li&gt;出于备份/恢复等需求，应用层需要 dump 一份 State Machine 的完整数据。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;TiKV 涉及到的是 1 和 2 这两种情况。在我们的实现中，Snapshot 总是由 Region leader 所在的 TiKV 生成，通过网络发送给 Region follower/learner 所在的 TiKV。&lt;/p&gt;&lt;p&gt;理论上讲，我们完全可以把 Snapshot 当作普通的 &lt;code&gt;RaftMessage&lt;/code&gt; 来发送，但这样做实践上会产生一些问题，主要是因为 Snapshot 消息的尺寸远大于其他 &lt;code&gt;RaftMessage&lt;/code&gt;：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Snapshot 消息需要花费更长的时间来发送，如果共用网络连接容易导致网络拥塞，进而引起其他 Region 出现 Raft 选举超时等问题。&lt;/li&gt;&lt;li&gt;构建待发送 Snapshot 消息需要消耗更多的内存。&lt;/li&gt;&lt;li&gt;过大的消息可能导致超出 gRPC 的 Message Size 限制等问题。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;基于上面的原因，TiKV 对 Snapshot 的发送和接收进行了特殊处理，为每个 Snapshot 创建单独的网络连接，并将 Snapshot 拆分成 1M 大小的多个 Chunk 进行传输。&lt;/p&gt;&lt;h2&gt;源码解读&lt;/h2&gt;&lt;p&gt;下面我们分别从 RPC 协议、发送 Snapshot、收取 Snapshot 三个方面来解读相关源代码。本文的所有内容都基于 v3.0.0-rc.2 版本。&lt;/p&gt;&lt;h3&gt;Snapshot RPC call 的定义&lt;/h3&gt;&lt;p&gt;与普通的 raft message 类似，Snapshot 消息也是使用 gRPC 远程调用的方式来传输的。在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/kvproto&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pingcap/kvproto&lt;/a&gt; 项目中可以找到相关 RPC Call 的定义，具体在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/kvproto/blob/5cb23649b361013f929e0d46a166ae24848fbcbb/proto/tikvpb.proto%23L57&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tikvpb.proto&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/kvproto/blob/5cb23649b361013f929e0d46a166ae24848fbcbb/proto/raft_serverpb.proto%23L42-L47&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;raft_serverpb.proto&lt;/a&gt; 文件中。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;rpc Snapshot(stream raft_serverpb.SnapshotChunk) returns (raft_serverpb.Done) {}
...
message SnapshotChunk {
  RaftMessage message = 1;
  bytes data = 2;
}

message Done {}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看出，Snapshot 被定义成 client streaming 调用，即对于每个 Call，客户端依次向服务器发送多个相同类型的请求，服务器接收并处理完所有请求后，向客户端返回处理结果。具体在这里，每个请求的类型是 &lt;code&gt;SnapshotChunk&lt;/code&gt;，其中包含了 Snapshot 对应的 &lt;code&gt;RaftMessage&lt;/code&gt;，或者携带一段 Snapshot 数据；回复消息是一个简单的空消息 &lt;code&gt;Done&lt;/code&gt;，因为我们在这里实际不需要返回任何信息给客户端，只需要关闭对应的 stream。&lt;/p&gt;&lt;h3&gt;Snapshot 的发送流程&lt;/h3&gt;&lt;p&gt;Snapshot 的发送过程的处理比较简单粗暴，直接在将要发送 &lt;code&gt;RaftMessage&lt;/code&gt; 的地方截获 Snapshot 类型的消息，转而通过特殊的方式进行发送。相关代码可以在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/transport.rs%23L313-L344&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;server/transport.rs&lt;/a&gt; 中找到：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn write_data(&amp;amp;self, store_id: u64, addr: &amp;amp;str, msg: RaftMessage) {
  if msg.get_message().has_snapshot() {
      return self.send_snapshot_sock(addr, msg);
  }
  if let Err(e) = self.raft_client.wl().send(store_id, addr, msg) {
      error!(&amp;#34;send raft msg err&amp;#34;; &amp;#34;err&amp;#34; =&amp;gt; ?e);
  }
}

fn send_snapshot_sock(&amp;amp;self, addr: &amp;amp;str, msg: RaftMessage) {
  ...
  if let Err(e) = self.snap_scheduler.schedule(SnapTask::Send {
      addr: addr.to_owned(),
      msg,
      cb,
  }) {
      ...
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从代码中可以看出，这里简单地把对应的 &lt;code&gt;RaftMessage&lt;/code&gt; 包装成一个 &lt;code&gt;SnapTask::Send&lt;/code&gt; 任务，并将其交给独立的 &lt;code&gt;snap-worker&lt;/code&gt; 去处理。值得注意的是，这里的 &lt;code&gt;RaftMessage&lt;/code&gt; 只包含 Snapshot 的元信息，而不包括真正的快照数据。TiKV 中有一个单独的模块叫做 &lt;code&gt;SnapManager&lt;/code&gt; ，用来专门处理数据快照的生成与转存，稍后我们将会看到从 &lt;code&gt;SnapManager&lt;/code&gt; 模块读取 Snapshot 数据块并进行发送的相关代码。&lt;/p&gt;&lt;p&gt;我们不妨顺藤摸瓜来看看 &lt;code&gt;snap-worker&lt;/code&gt; 是如何处理这个任务的，相关代码在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/snap.rs%23L329-L398&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;server/snap.rs&lt;/a&gt;，精简掉非核心逻辑后的代码引用如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn run(&amp;amp;mut self, task: Task) {
  match task {
      Task::Recv { stream, sink } =&amp;gt; {
           ...
           let f = recv_snap(stream, sink, ...).then(move |result| {
               ...
           });
           self.pool.spawn(f).forget();
      }
      Task::Send { addr, msg, cb } =&amp;gt; {
          ...
          let f = future::result(send_snap(..., &amp;amp;addr, msg))
              .flatten()
              .then(move |res| {
                  ...
              });
          self.pool.spawn(f).forget();
      }
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;snap-worker&lt;/code&gt; 使用了 &lt;code&gt;future&lt;/code&gt; 来完成收发 Snapshot 任务：通过调用 &lt;code&gt;send_snap()&lt;/code&gt; 或 &lt;code&gt;recv_snap()&lt;/code&gt; 生成一个 future 对象，并将其交给 &lt;code&gt;FuturePool&lt;/code&gt; 异步执行。&lt;/p&gt;&lt;p&gt;现在我们暂且只关注 &lt;code&gt;send_snap()&lt;/code&gt; 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/snap.rs%23L103-L175&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;实现&lt;/a&gt;：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn send_snap(
  ...
  addr: &amp;amp;str,
  msg: RaftMessage,
) -&amp;gt; Result&amp;lt;impl Future&amp;lt;Item = SendStat, Error = Error&amp;gt;&amp;gt; {
  ...
  let key = {
      let snap = msg.get_message().get_snapshot();
      SnapKey::from_snap(snap)?
  };
  ...
  let s = box_try!(mgr.get_snapshot_for_sending(&amp;amp;key));
  if !s.exists() {
      return Err(box_err!(&amp;#34;missing snap file: {:?}&amp;#34;, s.path()));
  }
  let total_size = s.total_size()?;
  let chunks = {
      let mut first_chunk = SnapshotChunk::new();
      first_chunk.set_message(msg);

      SnapChunk {
          first: Some(first_chunk),
          snap: s,
          remain_bytes: total_size as usize,
      }
  };

  let cb = ChannelBuilder::new(env);
  let channel = security_mgr.connect(cb, addr);
  let client = TikvClient::new(channel);
  let (sink, receiver) = client.snapshot()?;

  let send = chunks.forward(sink).map_err(Error::from);
  let send = send
      .and_then(|(s, _)| receiver.map_err(Error::from).map(|_| s))
      .then(move |result| {
          ...
      });
  Ok(send)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这一段流程还是比较清晰的：先是用 Snapshot 元信息从 &lt;code&gt;SnapManager&lt;/code&gt; 取到待发送的快照数据，然后将 &lt;code&gt;RaftMessage&lt;/code&gt; 和 &lt;code&gt;Snap&lt;/code&gt; 一起封装进 &lt;code&gt;SnapChunk&lt;/code&gt; 结构，最后创建全新的 gRPC 连接及一个 Snapshot stream 并将 &lt;code&gt;SnapChunk&lt;/code&gt; 写入。这里引入 &lt;code&gt;SnapChunk&lt;/code&gt; 是为了避免将整块 Snapshot 快照一次性加载进内存，它 impl 了 &lt;code&gt;futures::Stream&lt;/code&gt; 这个 trait 来达成按需加载流式发送的效果。如果感兴趣可以参考它的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/snap.rs%23L55-L92&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体实现&lt;/a&gt;，本文就暂不展开了。&lt;/p&gt;&lt;h3&gt;Snapshot 的收取流程&lt;/h3&gt;&lt;p&gt;最后我们来简单看一下 Snapshot 的收取流程，其实也就是 gRPC Call 的 server 端对应的处理，整个流程的入口我们可以在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/service/kv.rs%23L714-L729&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;server/service/kv.rs&lt;/a&gt; 中找到：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn snapshot(
  &amp;amp;mut self,
  ctx: RpcContext&amp;lt;&amp;#39;_&amp;gt;,
  stream: RequestStream&amp;lt;SnapshotChunk&amp;gt;,
  sink: ClientStreamingSink&amp;lt;Done&amp;gt;,
) {
  let task = SnapTask::Recv { stream, sink };
  if let Err(e) = self.snap_scheduler.schedule(task) {
      ...
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;与发送过程类似，也是直接构建 &lt;code&gt;SnapTask::Recv&lt;/code&gt; 任务并转发给 &lt;code&gt;snap-worker&lt;/code&gt; 了，这里会调用上面出现过的 &lt;code&gt;recv_snap()&lt;/code&gt; 函数，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/snap.rs%23L237-L291&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体实现&lt;/a&gt; 如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn recv_snap&amp;lt;R: RaftStoreRouter + &amp;#39;static&amp;gt;(
  stream: RequestStream&amp;lt;SnapshotChunk&amp;gt;,
  sink: ClientStreamingSink&amp;lt;Done&amp;gt;,
  ...
) -&amp;gt; impl Future&amp;lt;Item = (), Error = Error&amp;gt; {
  ...
  let f = stream.into_future().map_err(|(e, _)| e).and_then(
      move |(head, chunks)| -&amp;gt; Box&amp;lt;dyn Future&amp;lt;Item = (), Error = Error&amp;gt; + Send&amp;gt; {
          let context = match RecvSnapContext::new(head, &amp;amp;snap_mgr) {
              Ok(context) =&amp;gt; context,
              Err(e) =&amp;gt; return Box::new(future::err(e)),
          };

          ...
          let recv_chunks = chunks.fold(context, |mut context, mut chunk| -&amp;gt; Result&amp;lt;_&amp;gt; {
              let data = chunk.take_data();
              ...
              if let Err(e) = context.file.as_mut().unwrap().write_all(&amp;amp;data) {
                  ...
              }
              Ok(context)
          });

          Box::new(
              recv_chunks
                  .and_then(move |context| context.finish(raft_router))
                  .then(move |r| {
                      snap_mgr.deregister(&amp;amp;context_key, &amp;amp;SnapEntry::Receiving);
                      r
                  }),
          )
      },
  );
  f.then(move |res| match res {
      ...
  })
  .map_err(Error::from)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;值得留意的是 stream 中的第一个消息（其中包含有 &lt;code&gt;RaftMessage&lt;/code&gt;）被用来创建 &lt;code&gt;RecvSnapContext&lt;/code&gt; 对象，其后的每个 chunk 收取后都依次写入文件，最后调用 &lt;code&gt;context.finish()&lt;/code&gt; 把之前保存的 &lt;code&gt;RaftMessage&lt;/code&gt; 发送给 &lt;code&gt;raftstore&lt;/code&gt; 完成整个接收过程。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;以上就是 TiKV 发送和接收 Snapshot 相关的代码解析了。这是 TiKV 代码库中较小的一个模块，它很好地解决了由于 Snapshot 消息特殊性所带来的一系列问题，充分应用了 &lt;code&gt;grpc-rs&lt;/code&gt; 组件及 &lt;code&gt;futures&lt;/code&gt;/&lt;code&gt;FuturePool&lt;/code&gt; 模型，大家可以结合本系列文章的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tikv-source-code-reading-7/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;第七篇&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tikv-source-code-reading-8/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;第八篇&lt;/a&gt; 进一步拓展学习。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tikv-source-code-reading-10/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章（十）Snapshot 的发送和接收&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多 TiKV 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-09-72880848</guid>
<pubDate>Tue, 09 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 源码解析系列文章（九）Service 层处理流程解析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-08-72640867.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/72640867&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3505224dad3b818557ddf813893b94ba_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：周振靖&lt;/p&gt;&lt;p&gt;之前的 TiKV 源码解析系列文章介绍了 TiKV 依赖的周边库，从本篇文章开始，我们将开始介绍 TiKV 自身的代码。本文重点介绍 TiKV 最外面的一层——Service 层。&lt;/p&gt;&lt;p&gt;TiKV 的 Service 层的代码位于 &lt;code&gt;src/server&lt;/code&gt; 文件夹下，其职责包括提供 RPC 服务、将 store id 解析成地址、TiKV 之间的相互通信等。这一部分的代码并不是特别复杂。本篇将会简要地介绍 Service 层的整体结构和组成 Service 层的各个组件。&lt;/p&gt;&lt;h2&gt;整体结构&lt;/h2&gt;&lt;p&gt;位于 &lt;code&gt;src/server/server.rs&lt;/code&gt; 文件中的 &lt;code&gt;Server&lt;/code&gt; 是我们本次介绍的 Service 层的主体。它封装了 TiKV 在网络上提供服务和 Raft group 成员之间相互通信的逻辑。&lt;code&gt;Server&lt;/code&gt; 本身的代码比较简短，大部分代码都被分离到 &lt;code&gt;RaftClient&lt;/code&gt;，&lt;code&gt;Transport&lt;/code&gt;，&lt;code&gt;SnapRunner&lt;/code&gt; 和几个 gRPC service 中。上述组件的层次关系如下图所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1288&quot; data-rawheight=&quot;618&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1288&quot; data-original=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1288&quot; data-rawheight=&quot;618&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1288&quot; data-original=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;接下来，我们将详细介绍这些组件。&lt;/p&gt;&lt;h2&gt;Resolver&lt;/h2&gt;&lt;p&gt;在一个集群中，每个 TiKV 实例都由一个唯一的 store id 进行标识。Resolver 的功能是将 store id 解析成 TiKV 的地址和端口，用于建立网络通信。&lt;/p&gt;&lt;p&gt;Resolver 是一个很简单的组件，其接口仅包含一个函数：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub trait StoreAddrResolver: Send + Clone {
   fn resolve(&amp;amp;self, store_id: u64, cb: Callback) -&amp;gt; Result&amp;lt;()&amp;gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中 &lt;code&gt;Callback&lt;/code&gt; 用于异步地返回结果。&lt;code&gt;PdStoreAddrResolver&lt;/code&gt; 实现了该 trait，它的 &lt;code&gt;resolve&lt;/code&gt; 方法的实现则是简单地将查询任务通过其 &lt;code&gt;sched&lt;/code&gt; 成员发送给 &lt;code&gt;Runner&lt;/code&gt;。而 &lt;code&gt;Runner&lt;/code&gt; 则实现了 &lt;code&gt;Runnable&amp;lt;Task&amp;gt;&lt;/code&gt;，其意义是 &lt;code&gt;Runner&lt;/code&gt; 可以在自己的一个线程里运行，外界将会向 &lt;code&gt;Runner&lt;/code&gt; 发送 &lt;code&gt;Task&lt;/code&gt; 类型的消息，&lt;code&gt;Runner&lt;/code&gt; 将对收到的 &lt;code&gt;Task&lt;/code&gt; 进行处理。 这里使用了由 TiKV 的 util 提供的一个单线程 worker 框架，在 TiKV 的很多处代码中都有应用。&lt;code&gt;Runner&lt;/code&gt; 的 &lt;code&gt;store_addrs&lt;/code&gt; 字段是个 cache，它在执行任务时首先尝试在这个 cache 中找，找不到则向 PD 发送 RPC 请求来进行查询，并将查询结果添加进 cache 里。&lt;/p&gt;&lt;h2&gt;RaftClient&lt;/h2&gt;&lt;p&gt;TiKV 是一个 Multi Raft 的结构，Region 的副本之间，即 Raft group 的成员之间需要相互通信，&lt;code&gt;RaftClient&lt;/code&gt; 的作用便是管理 TiKV 之间的连接，并用于向其它 TiKV 节点发送 Raft 消息。&lt;code&gt;RaftClient&lt;/code&gt; 可以和另一个节点建立多个连接，并把不同 Region 的请求均摊到这些连接上。这部分代码的主要的复杂性就在于连接的建立，也就是 &lt;code&gt;Conn::new&lt;/code&gt; 这个函数。建立连接的代码的关键部分如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;let client1 = TikvClient::new(channel);

let (tx, rx) = batch::unbounded::&amp;lt;RaftMessage&amp;gt;(RAFT_MSG_NOTIFY_SIZE);
let rx = batch::BatchReceiver::new(rx, RAFT_MSG_MAX_BATCH_SIZE, Vec::new, |v, e| v.push(e));
let rx1 = Arc::new(Mutex::new(rx));

let (batch_sink, batch_receiver) = client1.batch_raft().unwrap();
let batch_send_or_fallback = batch_sink
   .send_all(Reusable(rx1).map(move |v| {
       let mut batch_msgs = BatchRaftMessage::new();
       batch_msgs.set_msgs(RepeatedField::from(v));
       (batch_msgs, WriteFlags::default().buffer_hint(false))
   })).then(/*...*/);

client1.spawn(batch_send_or_fallback.map_err(/*...*/));&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上述代码向指定地址调用了 &lt;code&gt;batch_raft&lt;/code&gt; 这个 gRPC 接口。&lt;code&gt;batch_raft&lt;/code&gt; 和 &lt;code&gt;raft&lt;/code&gt; 都是 stream 接口。对 &lt;code&gt;RaftClient&lt;/code&gt; 调用 &lt;code&gt;send&lt;/code&gt; 方法会将消息发送到对应的 &lt;code&gt;Conn&lt;/code&gt; 的 &lt;code&gt;stream&lt;/code&gt; 成员，即上述代码的 &lt;code&gt;tx&lt;/code&gt; 中，而在 gRPC 的线程中则会从 &lt;code&gt;rx&lt;/code&gt; 中取出这些消息（这些消息被 &lt;code&gt;BatchReceiver&lt;/code&gt; 这一层 batch 起来以提升性能），并通过网络发送出去。&lt;/p&gt;&lt;p&gt;如果对方不支持 batch，则会 fallback 到 &lt;code&gt;raft&lt;/code&gt; 接口。这种情况通常仅在从旧版本升级的过程中发生。&lt;/p&gt;&lt;h2&gt;RaftStoreRouter 与 Transport&lt;/h2&gt;&lt;p&gt;&lt;code&gt;RaftStoreRouter&lt;/code&gt; 负责将收到的 Raft 消息转发给 raftstore 中对应的 Region，而 &lt;code&gt;Transport&lt;/code&gt; 负责将 Raft 消息发送到指定的 store。&lt;/p&gt;&lt;p&gt;&lt;code&gt;ServerRaftStoreRouter&lt;/code&gt; 是在 TiKV 实际运行时将会使用的 &lt;code&gt;RaftStoreRouter&lt;/code&gt; 的实现，它包含一个内层的、由 raftstore 提供的 &lt;code&gt;RaftRouter&lt;/code&gt; 对象和一个 &lt;code&gt;LocalReader&lt;/code&gt; 对象。收到的请求如果是一个只读的请求，则会由 &lt;code&gt;LocalReader&lt;/code&gt; 处理；其它情况则是交给内层的 router 来处理。&lt;/p&gt;&lt;p&gt;&lt;code&gt;ServerTransport&lt;/code&gt; 则是 TiKV 实际运行时使用的 &lt;code&gt;Transport&lt;/code&gt; 的实现（&lt;code&gt;Transport&lt;/code&gt; trait 的定义在 raftstore 中），其内部包含一个 &lt;code&gt;RaftClient&lt;/code&gt; 用于进行 RPC 通信。发送消息时，&lt;code&gt;ServerTransport&lt;/code&gt; 通过上面说到的 Resolver 将消息中的 store id 解析为地址，并将解析的结果存入 &lt;code&gt;raft_client.addrs&lt;/code&gt; 中；下次向同一个 store 发送消息时便不再需要再次解析。接下来，再通过 &lt;code&gt;RaftClient&lt;/code&gt; 进行 RPC 请求，将消息发送出去。&lt;/p&gt;&lt;h2&gt;Node&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Node&lt;/code&gt; 可以认为是将 raftstore 的复杂的创建、启动和停止逻辑进行封装的一层，其内部的 &lt;code&gt;RaftBatchSystem&lt;/code&gt; 便是 raftstore 的核心。在启动过程中（即 &lt;code&gt;Node&lt;/code&gt; 的 &lt;code&gt;start&lt;/code&gt; 函数中），如果该节点是一个新建的节点，那么会进行 bootstrap 的过程，包括分配 store id、分配第一个 Region 等操作。&lt;/p&gt;&lt;p&gt;&lt;code&gt;Node&lt;/code&gt; 并没有直接包含在 &lt;code&gt;Server&lt;/code&gt; 之内，但是 raftstore 的运行需要有用于向其它 TiKV 发送消息的 &lt;code&gt;Transport&lt;/code&gt;，而 &lt;code&gt;Transport&lt;/code&gt; 作为提供网络通信功能的一部分，则是包含在 &lt;code&gt;Server&lt;/code&gt; 内。所以我们可以看到，在 &lt;code&gt;src/binutil/server.rs&lt;/code&gt;文件的 &lt;code&gt;run_raft_server&lt;/code&gt; 中（被 tikv-server 的 &lt;code&gt;main&lt;/code&gt; 函数调用），启动过程中需要先创建 &lt;code&gt;Server&lt;/code&gt;，然后创建并启动 &lt;code&gt;Node&lt;/code&gt; 并把 &lt;code&gt;Server&lt;/code&gt; 所创建的 &lt;code&gt;Transport&lt;/code&gt; 传给 &lt;code&gt;Node&lt;/code&gt;，最后再启动 &lt;code&gt;Node&lt;/code&gt;。&lt;/p&gt;&lt;h2&gt;Service&lt;/h2&gt;&lt;p&gt;TiKV 包含多个 gRPC service。其中，最重要的一个是 &lt;code&gt;KvService&lt;/code&gt;，位于 &lt;code&gt;src/server/service/kv.rs&lt;/code&gt; 文件中。&lt;/p&gt;&lt;p&gt;&lt;code&gt;KvService&lt;/code&gt; 定义了 TiKV 的 &lt;code&gt;kv_get&lt;/code&gt;，&lt;code&gt;kv_scan&lt;/code&gt;，&lt;code&gt;kv_prewrite&lt;/code&gt;，&lt;code&gt;kv_commit&lt;/code&gt; 等事务操作的 API，用于执行 TiDB 下推下来的复杂查询和计算的 &lt;code&gt;coprocessor&lt;/code&gt; API，以及 &lt;code&gt;raw_get&lt;/code&gt;，&lt;code&gt;raw_put&lt;/code&gt; 等 Raw KV API。&lt;code&gt;batch_commands&lt;/code&gt; 接口则是用于将上述的接口 batch 起来，以优化高吞吐量的场景。当我们要为 TiKV 添加一个新的 API 时，首先就要在 kvproto 项目中添加相关消息体的定义，并在这里添加相关代码。另外，TiKV 的 Raft group 各成员之间通信用到的 &lt;code&gt;raft&lt;/code&gt; 和 &lt;code&gt;batch_raft&lt;/code&gt; 接口也是在这里提供的。&lt;/p&gt;&lt;p&gt;下面以 &lt;code&gt;kv_prewrite&lt;/code&gt; 为例，介绍 TiKV 处理一个请求的流程。首先，无论是直接调用还是通过 &lt;code&gt;batch_commands&lt;/code&gt; 接口调用，都会调用 &lt;code&gt;future_prewrite&lt;/code&gt; 函数，并在该函数返回的 future 附加上根据结果发送响应的操作，再将得到的 future spawn 到 &lt;code&gt;RpcContext&lt;/code&gt;，也就是一个线程池里。&lt;code&gt;future_prewrite&lt;/code&gt; 的逻辑如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// 从请求体中取出调用 prewrite 所需的参数

let (cb, f) = paired_future_callback();
let res = storage.async_prewrite(/*其它参数*/, cb);

AndThenWith::new(res, f.map_err(Error::from)).map(|v| {
   let mut resp = PrewriteResponse::new();
   if let Some(err) = extract_region_error(&amp;amp;v) {
       resp.set_region_error(err);
   } else {
       resp.set_errors(RepeatedField::from_vec(extract_key_errors(v)));
   }
   resp
})&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里的 &lt;code&gt;paired_future_callback&lt;/code&gt; 是一个 util 函数，它返回一个闭包 &lt;code&gt;cb&lt;/code&gt; 和一个 future &lt;code&gt;f&lt;/code&gt;，当 &lt;code&gt;cb&lt;/code&gt; 被调用时 &lt;code&gt;f&lt;/code&gt; 就会返回被传入 &lt;code&gt;cb&lt;/code&gt; 的值。上述代码会立刻返回，但 future 中的逻辑在 &lt;code&gt;async_prewrite&lt;/code&gt; 中的异步操作完成之后才会执行。一旦 prewrite 操作完成，&lt;code&gt;cb&lt;/code&gt; 便会被调用，将结果传给 &lt;code&gt;f&lt;/code&gt;，接下来，我们写在 &lt;code&gt;future&lt;/code&gt; 中的创建和发送 Response 的逻辑便会继续执行。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;以上就是 TiKV 的 Service 层的代码解析。大家可以看到这些代码大量使用 trait 和泛型，这是为了方便将其中一些组件替换成另外一些实现，方便编写测试代码。另外，在 &lt;code&gt;src/server/snap.rs&lt;/code&gt; 中，我们还有一个专门用于处理 Snapshot 的模块，由于 Snapshot 消息的特殊性，在其它模块中也有一些针对 snapshot 的代码。关于 Snapshot，我们将在另一篇文章里进行详细讲解，敬请期待。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tikv-source-code-reading-9/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/blog-cn/tik&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;v-source-code-reading-9/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多 TiKV 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-08-72640867</guid>
<pubDate>Mon, 08 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Binlog 源码阅读系列文章（二）初识 TiDB Binlog 源码</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-05-72306986.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/72306986&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7e9b57ad1e4c1ebaae5bfabfa6d28ffb_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：satoru&lt;/p&gt;&lt;h2&gt;TiDB Binlog 架构简介&lt;/h2&gt;&lt;p&gt;TiDB Binlog 主要由 Pump 和 Drainer 两部分组成，其中 Pump 负责存储 TiDB 产生的 binlog 并向 Drainer 提供按时间戳查询和读取 binlog 的服务，Drainer 负责将获取后的 binlog 合并排序再以合适的格式保存到对接的下游组件。&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;402&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;402&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;br/&gt;&lt;b&gt;在《&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog 架构演进与实现原理&lt;/a&gt;》一文中，我们对 TiDB Binlog 整体架构有更详细的说明，建议先行阅读该文。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;相关源码仓库&lt;/h2&gt;&lt;p&gt;TiDB Binlog 的实现主要分布在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb-tools&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb-binlog&lt;/a&gt; 两个源码仓库中，我们先介绍一下这两个源码仓库中的关键目录。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. tidb-tools&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Repo: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-tools/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这个仓库除了 TiDB Binlog 还有其他工具的组件，在这里与 TiDB Binlog 关系最密切的是 &lt;code&gt;tidb-binlog/pump_client&lt;/code&gt; 这个 package。&lt;code&gt;pump_client&lt;/code&gt; 实现了 Pump 的客户端接口，当 binlog 功能开启时，TiDB 使用它来给 pump &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/v3.0.0-rc.3/tidb-binlog/pump_client/client.go%23L242&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;发送 binlog&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. tidb-binlog&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Repo: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-binlog&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;TiDB-Binlog 的核心组件都在这个仓库，下面是各个关键目录：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;cmd&lt;/code&gt;：包含 &lt;code&gt;pump&lt;/code&gt;，&lt;code&gt;drainer&lt;/code&gt;，&lt;code&gt;binlogctl&lt;/code&gt;，&lt;code&gt;reparo&lt;/code&gt;，&lt;code&gt;arbiter&lt;/code&gt; 等 5 个子目录，分别对应 5 个同名命令行工具。这些子目录下面的 &lt;code&gt;main.go&lt;/code&gt; 是对应命令行工具的入口，而主要功能的实现则依赖下面将介绍到的各个同名 packages。&lt;/li&gt;&lt;li&gt;&lt;code&gt;pump&lt;/code&gt;：Pump 源码，主要入口是 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/pump/server.go%23L103&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pump.NewServer&lt;/a&gt;&lt;/code&gt; 和 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/pump/server.go%23L313&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Server.Start&lt;/a&gt;&lt;/code&gt;；服务启动后，主要的功能是 &lt;code&gt;WriteBinlog&lt;/code&gt;（面向 &lt;code&gt;TiDB/pump_client&lt;/code&gt;） 和 &lt;code&gt;PullBinlogs&lt;/code&gt;（面向 &lt;code&gt;Drainer&lt;/code&gt;）。&lt;/li&gt;&lt;li&gt;&lt;code&gt;drainer&lt;/code&gt;：Drainer 源码，主要入口是 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/drainer/server.go%23L88&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;drainer.NewServer&lt;/a&gt;&lt;/code&gt; 和 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/drainer/server.go%23L238&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Server.Start&lt;/a&gt;&lt;/code&gt;；服务启动后，Drainer 会先找到所有 Pump 节点，然后调用 Pump 节点的 &lt;code&gt;PullBinlogs&lt;/code&gt; 接口同步 binlog 到下游。目前支持的下游有：mysql/tidb，file（文件增量备份），kafka 。&lt;/li&gt;&lt;li&gt;&lt;code&gt;binlogctl&lt;/code&gt;：Binlogctl 源码，实现一些常用的 Binlog 运维操作，例如用 &lt;code&gt;-cmd pumps&lt;/code&gt; 参数可以查看当前注册的各个 Pump 节点信息，相应的实现就是 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/binlogctl/nodes.go%23L37&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;QueryNodesByKind&lt;/a&gt;&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code&gt;reparo&lt;/code&gt;：Reparo 源码，实现从备份文件（Drainer 选择 file 下游时保存的文件）恢复数据到指定数据库的功能。&lt;/li&gt;&lt;li&gt;&lt;code&gt;arbiter&lt;/code&gt;：Arbiter 源码，实现从 Kafka 消息队列中读取 binlog 同步到指定数据库的功能，binlog 在消息中以 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/v3.0.0-rc.3/tidb-binlog/slave_binlog_proto/proto/binlog.proto%23L85&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Protobuf&lt;/a&gt;&lt;/code&gt; 格式编码。&lt;/li&gt;&lt;li&gt;&lt;code&gt;pkg&lt;/code&gt;：各个工具公用的一些辅助类的 packages，例如 &lt;code&gt;pkg/util&lt;/code&gt; 下面有用于重试函数执行的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/pkg/util/util.go%23L145&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RetryOnError&lt;/a&gt;&lt;/code&gt;，pkg/version 下面有用于打印版本信息的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/pkg/version/version.go%23L45&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PrintVersionInfo&lt;/a&gt;&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code&gt;tests&lt;/code&gt;：集成测试。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;启动测试集群&lt;/h2&gt;&lt;p&gt;上个小节提到的 &lt;code&gt;tests&lt;/code&gt; 目录里有一个名为 &lt;code&gt;run.sh&lt;/code&gt; 脚本，我们一般会使用 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/Makefile%23L68&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;make integration_test&lt;/a&gt;&lt;/code&gt; 命令，通过该脚本执行一次完整的集成测试，不过现在我们先介绍如何用它来启动一个测试集群。&lt;br/&gt;启动测试集群前，需要在 &lt;code&gt;bin&lt;/code&gt; 目录下准备好相关组件的可执行文件：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;pd-server：下载链接（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/pd-master-linux-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Linux&lt;/a&gt; / &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/pd-master-darwin-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;macOS&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;tikv-server：下载链接（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/tikv-master-linux-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Linux&lt;/a&gt; / &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/tikv-master-darwin-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;macOS&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;tidb-server：下载链接（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/tidb-master-linux-amd64.tar.g&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Linux&lt;/a&gt; / &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/tidb-master-darwin-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;macOS&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;&lt;code&gt;pump&lt;/code&gt;, &lt;code&gt;drainer&lt;/code&gt;, &lt;code&gt;binlogctl&lt;/code&gt;：在 tidb-binlog 目录执行 &lt;code&gt;make build&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;脚本依赖 MySQL 命令行客户端来确定 TiDB 已经成功启动，所以我们还需要安装一个 MySQL 客户端。&lt;/p&gt;&lt;p&gt;准备好以上依赖，运行 &lt;code&gt;tests/run.sh --debug&lt;/code&gt;，就可以启动一个测试集群。启动过程中会输出一些进度信息，看到以下提示就说明已成功启动：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;Starting Drainer... 
You may now debug from another terminal. Press [ENTER] to continue.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;测试集群包含以下服务：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;2 个作为上游的 TiDB 实例，分别使用端口 4000 和 4001&lt;/li&gt;&lt;li&gt;1 个作为下游的 TiDB 实例， 使用端口 3306&lt;/li&gt;&lt;li&gt;PD 实例，使用端口 2379&lt;/li&gt;&lt;li&gt;TiKV，使用端口 20160&lt;/li&gt;&lt;li&gt;Pump ，使用端口 8250&lt;/li&gt;&lt;li&gt;Drainer，使用端口 8249&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;使用 MySQL 客户端连接任意一个上游 TiDB，可以用 &lt;code&gt;SHOW PUMP STATUS&lt;/code&gt; 和 &lt;code&gt;SHOW DRAINER STATUS&lt;/code&gt; 查询对应工具的运行状态，例如：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;350&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;350&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;br/&gt;通过 &lt;code&gt;binlogctl&lt;/code&gt; 也可以查询到同样的信息，例如：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;$ bin/binlogctl -pd-urls=localhost:2379 -cmd pumps
[2019/06/26 14:36:29.158 +08:00] [INFO] [nodes.go:49] [&amp;#34;query node&amp;#34;] [type=pump] [node=&amp;#34;{NodeID: pump:8250, Addr: 127.0.0.1:8250, State: online, MaxCommitTS: 409345979065827329, UpdateTime: 2019-06-26 14:36:27 +0800 CST}&amp;#34;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br/&gt;接下来我们可以用 MySQL 客户端连接上端口为 4000 或 4001 的 TiDB 数据库，插入一些测试数据。&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;295&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;295&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;br/&gt;上图的演示中创建了一个叫 &lt;code&gt;hello_binlog&lt;/code&gt; 的 database，在里面新建了 &lt;code&gt;user_info&lt;/code&gt; 表并插入了两行数据。完成上述操作后，就可以连接到端口为 3306 的下游数据库验证同步是否成功：&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;252&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;252&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h2&gt;&lt;br/&gt;小结&lt;/h2&gt;&lt;p&gt;本文简单介绍了 tidb-tools 和 tidb-binlog 及其中的目录，并且展示了如何启动测试集群。有了这些准备，大家就可以进一步了解各个功能的源码实现。下篇文章将会介绍 &lt;code&gt;pump_client&lt;/code&gt; 如何将一条 binlog 发送往 Pump Server。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;原文链接：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-binlog-source-code-reading-2/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/blog-cn/tid&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;b-binlog-source-code-reading-2/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;延展阅读：&lt;/p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/69587196&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic3.zhimg.com/v2-c3da67191753fc9376f711e1c9dbec9a_180x120.jpg&quot; data-image-width=&quot;1280&quot; data-image-height=&quot;593&quot; class=&quot;internal&quot;&gt;ZoeyZhai：TiDB Binlog 源码阅读系列文章（一）序&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-05-72306986</guid>
<pubDate>Fri, 05 Jul 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
