<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Fri, 01 Nov 2019 06:52:35 +0800</lastBuildDate>
<item>
<title>TiKV 源码解析系列文章（十四）Coprocessor 概览</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-31-89518391.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/89518391&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-20f8a3ce333d33a342041949f11ab72d_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：Shirly&lt;/p&gt;&lt;p&gt;本文将简要介绍 TiKV Coprocessor 的基本原理，面向想要了解 TiKV 数据读取执行过程的同学，同时也面向想对该模块贡献代码的同学。阅读本文前，建议读者对 TiDB 整体架构有所了解，先阅读三篇文章了解 TiDB 技术内幕：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-internal-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;说存储&lt;/a&gt;、&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-internal-2/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;说计算&lt;/a&gt;、&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-internal-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;谈调度&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;什么是 Coprocessor&lt;/h2&gt;&lt;p&gt;熟悉 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/overview/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 整体框架&lt;/a&gt; 的同学可能记得，TiDB 是无状态的，数据存储在 TiKV 层。当 TiDB 在收到一个来自客户端的查询请求时，会向 TiKV 获取具体的数据信息。那么一个读请求最朴素的处理过程如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aa3dee0694dd1c0d14dc9e380e154ec2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;516&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-aa3dee0694dd1c0d14dc9e380e154ec2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-aa3dee0694dd1c0d14dc9e380e154ec2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;516&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-aa3dee0694dd1c0d14dc9e380e154ec2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-aa3dee0694dd1c0d14dc9e380e154ec2_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;首先需要肯定的是这种方式固然能解决问题，但是性能如何呢？我们来一起分析一下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;TiKV  将所有数据返回，网络开销太大。&lt;/li&gt;&lt;li&gt;TiDB 需要计算所有数据，CPU 消耗很大，相对的，TiKV 却并没有什么计算，很闲。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;看到以上问题后，聪明如你，可能很容易就想到，能不能让 TiKV 把自己负责的那部分数据做一次计算，再返回给 TiDB 呢？&lt;/p&gt;&lt;p&gt;有何不可呢？&lt;/p&gt;&lt;p&gt;TiKV 读取数据并计算的模块，我们定义为 Coprocessor，该概念灵感来自于 HBase，目前在 TiDB 中的实现类似于 HBase 中的 Coprocessor 的 Endpoint 部分，也可类比 MySQL 存储过程。&lt;/p&gt;&lt;p&gt;有了 Coprocessor 后，从宏观看一个读请求是如何下发到 TiKV 的呢？以下面的请求为例：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5182af7a6ac373f61670d3a75f74fbad_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5182af7a6ac373f61670d3a75f74fbad_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5182af7a6ac373f61670d3a75f74fbad_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;704&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5182af7a6ac373f61670d3a75f74fbad_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5182af7a6ac373f61670d3a75f74fbad_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;如图，以上查询语句在 TiDB 中处理如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;TiDB 收到查询语句，对语句进行分析，计算出物理执行计划，组织称 TiKV 的 Coprocessor 请求。&lt;/li&gt;&lt;li&gt;TiDB 将该 Coprocessor 请求根据数据的分布，分发到所有相关的 TiKV 上。&lt;/li&gt;&lt;li&gt;TiKV 在收到该 Coprocessor 请求后，根据请求算子对数据进行过滤聚合，然后返回给 TiDB。&lt;/li&gt;&lt;li&gt;TiDB 在收到所有数据的返回结果后，进行二次聚合，并将最终结果计算出来，返回给客户端。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;主要功能及处理概览&lt;/h2&gt;&lt;p&gt;TiKV Coprocessor 处理的读请求目前主要分类三种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;DAG：执行物理算子，为 SQL 计算出中间结果，从而减少 TiDB 的计算和网络开销。这个是绝大多数场景下 Coprocessor 执行的任务。&lt;/li&gt;&lt;li&gt;Analyze：分析表数据，统计、采样表数据信息，持久化后被 TiDB 的优化器采用。&lt;/li&gt;&lt;li&gt;CheckSum：对表数据进行校验，用于导入数据后一致性校验。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;那么 TiKV 在收到 Coprocessor 请求后，何时区分这三种请求的呢？&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0b450b6d1c6e8acbc44cd91a5d38e934_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;441&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-0b450b6d1c6e8acbc44cd91a5d38e934_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0b450b6d1c6e8acbc44cd91a5d38e934_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;441&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-0b450b6d1c6e8acbc44cd91a5d38e934_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-0b450b6d1c6e8acbc44cd91a5d38e934_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;请求到了 TiKV 层，处理过程如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;由 gRPC server 接收并将请求分发给 Coprocessor Endpoint 进行处理。&lt;/li&gt;&lt;li&gt;Endpoint 在收到请求后，根据请求的优先级，将请求分发给对应的线程池。&lt;/li&gt;&lt;li&gt;所有请求会先异步从存储层获取 snapshot，然后开始真正的处理阶段。&lt;/li&gt;&lt;li&gt;根据请求的不同类型，构造不同的 Handler 进行数据的处理。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;目前 Coprocessor 支持的三种接口中，后面两种接口相对比较简单，而 DAG 是里面最复杂也是最常用的，所以本文后续将重点介绍 DAG 类请求。&lt;/p&gt;&lt;h2&gt;DAG Request 概览&lt;/h2&gt;&lt;p&gt;DAG 顾名思义，是由一系列算子组成的有向无环图，算子在代码中称为 Executors。&lt;/p&gt;&lt;p&gt;目前 DAG 请求主要实现了两种计算模型：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;火山模型：每个算子按行按需吐出，3.0 之后开始弃用。&lt;/li&gt;&lt;li&gt;向量化计算模型：每个算子批量化处理数据，3.0 之后开始推广。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在目前的 TiKV master 上，处于火山模型向向量化模型的过度阶段，因而两种计算模型同时存在。TiKV 收到请求时，会优先检测是否可走向量化模型，若部分功能在向量化模型中没有实现，则走旧的计算模型，具体处理逻辑流程如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e8396857c846de7980df121bb3b1f9c8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;474&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-e8396857c846de7980df121bb3b1f9c8_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e8396857c846de7980df121bb3b1f9c8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;474&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-e8396857c846de7980df121bb3b1f9c8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-e8396857c846de7980df121bb3b1f9c8_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;相关代码在：&lt;code&gt;src/coprocessor/dag/mod.rs&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;因为火山模型已在被弃用中，所以下文我们只讲向量化计算模型。&lt;/p&gt;&lt;h2&gt;算子概览&lt;/h2&gt;&lt;p&gt;在向量化计算模型中，所有算子都实现了 &lt;code&gt;BatchExecutor&lt;/code&gt;接口，其主要定义了一个 &lt;code&gt;get_batch&lt;/code&gt; 的函数：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub trait BatchExecutor: Send {
   fn next_batch(&amp;amp;mut self, scan_rows: usize) -&amp;gt; BatchExecuteResult;
}

pub struct BatchExecuteResult {
   pub physical_columns: LazyBatchColumnVec,
   pub logical_rows: Vec&amp;lt;usize&amp;gt;,
   pub is_drained: Result&amp;lt;bool, Error&amp;gt;,
   ...
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;参数说明：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;next_batch&lt;/code&gt; 中 &lt;code&gt;scan_rows&lt;/code&gt; 由上层控制，由于扫的数据过多会慢，因此该数字从 32 倍增到 1024。&lt;/li&gt;&lt;li&gt;返回值 &lt;code&gt;BatchExecuteResult&lt;/code&gt; 中，由于返回了一批空数据不代表所有数据都处理完毕了，例如可能只是全被过滤，因而使用单独字段表示所有数据处理完毕。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;目前 TiKV 支持的算子主要有以下几类。&lt;/p&gt;&lt;h3&gt;TableScan&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;定义：根据指定主键范围扫表数据，并过滤出一部分列返回。它只会作为最底层算子出现，从底层 KV 获取数据。&lt;/li&gt;&lt;li&gt;源码路径：&lt;code&gt;components/tidb_query/src/batch/executors/table_scan_executor.rs&lt;/code&gt;&lt;/li&gt;&lt;li&gt;案例：&lt;code&gt;select col from t&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;IndexScan&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;定义：根据指定索引返回扫索引数据，并过滤出一部分索引列返回。它只会作为最底层算子出现，从底层 KV 获取数据。&lt;/li&gt;&lt;li&gt;源码路径：&lt;code&gt;components/tidb_query/src/batch/executors/index_scan_executor.rs&lt;/code&gt;&lt;/li&gt;&lt;li&gt;案例：&lt;code&gt;select index from t&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;Selection&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;定义：对底层算子的结果按照过滤条件进行过滤，其中这些条件由多个表达式组成。&lt;/li&gt;&lt;li&gt;源码路径：&lt;code&gt;components/tidb_query/src/batch/executors/selection_executor.rs&lt;/code&gt;&lt;/li&gt;&lt;li&gt;案例：&lt;code&gt;select col from t where a+b=10&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9537e60316ef13101e74f7c99b008f0c_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;177&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-9537e60316ef13101e74f7c99b008f0c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9537e60316ef13101e74f7c99b008f0c_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;177&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-9537e60316ef13101e74f7c99b008f0c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-9537e60316ef13101e74f7c99b008f0c_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;Limit&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;定义：从底层算子吐出的数据中，限定返回若干行。&lt;/li&gt;&lt;li&gt;源码路径：&lt;code&gt;components/tidb_query/src/batch/executors/limit_executor.rs&lt;/code&gt;&lt;/li&gt;&lt;li&gt;案例：&lt;code&gt;select col from t limit 10&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a86af3a75066450b0f1f2ffd99d742cd_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;241&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-a86af3a75066450b0f1f2ffd99d742cd_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a86af3a75066450b0f1f2ffd99d742cd_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;241&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-a86af3a75066450b0f1f2ffd99d742cd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a86af3a75066450b0f1f2ffd99d742cd_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;TopN&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;定义：按照给定表达式进行排序后，取出前若干行数据。&lt;/li&gt;&lt;li&gt;源码路径：&lt;code&gt;components/tidb_query/src/batch/executors/top_n_executor.rs&lt;/code&gt;&lt;/li&gt;&lt;li&gt;案例：&lt;code&gt;select col from t order by a+1 limit 10&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f81a619595326369d12d9259d1773b01_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;172&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-f81a619595326369d12d9259d1773b01_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f81a619595326369d12d9259d1773b01_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;172&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-f81a619595326369d12d9259d1773b01_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f81a619595326369d12d9259d1773b01_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;Aggregation&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;定义：按照给定表达式进行分组、聚合。&lt;/li&gt;&lt;li&gt;源码路径：&lt;code&gt;components/tidb_query/src/batch/executors/*_aggr_executor.rs&lt;/code&gt;&lt;/li&gt;&lt;li&gt;案例： &lt;code&gt;select count(1) from t group by score + 1&lt;/code&gt; &lt;/li&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8669a7a532d4dae1e3ace722d3eb5a5c_b.png&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;133&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-8669a7a532d4dae1e3ace722d3eb5a5c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8669a7a532d4dae1e3ace722d3eb5a5c_b.png&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;133&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-8669a7a532d4dae1e3ace722d3eb5a5c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-8669a7a532d4dae1e3ace722d3eb5a5c_b.png&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;混合使用各个算子&lt;/h3&gt;&lt;p&gt;综上，各个算子之间可以按照以下方式任意组合，如下图所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5c5b39a2ad24c059413f961ef11470dd_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;905&quot; data-rawheight=&quot;524&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;905&quot; data-original=&quot;https://pic2.zhimg.com/v2-5c5b39a2ad24c059413f961ef11470dd_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5c5b39a2ad24c059413f961ef11470dd_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;905&quot; data-rawheight=&quot;524&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;905&quot; data-original=&quot;https://pic2.zhimg.com/v2-5c5b39a2ad24c059413f961ef11470dd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5c5b39a2ad24c059413f961ef11470dd_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;案例：&lt;code&gt;select count(1) from t where age&amp;gt;10&lt;/code&gt; &lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5b2a0a955e0c4051716a04f1118c7559_b.png&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;124&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5b2a0a955e0c4051716a04f1118c7559_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5b2a0a955e0c4051716a04f1118c7559_b.png&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;124&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5b2a0a955e0c4051716a04f1118c7559_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5b2a0a955e0c4051716a04f1118c7559_b.png&quot;/&gt;&lt;/figure&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;由于篇幅原因，本文只是讲了一些 Coprocessor 的概要，读者对此有个概念即可。后续我们将推出该模块相关的更多更深的源码细节分析，欢迎大家继续阅读并给出建设性的改进意见。&lt;/p&gt;&lt;p&gt;原文阅读：&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tikv-source-code-reading-14/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章（十四）Coprocessor 概览 | PingCAP&lt;/a&gt;&lt;p&gt;更多 TiKV 源码阅读：&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Blog-cns | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-31-89518391</guid>
<pubDate>Thu, 31 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>据说今年黑客马拉松项目又多又猛？| TiDB Hackathon 回顾</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-31-89438349.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/89438349&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a1bf6fcf4f4d3255605d213ca373195b_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;TiDB Hackathon 2019  在 10 月 27 日完美收官！北京、上海、广州三地共 &lt;b&gt;39 支队伍&lt;/b&gt;参赛，两天一夜的 Hacking Time，大家围绕着&lt;b&gt;「Improve」&lt;/b&gt;主题，为 TiDB 性能、易用性、稳定性、功能等各方面做出提升，最终 6 支队伍瓜分了一、二、三等奖的 15 万元现金奖励，另有 4 支队伍分获最佳贡献奖、最佳创意奖、最具潜力奖和 CTO 特别奖。&lt;br/&gt;&lt;b&gt;话说，本届 Hackathon 不管是从比赛的形式、规模，还是完赛项目的质量，相比去年可以说是颠覆性的提升。&lt;/b&gt;&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;项目超猛，Demo Show 超长&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;今年总共 39 支队伍参赛，几乎较去年翻倍。从质量来看，&lt;b&gt;优秀项目又多又猛&lt;/b&gt;，奖项角逐非常激烈，三地联动直播的 Demo Show 从下午 14:30 持续到 21:30（没看错，7 个小时），但是大家越看越兴奋——兴奋程度堪比看到编译完没有报错和 Warning 的电脑屏幕放几张（评委 Ed Huang&amp;#34;激动盗摄&amp;#34;的）PPT 大家随意感受一下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5a3d6e2b5f8e127ae04c64623c027efe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-5a3d6e2b5f8e127ae04c64623c027efe_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5a3d6e2b5f8e127ae04c64623c027efe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-5a3d6e2b5f8e127ae04c64623c027efe_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-5a3d6e2b5f8e127ae04c64623c027efe_b.jpg&quot;/&gt;&lt;figcaption&gt;「Improve」不是说着玩儿的&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;经过紧张评选，评委团最终共评出了一、二、三等奖和最佳创意、最佳贡献、最佳潜力奖。由于奖项角逐太激烈，有很多优秀项目遗憾落选，我司 CTO 黄东旭现场临时增设了“CTO 特别奖” ，以下是全部获奖名单：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0428c4452cfad85a8e6a2288bc65ee45_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;834&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-0428c4452cfad85a8e6a2288bc65ee45_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-0428c4452cfad85a8e6a2288bc65ee45_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;834&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-0428c4452cfad85a8e6a2288bc65ee45_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-0428c4452cfad85a8e6a2288bc65ee45_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;再次恭喜各位获奖选手～🎉 &lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d55aec35ff71271514d24cc3889e39f5_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1439&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-d55aec35ff71271514d24cc3889e39f5_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d55aec35ff71271514d24cc3889e39f5_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1439&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-d55aec35ff71271514d24cc3889e39f5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-d55aec35ff71271514d24cc3889e39f5_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-80e31cd113b25356ab37cca5e3b3fcb8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;810&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-80e31cd113b25356ab37cca5e3b3fcb8_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-80e31cd113b25356ab37cca5e3b3fcb8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;810&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-80e31cd113b25356ab37cca5e3b3fcb8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-80e31cd113b25356ab37cca5e3b3fcb8_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;这些项目深深刺激了大家的神经，小伙伴们看完 Demo Show 都开始了激烈的讨论、摩拳擦掌开始推动项目落地，而本届 Hackathon 导师、我司首席架构师唐刘老师，“开心到飞起”，火速写了一篇的点评文章（以下为节选内容）——&lt;/p&gt;&lt;p&gt;&lt;b&gt;一等奖项目：Unified Thread Pool&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个项目主要解决 TiKV 两个问题：1）线程池太多，频繁线程切换导致性能问题；2）大查询影响小查询问题。一开始我并不知道奕霖同学要挑战这个项目，看到的时候真的很震惊，毕竟之前我们内部尝试过几次，但都无疾而终。主要面临的困难是同一个线程池如何调度大小查询，包括大查询不能影响小查询，同时小查询的性能又要足够好。&lt;/p&gt;&lt;p&gt;奕霖同学参考 Linux 的 Schedule 算法，以及 Rust juliex 库，跟他同学一起完成了 Unified Thread Pool 的原型，实际的测试效果让我非常震惊，不光是纯 point select 性能有大量提升，在有大查询的情况下面，QPS 也能保持稳定。&lt;/p&gt;&lt;p&gt;&lt;b&gt;二等奖项目：tidb-wasm&lt;/b&gt;&lt;/p&gt;&lt;p&gt;不得不说，这个是一个大杀器，它通过 wasm 技术，让 TiDB 能跑在浏览器上面，这个就很有意思了，我们可以很方便的做一个 TiDB Playground 出来，放到 PingCAP University（PU）这边，或者可以官网上嵌入，让用户直接尝试使用。这对于让用户快速的感受 TiDB 非常重要，可以说极大降低了用户了解 TiDB 的门槛，我个人非常期望这个项目能够落地。&lt;/p&gt;&lt;p&gt;&lt;b&gt;二等奖项目：TiDB 跨数据中心的解决方案&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个项目主要是解决跨 DC 情况下面 TiDB 的 latency 问题，主要从 Raft 层面来解决的。两个核心的思想：&lt;/p&gt;&lt;p&gt;1. Follower replication - 引入 raft sub-group 概念，将一些节点按照 DC 等属性进行分组，每个组里面有一个 delegate 节点，leader 只会跟这个 delegate 节点交互，由这个 delegate 节点将数据转发给这个 group 里面的其他节点。&lt;/p&gt;&lt;p&gt;2. 对于异地 follower read，并发的发送获取 TSO 以及 ReadIndex 的请求，减少一次 RTT。&lt;/p&gt;&lt;p&gt;这个项目我个人在开赛之前非常看好，后面果然拿了第二名的好成绩。&lt;/p&gt;&lt;p&gt;&lt;i&gt;……此处省略八千字长文点评，以上点评仅代表首架个人观点。&lt;/i&gt;&lt;/p&gt;&lt;blockquote&gt;总之这次完赛的项目质量都非常高，除了获奖项目之外，还有一些很有意义、实用性很强同时极具想象力的项目，我们非常希望这些项目都能在社区的维护下完善成熟～&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;评委老师好严肃，好认真&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;第二天大家吃完最后的午餐，就进入紧张的 Demo Show 环节了。评委团从项目的实用性/易用性/性能（40%）、完成度（30%）、创新性（20%）、展示度（10%），四个方面进行打分。由于项目一个比一个精彩，竞争非常激烈，评委老师们也卯足了劲，聚精会神地看每个细节，甚至用笔认真记下每个项目的优缺点。在选手演示结束后，评委老师们也随机提问，当然也不乏对项目完善方向的建议。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-30bb33226f8a1f265083a39b366cbfd0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-30bb33226f8a1f265083a39b366cbfd0_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-30bb33226f8a1f265083a39b366cbfd0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-30bb33226f8a1f265083a39b366cbfd0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-30bb33226f8a1f265083a39b366cbfd0_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;尤其感谢美团数据库团队负责人&lt;b&gt;李凯&lt;/b&gt;，58 集团数据库高级经理&lt;b&gt;于伯伟&lt;/b&gt;，京东云高级总监&lt;b&gt;李道兵&lt;/b&gt;，美团点评分布式数据库平台开发和运维负责人、研究员&lt;b&gt;赵应钢&lt;/b&gt;，贝壳找房数据技术总监&lt;b&gt;侯圣文&lt;/b&gt;，五位老师认真负责的评审。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;分布式赛事保障&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;当然硬核的黑客马拉松少不了吃吃喝喝啦，为了保障北上广三地分布式赛事顺利，TiDB Robot 的分身们都拼了。希望所有参赛选手回忆起 2019 年参加过最好吃的马拉松，脑袋里会冒出 TiDB Robot 在会议室给大家摆自助大餐、小龙虾、披萨、啤酒的样子，毕竟 Robot 每顿都要在群里叉腰大喊：吃早饭啦，吃午饭啦，吃晚饭啦，趁热吃！还有夜宵零食随便吃！（都给我吃！）&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5594157428b4ae6372ca09622d86e135_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-5594157428b4ae6372ca09622d86e135_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5594157428b4ae6372ca09622d86e135_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-5594157428b4ae6372ca09622d86e135_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5594157428b4ae6372ca09622d86e135_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;不过最令 Robot 感到惊悚的事情莫过于，第二天早上看到一位同学头朝下，窝在沙发里，2 小时一动不动。Robot 心里咯噔了一下，静静观察了这位同学的呼吸起伏……（此处就不配图了）&lt;b&gt;大家都是通宵型选手，嗨起来其实都不用睡的……&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ffc7b4a0aa877407af75bc3b0935099a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-ffc7b4a0aa877407af75bc3b0935099a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ffc7b4a0aa877407af75bc3b0935099a_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-ffc7b4a0aa877407af75bc3b0935099a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-ffc7b4a0aa877407af75bc3b0935099a_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;完赛项目合集，有你感兴趣的吗？&lt;/b&gt;&lt;/h2&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-16758e456675d2cf9ac696384dce5411_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;2328&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-16758e456675d2cf9ac696384dce5411_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-16758e456675d2cf9ac696384dce5411_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;2328&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-16758e456675d2cf9ac696384dce5411_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-16758e456675d2cf9ac696384dce5411_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;以上是最后参与 Demo Show 的 37 个项目，不知有没有大家特别感兴趣的呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;为了社区小伙伴们都能参与进来，一起推动这些优秀项目的落地，我们将邀请部分参赛选手撰文，为大家深入介绍他们的项目设计思路、实现过程以及未来工作方向，敬请期待！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;* 完整项目介绍 &amp;amp; Repo 地址：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/presentations/blob/master/hackathon-2019/hackathon-2019-projects.md&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/pres&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;entations/blob/master/hackathon-2019/hackathon-2019-projects.md&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后，&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;感谢 UCloud 提供云计算基础设施支持，&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;感谢志愿者们的奉献！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;我们明年见～&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-08e50ecbec49e3daaab0e04e1ad6eabc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-08e50ecbec49e3daaab0e04e1ad6eabc_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-08e50ecbec49e3daaab0e04e1ad6eabc_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1080&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-08e50ecbec49e3daaab0e04e1ad6eabc_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-08e50ecbec49e3daaab0e04e1ad6eabc_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-31-89438349</guid>
<pubDate>Thu, 31 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>高效编排有状态应用——TiDB 的云原生实践与思考</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-29-89037305.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/89037305&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5a5103695217f561e4e3f24d2152e650_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者： &lt;a class=&quot;member_mention&quot; href=&quot;https://www.zhihu.com/people/012515fbeab1b0b61084100a805ecaa0&quot; data-hash=&quot;012515fbeab1b0b61084100a805ecaa0&quot; data-hovercard=&quot;p$b$012515fbeab1b0b61084100a805ecaa0&quot;&gt;@吴叶磊&lt;/a&gt; &lt;/p&gt;&lt;h2&gt;导语&lt;/h2&gt;&lt;p&gt;云原生时代以降，无状态应用以其天生的可替换性率先成为各类编排系统的宠儿。以 Kubernetes 为代表的编排系统能够充分利用云上的可编程基础设施，实现无状态应用的弹性伸缩与自动故障转移。这种基础能力的下沉无疑是对应用开发者生产力的又一次解放。 然而，在轻松地交付无状态应用时，我们应当注意到，状态本身并没有消失，而是按照各类最佳实践下推到了底层的数据库、对象存储等有状态应用上。那么，“负重前行”的有状态应用是否能充分利云与 Kubernetes 的潜力，复制无状态应用的成功呢？&lt;/p&gt;&lt;p&gt;或许你已经知道，Operator 模式已经成为社区在 Kubernetes 上编排有状态应用的最佳实践，脚手架项目 KubeBuilder 和 operator-sdk 也已经愈发成熟，而对磁盘 IO 有严苛要求的数据库等应用所必须的 Local PV（本地持久卷）也已经在 1.14 中 GA。这些积木似乎已经足够搭建出有状态应用在平稳运行在 Kubernetes 之上这一和谐景象。然而，书面上的最佳实践与生产环境之间还有无数工程细节造就的鸿沟，要在 Kubernetes 上可靠地运行有状态应用仍需要相当多的努力。下面我将以 TiDB 与 Kubernetes 的“爱恨情仇”为例，总结有状态应用走向云原生的工程最佳实践。&lt;/p&gt;&lt;h2&gt;TiDB 简介&lt;/h2&gt;&lt;p&gt;首先让我们先熟悉熟悉研究对象。TiDB 是一个分布式的关系型数据库，它采用了存储和计算分离的架构，并且分层十分清晰：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8f4eda9feef3eadca9c611672edf88b3_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8f4eda9feef3eadca9c611672edf88b3_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8f4eda9feef3eadca9c611672edf88b3_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8f4eda9feef3eadca9c611672edf88b3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8f4eda9feef3eadca9c611672edf88b3_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1 TiDB 架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其中 TiDB 是 SQL 计算层，TiDB 进程接收 SQL 请求，计算查询计划，再根据查询计划去查询存储层完成查询。&lt;/p&gt;&lt;p&gt;存储层就是图中的 TiKV，TiKV 会将数据拆分为一个个小的数据块，比如一张 1000000 行的表，在 TiKV 中就有可能被拆分为 200 个 5000 行的数据块。这些数据块在 TiKV 中叫做 Region，而为了确保可用性， 每个 Region 都对应一个 Raft Group，通过 Raft Log 复制实现每个 Region 至少有三副本。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5dd409c6382dfa04bf2d726cb9745bf2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-5dd409c6382dfa04bf2d726cb9745bf2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5dd409c6382dfa04bf2d726cb9745bf2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-5dd409c6382dfa04bf2d726cb9745bf2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-5dd409c6382dfa04bf2d726cb9745bf2_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2 TiKV Region 分布&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;而 PD 则是集群的大脑，它接收 TiKV 进程上报的存储信息，并计算出整个集群中的 Region 分布。借由此，TiDB 便能通过 PD 获知该如何访问某块数据。更重要的是，PD 还会基于集群 Region 分布与负载情况进行数据调度。比如，将过大的 Region 拆分为两个小 Region，避免 Region 大小由于写入而无限扩张；将部分 Leader 或数据副本从负载较高的 TiKV 实例迁移到负载较低的 TiKV 实例上，以最大化集群性能。这引出了一个很有趣的事实，也就是 TiKV 虽然是存储层，但它可以非常简单地进行水平伸缩。这有点意思对吧？在传统的存储中，假如我们通过分片打散数据，那么加减节点数往往需要重新分片或手工迁移大量的数据。而在 TiKV 中，以 Region 为抽象的数据块迁移能够在 PD 的调度下完全自动化地进行，而对于运维而言，只管加机器就行了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;了解有状态应用本身的架构与特性是进行编排的前提，比如通过前面的介绍我们就可以归纳出，TiDB 是无状态的，PD 和 TiKV 是有状态的，它们三者均能独立进行水平伸缩。我们也能看到，TiDB 本身的设计就是云原生的——它的容错能力和水平伸缩能力能够充分发挥云基础设施提供的弹性，既然如此，云原生“操作系统” Kubernetes 不正是云原生数据库 TiDB 的最佳载体吗？TiDB Operator 应运而生。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;TiDB Operator 简介&lt;/h2&gt;&lt;p&gt;Operator 大家都很熟悉了，目前几乎每个开源的存储项目都有自己的 Operator，比如鼻祖 etcd-operator 以及后来的 prometheus-operator、postgres-operator。Operator 的灵感很简单，Kubernetes 自身就用 Deployment、DaemonSet 等 API 对象来记录用户的意图，并通过 control loop 控制集群状态向目标状态收敛，那么我们当然也可以定义自己的 API 对象，记录自身领域中的特定意图，并通过自定义的 control loop 完成状态收敛。&lt;/p&gt;&lt;p&gt;在 Kubernetes 中，添加自定义 API 对象的最简单方式就是 CustomResourceDefinition（CRD），而添加自定义 control loop 的最简单方式则是部署一个自定义控制器。自定义控制器 + CRD 就是 Operator。具体到 TiDB 上，用户可以向 Kubernetes 提交一个 TidbCluster 对象来描述 TiDB 集群定义，假设我们这里描述说“集群有 3 个 PD 节点、3 个 TiDB 节点和 3 个 TiKV 节点”，这是我们的意图。 而 TiDB Operator 中的自定义控制器则会进行一系列的 Kubernetes 集群操作，比如分别创建 3 个 TiKV、TiDB、PD Pod，来让真实的集群符合我们的意图。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-39a45f3e0a57456a64ab8da808bc99be_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-39a45f3e0a57456a64ab8da808bc99be_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-39a45f3e0a57456a64ab8da808bc99be_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-39a45f3e0a57456a64ab8da808bc99be_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-39a45f3e0a57456a64ab8da808bc99be_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 TiDB Operator&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;TiDB Operator 的意义在于让 TiDB 能够无缝运行在 Kubernetes 上，而 Kubernetes 又为我们抽象了基础设施。因此，TiDB Operator 也是 TiDB 多种产品形态的内核。对于希望直接使用 TiDB Operator 的用户， TiDB Operator 能做到在既有 Kubernetes 集群或公有云上开箱即用；而对于不希望有太大运维负载，又需求一套完整的分布式数据库解决方案的用户，我们则提供了打包 Kubernetes 的 on-premise 部署解决方案，用户可以直接通过方案中打包的 GUI 操作 TiDB 集群，也能通过 OpenAPI 将集群管理能力接入到自己现有的 PaaS 平台中；另外，对于完全不想运维数据库，只希望购买 SQL 计算与存储能力的用户，我们则基于 TiDB Operator 提供托管的 TiDB 服务，也即 DBaaS（Database as a Service）。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fe4b8106d8ad8b64fc6f59f2653b8345_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-fe4b8106d8ad8b64fc6f59f2653b8345_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fe4b8106d8ad8b64fc6f59f2653b8345_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-fe4b8106d8ad8b64fc6f59f2653b8345_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-fe4b8106d8ad8b64fc6f59f2653b8345_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 TiDB Operator 的多种上层产品形态&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;多样的产品形态对作为内核的 TiDB Operator 提出了更高的要求与挑战——事实上，由于数据资产的宝贵性和引入状态后带来的复杂性，有状态应用的可靠性要求与运维复杂度往往远高于无状态应用，这从 TiDB Operator 所面临的挑战中就可见一斑。&lt;/p&gt;&lt;h2&gt;挑战&lt;/h2&gt;&lt;p&gt;描绘架构总是让人觉得美好，而生产中的实际挑战则将我们拖回现实。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB Operator 的最大挑战就是数据库的场景极其严苛，大量用户的期盼都是我的数据库能够“永不停机”，对于数据不一致或丢失更是零容忍&lt;/b&gt;。很多时候大家对于数据库等有状态应用的可用性要求甚至是高于承载线上服务的 Kubernetes 集群的，至少线上集群宕机还能补救，而数据一旦出问题，往往意味着巨大的损失和补救成本，甚至有可能“回天乏术”。这本身也会在很大程度上影响大家把有状态应用推上 Kubernetes 的信心。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第二个挑战是编排分布式系统这件事情本身的复杂性&lt;/b&gt;。Kubernetes 主导的 level driven 状态收敛模式虽然很好地解决了命令式编排在一致性、事务性上的种种问题，但它本身的心智模型是更为抽象的，我们需要考虑每一种可能的状态并针对性地设计收敛策略，而最后的实际状态收敛路径是随着环境而变化的，我们很难对整个过程进行准确的预测和验证。假如我们不能有效地控制编排层面的复杂度，最后的结果就是没有人能拍胸脯保证 TiDB Operator 能够满足上面提到的严苛挑战，那么走向生产也就无从谈起了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第三个挑战是存储&lt;/b&gt;。数据库对于磁盘和网络的 IO 性能相当敏感，而在 Kubernetes 上，最主流的各类网络存储很难满足 TiDB 对磁盘 IO 性能的要求。假如我们使用本地存储，则不得不面对本地存储的易失性问题——磁盘故障或节点故障都会导致某块存储不可用，而这两种故障在分布式系统中是家常便饭。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后的问题是，尽管 Kubernetes 成功抽象了基础设施的计算能力与存储能力，但在实际场景的成本优化上考虑得很少&lt;/b&gt;。对于公有云、私有云、裸金属等不同的基础设施环境，TiDB Operator 需要更高级、特化的调度策略来做成本优化。大家也知道，成本优化是没有尽头的，并且往往伴随着一些牺牲，怎么找到优化过程中边际收益最大化的点，同样也是非常有意思的问题之一。&lt;/p&gt;&lt;p&gt;其中，场景严苛可以作为一个前提条件，而针对性的成本优化则不够有普适性。我们接下来就从编排和存储两块入手，从实际例子来看 TiDB 与 TiDB Operator 如何解决这些问题，并推广到一般的有状态应用上。&lt;/p&gt;&lt;h2&gt;控制器——剪不断，理还乱&lt;/h2&gt;&lt;p&gt;TiDB Operator 需要驱动集群向期望状态收敛，而最简单的驱动方式就是创建一组 Pod 来组成 TiDB 集群。通过直接操作 Pod，我们可以自由地控制所有编排细节。举例来说，我们可以：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通过替换 Pod 中容器的 image 字段完成原地升级。&lt;/li&gt;&lt;li&gt;自由决定一组 Pod 的升级顺序。&lt;/li&gt;&lt;li&gt;自由下线任意 Pod。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;事实上我们也确实采用过完全操作 Pod 的方案，但是当真正推进该方案时我们才发现，这种完全“自己造轮子”的方案不仅开发复杂，而且验证成本非常高。试想，为什么大家对 Kubernetes 的接受度越来越高， 即使是传统上较为保守的公司现在也敢于拥抱 Kuberentes？除了 Kubernetes 本身项目素质过硬之外，更重要的是有整个社区为它背书。我们知道 Kubernetes 已经在各种场景下经受过大量的生产环境考验，这种信心是各类测试手段都没法给到我们的。回到 TiDB Operator 上，选择直接操作 Pod 就意味着我们抛弃了社区在 StatefulSet、Deployment 等对象中沉淀的编排经验，随之带来的巨大验证成本大大影响了整个项目的开发效率。&lt;/p&gt;&lt;p&gt;因此，在目前的 TiDB Operator 项目中，大家可以看到控制器的主要操作对象是 StatefulSet。StatefulSet 能够满足有状态应用的大部分通用编排需求。当然，StatefulSet 为了做到通用化，做了很多不必要的假设，比如高序号的 Pod 是隐式依赖低序号 Pod 的，这会给我们带来一些额外的限制，比如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;无法指定 Pod 进行下线缩容。&lt;/li&gt;&lt;li&gt;滚动更新顺序固定。&lt;/li&gt;&lt;li&gt;滚动更新需要后驱 Pod 全部 Ready。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;StatefulSet 和 Pod 的抉择，最终是灵活性和可靠性的权衡，而在 TiDB 面临的严苛场景下，我们只有先做到可靠，才能做开发、敢做开发。最后的选择自然就呼之欲出——StatefulSet。当然，这里并不是说，使用基于高级对象进行编排的方案要比基于 Pod 进行编排的方案更好，只是说我们在当时认为选择 StatefulSet 是一个更好的权衡。当然这个故事还没有结束，当我们基于 StatefulSet 把第一版 TiDB Operator 做稳定后，我们正在接下来的版本中开发一个新的对象来水平替换 StatefulSet，这个对象可以使用社区积累的 StatefulSet 测试用例进行验证，同时又可以解除上面提到的额外限制，给我们提供更好的灵活性。 假如你也在考虑从零开始搭建一个 Operator，或许也可以参考“先基于成熟的原生对象快速迭代，在验证了价值后再增强或替换原生对象来解决高级需求”这条落地路径。&lt;/p&gt;&lt;p&gt;接下来的问题是控制器如何协调基础设施层的状态与应用层的状态。举个例子，在滚动升级 TiKV 时，每次重启 TiKV 实例前，都要先驱逐该实例上的所有 Region Leader；而在缩容 TiKV 时，则要先在 PD 中将待缩容的 TiKV 下线，等待待缩容的 TiKV 实例上的 Region 全部迁移走，PD 认为 TiKV 下线完成时，再真正执行缩容操作调整 Pod 个数。这些都是在编排中协调应用层状态的例子，我们可以怎么做自动化呢？&lt;/p&gt;&lt;p&gt;大家也注意到了，上面的例子都和 Pod 下线挂钩，因此一个简单的方案就通过 container lifecycle hook，在 preStop 时执行一个脚本进行协调。这个方案碰到的第一个问题是缺乏全局信息，脚本中无法区分当前是在滚动升级还是缩容。当然，这可以通过在脚本中查询 apiserver 来绕过。更大的问题是 preStop hook 存在 grace period，kubelet 最多等待 .spec.terminationGracePeriodSeconds 这么长的时间，就会强制删除 Pod。对于 TiDB 的场景而言，我们更希望在自动的下线逻辑失败时进行等待并报警，通知运维人员介入，以便于最小化影响，因此基于 container hook 来做是不可接受的。&lt;/p&gt;&lt;p&gt;第二种方案是在控制循环中来协调应用层的状态。比如，我们可以通过 partition 字段来控制 StatefulSet 升级进度，并在升级前确保 leader 迁移完毕，如下图所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-803d2ddb2796b89e538432f4398c1611_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-803d2ddb2796b89e538432f4398c1611_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-803d2ddb2796b89e538432f4398c1611_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-803d2ddb2796b89e538432f4398c1611_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-803d2ddb2796b89e538432f4398c1611_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5 在控制循环中协调状态&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在伪代码中，每次我们因为要将所有 Pod 收敛到新版本而进入这段控制逻辑时，都会先检查下一个要待升级的 TiKV 实例上 leader 是否迁移完毕，直到迁移完毕才会继续往下走，调整 partition 参数，开始升级对应的 TiKV 实例。缩容也是类似的逻辑。但你可能已经意识到，缩容和滚动更新两个操作是有可能同时出现在状态收敛的过程中的，也就是同时修改 replicas 和 image 字段。这时候由于控制器需要区分缩容与滚动更新，诸如此类的边界条件会让控制器越来越复杂。&lt;/p&gt;&lt;p&gt;第三种方案是使用 Kubernetes 的 Admission Webhook 将一部分协调逻辑从控制器中拆出来，放到更纯粹的切面当中。针对这个例子，我们可以拦截 Pod 的 Delete 请求和针对上层对象的 Update 请求，检查缩容或滚动升级的前置条件，假如不满足，则拒绝请求并触发指令进行协调，比如驱逐 leader，假如满足，那么就放行请求。控制循环会不断下发指令直到状态收敛，因此 webhook 就相应地会不断进行检查直到条件满足，如下图所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fc1a229e38e2de39b8cf4a4c99c3cc42_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-fc1a229e38e2de39b8cf4a4c99c3cc42_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fc1a229e38e2de39b8cf4a4c99c3cc42_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-fc1a229e38e2de39b8cf4a4c99c3cc42_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-fc1a229e38e2de39b8cf4a4c99c3cc42_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6 在 Webhook 中协调状态&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这种方案的好处是我们把逻辑拆分到了一个与控制器垂直的单元中，从而可以更容易地编写业务代码和单元测试。当然，这个方案也有缺点，一是引入了新的错误模式，处理 webhook 的 server 假如宕机，会造成集群功能降级；二是该方案适用面并不广，只能用于状态协调与特定的 Kubernetes API 操作强相关的场景。在实际的代码实践中，我们会按照具体场景选择方案二或方案三，大家也可以到项目中一探究竟。&lt;/p&gt;&lt;p&gt;&lt;b&gt;上面的两个例子都是关于如何控制编排逻辑复杂度的，关于 Operator 的各类科普文中都会用一句“在自定义控制器中编写领域特定的运维知识”将这一部分轻描淡写地一笔带过，而我们的实践告诉我们，真正编写生产级 的自定义控制器充满挑战与抉择。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;Local PV —— 想说爱你不容易&lt;/h2&gt;&lt;p&gt;接下来是存储的问题。我们不妨看看 Kubernetes 为我们提供了哪些存储方案：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-943080c31fad592402b61e8dc462f044_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-943080c31fad592402b61e8dc462f044_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-943080c31fad592402b61e8dc462f044_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-943080c31fad592402b61e8dc462f044_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-943080c31fad592402b61e8dc462f044_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7 存储方案&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其中，本地临时存储中的数据会随着 Pod 被删除而清空，因此不适用于持久存储。&lt;/p&gt;&lt;p&gt;远程存储则面临两个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通常来说，远程存储的性能较差，这尤其体现在 IOPS 不够稳定上，因此对于磁盘性能有严格要求的有状态应用，大多数远程存储是不适用的。&lt;/li&gt;&lt;li&gt;通常来说，远程存储本身会做三副本，因此单位成本较高，这对于在存储层已经实现三副本的 TiDB 来说是不必要的成本开销。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;因此，最适用于 TiDB 的是本地持久存储。这其中，hostPath 的生命周期又不被 Kubernetes 管理，需要付出额外的维护成本，最终的选项就只剩下了 Local PV。&lt;/p&gt;&lt;p&gt;Local PV 并非免费的午餐，所有的文档都会告诉我们 Local PV 有以下限制：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据易失（相比于远程存储的三副本）。&lt;/li&gt;&lt;li&gt;节点故障会影响数据访问。&lt;/li&gt;&lt;li&gt;难以垂直扩展容量（相当一部分远程存储可以直接调整 volume 大小）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这些问题同样也是在传统的虚拟机运维场景下的痛点，因此 TiDB 本身设计就充分考虑了这些问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;本地存储的易失性要求应用自身实现数据冗余。&lt;/li&gt;&lt;ul&gt;&lt;li&gt;TiDB 的存储层 TiKV 默认就为每个 Region 维护至少三副本。&lt;/li&gt;&lt;li&gt;当副本缺失时，TiKV 能自动补齐副本数。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;节点故障会影响本地存储的数据访问。&lt;/li&gt;&lt;ul&gt;&lt;li&gt;节点故障后，相关 Region 会重新进行 leader 选举，将读写自动迁移到健康节点上。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;本地存储的容量难以垂直扩展。&lt;/li&gt;&lt;ul&gt;&lt;li&gt;TiKV 的自动数据切分与调度能够实现水平伸缩。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;存储层的这些关键特性是 TiDB 高效使用 Local PV 的前提条件，也是 TiDB 水平伸缩的关键所在。当然，在发生节点故障或磁盘故障时，由于旧 Pod 无法正常运行，我们需要自定义控制器帮助我们进行恢复，及时补齐实例数，确保有足够的健康实例来提供整个集群所需的存储空间、计算能力与 IO 能力。这也就是自动故障转移。&lt;/p&gt;&lt;p&gt;我们先看一看为什么 TiDB 的存储层不能像无状态应用或者使用远程存储的 Pod 那样自动进行故障转移。假设下图中的节点发生了故障，由于 TiKV-1 绑定了节点上的 PV，只能运行在该节点上，因此 在节点恢复前，TiKV-1 将一直处于 Pending 状态：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b90337525fa64a1e08842cb95db8ac62_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-b90337525fa64a1e08842cb95db8ac62_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b90337525fa64a1e08842cb95db8ac62_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-b90337525fa64a1e08842cb95db8ac62_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-b90337525fa64a1e08842cb95db8ac62_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8 节点故障&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;此时，假如我们能够确认 Node 已经宕机并且短期无法恢复，那么就可以删除 Node 对象（比如 NodeController 在公有上会查询公有云的 API 来删除已经释放的 Node）。此时，控制器通过 Node 对象不存在这一事实理解了 Node 已经无法恢复，就可以直接删除 pvc-1 来解绑 PV，并强制删除 TiKV-1，最终让 TiKV-1 调度到其它节点上。当然，我们同时也要做应用层状态的协调，也就是先在 PD 中下线 TiKV-1，再将新的 TiKV-1 作为一个新成员加入集群，此时，PD 就会通知 TiKV-1 创建 Region 副本来补齐集群中的 Region 副本数。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4def23bf88b88c266c411f25dedcbf3e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-4def23bf88b88c266c411f25dedcbf3e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4def23bf88b88c266c411f25dedcbf3e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-4def23bf88b88c266c411f25dedcbf3e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-4def23bf88b88c266c411f25dedcbf3e_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9 能够确定节点状态时的故障转移&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当然，更多的情况下，我们是无法在自定义控制器中确定节点状态的，此时就很难针对性地进行原地恢复，因此我们通过向集群中添加新 Pod 来进行故障转移：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a618bf0bde89d5f1a8862aeaf56e3f9a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-a618bf0bde89d5f1a8862aeaf56e3f9a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-a618bf0bde89d5f1a8862aeaf56e3f9a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-a618bf0bde89d5f1a8862aeaf56e3f9a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-a618bf0bde89d5f1a8862aeaf56e3f9a_b.jpg&quot;/&gt;&lt;figcaption&gt;图 10 无法确定节点状态时的故障转移&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;上面讲的是 TiDB 特有的故障转移策略，但其实可以类推到大部分的有状态应用上。比如对于 MySQL 的 slave，我们同样可以通过新增 slave 来做 failover，而在 failover 时，我们同样也要做应用层的一些事情， 比如说去 S3 上拉一个全量备份，再通过 binlog 把增量数据补上，当 lag 达到可接受的程度之后开始对外提供读服务。因此大家就可以发现，对于有状态应用的 failover 策略是共通的，也都需要应用本身支持某种 failover 形式。比如对于 MySQL 的 master，我们只能通过 M-M 模式做一定程度上的 failover，而且还会损失数据一致性。这当然不是 Kubernetes 或云原生本身有什么问题，而是说 Kubernetes 只是改变了应用的运维模式，但并不能影响应用本身的架构特性。假如应用本身的设计就不是云原生的，那只能从应用本身去解决。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;通过 TiDB Operator 的实践，我们有以下几条总结：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Operator 本身的复杂度不可忽视。&lt;/li&gt;&lt;li&gt;Local PV 能满足高 IO 性能需求，代价则是编排上额外的复杂度。&lt;/li&gt;&lt;li&gt;应用本身必须迈向云原生（meets kubernetes part way）。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，言语的描述总是不如代码本身来得简洁有力，TiDB Operator 是一个完全开源的项目，眼见为实，大家可以尽情到 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;项目仓库&lt;/a&gt; 中拍砖，也欢迎大家加入社区一起玩起来，期待你的 issue 和 PR！&lt;/p&gt;&lt;p&gt;假如你对于文章有任何问题或建议，或是想直接加入 PingCAP 鼓捣相关项目，欢迎通过我的邮箱 wuyelei@pingcap.com 联系我。&lt;/p&gt;&lt;blockquote&gt;本文为吴叶磊在 2019 QCon 全球软件开发大会（上海）上的专题演讲实录，Slides &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/presentations/blob/master/conference/%25E5%2590%25B4%25E5%258F%25B6%25E7%25A3%258A-QCon-2019-%25E9%25AB%2598%25E6%2595%2588%25E7%25BC%2596%25E6%258E%2592%25E6%259C%2589%25E7%258A%25B6%25E6%2580%2581%25E5%25BA%2594%25E7%2594%25A8-TiDB%25E7%259A%2584%25E4%25BA%2591%25E5%258E%259F%25E7%2594%259F%25E5%25AE%259E%25E8%25B7%25B5%25E4%25B8%258E%25E6%2580%259D%25E8%2580%2583.pdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;下载地址&lt;/a&gt;。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-29-89037305</guid>
<pubDate>Tue, 29 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>网易互娱的数据库选型和 TiDB 应用实践</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-27-87945199.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/87945199&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c7c2edf92e205ae540dc9e1f3bce5a97_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者介绍：李文杰，网易互娱计费组，高级数据库管理工程师，TiDB User Group Ambassador。&lt;/blockquote&gt;&lt;h2&gt;一、业务架构简介&lt;/h2&gt;&lt;p&gt;计费组是为网易互娱产品提供统一登录和支付高效解决方案的公共支持部门，对内是互娱的各个游戏工作室，对外是国内外数百个渠道。由于业务场景的特殊性，我们为各个游戏产品部署了不同的应用服务，其中大产品环境独立，小产品集中部署。&lt;/p&gt;&lt;p&gt;随着部门业务量的激增，单机 MySQL 在容量、性能、扩展性等方面都遇到了瓶颈，我们开始对其他数据库产品进行调研选型。本文将详细介绍网易互娱计费组针对自己场景的数据库选型对比方案，以及使用 TiDB 后解决的问题，并分享了使用 TiDB 过程中集群管理、监控和数据迁移等方面的最佳实践，以供大家参考和交流。&lt;/p&gt;&lt;h3&gt;1.1 MySQL 使用架构&lt;/h3&gt;&lt;p&gt;网易互娱计费组线上 MySQL 的基本使用架构，如下图所示，其中箭头方向表示数据或请求的指向：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1 网易互娱计费组线上 MySQL 使用架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;线上应用 Application 通过 Keepalive + 多机部署，流量经过负载均衡，可以有效保障应用服务的高可用；&lt;/li&gt;&lt;li&gt;数据库层架构是 Keepalive + 主从结构，利用半同步复制特性可以有效解决延迟和数据一致性的问题；&lt;/li&gt;&lt;li&gt;Application 通过 VIP 访问后端数据库，在数据库主节点宕机后通过 VIP 飘移到从节点，保证服务正常对外提供；&lt;/li&gt;&lt;li&gt;通过 Slave 节点进行数据备份和线上数据采集，经过全量和增量同步方式导出数据到数据中心，然后进行在线和离线计算任务；&lt;/li&gt;&lt;li&gt;类似这样的架构组合线上大概有 50+ 套，涉及服务器 200~400 台，日均新增数据 TB 级。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;1.2 MySQL 使用的现状与问题&lt;/h3&gt;&lt;p&gt;随着业务的发展，部门内各应用服务产生的数据量也在快速增长。业务落地数据量不断激增，导致单机 MySQL 不可避免地会出现性能瓶颈。主要体现在以下几个方面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;容量&lt;/li&gt;&lt;ul&gt;&lt;li&gt;单机 MySQL 实例存储空间有限，想要维持现有架构就得删除和轮转旧数据，达到释放空间的目的；&lt;/li&gt;&lt;li&gt;网易互娱某些场景单表容量达到 700GB 以上，订单数据需永久保存，同时也需要保持在线实时查询，按照之前的存储设计会出现明显的瓶颈。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;性能&lt;/li&gt;&lt;ul&gt;&lt;li&gt;最大单表 15 亿行，行数过大，导致读写性能受到影响。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;扩展性&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MySQL 无法在线灵活扩展，无法解决存储瓶颈。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;SQL 复杂&lt;/li&gt;&lt;ul&gt;&lt;li&gt;大表轮转后出现多个分表，联合查询时需要 join 多个分表，SQL 非常复杂并难以维护；&lt;/li&gt;&lt;li&gt;单机 MySQL 缺乏大规模数据分析的能力。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;数据壁垒&lt;/li&gt;&lt;ul&gt;&lt;li&gt;不同产品的数据库独立部署；&lt;/li&gt;&lt;li&gt;数据不互通，导致数据相关隔离，形成数据壁垒；&lt;/li&gt;&lt;li&gt;当进行跨产品计算时，需要维护多个异构数据源，访问方式复杂。数据分散在不同的数据孤岛上会增加数据分析难度，不利于共性价值的挖掘。如下图：&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2 现状之数据孤岛&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;二、数据库选型&lt;/h2&gt;&lt;h3&gt;2.1 调研目标&lt;/h3&gt;&lt;p&gt;针对目前存储架构存在的问题，有需要使用其他存储方案的可能。考虑到目前的业务与 MySQL 高度耦合，对数据库选型的主要要求有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;必须兼容 MySQL 协议；&lt;/li&gt;&lt;li&gt;支持事务，保证任务以事务为维度来执行或遇错回滚；&lt;/li&gt;&lt;li&gt;支持索引，尤其是二级索引；&lt;/li&gt;&lt;li&gt;扩展性，支持灵活在线扩展能力，包括性能扩展和容量扩展。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其他要求：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;稳定性和可靠性；&lt;/li&gt;&lt;li&gt;备份和恢复；&lt;/li&gt;&lt;li&gt;容灾等。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;2.2 可选方案&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1072&quot; data-rawheight=&quot;458&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1072&quot; data-original=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1072&quot; data-rawheight=&quot;458&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1072&quot; data-original=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;2.3 测试&lt;/h3&gt;&lt;h3&gt;2.3.1 基于 MySQL 的解决方案&lt;/h3&gt;&lt;p&gt;一开始仍然是倾向使用基于 MySQL 的解决方案，比如 MySQL InnoDB Cluster 或 MySQL + 中间件的方案。&lt;/p&gt;&lt;p&gt;我们测试了 MySQL 集群 5.7.25 版本对比 8.0.12 版本，在 128 并发写各 1000 万行的 10 个表，比较单节点、3 节点和 5 节点下的情况，如下图所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;462&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;462&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 对比结果&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在测试中发现，使用 MySQL InnoDB 集群的方案写性能比单机 MySQL 差约 30%，其他的读写测试结果也不甚满意。之后陆续测试 MySQL InnoDB Cluster 或 MySQL + 中间件的方案，不是测试结果性能不达要求，就是需要修改大量代码。&lt;/p&gt;&lt;p&gt;因此我们得出了基于 MySQL InnoDB Cluster 或 MySQL + 中间件的方案的不满足我们的业务场景的结论。总结来说，我们不使用 MySQL 分库分表、中间件或 MySQL 集群，原因主要是以下两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;方案过于复杂&lt;/li&gt;&lt;li&gt;需要改业务代码&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;仔细分析来看，其实基于 MySQL InnoDB Cluster 或 MySQL + 中间件的方案，本质上是 MySQL 主从结构的延伸，并非真正的分布式拓展，像是以打“补丁”的方式来实现横向扩展，很多功能特性自然也难以让人满意。&lt;/b&gt;&lt;/p&gt;&lt;h3&gt;2.3.2 CockroachDB VS TiDB&lt;/h3&gt;&lt;p&gt;在开源的分布式 NewSQL 领域，知名的有 TiDB 和 CockroachDB（简称 CRDB），二者都是基于 Google Spanner 论文的开源实现。我们对这两种数据库的功能和性能做了大量的调研和测试。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 天然兼容 MySQL 协议，而 CRDB 兼容 PostgreSQL ；&lt;/li&gt;&lt;li&gt;如果业务以 MySQL 为主，那 TiDB 可能是比较好的选择；如果是 PostgreSQL，那CRDB 可能是优先的选择。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;测试方面，我们也进行了全面地对比和测试。这里说其中一个测试案例：10 台机器 5 存储节点，160 并发访问单表 2 亿行，我们于 2018 年 7 月，对 CRDB-v2.1.0 版本和 TiDB-v2.0.5 版本进行了读写测试（CRDB 和 TiDB 集群均使用默认配置，未进行调优）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;集群拓扑&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;464&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;464&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 CockroachDB 测试集群搭建&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;531&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;531&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5 TiDB 测试集群搭建&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;测试语句&lt;/b&gt;&lt;/p&gt;&lt;p&gt;范围查询：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT c FROM sbtest%u WHERE id BETWEEN ? AND ?
SELECT SUM(k) FROM sbtest%u WHERE id BETWEEN ? AND ?
SELECT c FROM sbtest WHERE id BETWEEN ? AND ? ORDER BY c
SELECT DISTINCT c FROM sbtest%u WHERE id BETWEEN ? AND ? ORDER BY c&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;随机 IN 查询：&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT id, k, c, pad FROM sbtest1 WHERE k IN (?)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;随机范围查询：&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT count(k) FROM sbtest1 WHERE k BETWEEN ? AND ? OR k BETWEEN ? AND ?&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;更新索引列：&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;UPDATE sbtest%u SET k=k+1 WHERE id=?&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;更新非索引列：&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;UPDATE sbtest%u SET c=? WHERE id=?&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;读写混合：范围查询 + 更删改混合&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其中一个重要的测试结果如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;466&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;466&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6 测试结果&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;结论：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;CRDB 和 TiDB 在性能表现上不相上下；&lt;br/&gt;注：上面是 2018 年 7 月的基于 TiDB 2.0.5 版本的测试结果，现在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/releases/3.0-ga/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 已发布 3.0 GA 版本，在性能上有了质的提升&lt;/a&gt;，我们在近期进行了补充测试，大多数场景下 3.0 版本较 2.1 版本有数倍的性能提升，最新的测试结果图如下：&lt;br/&gt;&lt;/li&gt;&lt;/ol&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7 TiDB 2.1.15 vs 3.0.3：OLTP 峰值比较&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;306&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;306&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8 TiDB 2.1.15 vs 3.0.3：TPC-C&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;2. CRDB 兼容 PostgreSQL，如果需要迁移则需要转协议，需 MySQL → PostgreSQL  → CRDB。迁移过程复杂，成本高；&lt;/p&gt;&lt;p&gt;3. TiDB 兼容 MySQL，代码修改量不多，迁移成本低。&lt;/p&gt;&lt;h3&gt;2.3.3 最终选型&lt;/h3&gt;&lt;p&gt;综合对比结果如下表：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;970&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;970&quot; data-original=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;970&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;970&quot; data-original=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;经过谨慎的考量，我们选择了 TiDB。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;497&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;497&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9 选择 TiDB 的重要理由&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;三、TiDB 在网易互娱计费组的使用&lt;/h2&gt;&lt;h3&gt;3.1 TiDB 使用架构&lt;/h3&gt;&lt;p&gt;网易互娱使用 TiDB 的架构设计如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;479&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;479&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 10 基于 TiDB 的架构设计&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;整个集群分为 TiDB、TiKV 和 PD 3 个模块分层部署；&lt;/li&gt;&lt;li&gt;使用 Nginx 作为前端负载均衡。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;3.2 TiDB 解决了哪些需求&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1282&quot; data-rawheight=&quot;456&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1282&quot; data-original=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1282&quot; data-rawheight=&quot;456&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1282&quot; data-original=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;3.3 TiDB 使用现状&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;业务&lt;/li&gt;&lt;ul&gt;&lt;li&gt;TiDB 作为线上 MySQL 数据镜像，负责线上数据的收集和集中管理，形成数据湖泊；&lt;/li&gt;&lt;li&gt;应用于数据平台服务，包括报表、监控、运营、用户画像、大数据计算等场景；&lt;/li&gt;&lt;li&gt;HTAP：OLTP + OLAP。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;集群&lt;/li&gt;&lt;ul&gt;&lt;li&gt;测试集群：v2.1.15，用于功能测试、特性尝鲜；&lt;/li&gt;&lt;li&gt;线上集群：v2.1.15，80% 离线大数据计算任务 + 20% 线上业务。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;规模&lt;/li&gt;&lt;ul&gt;&lt;li&gt;41 台服务器，88 个实例节点，38 个 Syncer 实时同步流（将升级为 DM）；&lt;/li&gt;&lt;li&gt;存储：20TB/总 50TB，230 万个 Region；&lt;/li&gt;&lt;li&gt;QPS 均值 4k/s，高峰期万级 QPS，读写比约 1:5；&lt;/li&gt;&lt;li&gt;延迟时间：80% 在 8ms 以内，95% 在 125ms 以下，99.9% 在 500ms 以下。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;四、最佳实践分享&lt;/h2&gt;&lt;h3&gt;4.1 集群管理&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Ansible（推荐）&lt;/li&gt;&lt;ul&gt;&lt;li&gt;一键部署；&lt;/li&gt;&lt;li&gt;弹性伸缩，可在线灵活扩缩容；&lt;/li&gt;&lt;li&gt;升级，单节点轮转平滑升级；&lt;/li&gt;&lt;li&gt;集群启停和下线；&lt;/li&gt;&lt;li&gt;Prometheus 监控。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Docker&lt;/li&gt;&lt;li&gt;K8s&lt;/li&gt;&lt;ul&gt;&lt;li&gt;使用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Operator&lt;/a&gt; 可以在私有云和公有云上一键管理。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h3&gt;4.2 运维实践&lt;/h3&gt;&lt;h3&gt;4.2.1 Prometheus 监控&lt;/h3&gt;&lt;p&gt;官方集成了 Prometheus + Grafana 的实时监控平台，从集群的各个方面进行了完善的监控，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;服务器基础资源的监控：内存、CPU、存储空间、IO 等；&lt;/li&gt;&lt;li&gt;集群组件的监控：TiDB、PD、TiKV 等；&lt;/li&gt;&lt;li&gt;数据监控：实时同步流、上下游数据一致性检验等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;PD 监控示意图如下，集群管理员可以很方便地掌握集群的最新状态，包括集群的空间 Region 等所有情况。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_b.jpg&quot;/&gt;&lt;figcaption&gt;图 11 最佳运维实践：Prometheus 实时监控&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如果集群运行过程出错，在监控面板上很容易就发现，下图是使用过程中的一个案例：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;191&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;191&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_b.jpg&quot;/&gt;&lt;figcaption&gt;图 12 最佳运维实践案例&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;应用访问 TiDB 写入数据时发现特别慢，读请求正常。排查后，根据 TiKV 面板发现 Raft Store CPU 这项指标异常。深入了解原因是因为数据库副本复制是单线程操作，目前已经到了集群的瓶颈。解决办法有以下两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Region 数量过多，Raft Store 还要处理 heartbeat message。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;解决方法：删除过期数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Raft Store 单线程处理速度跟不上集群写入速度。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;解决方法：从 2.1.5 升级到 2.1.15，开启自动 Region Merge 功能。&lt;/p&gt;&lt;h3&gt;4.2.2 部分运维问题及解决方案&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1422&quot; data-rawheight=&quot;1314&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1422&quot; data-original=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1422&quot; data-rawheight=&quot;1314&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1422&quot; data-original=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;4.3 全网数据库遍历&lt;/h3&gt;&lt;p&gt;以前部分业务遍历全网数据库获取所需数据，需要维护多个源，而且是异构源，非常复杂和繁琐。使用 TiDB 很好地解决了这个问题，只需要访问一个源就可以获取到所有想要的数据。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;412&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;412&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_b.jpg&quot;/&gt;&lt;figcaption&gt;图 13 全网数据库遍历&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;4.4 数据迁移&lt;/h3&gt;&lt;h3&gt;4.4.1 MySQL 到 TiDB&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_b.jpg&quot;/&gt;&lt;figcaption&gt;图 14 数据从 MySQL 迁移到 TiDB&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;MySQL 数据库迁移到 TiDB 分为两个部分：全量和增量。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;全量&lt;/li&gt;&lt;ul&gt;&lt;li&gt;使用工具 （Mydumper 或 MySQL Dump 等）从 MySQL 导出数据，并且记录当前数据的 binlog 位置；&lt;/li&gt;&lt;li&gt;使用工具（Loader 或 Lightning 等）将数据导入到 TiDB 集群；&lt;/li&gt;&lt;li&gt;可以用作数据的备份和恢复操作。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;增量&lt;/li&gt;&lt;ul&gt;&lt;li&gt;TiDB 伪装成为上游 MySQL 的一个 Slave，通过工具（Syncer 或 DM）实时同步 binlog 到 TiDB 集群；&lt;/li&gt;&lt;li&gt;通常情况上游一旦有数据更新，下游就会实时同步过来。同步速度受网络和数据量大小的影响。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h3&gt;4.4.2 数据迁出 TiDB&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_b.jpg&quot;/&gt;&lt;figcaption&gt;图 15 数据迁出 TiDB&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如果数据需要反向导入或同步，可以利用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog&lt;/a&gt; 工具将 TiDB 集群的 binlog 同步到 MySQL。TiDB Binlog 支持以下功能场景：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;数据同步&lt;/b&gt;：同步 TiDB 集群数据到其他数据库；&lt;/li&gt;&lt;li&gt;&lt;b&gt;实时备份和恢复&lt;/b&gt;：备份 TiDB 集群数据，同时可以用于 TiDB 集群故障时恢复。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;导入的方式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;全量：TiDB 兼容 MySQL 协议，在 MySQL 容量足够大的情况下，也可用工具将数据从 TiDB 导出后再导入 MySQL。&lt;/li&gt;&lt;li&gt;增量：打开 TiDB 的 binlog 开关，部署 binlog 收集组件（Pump+Drainer），可以将 binlog 数据同步到下游存储架构（MySQL、TiDB、Kafka、S3 等）。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;4.5 优雅地「去分库分表」&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_b.jpg&quot;/&gt;&lt;figcaption&gt;图 16 去分库分表举例&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;举例：一个超级大表按天分表，现在打算查询某个账号一年间的信息。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;上游 MySQL&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT xx FROM HFeeall join HFee20190101 join ... join ...join ... join HFee20190917 WHERE xx;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;需要连接 N 个 join 条件，查询需要等待较长时间。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;下游 TiDB&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT xx  FROM SuperHfeeall WHERE xx ;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;应用此方案，最大单表 700+GB，13+ 亿行，索引查询秒返回。&lt;/p&gt;&lt;h3&gt;4.6  业务迁移&lt;/h3&gt;&lt;p&gt;&lt;b&gt;目标&lt;/b&gt;：利用 TiDB 的水平扩展特性，解决容量瓶颈和系统吞吐量瓶颈。&lt;/p&gt;&lt;p&gt;&lt;b&gt;迁移原则&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据完整和准确：数据很重要，保证数据不错、不丢；&lt;/li&gt;&lt;li&gt;迁移平滑和迅速：服务敏感度高，停服时间要短；&lt;/li&gt;&lt;li&gt;可回滚：遇到问题可随时切回到 MySQL。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;1）数据同步&lt;/b&gt;&lt;/p&gt;&lt;p&gt;使用 DM 或者 Syncer 将上游 MySQL 的数据同步到 TiDB 集群。同步流搭建后注意需要检查上下游数据一致性。&lt;/p&gt;&lt;p&gt;观察一段时间，同步无误后，可以根据业务需要迁移部分读流量到 TiDB 集群。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;899&quot; data-rawheight=&quot;831&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;899&quot; data-original=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;899&quot; data-rawheight=&quot;831&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;899&quot; data-original=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_b.jpg&quot;/&gt;&lt;figcaption&gt;图 17 业务迁移之数据同步&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;2）读写验证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这一阶段是验证应用访问 MySQL 和访问 TiDB 可以得到相同的结果，验证业务访问的准确性问题。&lt;/p&gt;&lt;p&gt;停止数据同步，使用流量复制工具将线上流量完全拷贝出来，同时读写 MySQL 和 TiDB。将两边的访问结果进行对比，核查 TiDB 是否可靠和可信。根据需要，这个阶段可以测试较长时间。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;772&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;772&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_b.jpg&quot;/&gt;&lt;figcaption&gt;图 18 业务迁移之读写验证&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;3）灰度切换&lt;/b&gt;&lt;/p&gt;&lt;p&gt;将步骤 2 的双写停止，即关双写，同时拉起上游的 DM 同步。&lt;/p&gt;&lt;p&gt;把访问部分非核心业务的库表写操作迁移到 TiDB，打开 TiDB 的 Binlog 开关对线上 MySQL 进行反向同步。这个操作，保证只写 MySQL 的数据同步到 TiDB ，只写 TiDB 的数据也可以反向同步到 MySQL，保证出了问题，随时可以回滚。当业务长时间访问正常，可以增加切换流量，进行灰度切换。建议观察一段时间，至少一个月。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;854&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;854&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_b.jpg&quot;/&gt;&lt;figcaption&gt;图 19 业务迁移之灰度切换&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;4）迁移完成&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当流量完全迁移完成，保持 TiDB 反同步到 MySQL 过程，继续观察一段时间，确认无误后，断开反向同步，100% 迁移完成。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;877&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;877&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 20 完成迁移&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;五、总结与展望&lt;/h2&gt;&lt;p&gt;TiDB 兼容 MySQL 协议，支持 TP/AP 事务且扩展性好，能很好地解决网易互娱计费组业务大容量、高可用等问题。目前我们的业务在不断深入和扩大规模使用 TiDB，因为看好它，所以这里提出一些使用中的问题以帮助原厂持续打磨产品：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;集群数据备份：希望提供集群更高效地备份和恢复 SST 文件的方式；&lt;/li&gt;&lt;li&gt;事务限制：希望可以放宽大事务的限制，现在仍需要人工切分大事务，比较复杂；&lt;/li&gt;&lt;li&gt;同步：希望 DM 支持上下游表结构不一致的同步；&lt;/li&gt;&lt;li&gt;数据热点问题：建议加强自动检测和清除热点功能；&lt;/li&gt;&lt;li&gt;客户端重试：目前客户端代码需要封装重试逻辑，对用户不友好，希望可以改进。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，根据网易互娱计费组已有的使用情况，我们计划继续加大、加深 TiDB 的使用场景，丰富业务类型和使用规模，期待 TiDB 给我们的业务带来更多便利。&lt;/p&gt;&lt;p&gt;&lt;b&gt;阅读原文：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/user-case-wangyihuyu/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;网易互娱的数据库选型和 TiDB 应用实践 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多精彩案例：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;案例 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-27-87945199</guid>
<pubDate>Sun, 27 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 项目首个 SIG 成立，一起走上 Contributor 进阶之路吧！</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-24-88343561.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/88343561&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-5fb74c191abbf1a382b33b960aac4f32_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：Long Heneg&lt;/p&gt;&lt;p&gt;社区是一个开源项目的灵魂，随着 TiDB/TiKV &lt;u&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247489956%26idx%3D1%26sn%3D44724f8b262ed562fd8f1f0b5afbd4ba%26chksm%3Deb163ecedc61b7d8af57d867efa7241edc4fc987136c28a2c0f561744d809a465d403923d564%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;新的社区架构升级&lt;/a&gt;&lt;/u&gt;， TiKV 社区也计划逐步成立更多个 Special Interest Group（SIG ）吸引更多社区力量，一起来改进和完善 TiKV 项目。SIG  将围绕着特定的模块进行开发和维护工作，并对该模块代码的质量负责。&lt;/p&gt;&lt;p&gt;今天是 1024 程序员节，我们正式成立  TiKV 项目的首个 SIG —— Coprocessor SIG，希望对 TiKV 项目（&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//github.com/tikv/tikv&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/tikv/tikv&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;） 感兴趣的小伙伴们都能加入进来，探索硬核的前沿技术，交流切磋，一起走上 Contributor 的进阶之路！&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Coprocessor 模块是什么？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;为了提升数据库的整体性能，TiDB 会将部分计算下推到 TiKV 执行，即 TiKV 的 Coprocessor 模块。本次成立的 Coprocessor SIG 就聚焦在 TiKV 项目 Coprocessor 模块。本 SIG 的主要职责是对 Coprocessor 模块进行未来发展的讨论、规划、开发和维护。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何加入 Coprocessor SIG？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;社区的 Reviewer 或更高级的贡献者（Committer，Maintainer）将提名Active Contributor加入 Coprocessor SIG。Active Contributor 是对于 TiKV Coprocessor 模块或者 TiKV 项目有浓厚兴趣的贡献者，在过去 1 年为 TiKV 项目贡献超过 8 个 PR。&lt;/b&gt;加入 SIG 后，Coprocessor SIG Tech Lead 将指导成员完成目标任务。在此过程中，成员可以从 Active Contributor 逐步晋升为 Reviewer、Committer 角色，解锁更多角色权利&amp;amp;义务。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Reviewer&lt;/b&gt;：从 Active Contributor 中诞生，当 Active Contributor 对 Coprocessor 模块拥有比较深度的贡献，并且得到 2 个或 2 个以上 Committer 的提名时，将被邀请成为该模块的 Reviewer，主要权利&amp;amp;义务：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;参与 Coprocessor PR Review 与质量控制；&lt;/li&gt;&lt;li&gt;对 Coprocessor 模块 PR 具有有效的 Approve / Request Change 权限；&lt;/li&gt;&lt;li&gt;参与项目设计决策。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;Committer&lt;/b&gt;：资深的社区开发者，从 Reviewer 中诞生。当 Reviewer 对 Coprocessor  模块拥有非常深度的贡献，或者在保持 Coprocessor  模块 Reviewer 角色的同时，也在别的模块深度贡献成为了 Reviewer，这时他就在深度或者广度上具备了成为 Committer 的条件，只要再得到 2 个或 2 个以上 Maintainer 的提名时，即可成为 Committer，主要权利及义务：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;拥有 Reviewer 具有的权利与义务；&lt;/li&gt;&lt;li&gt;整体把控项目的代码质量；&lt;/li&gt;&lt;li&gt;指导 Contributor 与 Reviewer。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;工作内容有哪些？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 完善测试&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;为了进一步提高 Coprocessor 的集成测试覆盖率，TiKV 社区开源了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/copr-test&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;copr-test&lt;/a&gt; 集成测试框架，便于社区为 Coprocessor 添加更多集成测试；&lt;/li&gt;&lt;li&gt;从 TiDB port 的函数需要同时 port 单元测试，如果 TiDB 的单元测试没有覆盖所有的分支，需要补全单元测试；&lt;/li&gt;&lt;li&gt;Expression 的集成测试需要构造使用这个 Expression 的算子进行测试。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 提升代码质量&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Framework: 计算框架改进，包括表达式计算框架、算子执行框架等；&lt;/li&gt;&lt;li&gt;Executor: 改进现有算子、与 TiDB 协作研发新算子；&lt;/li&gt;&lt;li&gt;Function: 维护现有的 UDF / AggrFn 实现或从  TiDB port 新的 UDF / AggrFn 实现；&lt;/li&gt;&lt;li&gt;代码位置：&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//github.com/tikv/tikv/src/tidb_query&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/tikv/tikv/sr&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;c/tidb_query&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 设计与演进 Proposal&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Review 相关项目代码&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;如何协同工作？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 为了协同效率，我们要求 SIG 成员遵守一致的代码风格、提交规范、PR Description 等规定。&lt;/b&gt;具体请参考&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//github.com/tikv/tikv/blob/master/CONTRIBUTING.md&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;文档&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 任务分配方式&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;SIG Tech Lead 在 &lt;a href=&quot;https://link.zhihu.com/?target=http%3A//github.com/tikv/community&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/tikv/communi&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ty&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; 维护公开的成员列表与任务列表链接；&lt;/li&gt;&lt;li&gt;新加入的 SIG 成员可有 2 周时间了解各个任务详情并认领一个任务，或参与一个现有任务的开发或推动。若未能在该时间内认领任务则会被移除 SIG；&lt;/li&gt;&lt;li&gt;SIG 成员需维持每个月参与开发任务，或参与关于现有功能或未来规划的设计与讨论。若连续一个季度不参与开发与讨论，视为不活跃状态，将会被移除 SIG。作为 acknowledgment，仍会处于成员列表的「Former Member」中。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. 定期同步进度，定期周会&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;每 2 周以文档形式同步一次当前各个项目的开发进度；&lt;/li&gt;&lt;li&gt;每 2 周召开一次全组进度会议，时间依据参会人员可用时间另行协商。目前没有项目正在开发的成员可选择性参加以便了解各个项目进度。若参与开发的成员不能参加，需提前请假且提前将自己的月度进度更新至文档；&lt;/li&gt;&lt;li&gt;每次会议由一名成员进行会议记录，在会议结束 24 小时内完成会议记录并公开。会议记录由小组成员轮流执行；&lt;/li&gt;&lt;li&gt;Slack: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//tikv-wg.slack.com/join/shared_invite/enQtNTUyODE4ODU2MzI0LWVlMWMzMDkyNWE5ZjY1ODAzMWUwZGVhNGNhYTc3MzJhYWE0Y2FjYjliYzY1OWJlYTc4OWVjZWM1NDkwN2QxNDE&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://tikv-wg.slack.com&lt;/a&gt; （Channel #copr-sig-china）&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;4. 通过更多线上、线下成员的活动进行交流合作。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Coprocessor SIG 运营制度&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. 考核 &amp;amp; 晋升制度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;a.&lt;/b&gt; Coprocessor SIG Tech Lead 以月为单位对小组成员进行考核，决定成员是否可由 Active Contributor 晋升为 Reviewer：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;熟悉代码库；&lt;/li&gt;&lt;li&gt;获得至少 2 位 TiKV Committer 的提名；&lt;/li&gt;&lt;li&gt;PR 贡献满足以下任意一点：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Merge Coprocessor PR 总数超过 10 个；&lt;/li&gt;&lt;li&gt;Merge Coprocessor PR 总行数超过 1000 行；&lt;/li&gt;&lt;li&gt;已完成一项难度为 Medium 或以上的任务；&lt;/li&gt;&lt;li&gt;提出设计想法并得到采纳成为可执行任务超过 3 个。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;b. &lt;/b&gt;Coprocessor SIG Tech Lead 和 TiKV Maintainer 以季度为单位对小组成员进行考核，决定成员是否可由 Reviewer 晋升为 Committer：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;表现出良好的技术判断力；&lt;/li&gt;&lt;li&gt;在 TiKV / PingCAP 至少两个子项目中是 Reviewer；&lt;/li&gt;&lt;li&gt;获得至少 2 位 TiKV Maintainer 的提名；&lt;/li&gt;&lt;li&gt;至少完成两项难度为 Medium 的任务，或一项难度为 High 的任务；&lt;/li&gt;&lt;li&gt;PR 贡献满足以下至少两点：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;半年内 Merge Coprocessor PR 总行数超过 1500 行；&lt;/li&gt;&lt;li&gt;有效 Review Coprocessor PR 总数超过 10 个；&lt;/li&gt;&lt;li&gt;有效 Review Coprocessor PR 总行数超过 1000 行。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. 退出制度&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;a.&lt;/b&gt; SIG 成员在以下情况中会被移除 SIG，但保留相应的 Active Contributor / Reviewer / Committer 身份：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;作为新成员未在指定时间内认领任务；&lt;/li&gt;&lt;li&gt;连续一个季度处于不活跃状态。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;b.&lt;/b&gt; Reviewer 满足以下条件之一会被取消 Reviewer 身份且收回权限（后续重新考核后可恢复）：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;超过一个季度没有 review 任何 Coprocessor 相关的 PR；&lt;/li&gt;&lt;li&gt;有 2 位以上 Committer 认为 Reviewer 能力不足或活跃度不足。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;3. Tech Lead 额外承担的职责&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;SIG 成员提出的问题需要在 2 个工作日给出回复；&lt;/li&gt;&lt;li&gt;及时 Review 代码；&lt;/li&gt;&lt;li&gt;定时发布任务（如果 SIG 成员退出后，未完成的任务需要重新分配）。&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;i&gt;通过上文相信大家对于 Coprocessor SIG 的工作内容、范围、方式以及运营制度有了初步的了解。如果你是一个开源爱好者，想要参与到一个工业级的开源项目中来，或者想了解社区的运行机制，想了解你的代码是如何从一个想法最终发布到生产环境中运行，那么加入 Coprocessor SIG 就是一个绝佳的机会！&lt;/i&gt;&lt;br/&gt;&lt;i&gt;&lt;b&gt;如果你仍对 SIG 有些疑问或者想要了解更多学习资料，欢迎点击加入 &lt;/b&gt;&lt;/i&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//tikv-wg.slack.com/join/shared_invite/enQtNTUyODE4ODU2MzI0LWVlMWMzMDkyNWE5ZjY1ODAzMWUwZGVhNGNhYTc3MzJhYWE0Y2FjYjliYzY1OWJlYTc4OWVjZWM1NDkwN2QxNDE&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tikv-wg.slack.com&lt;/a&gt;&lt;i&gt;&lt;b&gt;哦～&lt;/b&gt;&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-24-88343561</guid>
<pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 最佳实践系列（四）海量 Region 集群调优</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-24-88236773.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/88236773&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8c752c14802cae0efd8c3f438dde8246_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：张博康&lt;/p&gt;&lt;p&gt;在 TiDB 的架构中，所有的数据按照 range 划分成一个个 Region 分布在多个 TiKV 实例上。随着数据的写入，一个集群中会产生上百万，甚至千万个 Region。而量变引起质变，单 TiKV 实例上过多的 Region 无疑会带来比较大的负担，进而影响整个集群的性能表现。&lt;/p&gt;&lt;p&gt;本文将介绍 TiKV 核心模块 Raftstore 的处理流程以使大家更好得理解海量 Region 导致性能问题的根源，以及针对这种情况的一些优化手段。&lt;/p&gt;&lt;h2&gt;Raftstore 的处理流程&lt;/h2&gt;&lt;p&gt;大家都知道在一个 TiKV 实例上会有多个 Region，这些 Region 消息处理就是通过 Raftstore 这个模块来驱动的，包括 Region 上读写请求的处理，Raft log 的持久化以及复制，还有 Raft 的心跳处理等等。为什么 Region 数多了就会影响到整个集群的性能呢？为了解释这个问题，我们先来看看 TiKV 的核心模块 Raftstore 是怎样工作的。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4cfad7b6d0e555d173b880c59bfa06a7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;547&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-4cfad7b6d0e555d173b880c59bfa06a7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4cfad7b6d0e555d173b880c59bfa06a7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;547&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-4cfad7b6d0e555d173b880c59bfa06a7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4cfad7b6d0e555d173b880c59bfa06a7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1 Raftstore 处理流程示意图&lt;/figcaption&gt;&lt;/figure&gt;&lt;blockquote&gt;注：该示意图仅仅表意，不代表代码层面的实际结构。&lt;/blockquote&gt;&lt;p&gt;上图是 Raftstore 处理流程的示意图，可以看到，从 TiDB 发来的请求会通过 gRPC 和 storage 模块变成最终的 KV 读写消息发往相应的 Region，而这些消息并不会立即处理而是暂存下来。而在 Raftstore 中会轮询检查每个 Region 是否有需要处理的消息。如果有消息，那么 Raftstore 会驱动 Raft 状态机去处理这些消息，并根据这些消息所产生的状态变更去完成一些后续动作。比如，在有写请求时，Raft 状态机需要将日志落盘并且将日志发送给其他副本；在达到心跳间隔时，Raft 状态机需要将心跳信息发送给其他副本。&lt;/p&gt;&lt;h2&gt;性能问题及优化方法&lt;/h2&gt;&lt;p&gt;从上面我们可以看到，这么多 Region 的消息是一个接一个地处理。那么在 Region 很多的情况下，Raftstore 会需要花费一些时间处理大量 Region 的心跳，势必会引入一些延迟，导致一些读写请求得不到特别及时的处理。如果在读写压力大的情况下，很容易使得 Raftstore 线程 CPU 达到瓶颈，而延迟会被进一步放大，进而影响性能表现。&lt;/p&gt;&lt;h3&gt;常见现象&lt;/h3&gt;&lt;p&gt;一般来说，在有负载的情况下，如果 TiKV 的 Raftstore CPU 使用率达到了 85%+（指的是单线程的情况，多线程等比例放大），我们就认为 Raftstore 已经达到了比较繁忙的状态成为了瓶颈（由于 Raftstore 线程中有 IO 操作，所以 CPU 使用率不可能达到 100%），同时 propose wait duration 可能会达到百毫秒级别。&lt;/p&gt;&lt;p&gt;相关 metrics 可在 TiKV grafana 面板下查看：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Thread-CPU 下的 &lt;code&gt;Raft store CPU&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;参考值：最好低于 &lt;code&gt;raftstore.store-pool-size * 85%&lt;/code&gt;（v2.1 版本中无此配置项，可认为 &lt;code&gt;raftstore.store-pool-size = 1&lt;/code&gt;）。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4f3c5145d6658899524033d9c5e50957_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;275&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-4f3c5145d6658899524033d9c5e50957_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4f3c5145d6658899524033d9c5e50957_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;275&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-4f3c5145d6658899524033d9c5e50957_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4f3c5145d6658899524033d9c5e50957_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2 查看 Raft store CPU&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Raft Propose 下的 &lt;code&gt;Propose wait duration&lt;/code&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;Propose wait duration&lt;/code&gt; 是发送请求给 Raftstore、到 Raftstore 真正处理请求之间的延迟。如果该延迟比较长，说明 Raftstore 比较繁忙或者 append log 比较耗时导致 Raftstore 不能及时处理请求。&lt;/p&gt;&lt;p&gt;参考值：最好低于 50-100ms。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-476963b7a70656f6625ecca059995a70_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;275&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-476963b7a70656f6625ecca059995a70_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-476963b7a70656f6625ecca059995a70_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;275&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-476963b7a70656f6625ecca059995a70_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-476963b7a70656f6625ecca059995a70_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 查看 Propose wait duration&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;优化方法&lt;/h3&gt;&lt;p&gt;既然我们已经知道了性能问题的根源，那么就可以从两方面入手：减少单个 TiKV 实例的 Region 数；减少单个 Region 的消息数。根据不同版本，具体可以参考以下优化方法：&lt;/p&gt;&lt;h3&gt;v2.1 版本&lt;/h3&gt;&lt;p&gt;在 v2.1 版本中 Raftstore 只能是单线程，因此一般在 Region 数超过 10 万时就会逐渐成为瓶颈。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 增加 TiKV 实例&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果 IO 资源和 CPU 资源都还有比较多的盈余的话，可以在单个机器上部署多个 TiKV 实例，以减少单个 TiKV 实例上的 Region 个数，或者扩容集群的 TiKV 机器数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 开启 Region Merge&lt;/b&gt;&lt;/p&gt;&lt;p&gt;另外一种可以减少 Region 个数的办法是开启 Region Merge。与 Region Split 相反，Region Merge 是通过调度把相邻的小 Region 合并的过程。在集群有删除数据或者进行过 Drop Table/Truncate Table 后，可以将那些小 Region 甚至空 Region 进行合并以减少资源的消耗。&lt;/p&gt;&lt;p&gt;简单来说，通过 pd-ctl 设置相关参数即可开启 Region Merge&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&amp;gt;&amp;gt; pd-ctl config set max-merge-region-size 20
&amp;gt;&amp;gt; pd-ctl config set max-merge-region-keys 200000
&amp;gt;&amp;gt; pd-ctl config set merge-schedule-limit 8&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;关于更多详情请参考这两个文档 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/master/docs/how-to/configure/region-merge.md&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;如何配置 Region Merge&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/dev/reference/configuration/pd-server/configuration-file/%23schedule&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PD 配置文件描述&lt;/a&gt;，在此不再展开。&lt;/p&gt;&lt;p&gt;同时，默认配置的 Region Merge 默认参数设置相对保守，可以根据需求参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/best-practice-pd/%235-region-merge-%25E9%2580%259F%25E5%25BA%25A6%25E6%2585%25A2&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiDB 最佳实践系列（二）PD 调度策略》&lt;/a&gt; 中提及的具体方法加快 Region Merge 速度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. 调整 raft-base-tick-interval&lt;/b&gt;&lt;/p&gt;&lt;p&gt;除了减小 Region 个数，我们还可以通过尽量减少 Region 单位时间内的消息数量以减小 Raftstore 压力。比如，在 TiKV 配置中适当增大 &lt;code&gt;raft-base-tick-interval&lt;/code&gt;： &lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;[raftstore]
raft-base-tick-interval = “2s”&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;raft-base-tick-interval&lt;/code&gt; 是 Raftstore 驱动每个 Region 的 Raft 状态机的基本时间单位，也就是每隔这么久就需要向 Raft 状态机发送一个 tick 消息。显然增大这个时间间隔，可以有效减少 Raftstore 消息数量。&lt;/p&gt;&lt;p&gt;需要注意的是，这个 tick 也决定了 election timeout 和 heartbeat 的间隔。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;raft-election-timeout = raft-base-tick-interval * raft-election-timeout-ticks
raft-heartbeat-interval = raft-base-tick-interval * raft-heartbeat-ticks&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;follower 在 &lt;code&gt;raft-election-timeout&lt;/code&gt; 间隔内未收到来自 leader 的心跳会认为 leader 出现故障而发起新的选举，而 &lt;code&gt;raft-heartbeat-interval&lt;/code&gt; 是 leader 向 follower 发送心跳的间隔，因此增大 &lt;code&gt;raft-base-tick-interval&lt;/code&gt; 可以减少单位时间内 Raft 发送的网络消息，但也会让 Raft 检测到 leader 故障的时间更长。&lt;/p&gt;&lt;h3&gt;v3.0 版本&lt;/h3&gt;&lt;p&gt;除了以上提及的优化方法外（注：Region Merge 在 v3.0 版本中默认开启），v3.0 版本中还可以进行以下优化：&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 提高 Raftstore 并发数&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 v3.0 版本中 Raftstore 已经扩展为多线程，极大降低了 Raftstore 线程成为瓶颈的可能性。&lt;/p&gt;&lt;p&gt;默认 TiKV 配置 &lt;code&gt;raftstore.store-pool-size&lt;/code&gt; 为 &lt;code&gt;2&lt;/code&gt;，如果在 Raftstore 出现瓶颈的时候可以根据实际情况适当提高，但不建议设置过大以防引入不必要的线程切换开销。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 开启 Hibernate Region&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在实际情况下，读写请求并不会均匀的打在每个 Region 上，而是主要集中在少数的 Region 上，那么对于暂时空闲的 Region 我们是不是可以尽量减少它们的消息数量。这也就是 Hibernate Region 的主要思想，在无必要的时候不进行 &lt;code&gt;raft-base-tick&lt;/code&gt;，也就是不去驱动那些空闲 Region 的 Raft 状态机，那么就不会触发这些 Region 的 Raft 心跳信息的产生，极大得减小了 Raftstore 的工作负担。&lt;/p&gt;&lt;p&gt;截止发稿时 Hibernate Region 还是一个实验 feature，在 master 上已经默认开启。如有需要，可酌情开启，相关配置说明请参考 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/master/docs/reference/configuration/raftstore-config.md%23hibernate-region&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;配置 Hibernate Region&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;其他可能出现的问题&lt;/h2&gt;&lt;h3&gt;PD Leader 切换慢&lt;/h3&gt;&lt;p&gt;PD 需要将 Region Meta 信息持久化在 etcd 以保证 PD Leader 节点切换后能快速继续提供 Region 路由服务。随着 Region 数量的增长，Etcd 的性能问题会使得 PD 在切换 Leader 时从 etcd 获取这些信息时比较慢，在百万 Region 量级时可能达到十几秒甚至几十秒。&lt;/p&gt;&lt;p&gt;因此在 v3.0 版本中 PD 将 Region Meta 信息存在本地的 LevelDB 中，通过另外的机制同步 PD 节点间的信息。&lt;/p&gt;&lt;p&gt;在 v3.0 版本中 PD 已经默认开启配置 &lt;code&gt;use-region-storage&lt;/code&gt;，而 v2.1 版本如碰到类似问题建议升级到 v3.0。&lt;/p&gt;&lt;h3&gt;PD 路由信息更新不及时&lt;/h3&gt;&lt;p&gt;在 TiKV 中是由 pd-worker 这个模块来将 Region Meta 信息定期上报给 PD，在 TiKV 重启或者 Region Leader 切换时需要通过统计信息重新计算 Region 的 &lt;code&gt;approximate size/keys&lt;/code&gt;。那么在 Region 数量比较多的情况下，pd-worker 单线程可能成为瓶颈，造成任务的堆积不能及时处理，因此 PD 不能及时获取某些 Region Meta 信息以致路由信息更新不及时。该问题不会影响实际的读写，但可能导致 PD 调度的不准确以及 TiDB 更新 region cache 时需要多几次 round-trip。&lt;/p&gt;&lt;p&gt;可以在 TiKV grafana 面板中查看 Task 下的 Worker pending tasks 来确定 pd-worker 是否有任务堆积，正常来说 pending tasks 应该维持在一个比较低的值。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b63b732c01d613d5a4f856941328c4d5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-b63b732c01d613d5a4f856941328c4d5_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b63b732c01d613d5a4f856941328c4d5_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-b63b732c01d613d5a4f856941328c4d5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-b63b732c01d613d5a4f856941328c4d5_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 查看 pd-worker&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们在 master 上已经对 pd-worker 进行了效率优化，预计会在 v2.1.19 和 v3.0.5 中带上相关优化，如碰到类似问题建议升级。&lt;/p&gt;&lt;h3&gt;Prometheus 查询慢&lt;/h3&gt;&lt;p&gt;在大规模集群中，TiKV 实例数的增加会让 Prometheus 的查询时的计算压力较大导致 Grafana 查看 metrics 时较慢，在 v3.0 版本中通过设置了一些 metrics 的预计算有所缓解。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;本文介绍了百万级 Region 的集群规模下的常见问题以及相应的处理方法。总体来讲，在 v3.0 版本中我们做了比较多的优化，海量 Region 导致的性能问题上已经有了明显的改善。希望本文在问题根源的解释上能帮助读者更好的理解相关参数调整背后的逻辑，并能举一反三地应用在类似问题的解决上。最后，“量变引起质变”，大家的参与才能让我们的产品更进一步，期待你们的反馈和建议（zhangbokang@pingcap.com）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;阅读原文：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/best-practice-massive-regions-performance-improvement/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 最佳实践系列（四）海量 Region 集群调优 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多最佳实践&lt;/b&gt;：&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23%25E6%259C%2580%25E4%25BD%25B3%25E5%25AE%259E%25E8%25B7%25B5&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/blog-cn/#&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-24-88236773</guid>
<pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>新架构、新角色：TiDB Community Upgrade！</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-23-88015069.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/88015069&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c08ea34b4ac5c0903eaf665dd8b6654a_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：Jian Zhang&lt;/p&gt;&lt;p&gt;经过几年的发展，TiDB 社区已经逐渐成熟，但是随着社区的发展壮大，我们逐渐感受到了现在社区架构上的一些不足。经过一系列的思考和总结，我们决定升级和调整目前社区组织架构，引入更多的社区角色和社区组织，以便更好的激发社区活力，维护积极健康的社区环境。&lt;/p&gt;&lt;h2&gt;老社区架构&lt;/h2&gt;&lt;p&gt;下图是之前官网上的社区架构图：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-872d9336a37bca16b8279cc55368ac69_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1122&quot; data-rawheight=&quot;408&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1122&quot; data-original=&quot;https://pic2.zhimg.com/v2-872d9336a37bca16b8279cc55368ac69_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-872d9336a37bca16b8279cc55368ac69_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1122&quot; data-rawheight=&quot;408&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1122&quot; data-original=&quot;https://pic2.zhimg.com/v2-872d9336a37bca16b8279cc55368ac69_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-872d9336a37bca16b8279cc55368ac69_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1 老社区架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;老社区架构主要面向 TiDB 开发者社区（Developer Group），主要角色有 Maintainer、Committer、Contributor 等，其中：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Committer：由 Maintainer 或 PMC 推荐，是对 TiDB 有突出贡献的 Contributor。需要独立完成至少一个 feature 或修复重大 bug。&lt;/li&gt;&lt;li&gt;Maintainer：项目的规划和设计者，拥有合并主干分支的权限，从 Committer 中产生。他们必须对子项目的健康表现出良好的判断力和责任感。维护者必须直接或通过委派这些职责来设置技术方向并为子项目做出或批准设计决策。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;可以看到老社区架构屏蔽了日益壮大的、对产品打磨升级至关重要的 TiDB 用户群体，并且老架构中对于开发者社区角色的职责、角色之间关系的表述都比较简单，所以我们在新社区架构中做了一些加法，将 TiDB 用户社区纳入进来的同时，对 TiDB 开发者社区的每个角色定义、权责又做了明确的界定，同时也增加了一些新角色、新组织，下面让我们来详细地看一看。&lt;/p&gt;&lt;h2&gt;新社区架构&lt;/h2&gt;&lt;h3&gt;变化 1：将 TiDB 用户社区纳入整体社区架构&lt;/h3&gt;&lt;p&gt;随着 TiDB 产品的成熟，TiDB 用户群体愈发壮大，用户在使用过程中遇到的问题反馈及实践经验，对于 TiDB 产品的完善及应用推广有着不可忽视的重要作用。因此我们此次正式将 TiDB 用户社区（TiDB User Group，简称 TUG）纳入新的社区架构中来，希望用户与开发者有更好的交流互动，一起推动 TiDB 社区的健康发展。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0c6936dd513abe27678fbd4927b18742_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1090&quot; data-rawheight=&quot;440&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1090&quot; data-original=&quot;https://pic3.zhimg.com/v2-0c6936dd513abe27678fbd4927b18742_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0c6936dd513abe27678fbd4927b18742_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1090&quot; data-rawheight=&quot;440&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1090&quot; data-original=&quot;https://pic3.zhimg.com/v2-0c6936dd513abe27678fbd4927b18742_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-0c6936dd513abe27678fbd4927b18742_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2 新社区架构之 User Group&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;TiDB User Group（TUG）是由 TiDB 用户发起的独立、非盈利的第三方组织，用户实行自我管理，旨在加强 TiDB 用户之间的交流和学习。TUG 的形式包括但不限于线上问答和技术文章分享、线下技术沙龙、走进名企、官方互动活动等等。TUG 成员可以通过线上、线下的活动，学习前沿技术知识，发表技术见解，共同建设 TiDB 项目。更多信息可以登陆 TUG 问答论坛 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//asktug.com/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;asktug.com&lt;/a&gt; 查看。&lt;/p&gt;&lt;h3&gt;变化 2：Active Contributor 和 Reviewer&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5a05ddc0ca23390dc5d0fccdcd5be436_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1122&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1122&quot; data-original=&quot;https://pic3.zhimg.com/v2-5a05ddc0ca23390dc5d0fccdcd5be436_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-5a05ddc0ca23390dc5d0fccdcd5be436_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1122&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1122&quot; data-original=&quot;https://pic3.zhimg.com/v2-5a05ddc0ca23390dc5d0fccdcd5be436_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-5a05ddc0ca23390dc5d0fccdcd5be436_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 新社区架构之 Active Contributor、Reviewer&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;上图反映了这次社区架构升级的第 2 个变化：在开发者社区中，新增了 Reviewer 和 Active Contributor 的角色。&lt;/p&gt;&lt;p&gt;Active Contributor 是一年贡献超过 8 个 PR 的 Contributor。Reviewer 从 Active Contributor 中诞生，具有 Review PR 的义务，并且对 TiDB 或者 TiKV 某个子模块的 PR 的点赞（LGTM）有效。关于这些角色，我们将在后文介绍 Special Interest Group 时更详细地介绍。&lt;/p&gt;&lt;h3&gt;变化 3：Special Interest Group&lt;/h3&gt;&lt;p&gt;让我们把开发者社区架构图放大再看看：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7438105aa7bc9c04b2a515410ac1b3d9_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1122&quot; data-rawheight=&quot;883&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1122&quot; data-original=&quot;https://pic2.zhimg.com/v2-7438105aa7bc9c04b2a515410ac1b3d9_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7438105aa7bc9c04b2a515410ac1b3d9_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1122&quot; data-rawheight=&quot;883&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1122&quot; data-original=&quot;https://pic2.zhimg.com/v2-7438105aa7bc9c04b2a515410ac1b3d9_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-7438105aa7bc9c04b2a515410ac1b3d9_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 新社区架构之 Special Interest Group&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;上图展示了以垂直的视角来细看开发者社区的整体架构，反映了这次社区架构升级的第 3 个变化：引入了 “专项兴趣小组”（Special Interest Group，简称 SIG）。&lt;/p&gt;&lt;p&gt;专项兴趣小组主要负责 TiDB/TiKV 某个模块的开发和维护工作，对该模块代码的质量负责。我们将邀请满足条件的 Active Contributor 加入专项兴趣小组，开发者们将在专项兴趣小组中获得来自 Tech Lead 们的持续指导，一边锻炼技术能力，一边优化和完善该模块。社区开发者们可通过专项兴趣小组逐渐从初始的 Active Contributor 成长为受到社区认可的 Reviewer、Committer 和 Maintainer。一般而言每个专项兴趣小组都会周期性的组织会议，讨论最近进展和遇到的问题，所有的会议讨论都公开在社区上，方便感兴趣的同学一起参与和讨论。&lt;/p&gt;&lt;p&gt;具体可参考目前我们正在运营的表达式专项兴趣小组：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/community/tree/master/special-interest-groups/sig-expr&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Expression Special Interest Group&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;另外这张图也反映了社区角色和专项兴趣小组的关系，我们来仔细看看 SIG 中的社区角色：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Active Contributor&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;即一年贡献超过 8 个 PR 的 Contributor。&lt;/li&gt;&lt;li&gt;如果要加入某个 SIG，某个 Contributor 需要在 1 年内为该 SIG 所负责的模块贡献超过 8 个以上的 PR，这样即可获得邀请，加入该 SIG 进行针对性的学习和贡献。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2. Reviewer&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;隶属于某个 SIG，具有 Review PR 的义务。&lt;/li&gt;&lt;li&gt;Reviewer 从 Active Contributor 中诞生，当 Active Contributor 对该模块拥有比较深度的贡献，并且得到 2 个或 2 个以上 Committer 的提名时，将被邀请成为该模块的 Reviewer。&lt;/li&gt;&lt;li&gt;Reviewer 对该模块代码的点赞（LGTM）有效（注：TiDB 要求每个 PR 至少拥有 2 个 LGTM 后才能够合并到开发分支）。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;3. Tech Lead&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;即 SIG 的组织者，负责 SIG 的日常运营，包括组织会议，解答疑问等。&lt;/li&gt;&lt;li&gt;Tech Lead 需要为 SIG 的管理和成长负责，责任重大。目前暂时由 PingCAP 内部同事担任，将来可由社区开发者一起担任，和 PingCAP 同事一起为 SIG 的进步而努力。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;再来看看另外两个角色：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Committer&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;资深的社区开发者，从 Reviewer 中诞生。&lt;/li&gt;&lt;li&gt;当 Reviewer 对该模块拥有非常深度的贡献，或者在保持当前模块 Reviewer 角色的同时，也在别的模块深度贡献成为了 Reviewer，这时他就在深度或者广度上具备了成为 Committer 的条件，只要再得到 2 个或 2 个以上 Maintainer 的提名时，即可成为 Committer。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;2. Maintainer&lt;/p&gt;&lt;ul&gt;&lt;ul&gt;&lt;li&gt;重度参与 TiDB 社区的开发者，从 Committer 中诞生，对代码 repo 拥有写权限。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;blockquote&gt;以上社区角色的详细的定义和权责内容可以在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/community-cn/developer-group/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;这里&lt;/a&gt; 查看。&lt;/blockquote&gt;&lt;h3&gt;变化 4：Working Group&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ff195685d0159a0cb75bbe7cb3d3e606_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1122&quot; data-rawheight=&quot;660&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1122&quot; data-original=&quot;https://pic3.zhimg.com/v2-ff195685d0159a0cb75bbe7cb3d3e606_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ff195685d0159a0cb75bbe7cb3d3e606_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1122&quot; data-rawheight=&quot;660&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1122&quot; data-original=&quot;https://pic3.zhimg.com/v2-ff195685d0159a0cb75bbe7cb3d3e606_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-ff195685d0159a0cb75bbe7cb3d3e606_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5 新社区架构之 Working Group&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;第 4 个变化是开发者社区架构中引入了 “工作小组”（Working Group，简称 WG）。工作小组是由为了完成某个特定目标而聚集在一起的社区开发者与 PingCAP 同事一起成立。为了完成目标，有些工作小组可能跨越多个 SIG，有些小组可能只会专注在某个具体的 SIG 中做某个具体的事情。&lt;/p&gt;&lt;p&gt;工作小组具有生命周期，一旦目标完成，工作小组即可解散。工作小组运营和管理的唯一目标是确保该小组成立时设置的目标在适当的时间内完成。一般而言，工作小组也会有周期性的会议，用于总结目前项目进展，确定下一步实施方案等。&lt;/p&gt;&lt;p&gt;可参考目前我们正在运营的表达式工作小组：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/community/blob/master/working-groups/wg-vec-expr.md&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Vectorized Expression Working Group&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;总结和未来的工作&lt;/h2&gt;&lt;p&gt;总的来说，这次社区架构升级主要有如下改进：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;引入了 TiDB 用户社区（TiDB User Group）。&lt;/li&gt;&lt;li&gt;引入了 Active Contributor、Reviewer 的社区角色。&lt;/li&gt;&lt;li&gt;引入了 Special Interest Group（SIG）。&lt;/li&gt;&lt;li&gt;引入了 Working Group（WG）。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在社区运营方面，我们未来还将继续：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;完善社区成员晋级的指导机制，让社区同学从 Contributor 成长到 Committer 或 Maintainer 有路可循。&lt;/li&gt;&lt;li&gt;让社区上的事情更加成体系，做事不乱。&lt;/li&gt;&lt;li&gt;让社区同学更有归属感，加强和其他社区成员的沟通。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;在未来，我们将陆续开放更多的专项兴趣小组和工作小组。在专项兴趣小组中，还将持续发放更多数据库相关的资料，帮助成员在专项兴趣小组中逐渐深度参与 TiDB 的开发工作。希望大家都能够多多参与进来，一起将 TiDB 打造成开源分布式关系型数据库的事实标准！&lt;/p&gt;&lt;p&gt;&lt;b&gt;阅读原文：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-community-upgrade/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic4.zhimg.com/v2-6748fc5f8128495a8afbfb40de50dc27_180x120.jpg&quot; data-image-width=&quot;1500&quot; data-image-height=&quot;1000&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;新架构、新角色：TiDB Community Upgrade！ | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-23-88015069</guid>
<pubDate>Wed, 23 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>网易互娱的数据库选型和 TiDB 应用实践</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-22-87945199.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/87945199&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-c7c2edf92e205ae540dc9e1f3bce5a97_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;作者介绍：李文杰，网易互娱计费组，高级数据库管理工程师，TiDB User Group Ambassador。&lt;/blockquote&gt;&lt;h2&gt;一、业务架构简介&lt;/h2&gt;&lt;p&gt;计费组是为网易互娱产品提供统一登录和支付高效解决方案的公共支持部门，对内是互娱的各个游戏工作室，对外是国内外数百个渠道。由于业务场景的特殊性，我们为各个游戏产品部署了不同的应用服务，其中大产品环境独立，小产品集中部署。&lt;/p&gt;&lt;p&gt;随着部门业务量的激增，单机 MySQL 在容量、性能、扩展性等方面都遇到了瓶颈，我们开始对其他数据库产品进行调研选型。本文将详细介绍网易互娱计费组针对自己场景的数据库选型对比方案，以及使用 TiDB 后解决的问题，并分享了使用 TiDB 过程中集群管理、监控和数据迁移等方面的最佳实践，以供大家参考和交流。&lt;/p&gt;&lt;h3&gt;1.1 MySQL 使用架构&lt;/h3&gt;&lt;p&gt;网易互娱计费组线上 MySQL 的基本使用架构，如下图所示，其中箭头方向表示数据或请求的指向：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-e701bf359f428776beadb051bd789a73_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1 网易互娱计费组线上 MySQL 使用架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;线上应用 Application 通过 Keepalive + 多机部署，流量经过负载均衡，可以有效保障应用服务的高可用；&lt;/li&gt;&lt;li&gt;数据库层架构是 Keepalive + 主从结构，利用半同步复制特性可以有效解决延迟和数据一致性的问题；&lt;/li&gt;&lt;li&gt;Application 通过 VIP 访问后端数据库，在数据库主节点宕机后通过 VIP 飘移到从节点，保证服务正常对外提供；&lt;/li&gt;&lt;li&gt;通过 Slave 节点进行数据备份和线上数据采集，经过全量和增量同步方式导出数据到数据中心，然后进行在线和离线计算任务；&lt;/li&gt;&lt;li&gt;类似这样的架构组合线上大概有 50+ 套，涉及服务器 200~400 台，日均新增数据 TB 级。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;1.2 MySQL 使用的现状与问题&lt;/h3&gt;&lt;p&gt;随着业务的发展，部门内各应用服务产生的数据量也在快速增长。业务落地数据量不断激增，导致单机 MySQL 不可避免地会出现性能瓶颈。主要体现在以下几个方面：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;容量&lt;/li&gt;&lt;ul&gt;&lt;li&gt;单机 MySQL 实例存储空间有限，想要维持现有架构就得删除和轮转旧数据，达到释放空间的目的；&lt;/li&gt;&lt;li&gt;网易互娱某些场景单表容量达到 700GB 以上，订单数据需永久保存，同时也需要保持在线实时查询，按照之前的存储设计会出现明显的瓶颈。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;性能&lt;/li&gt;&lt;ul&gt;&lt;li&gt;最大单表 15 亿行，行数过大，导致读写性能受到影响。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;扩展性&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MySQL 无法在线灵活扩展，无法解决存储瓶颈。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;SQL 复杂&lt;/li&gt;&lt;ul&gt;&lt;li&gt;大表轮转后出现多个分表，联合查询时需要 join 多个分表，SQL 非常复杂并难以维护；&lt;/li&gt;&lt;li&gt;单机 MySQL 缺乏大规模数据分析的能力。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;数据壁垒&lt;/li&gt;&lt;ul&gt;&lt;li&gt;不同产品的数据库独立部署；&lt;/li&gt;&lt;li&gt;数据不互通，导致数据相关隔离，形成数据壁垒；&lt;/li&gt;&lt;li&gt;当进行跨产品计算时，需要维护多个异构数据源，访问方式复杂。数据分散在不同的数据孤岛上会增加数据分析难度，不利于共性价值的挖掘。如下图：&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-19112c8c75fc02890d7d5fb7c83d082f_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2 现状之数据孤岛&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;二、数据库选型&lt;/h2&gt;&lt;h3&gt;2.1 调研目标&lt;/h3&gt;&lt;p&gt;针对目前存储架构存在的问题，有需要使用其他存储方案的可能。考虑到目前的业务与 MySQL 高度耦合，对数据库选型的主要要求有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;必须兼容 MySQL 协议；&lt;/li&gt;&lt;li&gt;支持事务，保证任务以事务为维度来执行或遇错回滚；&lt;/li&gt;&lt;li&gt;支持索引，尤其是二级索引；&lt;/li&gt;&lt;li&gt;扩展性，支持灵活在线扩展能力，包括性能扩展和容量扩展。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其他要求：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;稳定性和可靠性；&lt;/li&gt;&lt;li&gt;备份和恢复；&lt;/li&gt;&lt;li&gt;容灾等。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;2.2 可选方案&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1072&quot; data-rawheight=&quot;458&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1072&quot; data-original=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1072&quot; data-rawheight=&quot;458&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1072&quot; data-original=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4e2c5d05280b41a4559bf6e8e882e5d3_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;2.3 测试&lt;/h3&gt;&lt;h3&gt;2.3.1 基于 MySQL 的解决方案&lt;/h3&gt;&lt;p&gt;一开始仍然是倾向使用基于 MySQL 的解决方案，比如 MySQL InnoDB Cluster 或 MySQL + 中间件的方案。&lt;/p&gt;&lt;p&gt;我们测试了 MySQL 集群 5.7.25 版本对比 8.0.12 版本，在 128 并发写各 1000 万行的 10 个表，比较单节点、3 节点和 5 节点下的情况，如下图所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;462&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;462&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-e043a4e85967a6d57373fab5b129d034_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3 对比结果&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在测试中发现，使用 MySQL InnoDB 集群的方案写性能比单机 MySQL 差约 30%，其他的读写测试结果也不甚满意。之后陆续测试 MySQL InnoDB Cluster 或 MySQL + 中间件的方案，不是测试结果性能不达要求，就是需要修改大量代码。&lt;/p&gt;&lt;p&gt;因此我们得出了基于 MySQL InnoDB Cluster 或 MySQL + 中间件的方案的不满足我们的业务场景的结论。总结来说，我们不使用 MySQL 分库分表、中间件或 MySQL 集群，原因主要是以下两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;方案过于复杂&lt;/li&gt;&lt;li&gt;需要改业务代码&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;仔细分析来看，其实基于 MySQL InnoDB Cluster 或 MySQL + 中间件的方案，本质上是 MySQL 主从结构的延伸，并非真正的分布式拓展，像是以打“补丁”的方式来实现横向扩展，很多功能特性自然也难以让人满意。&lt;/b&gt;&lt;/p&gt;&lt;h3&gt;2.3.2 CockroachDB VS TiDB&lt;/h3&gt;&lt;p&gt;在开源的分布式 NewSQL 领域，知名的有 TiDB 和 CockroachDB（简称 CRDB），二者都是基于 Google Spanner 论文的开源实现。我们对这两种数据库的功能和性能做了大量的调研和测试。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 天然兼容 MySQL 协议，而 CRDB 兼容 PostgreSQL ；&lt;/li&gt;&lt;li&gt;如果业务以 MySQL 为主，那 TiDB 可能是比较好的选择；如果是 PostgreSQL，那CRDB 可能是优先的选择。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;测试方面，我们也进行了全面地对比和测试。这里说其中一个测试案例：10 台机器 5 存储节点，160 并发访问单表 2 亿行，我们于 2018 年 7 月，对 CRDB-v2.1.0 版本和 TiDB-v2.0.5 版本进行了读写测试（CRDB 和 TiDB 集群均使用默认配置，未进行调优）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;集群拓扑&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;464&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;464&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-b87fcc3484e0ed11b7415194bacca766_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4 CockroachDB 测试集群搭建&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;531&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;531&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-2df8880a5aadd7b661237be784bd12c0_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5 TiDB 测试集群搭建&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;测试语句&lt;/b&gt;&lt;/p&gt;&lt;p&gt;范围查询：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT c FROM sbtest%u WHERE id BETWEEN ? AND ?
SELECT SUM(k) FROM sbtest%u WHERE id BETWEEN ? AND ?
SELECT c FROM sbtest WHERE id BETWEEN ? AND ? ORDER BY c
SELECT DISTINCT c FROM sbtest%u WHERE id BETWEEN ? AND ? ORDER BY c&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;随机 IN 查询：&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT id, k, c, pad FROM sbtest1 WHERE k IN (?)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;随机范围查询：&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT count(k) FROM sbtest1 WHERE k BETWEEN ? AND ? OR k BETWEEN ? AND ?&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;更新索引列：&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;UPDATE sbtest%u SET k=k+1 WHERE id=?&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;更新非索引列：&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;UPDATE sbtest%u SET c=? WHERE id=?&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;读写混合：范围查询 + 更删改混合&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其中一个重要的测试结果如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;466&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;466&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-bcc6cccccdc3e119d41a5e053187f2b7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6 测试结果&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;结论：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;CRDB 和 TiDB 在性能表现上不相上下；&lt;br/&gt;注：上面是 2018 年 7 月的基于 TiDB 2.0.5 版本的测试结果，现在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/releases/3.0-ga/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 已发布 3.0 GA 版本，在性能上有了质的提升&lt;/a&gt;，我们在近期进行了补充测试，大多数场景下 3.0 版本较 2.1 版本有数倍的性能提升，最新的测试结果图如下：&lt;br/&gt;&lt;/li&gt;&lt;/ol&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-126246b456e76b94f40b965841eebc36_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7 TiDB 2.1.15 vs 3.0.3：OLTP 峰值比较&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;306&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;306&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-d6716918375f8a4f1142a803e01dbc0c_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8 TiDB 2.1.15 vs 3.0.3：TPC-C&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;2. CRDB 兼容 PostgreSQL，如果需要迁移则需要转协议，需 MySQL → PostgreSQL  → CRDB。迁移过程复杂，成本高；&lt;/p&gt;&lt;p&gt;3. TiDB 兼容 MySQL，代码修改量不多，迁移成本低。&lt;/p&gt;&lt;h3&gt;2.3.3 最终选型&lt;/h3&gt;&lt;p&gt;综合对比结果如下表：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;970&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;970&quot; data-original=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;970&quot; data-rawheight=&quot;452&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;970&quot; data-original=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-7f97b461d169b0fd1ca972e1f527e5b7_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;经过谨慎的考量，我们选择了 TiDB。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;497&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;497&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-feae600e94813ac3a015f6b923bbeaa1_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9 选择 TiDB 的重要理由&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;三、TiDB 在网易互娱计费组的使用&lt;/h2&gt;&lt;h3&gt;3.1 TiDB 使用架构&lt;/h3&gt;&lt;p&gt;网易互娱使用 TiDB 的架构设计如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;479&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;479&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8f6d6f5043b3a90ea15ca410117e07c7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 10 基于 TiDB 的架构设计&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;整个集群分为 TiDB、TiKV 和 PD 3 个模块分层部署；&lt;/li&gt;&lt;li&gt;使用 Nginx 作为前端负载均衡。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;3.2 TiDB 解决了哪些需求&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1282&quot; data-rawheight=&quot;456&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1282&quot; data-original=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1282&quot; data-rawheight=&quot;456&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1282&quot; data-original=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-4bb5735becad11940999f16df0597ad0_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;3.3 TiDB 使用现状&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;业务&lt;/li&gt;&lt;ul&gt;&lt;li&gt;TiDB 作为线上 MySQL 数据镜像，负责线上数据的收集和集中管理，形成数据湖泊；&lt;/li&gt;&lt;li&gt;应用于数据平台服务，包括报表、监控、运营、用户画像、大数据计算等场景；&lt;/li&gt;&lt;li&gt;HTAP：OLTP + OLAP。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;集群&lt;/li&gt;&lt;ul&gt;&lt;li&gt;测试集群：v2.1.15，用于功能测试、特性尝鲜；&lt;/li&gt;&lt;li&gt;线上集群：v2.1.15，80% 离线大数据计算任务 + 20% 线上业务。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;规模&lt;/li&gt;&lt;ul&gt;&lt;li&gt;41 台服务器，88 个实例节点，38 个 Syncer 实时同步流（将升级为 DM）；&lt;/li&gt;&lt;li&gt;存储：20TB/总 50TB，230 万个 Region；&lt;/li&gt;&lt;li&gt;QPS 均值 4k/s，高峰期万级 QPS，读写比约 1:5；&lt;/li&gt;&lt;li&gt;延迟时间：80% 在 8ms 以内，95% 在 125ms 以下，99.9% 在 500ms 以下。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;四、最佳实践分享&lt;/h2&gt;&lt;h3&gt;4.1 集群管理&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;Ansible（推荐）&lt;/li&gt;&lt;ul&gt;&lt;li&gt;一键部署；&lt;/li&gt;&lt;li&gt;弹性伸缩，可在线灵活扩缩容；&lt;/li&gt;&lt;li&gt;升级，单节点轮转平滑升级；&lt;/li&gt;&lt;li&gt;集群启停和下线；&lt;/li&gt;&lt;li&gt;Prometheus 监控。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Docker&lt;/li&gt;&lt;li&gt;K8s&lt;/li&gt;&lt;ul&gt;&lt;li&gt;使用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-operator&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Operator&lt;/a&gt; 可以在私有云和公有云上一键管理。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h3&gt;4.2 运维实践&lt;/h3&gt;&lt;h3&gt;4.2.1 Prometheus 监控&lt;/h3&gt;&lt;p&gt;官方集成了 Prometheus + Grafana 的实时监控平台，从集群的各个方面进行了完善的监控，包括：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;服务器基础资源的监控：内存、CPU、存储空间、IO 等；&lt;/li&gt;&lt;li&gt;集群组件的监控：TiDB、PD、TiKV 等；&lt;/li&gt;&lt;li&gt;数据监控：实时同步流、上下游数据一致性检验等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;PD 监控示意图如下，集群管理员可以很方便地掌握集群的最新状态，包括集群的空间 Region 等所有情况。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8a54da195fdd0a5e3c085dda6c351793_b.jpg&quot;/&gt;&lt;figcaption&gt;图 11 最佳运维实践：Prometheus 实时监控&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如果集群运行过程出错，在监控面板上很容易就发现，下图是使用过程中的一个案例：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;191&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;191&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-21ad70df0265e812b94011451e9cc4fe_b.jpg&quot;/&gt;&lt;figcaption&gt;图 12 最佳运维实践案例&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;应用访问 TiDB 写入数据时发现特别慢，读请求正常。排查后，根据 TiKV 面板发现 Raft Store CPU 这项指标异常。深入了解原因是因为数据库副本复制是单线程操作，目前已经到了集群的瓶颈。解决办法有以下两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Region 数量过多，Raft Store 还要处理 heartbeat message。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;解决方法：删除过期数据。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Raft Store 单线程处理速度跟不上集群写入速度。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;解决方法：从 2.1.5 升级到 2.1.15，开启自动 Region Merge 功能。&lt;/p&gt;&lt;h3&gt;4.2.2 部分运维问题及解决方案&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1422&quot; data-rawheight=&quot;1314&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1422&quot; data-original=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1422&quot; data-rawheight=&quot;1314&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1422&quot; data-original=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-645d348684315ec58be5a3569c76122e_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h3&gt;4.3 全网数据库遍历&lt;/h3&gt;&lt;p&gt;以前部分业务遍历全网数据库获取所需数据，需要维护多个源，而且是异构源，非常复杂和繁琐。使用 TiDB 很好地解决了这个问题，只需要访问一个源就可以获取到所有想要的数据。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;412&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;412&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-64e0713ff4ee564dec1b643c539dbfc2_b.jpg&quot;/&gt;&lt;figcaption&gt;图 13 全网数据库遍历&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;4.4 数据迁移&lt;/h3&gt;&lt;h3&gt;4.4.1 MySQL 到 TiDB&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-84ca591d0ad68ce644f3a2816018b44c_b.jpg&quot;/&gt;&lt;figcaption&gt;图 14 数据从 MySQL 迁移到 TiDB&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;MySQL 数据库迁移到 TiDB 分为两个部分：全量和增量。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;全量&lt;/li&gt;&lt;ul&gt;&lt;li&gt;使用工具 （Mydumper 或 MySQL Dump 等）从 MySQL 导出数据，并且记录当前数据的 binlog 位置；&lt;/li&gt;&lt;li&gt;使用工具（Loader 或 Lightning 等）将数据导入到 TiDB 集群；&lt;/li&gt;&lt;li&gt;可以用作数据的备份和恢复操作。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;增量&lt;/li&gt;&lt;ul&gt;&lt;li&gt;TiDB 伪装成为上游 MySQL 的一个 Slave，通过工具（Syncer 或 DM）实时同步 binlog 到 TiDB 集群；&lt;/li&gt;&lt;li&gt;通常情况上游一旦有数据更新，下游就会实时同步过来。同步速度受网络和数据量大小的影响。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h3&gt;4.4.2 数据迁出 TiDB&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-de4230f3e89b1b8fd0b59cb618fd1329_b.jpg&quot;/&gt;&lt;figcaption&gt;图 15 数据迁出 TiDB&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如果数据需要反向导入或同步，可以利用 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog&lt;/a&gt; 工具将 TiDB 集群的 binlog 同步到 MySQL。TiDB Binlog 支持以下功能场景：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;数据同步&lt;/b&gt;：同步 TiDB 集群数据到其他数据库；&lt;/li&gt;&lt;li&gt;&lt;b&gt;实时备份和恢复&lt;/b&gt;：备份 TiDB 集群数据，同时可以用于 TiDB 集群故障时恢复。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;导入的方式：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;全量：TiDB 兼容 MySQL 协议，在 MySQL 容量足够大的情况下，也可用工具将数据从 TiDB 导出后再导入 MySQL。&lt;/li&gt;&lt;li&gt;增量：打开 TiDB 的 binlog 开关，部署 binlog 收集组件（Pump+Drainer），可以将 binlog 数据同步到下游存储架构（MySQL、TiDB、Kafka、S3 等）。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;4.5 优雅地「去分库分表」&lt;/h3&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-27d611443fcb9818371be07e692cfb61_b.jpg&quot;/&gt;&lt;figcaption&gt;图 16 去分库分表举例&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;举例：一个超级大表按天分表，现在打算查询某个账号一年间的信息。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;上游 MySQL&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT xx FROM HFeeall join HFee20190101 join ... join ...join ... join HFee20190917 WHERE xx;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;需要连接 N 个 join 条件，查询需要等待较长时间。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;下游 TiDB&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;SELECT xx  FROM SuperHfeeall WHERE xx ;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;应用此方案，最大单表 700+GB，13+ 亿行，索引查询秒返回。&lt;/p&gt;&lt;h3&gt;4.6  业务迁移&lt;/h3&gt;&lt;p&gt;&lt;b&gt;目标&lt;/b&gt;：利用 TiDB 的水平扩展特性，解决容量瓶颈和系统吞吐量瓶颈。&lt;/p&gt;&lt;p&gt;&lt;b&gt;迁移原则&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据完整和准确：数据很重要，保证数据不错、不丢；&lt;/li&gt;&lt;li&gt;迁移平滑和迅速：服务敏感度高，停服时间要短；&lt;/li&gt;&lt;li&gt;可回滚：遇到问题可随时切回到 MySQL。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;1）数据同步&lt;/b&gt;&lt;/p&gt;&lt;p&gt;使用 DM 或者 Syncer 将上游 MySQL 的数据同步到 TiDB 集群。同步流搭建后注意需要检查上下游数据一致性。&lt;/p&gt;&lt;p&gt;观察一段时间，同步无误后，可以根据业务需要迁移部分读流量到 TiDB 集群。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;899&quot; data-rawheight=&quot;831&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;899&quot; data-original=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;899&quot; data-rawheight=&quot;831&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;899&quot; data-original=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-55e50472d089594c5b5be928e744d5ae_b.jpg&quot;/&gt;&lt;figcaption&gt;图 17 业务迁移之数据同步&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;2）读写验证&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这一阶段是验证应用访问 MySQL 和访问 TiDB 可以得到相同的结果，验证业务访问的准确性问题。&lt;/p&gt;&lt;p&gt;停止数据同步，使用流量复制工具将线上流量完全拷贝出来，同时读写 MySQL 和 TiDB。将两边的访问结果进行对比，核查 TiDB 是否可靠和可信。根据需要，这个阶段可以测试较长时间。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;772&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;772&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-2e337efcdd9782d068cde17c8728c3fd_b.jpg&quot;/&gt;&lt;figcaption&gt;图 18 业务迁移之读写验证&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;3）灰度切换&lt;/b&gt;&lt;/p&gt;&lt;p&gt;将步骤 2 的双写停止，即关双写，同时拉起上游的 DM 同步。&lt;/p&gt;&lt;p&gt;把访问部分非核心业务的库表写操作迁移到 TiDB，打开 TiDB 的 Binlog 开关对线上 MySQL 进行反向同步。这个操作，保证只写 MySQL 的数据同步到 TiDB ，只写 TiDB 的数据也可以反向同步到 MySQL，保证出了问题，随时可以回滚。当业务长时间访问正常，可以增加切换流量，进行灰度切换。建议观察一段时间，至少一个月。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;854&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;854&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-b3ab62e04da1be6385b531bb98cb039d_b.jpg&quot;/&gt;&lt;figcaption&gt;图 19 业务迁移之灰度切换&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;4）迁移完成&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当流量完全迁移完成，保持 TiDB 反同步到 MySQL 过程，继续观察一段时间，确认无误后，断开反向同步，100% 迁移完成。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;877&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;877&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-8194bf59c99fbcf65acd3617bbf620b7_b.jpg&quot;/&gt;&lt;figcaption&gt;图 20 完成迁移&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;五、总结与展望&lt;/h2&gt;&lt;p&gt;TiDB 兼容 MySQL 协议，支持 TP/AP 事务且扩展性好，能很好地解决网易互娱计费组业务大容量、高可用等问题。目前我们的业务在不断深入和扩大规模使用 TiDB，因为看好它，所以这里提出一些使用中的问题以帮助原厂持续打磨产品：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;集群数据备份：希望提供集群更高效地备份和恢复 SST 文件的方式；&lt;/li&gt;&lt;li&gt;事务限制：希望可以放宽大事务的限制，现在仍需要人工切分大事务，比较复杂；&lt;/li&gt;&lt;li&gt;同步：希望 DM 支持上下游表结构不一致的同步；&lt;/li&gt;&lt;li&gt;数据热点问题：建议加强自动检测和清除热点功能；&lt;/li&gt;&lt;li&gt;客户端重试：目前客户端代码需要封装重试逻辑，对用户不友好，希望可以改进。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后，根据网易互娱计费组已有的使用情况，我们计划继续加大、加深 TiDB 的使用场景，丰富业务类型和使用规模，期待 TiDB 给我们的业务带来更多便利。&lt;/p&gt;&lt;p&gt;&lt;b&gt;阅读原文：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/user-case-wangyihuyu/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;网易互娱的数据库选型和 TiDB 应用实践 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多精彩案例：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;案例 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-22-87945199</guid>
<pubDate>Tue, 22 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 最佳实践系列（三）乐观锁事务</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-20-87608202.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/87608202&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9c88b5ce36b4a8fb1354e4e2c1c4a086_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：Shirly&lt;/p&gt;&lt;blockquote&gt;TiDB 最佳实践系列是面向广大 TiDB 用户的系列教程，旨在深入浅出介绍 TiDB 的架构与原理，帮助用户在生产环境中最大限度发挥 TiDB 的优势。我们将分享一系列典型场景下的最佳实践路径，便于大家快速上手，迅速定位并解决问题。&lt;/blockquote&gt;&lt;p&gt;在前两篇的文章中，我们分别介绍了 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-in-high-concurrency-scenarios/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 高并发写入常见热点问题及规避方法&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/best-practice-pd/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PD 调度策略最佳实践&lt;/a&gt;，本文我们将深入浅出介绍 TiDB 乐观事务原理，并给出多种场景下的最佳实践，希望大家能够从中收益。同时，也欢迎大家给我们提供相关的优化建议，参与到我们的优化工作中来。&lt;/p&gt;&lt;p&gt;建议大家在阅读之前先了解 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/architecture/%23tidb-%25E6%2595%25B4%25E4%25BD%2593%25E6%259E%25B6%25E6%259E%2584&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 的整体架构&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Percollator&lt;/a&gt; 事务模型。另外，本文重点关注原理及最佳实践路径，具体的 TiDB 事务语句大家可以在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/reference/transactions/overview/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;官方文档&lt;/a&gt; 中查阅。&lt;/p&gt;&lt;h2&gt;TiDB 事务定义&lt;/h2&gt;&lt;p&gt;TiDB 使用 Percolator 事务模型，实现了分布式事务（建议未读过该论文的同学先浏览一下 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;论文&lt;/a&gt; 中事务部分内容）。&lt;/p&gt;&lt;p&gt;说到事务，不得不先抛出事务的基本概念。通常我们用 ACID 来定义事务（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/ACID&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;ACID 概念定义&lt;/a&gt;）。下面我们简单说一下 TiDB 是怎么实现 ACID 的：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A（原子性）：基于单实例的原子性来实现分布式事务的原子性，和 Percolator 论文一样，TiDB 通过使用 Primary Key 所在 region 的原子性来保证。&lt;/li&gt;&lt;li&gt;C（一致性）：本身 TiDB 在写入数据之前，会对数据的一致性进行校验，校验通过才会写入内存并返回成功。&lt;/li&gt;&lt;li&gt;I（隔离性）：隔离性主要用于处理并发场景，TiDB 目前只支持一种隔离级别 Repeatable Read，即在事务内可重复读。&lt;/li&gt;&lt;li&gt;D（持久性）：事务一旦提交成功，数据全部持久化到 TiKV， 此时即使 TiDB 服务器宕机也不会出现数据丢失。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;截止本文发稿时，TiDB 一共提供了两种事务模式：乐观事务和悲观事务。那么乐观事务和悲观事务有什么区别呢？最本质的区别就是什么时候检测冲突：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;悲观事务：顾名思义，比较悲观，对于每一条 SQL 都会检测冲突。&lt;/li&gt;&lt;li&gt;乐观事务：只有在事务最终提交 commit 时才会检测冲突。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下面我们将着重介绍乐观事务在 TiDB 中的实现。另外，想要了解 TiDB 悲观事务更多细节的同学，可以先阅读本文，思考一下在 TiDB 中如何实现悲观事务，我们后续也会提供《悲观锁事务最佳实践》给大家参考。&lt;/p&gt;&lt;h2&gt;乐观事务原理&lt;/h2&gt;&lt;p&gt;有了 Percolator 基础后，下面我们来介绍 TiDB 乐观锁事务处理流程。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a0be1fb55caf7afe189c1d0f8e2b1c35_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1199&quot; data-rawheight=&quot;1228&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1199&quot; data-original=&quot;https://pic2.zhimg.com/v2-a0be1fb55caf7afe189c1d0f8e2b1c35_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a0be1fb55caf7afe189c1d0f8e2b1c35_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1199&quot; data-rawheight=&quot;1228&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1199&quot; data-original=&quot;https://pic2.zhimg.com/v2-a0be1fb55caf7afe189c1d0f8e2b1c35_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a0be1fb55caf7afe189c1d0f8e2b1c35_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;TiDB 在处理一个事务时，处理流程如下：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;客户端 begin 了一个事务。&lt;br/&gt;a. TiDB 从 PD 获取一个全局唯一递增的版本号作为当前事务的开始版本号，这里我们定义为该事务的 &lt;code&gt;start_ts&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;客户端发起读请求。&lt;br/&gt;a. TiDB 从 PD  获取数据路由信息，数据具体存在哪个 TiKV 上。&lt;br/&gt;b. TiDB 向 TiKV 获取 &lt;code&gt;start_ts&lt;/code&gt; 版本下对应的数据信息。&lt;/li&gt;&lt;li&gt;客户端发起写请求。&lt;br/&gt;a. TiDB 对写入数据进行校验，如数据类型是否正确、是否符合唯一索引约束等，确保新写入数据事务符合一致性约束，&lt;b&gt;将检查通过的数据存放在内存里&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;客户端发起 commit。&lt;/li&gt;&lt;li&gt;TiDB 开始两阶段提交将事务原子地提交，数据真正落盘。&lt;br/&gt;a. TiDB 从当前要写入的数据中选择一个 Key 作为当前事务的 Primary Key。&lt;br/&gt;b. TiDB 从 PD 获取所有数据的写入路由信息，并将所有的 Key 按照所有的路由进行分类。&lt;br/&gt;c. TiDB 并发向所有涉及的 TiKV 发起 prewrite 请求，TiKV 收到 prewrite 数据后，检查数据版本信息是否存在冲突、过期，符合条件给数据加锁。&lt;br/&gt;d. TiDB 收到所有的 prewrite 成功。&lt;br/&gt;e. TiDB 向 PD 获取第二个全局唯一递增版本，作为本次事务的 &lt;code&gt;commit_ts&lt;/code&gt;。&lt;br/&gt;f. TiDB 向 Primary Key 所在 TiKV 发起第二阶段提交 commit 操作，TiKV 收到 commit 操作后，检查数据合法性，清理 prewrite 阶段留下的锁。&lt;br/&gt;g. TiDB 收到 f 成功信息。&lt;/li&gt;&lt;li&gt;TiDB 向客户端返回事务提交成功。&lt;/li&gt;&lt;li&gt;TiDB 异步清理本次事务遗留的锁信息。&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;优缺点分析&lt;/h3&gt;&lt;p&gt;从上面这个过程可以看到， TiDB 事务存在以下优点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;简单，好理解。&lt;/li&gt;&lt;li&gt;基于单实例事务实现了跨节点事务。&lt;/li&gt;&lt;li&gt;去中心化的锁管理。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;缺点如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;两阶段提交，网络交互多。&lt;/li&gt;&lt;li&gt;需要一个中心化的版本管理服务。&lt;/li&gt;&lt;li&gt;事务在 commit 之前，数据写在内存里，数据过大内存就会暴涨。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;基于以上缺点的分析，我们有了一些实践建议，将在下文详细介绍。&lt;/p&gt;&lt;h2&gt;事务大小&lt;/h2&gt;&lt;h3&gt;1. 小事务&lt;/h3&gt;&lt;p&gt;为了降低网络交互对于小事务的影响，我们建议小事务打包来做。如在 auto commit 模式下，下面每条语句成为了一个事务：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;# original version with auto_commit
UPDATE my_table SET a=&amp;#39;new_value&amp;#39; WHERE id = 1; 
UPDATE my_table SET a=&amp;#39;newer_value&amp;#39; WHERE id = 2;
UPDATE my_table SET a=&amp;#39;newest_value&amp;#39; WHERE id = 3;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;以上每一条语句，都需要经过两阶段提交，网络交互就直接 *3， 如果我们能够打包成一个事务提交，性能上会有一个显著的提升，如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;# improved version
START TRANSACTION;
UPDATE my_table SET a=&amp;#39;new_value&amp;#39; WHERE id = 1; 
UPDATE my_table SET a=&amp;#39;newer_value&amp;#39; WHERE id = 2;
UPDATE my_table SET a=&amp;#39;newest_value&amp;#39; WHERE id = 3;
COMMIT;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;同理，对于 insert 语句也建议打包成事务来处理。&lt;/p&gt;&lt;h3&gt;2. 大事务&lt;/h3&gt;&lt;p&gt;既然小事务有问题，我们的事务是不是越大越好呢？&lt;/p&gt;&lt;p&gt;我们回过头来分析两阶段提交的过程，聪明如你，很容易就可以发现，当事务过大时，会有以下问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;客户端 commit 之前写入数据都在内存里面，TiDB 内存暴涨，一不小心就会 OOM。&lt;/li&gt;&lt;li&gt;第一阶段写入与其他事务出现冲突的概率就会指数级上升，事务之间相互阻塞影响。&lt;/li&gt;&lt;li&gt;事务的提交完成会变得很长很长 ～～～&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为了解决这个问题，我们对事务的大小做了一些限制：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;单个事务包含的 SQL 语句不超过 5000 条（默认）&lt;/li&gt;&lt;li&gt;每个键值对不超过 6MB&lt;/li&gt;&lt;li&gt;键值对的总数不超过 300,000&lt;/li&gt;&lt;li&gt;键值对的总大小不超过 100MB&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;因此，对于 TiDB 乐观事务而言，事务太大或者太小，都会出现性能上的问题。我们建议每 100～500 行写入一个事务，可以达到一个比较优的性能。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;事务冲突&lt;/h2&gt;&lt;p&gt;事务的冲突，主要指事务并发执行时，对相同的 Key 有读写操作，主要分两种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;读写冲突：存在并发的事务，部分事务对相同的 Key 读，部分事务对相同的 Key 进行写。&lt;/li&gt;&lt;li&gt;写写冲突：存在并发的事务，同时对相同的 Key 进行写入。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 TiDB 的乐观锁机制中，因为是在客户端对事务 commit 时，才会触发两阶段提交，检测是否存在写写冲突。所以，在乐观锁中，存在写写冲突时，很容易在事务提交时暴露，因而更容易被用户感知。&lt;/p&gt;&lt;h3&gt;默认冲突行为&lt;/h3&gt;&lt;p&gt;因为我们本文着重将乐观锁的最佳实践，那么我们这边来分析一下乐观事务下，TiDB 的行为。&lt;/p&gt;&lt;p&gt;默认配置下，以下并发事务存在冲突时，结果如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-179f06f5ef72ab87440529ffe9aeb52e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1400&quot; data-rawheight=&quot;1208&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1400&quot; data-original=&quot;https://pic3.zhimg.com/v2-179f06f5ef72ab87440529ffe9aeb52e_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-179f06f5ef72ab87440529ffe9aeb52e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1400&quot; data-rawheight=&quot;1208&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1400&quot; data-original=&quot;https://pic3.zhimg.com/v2-179f06f5ef72ab87440529ffe9aeb52e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-179f06f5ef72ab87440529ffe9aeb52e_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;在这个 case 中，现象分析如下：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-401a4717b25b742d7df7143752c9745b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;960&quot; data-rawheight=&quot;239&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;960&quot; data-original=&quot;https://pic4.zhimg.com/v2-401a4717b25b742d7df7143752c9745b_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-401a4717b25b742d7df7143752c9745b_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;960&quot; data-rawheight=&quot;239&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;960&quot; data-original=&quot;https://pic4.zhimg.com/v2-401a4717b25b742d7df7143752c9745b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-401a4717b25b742d7df7143752c9745b_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;如上图，事务 A  在时间点 &lt;code&gt;t1&lt;/code&gt; 开始事务，事务 B 在事务 &lt;code&gt;t1&lt;/code&gt; 之后的 &lt;code&gt;t2&lt;/code&gt; 开始。&lt;/li&gt;&lt;li&gt;事务 A、事务 B 会同时去更新同一行数据。&lt;/li&gt;&lt;li&gt;时间点 &lt;code&gt;t4&lt;/code&gt; 时，事务 A 想要更新 &lt;code&gt;id = 1&lt;/code&gt; 的这一行数据，虽然此时这行数据在 &lt;code&gt;t3&lt;/code&gt; 这个时间点被事务 B 已经更新了，但是因为 TiDB 乐观事务只有在事务 commit 时才检测冲突，所以时间点 &lt;code&gt;t4&lt;/code&gt; 的执行成功了。&lt;/li&gt;&lt;li&gt;时间点 &lt;code&gt;t5&lt;/code&gt;，事务 B 成功提交，数据落盘。&lt;/li&gt;&lt;li&gt;时间点 &lt;code&gt;t6&lt;/code&gt;，事务 A 尝试提交，检测冲突时发现 &lt;code&gt;t1&lt;/code&gt; 之后有新的数据写入，返回冲突，事务 A 提交失败，提示客户端进行重试。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;根据乐观锁的定义，这样做完全符合逻辑。&lt;/p&gt;&lt;h3&gt;重试机制&lt;/h3&gt;&lt;p&gt;我们知道了乐观锁下事务的默认行为，可以知道在冲突比较大的时候，Commit 很容易出现失败。然而，TiDB 的大部分用户，都是来自于 MySQL；而 MySQL 内部使用的是悲观锁。对应到这个 case，就是事务 A 在 &lt;code&gt;t4&lt;/code&gt; 更新时就会报失败，客户端就会根据需求去重试。&lt;/p&gt;&lt;p&gt;换言之，MySQL 的冲突检测在 SQL 执行过程中执行，所以 commit 时很难出现异常。而 TiDB 使用乐观锁机制造成的两边行为不一致，则需要客户端修改大量的代码。 为了解决广大 MySQL 用户的这个问题，TiDB 提供了内部默认重试机制，这里，也就是当事务 A commit 发现冲突时，TiDB 内部重新回放带写入的 SQL。为此 TiDB 提供了以下参数,&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/reference/configuration/tidb-server/tidb-specific-variables/%23tidb_disable_txn_auto_retry&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb_disable_txn_auto_retry&lt;/a&gt;&lt;/code&gt;：这个参数控制是否自动重试，默认为 &lt;code&gt;1&lt;/code&gt;，即不重试。&lt;/li&gt;&lt;li&gt;&lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/reference/configuration/tidb-server/tidb-specific-variables/%23tidb_retry_limit&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb_retry_limit&lt;/a&gt;&lt;/code&gt;：用来控制重试次数，注意只有第一个参数启用时该参数才会生效。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如何设置以上参数呢？推荐两种方式设置：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;session 级别设置：&lt;br/&gt;set @@tidb_disable_txn_auto_retry = 0; set @@tidb_retry_limit = 10;&lt;/li&gt;&lt;li&gt;全局设置：&lt;br/&gt;set @@global.tidb_disable_txn_auto_retry = 0; set @@global.tidb_retry_limit = 10;&lt;/li&gt;&lt;/ol&gt;&lt;h3&gt;万能重试&lt;/h3&gt;&lt;p&gt;那么重试是不是万能的呢？这要从重试的原理出发，重试的步骤：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;重新获取 &lt;code&gt;start_ts&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;对带写入的 SQL 进行重放。&lt;/li&gt;&lt;li&gt;两阶段提交。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;细心如你可能会发现，我们这边只对写入的 SQL 进行回放，并没有提及读取 SQL。这个行为看似很合理，但是这个会引发其他问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;code&gt;start_ts&lt;/code&gt; 发生了变更，当前这个事务中，读到的数据与事务真正开始的那个时间发生了变化，写入的版本也是同理变成了重试时获取的 &lt;code&gt;start_ts&lt;/code&gt; 而不是事务一开始时的那个。&lt;/li&gt;&lt;li&gt;如果当前事务中存在更新依赖于读到的数据，结果变得不可控。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;打开了重试后，我们来看下面的例子：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fdca1934303729678d5445a7ac5076fb_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1490&quot; data-rawheight=&quot;1597&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1490&quot; data-original=&quot;https://pic4.zhimg.com/v2-fdca1934303729678d5445a7ac5076fb_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-fdca1934303729678d5445a7ac5076fb_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1490&quot; data-rawheight=&quot;1597&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1490&quot; data-original=&quot;https://pic4.zhimg.com/v2-fdca1934303729678d5445a7ac5076fb_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-fdca1934303729678d5445a7ac5076fb_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;我们来详细分析以下这个 case：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e906a1014eba73dd01016cd55fc0e207_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;891&quot; data-rawheight=&quot;257&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;891&quot; data-original=&quot;https://pic4.zhimg.com/v2-e906a1014eba73dd01016cd55fc0e207_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e906a1014eba73dd01016cd55fc0e207_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;891&quot; data-rawheight=&quot;257&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;891&quot; data-original=&quot;https://pic4.zhimg.com/v2-e906a1014eba73dd01016cd55fc0e207_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-e906a1014eba73dd01016cd55fc0e207_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;如图，在 session B 在 &lt;code&gt;t2&lt;/code&gt; 开始事务 2，&lt;code&gt;t5&lt;/code&gt; 提交成功。session A 的事务 1 在事务 2 之前开始，在事务 n2 提交完成后提交。&lt;/li&gt;&lt;li&gt;事务 1、事务 2 会同时去更新同一行数据。&lt;/li&gt;&lt;li&gt;session A 提交事务 1 时，发现冲突，tidb 内部重试事务 1。&lt;br/&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;重试时，重新取得新的 &lt;code&gt;start_ts&lt;/code&gt; 为 &lt;code&gt;t8’&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;回放更新语句 &lt;code&gt;update tidb set name=&amp;#39;pd&amp;#39; where id =1 and status=1&lt;/code&gt;。&lt;br/&gt;i. 发现当前版本 &lt;code&gt;t8’&lt;/code&gt; 下并不存在符合条件的语句，不需要更新。&lt;br/&gt;ii. 没有数据更新，返回上层成功。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;tidb 认为事务 1 重试成功，返回客户端成功。&lt;/li&gt;&lt;li&gt;session A 认为事务执行成功，查询结果，在不存在其他更新的情况下，发现数据与预想的不一致。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这里我们可以看到，对于重试事务，如果本身事务中更新语句需要依赖查询结果时，因为重试时会重新取版本号作为 &lt;code&gt;start_ts&lt;/code&gt;，因而无法保证事务原本的 &lt;code&gt;ReadRepeatable&lt;/code&gt; 隔离型，结果与预测可能出现不一致。&lt;/p&gt;&lt;p&gt;综上所述，如果存在依赖查询结果来更新 SQL 语句的事务，建议不要打开 TiDB 乐观锁的重试机制。&lt;/p&gt;&lt;h3&gt;冲突预检&lt;/h3&gt;&lt;p&gt;从上文我们可以知道，检测底层数据是否存在写写冲突是一个很重的操作，因为要读取到数据进行检测，这个操作在 prewrite 时 TiKV 中具体执行。为了优化这一块性能，TiDB 集群会在内存里面进行一次冲突预检测。&lt;/p&gt;&lt;p&gt;TiDB 作为一个分布式系统，我们在内存中的冲突检测主要在两个模块进行：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 层，如果在 TiDB 实例本身发现存在写写冲突，那么第一个写入发出去后，后面的写入就已经能清楚地知道自己冲突了，没必要再往下层 TiKV 发送请求去检测冲突。&lt;/li&gt;&lt;li&gt;TiKV 层，主要发生在 prewrite 阶段。因为 TiDB 集群是一个分布式系统，TiDB 实例本身无状态，实例之间无法感知到彼此的存在，也就无法确认自己的写入与别的 TiDB 实例是否存在冲突，所以会在 TiKV 这一层检测具体的数据是否有冲突。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其中 TiDB 层的冲突检测可以关闭，配置项可以启用：&lt;/p&gt;&lt;p&gt;txn-local-latches：事务内存锁相关配置，当本地事务冲突比较多时建议开启。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;enable&lt;/li&gt;&lt;ul&gt;&lt;li&gt;开启&lt;/li&gt;&lt;li&gt;默认值：false&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;capacity&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Hash 对应的 slot 数，会自动向上调整为 2 的指数倍。每个 slot 占 32 Bytes 内存。当写入数据的范围比较广时（如导数据），设置过小会导致变慢，性能下降。&lt;/li&gt;&lt;li&gt;默认值：1024000&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;细心的朋友可能又注意到，这边有个 capacity 的配置，它的设置主要会影响到冲突判断的正确性。在实现冲突检测时，我们不可能把所有的 Key 都存到内存里，占空间太大，得不偿失。所以，真正存下来的是每个 Key 的 hash 值，有 hash 算法就有碰撞也就是误判的概率，这里我们通过 capacity 来控制 hash 取模的值：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;capacity 值越小，占用内存小，误判概率越大。&lt;/li&gt;&lt;li&gt;capacity 值越大，占用内存大，误判概率越小。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在真实使用时，如果业务场景能够预判断写入不存在冲突，如导入数据操作，建议关闭。&lt;/p&gt;&lt;p&gt;相应地，TiKV 内存中的冲突检测也有一套类似的东西。不同的是，TiKV 的检测会更严格，不允许关闭，只提供了一个 hash 取模值的配置项：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;scheduler-concurrency&lt;/li&gt;&lt;ul&gt;&lt;li&gt;scheduler 内置一个内存锁机制，防止同时对一个 Key 进行操作。每个 Key hash 到不同的槽。&lt;/li&gt;&lt;li&gt;默认值：2048000&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;此外，TiKV 提供了监控查看具体消耗在 latch 等待的时间：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-482cbf21c935f2b5b8e68bafeb9c9510_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-482cbf21c935f2b5b8e68bafeb9c9510_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-482cbf21c935f2b5b8e68bafeb9c9510_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-482cbf21c935f2b5b8e68bafeb9c9510_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-482cbf21c935f2b5b8e68bafeb9c9510_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;如果发现这个 wait duration 特别高，说明耗在等待锁的请求上比较久，如果不存在底层写入慢问题的话，基本上可以判断这段时间内冲突比较多。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;综上所述，Percolator 乐观事务实现原理简单，但是缺点诸多，为了优化这些缺陷带来的性能上和功能上的开销，我们做了诸多努力。但是谁也不敢自信满满地说：这一块的性能已经达到了极致。&lt;/p&gt;&lt;p&gt;时至今日，我们还在持续努力将这一块做得更好更远，希望能让更多使用 TiDB 的小伙伴能从中受益。与此同时，我们也非常期待大家在使用过程中的反馈，如果大家对 TiDB 事务有更多优化建议，欢迎联系我 &lt;a href=&quot;mailto:wuxuelian@pingcap.com&quot;&gt;wuxuelian@pingcap.com&lt;/a&gt; 。您看似不经意的一个举动，都有可能使更多饱受折磨的互联网同学们从中享受到分布式事务的乐趣。&lt;/p&gt;&lt;p&gt;原文阅读：&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/best-practice-optimistic-transaction/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 最佳实践系列（三）乐观锁事务 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-20-87608202</guid>
<pubDate>Sun, 20 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>Hands-on! 如何给 TiDB 添加新系统表</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-10-18-87280459.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/87280459&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-206d9738b5b3622f8c51640f2a38a2e6_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄东旭&lt;/p&gt;&lt;blockquote&gt;“TiDB，你已经是一个成熟的数据库了，该学会用自己的 SQL 查自己的状态了。”&lt;/blockquote&gt;&lt;p&gt;对于一个成熟的数据库来说，通过 SQL 来查询系统本身的状态再正常不过，对于 MySQL 来说 &lt;code&gt;INFOMATION_SCHEMA&lt;/code&gt; 和 &lt;code&gt;PERFORMANCE_SCHEMA&lt;/code&gt; 里面有大量的信息，基本上通过查询些信息，DBA 就能对整个系统的运行状态一目了然。最棒的是，查询的接口正是 SQL，不需要依赖其他的第三方工具，运用表达力强大的 SQL 甚至可以对这些信息进行二次加工或者过滤，另外接入第三方的运维监控工具也很自然，不需要引入新的依赖。&lt;/p&gt;&lt;p&gt;过去由于种种原因，TiDB 很多的内部状态信息是通过不同组件暴露 RESTFul API 来实现，这个方案也不是不好，但是随着 API 的增多，管理成本越来越高，举一个例子：在不参考文档的前提下，用户是很难记住那么多 RESTFul API 的路径的，只能通过将这些 API 封装成命令行工具来使用，但是如果这是一张系统表，只需要一句 &lt;code&gt;SHOW TABLES&lt;/code&gt; 和几条 &lt;code&gt;SELECT&lt;/code&gt; 就能够了。当然选择 RESTFul API 还有其他的原因，例如有些操作并不是只读的，是类似命令的形式，例如：手动 split region 这类操作，使用 RESTFul API 会更好，这两者其实并不矛盾，系统表当然是一个很好的补充，这是提升整体软件易用性的一个好例子。&lt;/p&gt;&lt;p&gt;&lt;b&gt;今天正好有一些时间，花了几十分钟完整的走了一遍流程，给 TiDB 的&lt;/b&gt; &lt;b&gt;&lt;code&gt;INFORMATION_SCHEMA&lt;/code&gt;&lt;/b&gt; &lt;b&gt;添加了一张名为&lt;/b&gt; &lt;b&gt;&lt;code&gt;TIDB_SERVERS_INFO&lt;/code&gt;&lt;/b&gt; &lt;b&gt;的表，用来显示集群中所有活着的 tidb-server 的状态信息（基本和&lt;/b&gt; &lt;b&gt;&lt;code&gt;/info/all&lt;/code&gt;&lt;/b&gt; &lt;b&gt;做的事情差不多），意在抛砖引玉，社区的小伙伴可以参照这篇博客添加新的有用的信息。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有这个想法后，我的直觉是去找 &lt;code&gt;information_schema&lt;/code&gt; 的代码看看别的系统表是怎么实现的，照猫画虎就 OK 了（😁没毛病）。 TiDB 的代码组织还算比较直观，在 tidb repo 的根目录下直接看到了一个包叫 &lt;code&gt;infoschema&lt;/code&gt;，感觉就是它，打开 &lt;code&gt;inforschema/table.go&lt;/code&gt; 后确实应证了我的猜想，文件开头集中定义了很多字符串常量：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;...
tableTiKVStoreStatus                	= &amp;#34;TIKV_STORE_STATUS&amp;#34;
tableAnalyzeStatus                  	= &amp;#34;ANALYZE_STATUS&amp;#34;
tableTiKVRegionStatus               	= &amp;#34;TIKV_REGION_STATUS&amp;#34;
tableTiKVRegionPeers                	= &amp;#34;TIKV_REGION_PEERS&amp;#34;
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这些常量正是 TiDB 的 &lt;code&gt;INFOMATION_SCHEMA&lt;/code&gt; 中的表名，根据这些变量顺藤摸瓜可以找到同文件里面的 &lt;code&gt;tableNameToColumns&lt;/code&gt; 这个 map，顾名思义应该是这个 map 通过表名映射到表结构定义，随便打开一个，果然如此：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;var columnStatisticsCols = []columnInfo{
	{&amp;#34;SCHEMA_NAME&amp;#34;, mysql.TypeVarchar, 64, mysql.NotNullFlag, nil, nil}, 
	{&amp;#34;TABLE_NAME&amp;#34;, mysql.TypeVarchar, 64, mysql.NotNullFlag, nil, nil}, 
	{&amp;#34;COLUMN_NAME&amp;#34;, mysql.TypeVarchar, 64, mysql.NotNullFlag, nil, nil}, 
	{&amp;#34;HISTOGRAM&amp;#34;, mysql.TypeJSON, 51, 0, nil, nil}, 
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;下一步需要如何填充数据返回给 TiDB 的 SQL Engine，我们注意到 &lt;code&gt;infoschemaTable&lt;/code&gt; 这个类实现了 &lt;code&gt;table.Table interface&lt;/code&gt;，很显然这个 interface 就是 TiDB 中对于 Table 获取数据/修改数据的接口，有关获取数据的方法是 &lt;code&gt;IterRecords&lt;/code&gt;，我们只需要看到 &lt;code&gt;IterRecords&lt;/code&gt; 中的实现就能知道这些系统表的数据是如何返回给 SQL Engine 的，果然在 &lt;code&gt;IterRecords&lt;/code&gt; 里面有一个方法，&lt;code&gt;inforschemaTable.getRows()&lt;/code&gt;，这个方法的定义中有一个巨大的 switch 语句，用于判断是在哪个系统表上，根据这个信息然后返回不同的数据：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;...
switch it.meta.Name.O {
	case tableSchemata:
		fullRows = dataForSchemata(dbs)
	case tableTables:
		fullRows, err = dataForTables(ctx, dbs) 
	case tableTiDBIndexes: 
		fullRows, err = dataForIndexes(ctx, dbs) 
...
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Bingo! 感觉就是我们需要的东西。&lt;/p&gt;&lt;p&gt;&lt;b&gt;现在步骤就很清楚了：&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;在 &lt;code&gt;infoschema/tables.go&lt;/code&gt; 中添加一个新的字符串常量 &lt;code&gt;tableTiDBServersInfo&lt;/code&gt; 用于定义表名；&lt;/li&gt;&lt;li&gt;定义一个 &lt;code&gt;[]columnInfo：tableTiDBServersInfoCols&lt;/code&gt;，用于定义这张系统表的结构；&lt;/li&gt;&lt;li&gt;在 &lt;code&gt;tableNameToColumns&lt;/code&gt; 这个 map 中添加一个新的映射关系 &lt;code&gt;tableTiDBServersInfo =&amp;gt; tableTiDBServersInfoCols&lt;/code&gt;；&lt;/li&gt;&lt;li&gt;在 &lt;code&gt;infoschemaTable.getRows()&lt;/code&gt; 方法中加入一个新的 &lt;code&gt;dataForTableTiDBServersInfo&lt;/code&gt; 的 swtich case；&lt;/li&gt;&lt;li&gt;搞定。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;下一个目标是实现 &lt;code&gt;dataForTableTiDBServersInfo&lt;/code&gt;，很显然，大致的思路是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;找到这个集群的 PD，因为这些集群拓扑信息；&lt;/li&gt;&lt;li&gt;将这些信息封装成 &lt;code&gt;tableTiDBServersInfoCols&lt;/code&gt; 中定义的形式，返回给 &lt;code&gt;getRows&lt;/code&gt; 方法。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;通过传入的 ctx 对象，获取到 Store 的信息， &lt;code&gt;sessionctx.Context&lt;/code&gt; 是 TiDB 中一个很重要的对象，也是 TiDB 贯穿整个 SQL 引擎的一个设计模式，这个 Context 中间存储在这个 session 生命周期中的一些重要信息，例如我们可以通过 &lt;code&gt;sessionctx.Context&lt;/code&gt; 获取底层的 Storage 对象，拿到 Storage 对象后，能干的事情就很多了。&lt;/p&gt;&lt;p&gt;本着照猫画虎的原则，参考了一下 &lt;code&gt;dataForTiDBHotRegions&lt;/code&gt; 的实现：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;tikvStore, ok := ctx.GetStore().(tikv.Storage) &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;因为我们的目标是获取 PD 对象，必然地，只有 TiKV 作为 backend 的时候才有 PD，所以这里的类型转换判断是必要的。&lt;/p&gt;&lt;p&gt;其实，通过 PD 获取集群信息这样的逻辑已经在 TiDB 中封装好了，我发现在 &lt;code&gt;domain/info.go&lt;/code&gt; 中的这个方法正是我们想要的：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// GetAllServerInfo gets all servers static information from etcd. func (is *InfoSyncer) 
GetAllServerInfo(ctx context.Context) (map[string]*ServerInfo, error)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;实际上，TiDB 的 &lt;code&gt;/info/all&lt;/code&gt; 这个 REST API 正是通过调用这个函数实现，我们只需要调用这个方法，将返回值封装好就完成了。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5ecb9035e5753574aa3efa1f5119d097_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;275&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-5ecb9035e5753574aa3efa1f5119d097_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5ecb9035e5753574aa3efa1f5119d097_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;275&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-5ecb9035e5753574aa3efa1f5119d097_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-5ecb9035e5753574aa3efa1f5119d097_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;自此，我们就完成了一个新的系统表的添加。在自己添加的新表上 SELECT 一下，是不是很有成就感 :) 欢迎大家在此基础上添加更多有用的信息。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;阅读原文：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/hands-on-build-a-new-system-table-for-tidb/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Hands-on! 如何给 TiDB 添加新系统表 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-10-18-87280459</guid>
<pubDate>Fri, 18 Oct 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
