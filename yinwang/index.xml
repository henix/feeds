<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>当然我在扯淡</title>
<link>https://henix.github.io/feeds/yinwang/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Wed, 25 Dec 2019 06:58:31 +0800</lastBuildDate>
<item>
<title>我不是编译器专家</title>
<link>https://henix.github.io/feeds/yinwang/2019-12-24-compilers.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/12/24/compilers&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;我不是编译器专家&lt;/h2&gt;
            &lt;p&gt;工作多年以来，我深刻的体会到一个规律，那就是做过编译器工作的人，似乎很容易产生高人一等的心理，以至于在与人合作中出现各种问题。由于他们往往也存在偏执心理和理想主义，所以在恶化人际关系的同时，也可能设计出非常不合理的软件构架，浪费大量的人力物力。&lt;/p&gt;

&lt;p&gt;我曾经提到的 &lt;a href=&quot;http://www.yinwang.org/blog-cn/2017/05/25/dsl&quot;&gt;DSL&lt;/a&gt; 例子，就是这样的两个人。他们都自称做过编译器，所以成天在我面前高谈阔论，甚至在最基础的概念上班门弄斧，显示出一副“教育”其他人的姿态。其实他们只有一个人做过 &lt;a href=&quot;http://www.yinwang.org/blog-cn/2015/09/19/parser&quot;&gt;parser&lt;/a&gt;，还不算是真正的编译器工作，却总显示出高深莫测的模样。像哲人一样捋捋胡子，摇摇脑袋，慢条斯理，嗯…… 另外一个完全就是外行，只是知道一些术语，成天挂在嘴边。每次他一开口，我都发现这个人并不知道他自己在说什么，却仍然洋洋得意的样子。&lt;/p&gt;

&lt;p&gt;我是被他们作为专家请来这个公司的，来了之后却发现他们唯一想做的事情，是在我面前显示他们才是“专家”。他们也问过我问题，可是我发现他们并不想知道答案，因为我说话的时候他们并没有在听。不管说什么问什么，他们似乎只想别人觉得他们是最聪明的人。“Yin，你知道 X 吗？” 当然他期望的是你说不知道，这样他就能像大师一样，把这个刚学到的术语给你讲半天。&lt;/p&gt;

&lt;p&gt;每当这个时候，我就想起一个前同事喜欢说的一句话：“你问我，是因为你不知道，还是因为你知道？”&lt;/p&gt;

&lt;p&gt;更糟的事情是，这其中一人还是 Haskell 语言的忠实粉丝，他总是有这样的雄心壮志，要用“&lt;a href=&quot;http://www.yinwang.org/blog-cn/2013/03/31/purely-functional&quot;&gt;纯函数式编程&lt;/a&gt;”改写全公司的代码……&lt;/p&gt;

&lt;p&gt;遇到这样的人是非常闹心的，到了什么程度？他们经常雄心勃勃用一种新的语言（Scala，Go……）试图改写全公司的代码，一个月之后开始唾骂这语言，两个月之后他们的项目不了了之，代码也不知道哪里去了。然后换一种语言，如此反复…… 因为烦于他们在我面前高谈阔论，我干脆换了一个部门，不再做跟语言和编译器相关的事情。&lt;/p&gt;

&lt;p&gt;有些美国公司在招人的时候表示，对简历里提到“做过编译器”的求职者有戒备心理，甚至直接说“我们不招编译器专业的人”。以至于我也曾经被过滤掉，因为我在 Coverity 做过编译器相关工作。编译器专业的人本来可以做普通的程序员工作，为什么有公司如此明确不要他们呢？我现在明白为什么了，因为编译器专业人士有大概率是性格很差的团队合作者，喜欢显示出高高在上，拯救世界的姿态，无法平等而尊重的对待其他人。&lt;/p&gt;

&lt;p&gt;在 Coverity 和之后的其它公司遇到的编译器人，也或多或少存在差不多的问题。他们下意识里把自己看成是最高档次的程序员，所以对其他人总是高高在上的气势。&lt;/p&gt;

&lt;p&gt;很多人也把我叫做“编译器专家”。我一直没有正式拒绝这个称呼，每每遇到真正的编译器专家，我总觉得自己不是那个圈子的。不是我不能做编译器的工作，而是他们的认识水平，理念和态度和我格格不入。&lt;/p&gt;

&lt;p&gt;所以我应该明确表个态：我不是编译器专家，而且我看不起编译器这个领域。就最后学习的专业，我是一个编程语言（PL）研究者，从更广的角度来看，我是一个计算机科学家。有人听了“科学家”一词总是误以为我在抬高自己，而在我心目中“科学家”仅仅是一个职业，就像“厨师”一样，并不说明一个人的水平。科学家有好的，也有很差，素质很低的。&lt;/p&gt;

&lt;p&gt;业内人士经常混淆编程语言（PL）和编译器两个领域，而其实 PL 和编译器是很不一样的。真懂 PL 的人去做编译器也会比较顺手，而编译器专业的却不一定懂 PL。为什么呢？因为做编译器一般是专注于“实现”别人已经设计好的语言，比如 C，C++。他们必须按照语言设计者写好的语言规范（specification）来写编译器，所以在语言方面并没有发挥的空间，没有机会去理解语言设计的微妙之处。&lt;/p&gt;

&lt;p&gt;很多编译器工程师并没有接受过系统的 PL 理论教育，有些甚至是半路出家，在学校里根本没碰过编译器，也没研究过 PL。比如我的第一个公司 Coverity，招进去的很多人从来没碰过编译器，也不懂 PL。Coverity 的领导天真的向他们宣布：“我们能教会你们一切！” 然而很可惜，PL 的功夫根本不是一个公司在短期能够传授的。Coverity 没有这个能力，Google，Facebook，Intel，微软…… 都没有这个能力。&lt;/p&gt;

&lt;p&gt;由于缺乏对 PL 理论的深入研究，编译器人往往用井底之蛙的眼光来看待语言，总以为他们实现过的语言（比如 C++）就是一切。一个语言为什么那样设计？不知道。它还可以如何改进？不知道。“它就是那个样子！” 这是我常听编译器人说的话。&lt;/p&gt;

&lt;p&gt;许多编译器人把 C++ 的创造者 Stroustrup 奉为神圣，却不知道 Stroustrup 在 PL 领域算是实力比较弱的。Stroustrup 曾经在 2011 年 11 月 11 日来到 IU 进行关于 C++11 的演讲，IU 的资深 PL 教授们都有到场。Stroustrup 谦卑的说：“我需要向你们学习很多东西来改进 C++。” 他说的是实话，因为 IU 的教授们在语言设计上确实比他强很多。&lt;/p&gt;

&lt;p&gt;编译器人所崇拜的大师，在 PL 研究者眼里其实不算什么。编译器人与 PL 研究者在这类见识上的差距，足以说明编译器人并不真懂 PL。&lt;/p&gt;

&lt;p&gt;实际上做编译器是很无聊的工作，大部分时候只是把别人设计的语言，翻译成另外的人设计的硬件指令。所以编译器领域处于编程语言（PL）和计算机体系构架（computer architecture）两个领域的夹缝中，上面的语言不能改，下面的指令也不能改，并没有很大的创造空间。&lt;/p&gt;

&lt;p&gt;编译器领域几十年来翻来覆去都是那几个编程模式和技巧，玩来玩去也真够无聊的。起初觉得新鲜，熟悉了之后也就那个样了。很多程序员都懂得避免“低水平重复”，可是由于没有系统的学习过编译器，他们往往误以为做编译器是更高级，更有趣的工作，而其实编译器领域是更加容易出现低水平重复的地方，因为它的创造空间非常有限。&lt;/p&gt;

&lt;p&gt;同样的编译优化技巧，在 A 公司拿来做 A 语言的编译器，到了 B 公司拿来做 B 语言的编译器…… 大同小异，如此反复。运气好点，你可能遇到 C，C++，Java。运气不好，你可能遇到 JavaScript，PHP，Go 之类的怪胎，甚至某种垃圾 DSL。但公司有要求，无论语言设计如何垃圾，硬件指令设计如何繁琐，你编译出来的指令必须能正确运行所有这语言写出来的代码。你说这活是不是很苦逼？&lt;/p&gt;

&lt;p&gt;虽然苦逼，编译器人往往自高自大，高估自己在整个 IT 领域里的地位，看低其它程序员。编译器人很多认为自己懂了编程语言的一切，而其实他们只是一知半解。从我之前怼 Chris Lattner 的一些文章（&lt;a href=&quot;http://www.yinwang.org/blog-cn/2016/06/06/swift&quot;&gt;链接1&lt;/a&gt;，&lt;a href=&quot;http://www.yinwang.org/blog-cn/2016/10/12/compiler-bug&quot;&gt;链接2&lt;/a&gt;）你也许可以看出来，虽然是编译器领域声名显赫的人物，却在设计 Swift 语言的早期犯下我一眼就看出来的低级错误，改了一次居然还没对。随便找个 PL 专家商量一下，也不至于犯这样的错误。这就是所谓“骄傲使人落后”吧。&lt;/p&gt;

&lt;p&gt;编译器领域最重要的教材，龙书和虎书，在我看来也有很多一知半解，作者自己都稀里糊涂的内容。而且花了大量篇幅讲 &lt;a href=&quot;http://www.yinwang.org/blog-cn/2015/09/19/parser&quot;&gt;parser&lt;/a&gt; 这种看似高深，实则肤浅的话题，浪费读者太多时间，误导他们认为 parser 是至关重要的技术。以至于很多人上完编译器课程，只学会了写 parser，对真正关键的部分没能理解。龙书很难啃，为什么呢，因为作者自己都不怎么懂。虎书号称改进了龙书，结果还是很难啃，感觉只是换了一个封面而已。&lt;/p&gt;

&lt;p&gt;我曾经跟虎书作者 Andrew Appel 的一个门徒合作过，当时这人是 IU 的助理教授。借着一次我跟她做 independent study 的机会，逼我写扯淡而毫无意义的论文，而且对人非常的 push 和虚伪。作为普林斯顿大学毕业的 PhD，学识水平跟 IU 的其他教授格格不入，却在待人接物方面显示出各种“贱”，对编译器领域的“牛人”各种跪舔，随时都在显示自己以前在某某人身边工作过，那神情好像在说“你们见识过吗？” 那是我在 IU 度过的最难受的一个学期，这使我对“编译器人”的偏见又加深一层。&lt;/p&gt;

&lt;p&gt;编译器领域的顶级人物如此，其它声称做过编译器的人也可想而知了。大部分自称做过编译器的人，恐怕连最基本的的编译器都没法从头写出来。利用 LLVM 已有的框架做点小打小闹的优化，就号称自己做过编译器了。许多编译器人士死啃书本，肤浅的记忆各种术语（比如 SSA），死记硬背具体实现细节，无法灵活变通。&lt;/p&gt;

&lt;p&gt;所以我常说，编译器是计算机界死知识最多，教条主义最严重的领域。经常是某人想出一个做法，起个名字，其他人就照做，死记硬背，而且把这名字叫得特别响亮。你要是一时想不起这名字是什么意思，立马被认为是法国人不知道拿破仑，中国人不知道毛泽东。你不是做编译器的！&lt;/p&gt;

&lt;p&gt;这就是为什么虽然有多次编译器的工作机会，包括 Apple 的 LLVM 部门，我最后都没去。进入 Intel 的时候，本来编译器部门也是一个选择，可是再三考虑之后还是选择了其它方向。因为我很清楚的记得，每一次做编译器相关工作都是非常压抑的，需要面对一些沉闷古板而自以为是的人，而且内容真的是重复，无聊和枯燥。&lt;/p&gt;

&lt;p&gt;我唯一敬佩的编译器作者是 &lt;a href=&quot;http://www.yinwang.org/blog-cn/2013/03/28/chez-scheme&quot;&gt;Kent Dybvig&lt;/a&gt;，但我也不想跟他一起做编译器。最近很多芯片公司的“AI 编译器”部门找我，我全都拒绝了。我不喜欢身边围绕着这些人，做着这些事。我宁愿去卖烧饼也不想做编译器。&lt;/p&gt;

&lt;p&gt;由于编译器人的性格特征，除非一个公司专门要做编译器，否则对于曾经做过编译器，想换个方向的求职者，在面试的时候最好深刻了解他们的性格，态度和做事方式，看他们是否能看淡这些，能否平等对待其他人，能否理性而实在的对待工程。否则自视很高的“编译器人”进了公司，很可能对团队成为一种灾难。&lt;/p&gt;

&lt;p&gt;我写这篇文章是为了警醒广大 IT 公司，也是为了在精神上支持其它程序员。我希望他们不要被编译器的“难度”迷惑了，不要被编译器人吓唬和打压。你们做的并不是更低级，更无聊的工作。正好相反，真正可以发挥创造力的空间并不在底层的编译器一类的东西，而在更接近应用和现实的地方。&lt;/p&gt;

&lt;p&gt;每当有人向我表示编译器高深莫测，向往却又高攀不上，我都会给他打一个比方：做编译器就像做菜刀。你可以做出非常好的菜刀，然而你终究只是一个铁匠。铁匠不知道如何用这菜刀做出五花八门，让人心旷神怡，米其林级别的菜肴，因为那是大厨的工作。要做菜还是要打铁，那是你自己的选择，并没有贵贱之分。&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-12-24-compilers</guid>
<pubDate>Tue, 24 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>抱怨与观察的差别</title>
<link>https://henix.github.io/feeds/yinwang/2019-12-23-complaint-observation.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/12/23/complaint-observation&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;抱怨与观察的差别&lt;/h2&gt;
            &lt;p&gt;我发现很多国人分不清“抱怨”和“观察”。可能因为大部分人只会抱怨，所以每当他们听到别人对某些事情的“负面”反馈，就会觉得别人在抱怨，甚至以为别人生气了。事情越是超越自己的格局，就越是觉得别人在抱怨。因为在潜意识里，那是他们自己会抱怨的时候，所以转移一下主角，就以为别人说点话就是在抱怨，而其实可能完全不是。就像我的博客和微博的很多内容，很多只是上帝视角的“观察”，却很容易被某些人以为是“抱怨”。&lt;/p&gt;

&lt;p&gt;美国也有类似的文化。很多美国人喜欢表现得很“正面”，“乐观”，不习惯客观的“观察”和诙谐的“讽刺”。说话语气也是假得很戏剧化，一点点小事总是喜欢说得眉飞色舞的。这一差别，我已经在很久以前和欧洲人的交流中体会到了。&lt;/p&gt;

&lt;p&gt;有次我去参加学术会议，晚上和十几个学者去饭店吃饭，其中大部分是欧洲人。大家聊得很开心，到了付账的时候，服务员拿来账单。一个英国来的教授接过账单，开始算大家应该 AA 多少钱。当看到饭店往账单里加了 15% 的小费，他摇摇头，说：“哎，这些美国人，把 bill 叫做 check，把 check 叫做 bill，还往账单里直接加上了小费……” 大家听了都笑了。&lt;/p&gt;

&lt;p&gt;解释一下，按照一般美国饭店的规矩，小费本来应该是自愿给的。基本满意的服务给 15%，如果服务让你觉得很贴心，也可以多给（18% 或者 20%），不满意的也可以少到 10%，甚至可以干脆不给小费，在收据上签字时写上“服务态度很差”。顾客给多少小费，饭店或者服务员不应该有任何异议。但是某些饭店规定，当一桌就餐人数超过一定人数（比如 8 人），饭店会强制往账单里加上小费。看似合理，然而这并不符合欧洲人的观念。欧洲人其实一直鄙视美国饭店不给够服务员“法定最低工资”的做法，导致很多服务员完全靠小费为生，跟讨饭似的。这是很不好的社会制度。&lt;/p&gt;

&lt;p&gt;所以这位教授是在“抱怨”饭店的做法吗？你用“讽刺”这个词可能更加贴切点。面对不好的做法，欧洲人和美国人，中国人的表现是很不一样的。美国人和中国人喜欢盲目的“正面”，显示出一种不敢面对负面信息的伪善文化，表面上显得乐观，体面，无谓的“宽容”。而大部分欧洲人更加直率，是好的就说好，不喜欢的就随便“观察”或者“戏谑”几句。听了这些话的人，也会为此开怀。&lt;/p&gt;

&lt;p&gt;所以当你觉得别人在“抱怨”的时候，也许别人只是在告诉你：我见过更好的。&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-12-23-complaint-observation</guid>
<pubDate>Mon, 23 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>自动化服务的误区</title>
<link>https://henix.github.io/feeds/yinwang/2019-12-15-automation.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/12/15/automation&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;自动化服务的误区&lt;/h2&gt;
            &lt;p&gt;说到当今的创业方向，很多人会有利用技术让某些事情“自动化”的想法。我承认其中某些做法是有好处的，但也需要认识到，并不是所有的场景顾客都会认可自动化。商家不可以把自动化服务强加在顾客身上，否则就可能输给人工服务的竞争者。&lt;/p&gt;

&lt;p&gt;这段时间听说的一个想法，是利用机器人让服装导购自动化。也就是说，当我走进一家服装店，有一个机器人来回答我的问题，带我去找我想要的衣服。虽然我尊重这位朋友的想法，但这显然不是他最好的想法。我觉得这是“自动化方向”比较有代表性的例子，有必要分析一下，再附带讲一下我对最近出现的一些“自动化服务”的看法。&lt;/p&gt;

&lt;p&gt;简言之，我非常的不希望走进一家服装店，是一个机器人带我去找衣服，管你把它做得多漂亮。作为顾客，我需要一个人，一个有礼貌，不俗气，尊重顾客，有点品位，甚至有点幽默感的人，来为我服务。我觉得这是商店主人对我起码的尊重。这个人代表了商店主人来欢迎我，给我推荐衣服，帮我拿衣服，穿上之后给我建议，尺码不对帮我去换，走的时候跟我说再见，谢谢光临！各种贴心，我就很愿意买单，下次再来。但如果是机器人来给我服务，我就觉得少了很多关怀，就觉得买衣服是完成任务，冷冰冰的，而不是一件有趣的事情。如果机器人服务员夸我穿上一件衣服好看，我会相信它吗？但如果是一个人，特别是女性夸我穿上好看，我可能就买下了。而且我发现不管是男性还是女性，都喜欢女性服务员给自己服务，而且比较相信她们的眼光。所以你发现服装店里导购都是女性，男的基本都是搬东西或者收款的。我恐怕是买最便宜，最无聊的衣服，才会去机器人服务的商店。&lt;/p&gt;

&lt;p&gt;所以呢要做自动化，不能只从技术人的角度出发来看问题。要知道世界上不止有追求效率的人，还有喜欢乐趣和文化的人，“无人服务”会少了很多文化。服装行业需要很多的文化，使用机器人来服务，会把很多顾客给无聊走的。所以作为“十年内 AI 无法取代的人类工作”的例子，服装导购也是一个 AI 技术无法取代的工作。不是因为技术完全没有能力做服装导购的事情，而是你换成机器之后，很多人不来你的店了，因为失去了文化的感觉。&lt;/p&gt;

&lt;p&gt;我曾经在湾区吃过一次“全自动服务”的寿司。每桌顾客头上有一个 iPad，你在 iPad 上下单，然后你点的寿司就会通过传送带嗖的送到你面前。如果你买够一定数量的寿司，你头上的机器上还会出来一些奖品玩具给你。似乎很热闹，跟打游戏一样。结果呢，我和朋友去了一次就再也不想去了。我喜欢去有人为我服务的饭店，我喜欢“文化”的感觉，我不喜欢机械化的服务。我不喜欢菜单是个 iPad，我喜欢纸质菜单。实际上，纸质菜单不但更有文化感，而且比起电子屏幕在操作上有优点。因为纸质菜单翻开可以很大，所以你可以同时看到很多的菜，而且来回翻起来也更快，定位更准。所以你发现高档餐馆全都是纸质菜单，几乎没人用电子屏。&lt;/p&gt;

&lt;p&gt;现在国内某些饭店也有滥用技术的问题，在桌上贴着二维码可以“扫码点餐”，同时又有服务员和菜单。我一般走进饭店，拿起桌上菜单看，跟朋友商量好，招呼服务员过来。这时某些饭店的服务员会跟我说：“扫桌上二维码就可以点餐。” 因为我已经看好了菜单，就不想打开手机再找一遍我已经选好的菜，所以听到这样的话，我心里会梗一下。我会对他说：“不用了，你帮我点单就行。” 作为顾客，我期望他为我服务。一般在我要求之下，服务员也会为我点单，但某些饭店做得过了，当我要求服务员给我点单的时候，他们会坚持说：“您扫码就可以了，我们这里都是扫码点餐。” 这时候我会觉得我没有受到应有的尊重，我会认为这家饭店为了省钱少请几个服务员，把麻烦推到顾客头上。况且这些服务员叫顾客自己扫码点菜，自己却站在那里不做事，这是什么鬼？我不想拿我的手机折腾，去做本来应该服务员为我做的事情。如果他坚持拒绝给我点单，我会站起来离开这家饭店，以后再也不来了。到处是有人为我服务的饭店，我为什么要来你这里呢？如果要求之后他帮我我点单，却拿起他的手持设备磨磨蹭蹭找不到菜，还叫我“等一下……”，我也会在心理上给这家饭店扣很多分，下次可能也不来了。&lt;/p&gt;

&lt;p&gt;所以你看，稍微好点的餐厅，服务员其实也是技术无法取代的工作。看过意大利电影《美丽人生》的人可能还记得，男主角去餐厅做服务员，年老的服务员在教他的时候说：“你是一个侍者，不是下人。服务是一门高度的艺术。上帝为人服务，但上帝不是下人。” 服务是一门艺术，有道理，所以它很难被技术取代。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.yinwang.org/images/life-is-beautiful-serving.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;我还曾经在门口的便利店遇到类似的情况。这便利店当时刚安装了自助扫码买单的机器。我拿着商品走到收款台，里面的服务员对我说：“那边机器上可以扫码买单。” 我走到机器那里，发现上面没有塑料袋可以装商品，所以又拿着东西走回柜台。里面的店员再次对我说：“跟你说了，那边机器上买单！” 我忽然就怒了，说：“你什么态度？叫你们经理过来！” 后来就再也不去这家店了。&lt;/p&gt;

&lt;p&gt;这个例子说明，超市便利店结账的自动化是可以做，能增加顾客的吞吐速度，大家都开心。但你也得小心，必须把每个细节都做对了，否则惹怒了顾客就不好了。有了自动收款台之后，商品价格并没有更便宜，顾客原来不需要自己动手，现在凭什么要自己动手扫码装袋，还得熟悉你们的系统界面？况且你的机器上塑料袋都没找到，也没人帮忙。有了机器，柜台里的服务员就可以不做事吗？所以人工服务必须能同时进行，不能拒绝顾客采用人工服务的要求。只有当他们意识到人多的时候用机器结账可能会快一些，自愿去用才是合理的。&lt;/p&gt;

&lt;p&gt;现在某些公司还设计了“无人酒店”，走进去都没人接待你，全自动的，装修也是科幻电影似的光秃秃。不觉得很无趣很不温馨吗？本来住酒店就少了家的感觉，现在连人都没有了。看似省了一些请服务人员的钱，而其实恐怕没法跟彬彬有礼人工服务的酒店竞争。&lt;/p&gt;

&lt;p&gt;所以我觉得，自动化做得过度了，甚至把自动化强加在顾客头上，会损失很多顾客，败给具有高素质人工服务的竞争者。随着人们越来越富裕，对效率要求没那么高，看过了世界很多地方的风土人情之后，对有文化有品质的服务要求越来越高，不喜欢自动化服务的顾客就会越来越多。所以即使要做自动化，也要选对合适的场景，考虑到人的心理需要，不然效果适得其反。&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-12-15-automation</guid>
<pubDate>Sun, 15 Dec 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>永恒</title>
<link>https://henix.github.io/feeds/yinwang/2019-11-05-timeless.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/11/05/timeless&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;永恒&lt;/h2&gt;
            &lt;p&gt;这个年代的人们经常使用“80后”，“90后”，“00后”这样的说法。也不知道这是从什么时候开始的，总之我记得在我小的时候是没有这些说法的，所以你可以想象我的年龄有多大了。虽说如此，我没有觉得自己老，也没希望自己年轻。因为对于我来说，年龄并不是那么让我紧张的事情。时间和经验让我懂得自己想要什么，让我积累自身的价值。&lt;/p&gt;

&lt;p&gt;“nn后”这些说法的流行，似乎说明我们下意识地认同了所谓的“年龄段”决定了一个人的性格和思想。似乎“00后”就一定得欣赏某种电影，听某些音乐，追捧某些明星，而“90后”，“80后”就会欣赏更加“老”一点的。很多人也认为年龄段决定了一个人的思维方式，价值观和做事方法。这就是所谓“成见”（stereotype）。&lt;/p&gt;

&lt;p&gt;经过很多事情之后，我发现这些所谓“nn后”的说法，往往是给人贴上标签，方便进行市场宣传。大部分人惧怕衰老，都想得到别人的喜爱，希望合群有朋友，所以商业宣传给事物打上年龄标签，就能吸引某些人群，甚至迫使很多人来追捧。如果你不认同这些事物，甚至没听说过，那你就老了，就落伍啦！&lt;/p&gt;

&lt;p&gt;比如把某个流行组合标记为“00后”，那么就显得非常年轻。有人要是对此风格有异议，比如觉得这些男明星太娘了，那么就只说明这些人老了，不属于“00后”。你是 80 后还是 70 后啊，居然不喜欢我们的小鲜肉？所以年龄标签可以保护商业操作的势力范围。&lt;/p&gt;

&lt;p&gt;年龄段的标签把某些事物显得“年轻”，而它们可能实质是没品位，短命，甚至愚蠢的。是的，我经常发现“年轻”的标签和愚蠢联系在一起。有些人想显得年轻，所以他选择了某个被称为低年龄段的事物，结果是显得没有品位，随波逐流。&lt;/p&gt;

&lt;p&gt;独立而优雅思考的人们，是不认这些“nn后”的年代标签的。他们只认同永不过时（timeless）的东西。由于他们有自己的品位和判断能力，他们可以吸收任何年代的优秀特征，而不会把自己局限于某个短暂时代的东西。不管是在衣着，艺术品位或者思想，他们都有自己的鉴别能力。&lt;/p&gt;

&lt;p&gt;这并不是说他们一定会选择“老”的事物而排斥新的，只是他们的标准不是事物出现的年代，不是它们是否当下流行，而是它们本身的风格和品质。也许某个今年出现的最新事物就满足他们的标准，那么他们也会毫不犹豫的拥抱它。他们选择的事物经常会成为经典，永恒的风格。&lt;/p&gt;

&lt;p&gt;不因为大家都追捧而去屈从大众的品位，不因为与众不同而害怕被人认为“老气”，由自己的美学和喜好而定，吸收天下一切可能的美好思想，这才是真正优雅而独立的人。这样的人面对岁月和年龄毫无畏惧。&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-11-05-timeless</guid>
<pubDate>Tue, 05 Nov 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>写在 1024 程序员节</title>
<link>https://henix.github.io/feeds/yinwang/2019-10-24-1024.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/10/24/1024&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;写在 1024 程序员节&lt;/h2&gt;
            &lt;p&gt;1024 程序员节，又一个程序员猝死。中国 IT 行业似乎以 996 加班著称，可是据我了解这不只是 IT 行业的问题，国内的其它各个行业也差不多的情况。我不得不深思 996 现象的起源，因为我发现在不提倡 996 的公司也有人自己 996，甚至促使同事一起加班，而这些都不是领导位置的人物，没有领导要求他们这样做，甚至对此完全不知情。&lt;/p&gt;

&lt;p&gt;我不是在为“公司的邪恶”开脱，制度化 996 的公司当然是邪恶的。我也不是针对这一次事件，我没有调查，也没有权利针对这个事情发言。我只是想提醒大家，996 是一种文化，它存在于很多中国人的心里。即使没有制度规定 996，也可能会不知不觉变成 996。开头是一小部分人，后来越来越多，形成整个公司的 996 文化。所以我在考虑的是一个更深层的问题，要是不喜欢加班的人建立一个新的公司，它要如何才能不被其他人拉下水，以至于同样落入 996 的圈套？这似乎不是一个意愿的问题，而是一个方法的问题。&lt;/p&gt;

&lt;p&gt;996 的心理来源，我觉得至少有一部分是很多人“挣表现”的心理。很多国人在公司里喜欢显得自己很勤快，做事麻利，不犯错误，这样上司就会赏识我，我就有晋级加薪的机会。这种心理来自于中国从小的教育，很多中国小孩从小养成的心理就是让父母开心，让老师开心，这样就会受到表彰和奖励。很少有人从小就有独立的思想，把自己和来自父母老师的奖赏分离开来，为自己考虑。&lt;/p&gt;

&lt;p&gt;很少有人考虑过自己的付出和回报的比例，也就是“小时工资”。他们只看到每个月的收入，却没算过除以工作的时间之后，每个小时的收入是多少。甚至有人在晚上或者周末加班到半夜，第二天早上还要早起按时去公司打卡，羞于向领导请求晚到或者休假。本来是理所当然的事情，却怕伤害到自己在领导心中的“表现”。甚至有些公司的员工形成一种“不用年假”的集体行为。本来公司制度给了一年这么多天的年假，可是所有人都不用年假。大家都觉得要是别人不用年假，而自己用了，那么领导就会更加器重其他人，觉得自己贪玩，不用功奋斗。&lt;/p&gt;

&lt;p&gt;我觉得这就是中国的文化意识导致的。在美国或者欧洲国家，这种显得勤快奋斗加班，不用年假的人，会因此受到上司的赏识吗？不会的。如果你需要用额外的时间，甚至牺牲年假来给公司做事，别人只会觉得你这个人很笨，以至于需要额外的时间。或者打心眼里瞧不起你，觉得你是弱国来的打工仔，居然不会享受自己的时间。所以你受到赏识，晋升的机会反而变小了。越是自信，按时休息，或者偶尔加班之后要求换休晚到的人，越是会让人觉得有能力，有思想，有尊严，从而受到尊敬和提拔。&lt;/p&gt;

&lt;p&gt;我看人也是一样的方式。我有一个很好的理发师，不但每次剪出来的效果很好，而且他周末是不上班的。工作日上午 10:30 上班，下午准时 6:30 下班。如果要晚上找他理发也行，但得提前两天预约。当然如果他晚上工作了，可能第二天就会晚到。这个人在那个理发店里是很受尊敬的，所有其他理发师都尊敬他，虽然他在那里并不是级别最高的。这也许出乎有些人的意料，但中国人的心理跟外国人的构造并没有不同。你越是过分在乎工作，别人就越觉得你地位低。你有自己的尊严和规则，你有自己的生活，自己的思想，别人就越是尊重你。&lt;/p&gt;

&lt;p&gt;所以改变 996 的现象，我觉得应该从每个人的行为开始。我们应该改变从小给家长老师表现，争做好学生拿小红花的心理，真正长大成为受人尊敬的成年人。从今天开始，你应该勇于提出自己正当的需要，需要休息的时候就休息。&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-10-24-1024</guid>
<pubDate>Thu, 24 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>免费食物不是好事</title>
<link>https://henix.github.io/feeds/yinwang/2019-10-17-free-food.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/10/17/free-food&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;免费食物不是好事&lt;/h2&gt;
            &lt;p&gt;很多大互联网公司（Google，Facebook 之类的）都提供免费的三餐，饮料和零食。到后来很多创业公司也开始效仿，以至于“免费午餐”成了一种风气，到现在已经不是什么新鲜事了。“传统”一点的 IT 公司（微软，Intel 等）的某些员工也对此表示羡慕，希望自己的公司也有免费的三餐，饮料和零食。他们没有看明白，完全免费的食物可能并不是什么好事。这个看似简单的问题，我经历了很多年才看明白其中的奥妙。&lt;/p&gt;

&lt;p&gt;首先在心理上，免费食物容易造成一种“我欠公司人情”的感觉。不知不觉的，很多年轻人就被洗脑了，觉得自己白吃了公司那么多东西，就该为公司多做“贡献”，从而付出比本来多的劳动，或者为公司高唱赞歌，显示出一副跪舔的姿态。有些人为了免费的晚餐会待到比较晚的时候，也没其他事干，就不知不觉多做工作，公司就因此赚了一把。&lt;/p&gt;

&lt;p&gt;因为闲得无聊多做点工作对于年轻员工也许不是坏事，但免费食物对于人的心理有很奇怪的作用。当年 Google 的免费三餐仍然是新鲜事的时候，很多 Google 员工打心眼里觉得外面的人都想去 Google 吃“free lunch”，显示出一种莫名的自豪感。外面的人总是听说 Google 有免费三餐，还请了大厨，也被洗脑了。总是有人两眼放光地说：“我也好想去 Google 吃免费大餐啊！” 让人很无语。&lt;/p&gt;

&lt;p&gt;所以大互联网公司提供免费三餐是有目的的，而且产生了一些意想不到的心理效果。反之如果食物不免费，员工就可以在心理上不欠公司什么：“这是我出钱买了的。” 这样他们就不会出现过度付出，或者对公司跪舔，过度自豪的现象。&lt;/p&gt;

&lt;p&gt;自从 Google 提出 free food 这个概念，导致了社会文化的低俗化，甚至传染到大学里去了。我在 Cornell 的时候，学校里的各种活动似乎也受了 Google 理念的影响，很多讲座一类的活动都提供免费食物。当然，学校里的免费食物就是 pizza 一类的而已，可是仍然引得很多人过度兴奋：“Go! Free food!” 甚至有学生做了一个“free food 搜索引擎”，列出学校里每天所有免费食物的时间和地点……&lt;/p&gt;

&lt;p&gt;我开头还没发现这有什么问题，直到有天一个巴西来的同学对我说：“我不明白这些人为什么对 free food 如此兴奋，跟傻子似的！谁稀罕他们的 pizza 啊？” 我才开始意识到，免费食物带来的是低级的文化。&lt;/p&gt;

&lt;p&gt;我不得不说，微软，Intel 一类的老牌公司的跪舔现象确实少很多。员工都更加成熟和独立，工作就是工作，很少有过度兴奋和“我属于这个公司”的集体自豪感。我觉得这与他们不提供免费食物有一定的关系。&lt;/p&gt;

&lt;p&gt;集体自豪感和集体主义并不是什么好东西，我们应该避免这种心理。每个人都应该保持自己心理的独立。传统公司的食物虽然收费，价格却比外面便宜很多，基本只是收回成本。餐费几乎可以忽略不计，可是带来的心理效果却是与完全免费很不一样的。没人会谈论公司的食物这个事，没人为此引以为豪，只是不好吃的时候会骂两句而已。很奇怪，这似乎帮助了员工保持心理上的独立。&lt;/p&gt;

&lt;p&gt;完全免费的食物容易被低素质人群占便宜，导致其它人没得吃。你可能以为 Google，Facebook 员工素质那么高，不可能过度利用公司的免费食物。可惜林子大了什么鸟都有。我听说 Google 总部曾经持续出现这样的情况，每当快到周末的时候，架子上的零食会被某些人拿背包全部收走带回家，囤起来给自己家里人用。甚至有 Facebook 员工收费带外面不认识的人去公司食堂吃饭，以此来赚外快，结果被发现开除，成为一时的新闻。另外 Google 还提供免费自助洗衣服务，导致很多人为了省水电费把家里衣服打包带到公司去洗…… 你说猥不猥琐？&lt;/p&gt;

&lt;p&gt;经济是小事，但这种现象降低了整个公司的文化品位。在免费食物的诱惑之下，爱占小便宜的人在各种地方显示他们的存在，就让公司显得很低级，怎么感觉身边都是难民似的，没见过吃的东西吗？要是食物不免费，而是收取基本的费用，就可以自然而然通过经济学原理防止占便宜。同时公司文化也能保持尊严，更加文明和互相尊重。&lt;/p&gt;

&lt;p&gt;为了避免有人占小便宜，Google 员工要带别人去公司吃饭，都是有配额有记录的。我当年在 Google 时的规矩是一个月只能带四人次去公司吃饭。这会出现什么问题呢？有一天我带了 5 个微软研究院的朋友去参观 Google，到了午饭时间门卫硬是不让我带他们进餐厅吃饭，说一个月只能带 4 个人吃饭，搞得很尴尬。看到问题了吗？如果食物不是免费的，我就可以随便带几个人吃饭，而不会出现这种尴尬的局面。我出了钱买的饭菜，不用别人来管 :)&lt;/p&gt;

&lt;p&gt;完全免费的食物还有营养方面的问题。一方面，员工可能吃太多零食和饮料而变胖，导致各种健康问题。另外，它限制了你吃到更好品质的食物。Google 的午餐也许够好了，可是有些人就是想要更好更健康的。然而因为是免费的，公司肯定会控制食物的成本，所以本来你花点钱就可以买到很好的东西，可是因为公司要控制成本，就没法给你更好的。免费的好东西（寿司一类的）总是很多人想要，所以食堂里最好的食物总是很快就没了，你晚一点去就吃不到。因为公司提供三餐，所以外面街上的餐厅都倒闭或者搬走了，你走出去也没什么好吃的。结果免费的公司午餐，导致了你要吃顿更好的饭菜需要跑很远。&lt;/p&gt;

&lt;p&gt;如果食物不免费，公司就可以用实惠的价格出售很好的东西给你，而且好东西稍微贵一些，就不会那么快被人拿完。某些公司甚至会在食堂里设置一个柜台，让外面的饭店也可以在里面以平价卖他们的食物。每天换一家饭店在那个窗口，所以你就有更多的选择。这会给很多人带来方便，多样和健康的生活。&lt;/p&gt;

&lt;p&gt;另外，公司收费的午餐平衡了公司餐厅与外面餐厅的利益关系。吃饭都要花钱的，所以员工有时候也会去外面吃饭。这就让外面的餐厅不至于因为这公司的存在而倒闭。社区和街道也因为公司员工出去吃饭而更加繁荣有趣，不会变的死气沉沉。&lt;/p&gt;

&lt;p&gt;所以呢，公司提供完全免费的食物并不是什么好事，它导致了各种不好的心理状态和不好解决的问题。提供平价而不是完全免费的食物，自动避免了这些问题，其实是更好的做法。&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-10-17-free-food</guid>
<pubDate>Thu, 17 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>欢迎关注我的微博</title>
<link>https://henix.github.io/feeds/yinwang/2019-10-11-weibo.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/10/11/weibo&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;欢迎关注我的微博&lt;/h2&gt;
            &lt;p&gt;经过一段时间的考虑和试用之后，我决定重新开始使用微博。这个博客仍然是我主要的表达思想的地方，但由于微博的形式更加灵活方便，会被用来发布比较短，有时效性的想法。&lt;/p&gt;

&lt;p&gt;我的微博地址是：&lt;a href=&quot;https://www.weibo.com/u/6347862377&quot;&gt;https://www.weibo.com/u/6347862377&lt;/a&gt;，你也可以在这个博客的右上方找到微博链接。欢迎关注。&lt;/p&gt;

&lt;p&gt;回国经过这么长时间的体验之后，我发现国内靠谱的声音还是太少了，不论是在技术领域还是在社会认知上，都有很多不足。中国其实仍然很落后，不只是在物质上，建设上，技术上，而且在社会心理上，有很多地方落后于发达国家。长期的自卑心理导致了自傲，很多人不再能看到我们的缺点，因为好像“有钱了”而自我膨胀，停滞不前。&lt;/p&gt;

&lt;p&gt;在很多国人心里，钱就是发言权，各种盲从和浮夸的风气盛行，价值观颠倒。许多有着千万粉丝的名人大腕，面对各种事情却说不出正确导向的话来。所以我觉得我应该影响更多的人，为改善社会尽自己的一份力，我这样的人应该有更多的“粉丝”。这就是为什么我重新打开微博这个重要的信息渠道，让那些对我的想法感兴趣的人可以关注。&lt;/p&gt;

&lt;p&gt;我知道很多支持我的人不用微博，已经几次为我重新打开微博。我恐怕得再次麻烦他们打开微博了。其实说实话，如果你跳出技术的圈子，微博上还是有蛮多有趣有价值的内容的，重要的是要会选择。&lt;/p&gt;

&lt;p&gt;为了维护健康的环境，避免评论里出现冗长或者恶意的争论，我不开放微博的评论功能。如果你需要表达自己的看法，可以在转发之后在自己的空间进行评论。我想这并不损害任何人的言论自由。&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-10-11-weibo</guid>
<pubDate>Fri, 11 Oct 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>自动驾驶车的责任和风险分析</title>
<link>https://henix.github.io/feeds/yinwang/2019-09-30-autopilot-responsibility.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/09/30/autopilot-responsibility&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;自动驾驶车的责任和风险分析&lt;/h2&gt;
            &lt;p&gt;说到“自动驾驶”，人们最熟悉的名字恐怕是 Tesla 的 Elon Musk 先生了。他总是对 Tesla 的 Autopilot 进行各种夸大宣传，让人误解 Autopilot 的能力。Autopilot 引起车祸死了人之后，Musk 先生总是在网上发话扭曲人们的逻辑，抓住“车主没有及时接管”等各种借口，逃避对事故的责任。&lt;/p&gt;

&lt;p&gt;很多人对他的言论感到荒谬和愤怒，却又难以说清楚他到底哪里错了，甚至政府监管机构都对各自动驾驶公司的歪理无能为力。我发现对于自动驾驶车的责任和风险问题，人们仍然缺乏一个精确的，使人信服的说法，所以我一直在思索这些问题。&lt;/p&gt;

&lt;p&gt;在本文里，我试图使用&lt;strong&gt;逻辑和概率&lt;/strong&gt;的工具来分析自动驾驶车的责任和风险问题。虽然我用的数学可能不是那么的细节到位，但是你可以从中获得分析这类问题的思路。逻辑推理和概率分析不仅可以用于科学研究，而且可以用于法律和各种社会现象。&lt;/p&gt;

&lt;p&gt;根据对 Elon Musk 的&lt;a href=&quot;https://youtu.be/dEv99vxKjVI&quot;&gt;采访&lt;/a&gt;，你可以看出他的言论大体包含以下内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;全自动驾驶很快就要实现了，Autopilot 的视觉识别能力成指数增长，所以实现全自动驾驶就在眼前。&lt;/li&gt;
  &lt;li&gt;现在出产的 Tesla 车已经安装了具有“全自动驾驶能力”（FSD）的硬件。只要我们在不久的将来更新车里的软件，你就能拥有全自动驾驶的车，所以现在买 Tesla 的车是一种升值的财富，而不是贬值的物品。&lt;/li&gt;
  &lt;li&gt;统计数字显示，Autopilot 的事故率远远低于人类驾驶员。Autopilot 比人类驾驶员安全很多，这是不可争辩的事实。如果你否认这个事实，你就是危害公共安全。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/musk-safter-than-human.jpg&quot; width=&quot;80%&quot;&gt;&lt;/p&gt;

&lt;p&gt;虽然各种证据都说明 Autopilot 几乎没有自动驾驶能力，Elon Musk 却仍然在宣扬这些歪理。很多书呆子极客会听信他的“事故率”，为他所谓的“高科技”欢呼，甚至有人跟风说“Autopilot 比人类驾驶员安全 6 倍”。这些人都不明白，统计数字对于事故责任分析，对于 Autopilot 的风险评估都是没用的，而且他们的统计方法，解释统计数字的方式都是错误的。&lt;/p&gt;

&lt;p&gt;在本文中我想说明以下几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;大范围的统计数字对于“责任”和“风险”的分析是没有任何关系的。&lt;/li&gt;
  &lt;li&gt;Autopilot 导致的任何一次车祸，Tesla 公司在法律上都是负有责任的。&lt;/li&gt;
  &lt;li&gt;要求驾驶员“随时接管”是推脱责任的手段，根本不符合法理。&lt;/li&gt;
  &lt;li&gt;自动驾驶行业对于车祸死亡率的数据解释是片面而错误的。由车祸引起的死亡，相对其它死亡因素并不是特别严重的问题。自动驾驶技术并不能降低车祸死亡率。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;责任&quot;&gt;责任&lt;/h3&gt;

&lt;p&gt;Elon Musk 和其他很多自动驾驶公司都喜欢拿“事故率”说事，总是说自动驾驶比人类驾驶员安全，因为统计数字显示它们的事故率低，其实那相当于在说：“我活了这么久，为这么多客户服务，没杀过其中任何一个人，我杀人的概率非常低，低于全国的谋杀犯罪率，所以我现在杀了你没什么大不了的。”&lt;/p&gt;

&lt;p&gt;先不说 Autopilot 的事故率是否真的那么低。即使它事故率是很低，难道弄死了人就可以不负责，甚至不受谴责吗？&lt;/p&gt;

&lt;p&gt;到底 Tesla 有没有责任，我们可以使用逻辑学的因果关系“反事实分析”（counterfactual analysis）。假设驾驶员没有使用 Autopilot 而是自己开车，那么这次事故还会不会发生？如果不会发生，那么我们得到因果关系：Autopilot 导致了事故。不管其他人用 Autopilot 有没有出事故，事故占多大比例，面对这里的因果关系都是无关紧要的。因果关系等于责任。&lt;/p&gt;

&lt;p&gt;如果是 Autopilot 导致了事故，即使总共只发生了一次事故，都该它的设计者 Tesla 公司负责。很多人都是混淆了“责任”和“事故率”，所以才会继续支持 Elon Musk 和 Tesla 的谬论。有些人以为“自动驾驶可能会降低全国的车祸率”，从而认为 Autopilot 引起少数几次车祸问题不大，而不明白“事故率”跟“责任”和“事故再次发生的风险”，完全是两码事。&lt;/p&gt;

&lt;p&gt;另外，如果你看透了这些吹嘘得神乎其神的“&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/09/14/machine-vs-human&quot;&gt;机器的视觉能力&lt;/a&gt;”有多假，就会知道“自动驾驶会降低车祸率”这个说法根本就不可能实现。&lt;/p&gt;

&lt;p&gt;为什么我强调“责任”呢？因为人如果自己开车，不小心出了车祸伤到自己，他自己是可以接受的，因为是自己的责任。然而要是 Autopilot 判断错误引起车祸，撞伤了自己，对于车主来说这就是不可接受的，必然要追究 Tesla 公司的责任。&lt;/p&gt;

&lt;p&gt;任何人都明白这个道理吧？这就跟自己开车不小心受了伤，和出租车司机不小心导致你受伤的差别一样。你会告那个出租车司机，你却不会上法庭告自己。简单吧？&lt;/p&gt;

&lt;p&gt;每一次 Autopilot 相关的事故，Tesla 公司都会在事后散布新闻说是驾驶员开车不认真，手没有在方向盘上准备“随时接管”，所以不是 Autopilot 的责任。驾驶员是否认真在开车，人死了无所对证，但这些全都成为了 Tesla 公司推脱责任的借口。&lt;/p&gt;

&lt;p&gt;如果发现 Autopilot 判断失误，你真来得及接管吗，你能在那么短的时间内做出正确的反应吗？就算你双手都在方向盘上，车到了离障碍物多近的地方不减速，你才会意识到它出错了，决定接管呢？恐怕到了自己接管的时候就已经晚了。所以要求车主随时接管，根本就不是一个合理的要求，不应该作为 Tesla 免责的理由。&lt;/p&gt;

&lt;h3 id=&quot;autopilot-的个人风险分析&quot;&gt;Autopilot 的个人风险分析&lt;/h3&gt;

&lt;p&gt;为什么每年几万起其它车祸没什么人关心，而 Autopilot 引起一两次车祸就这么多新闻舆论呢？因为要是车祸是由于 Autopilot 引起的，那么同样的车祸就可能发生在所有使用 Autopilot 的 Tesla 车主身上，“Autopilot 再次发生车祸”的后验概率就会大大提高。Autopilot 导致自己伤亡的风险就很高了。&lt;/p&gt;

&lt;p&gt;这里的核心问题就在于，到底是人开车还是 Autopilot 开车。人和软件不仅在技术能力上有很大差别，对于概率风险分析，人和软件的效果也是很不一样的。简言之，人是“独立随机变量”，而 Autopilot 不是独立变量。&lt;/p&gt;

&lt;p&gt;每个人都是不一样的，是独立的个体。有的人开车很稳，有的人开车一般，而少数人很鲁莽。这些人之间没有必然的联系，是“独立随机变量”。什么叫“独立”呢？意思是某个人自己开车不小心出车祸，其他人并不一定会出同样的车祸，因为每个人的开车方式都不一样。在概率论里面，这些人是否出现车祸完全是独立的事件。&lt;/p&gt;

&lt;p&gt;而 Autopilot 是一个软件系统，所有安装 Autopilot 的车都有一模一样的行为方式，所以使用 Autopilot 的许多 Tesla 车不是独立变量，而是“相关变量”，它们通过 Autopilot 系统的设计关联在了一起。如果 Autopilot 因为判断错误导致一次车祸，那么所有使用 Autopilot 的车都很可能发生同样的车祸。&lt;/p&gt;

&lt;p&gt;相应的随机变量是否“独立”，导致了人类驾驶员与 Autopilot 出现一次事故的风险分析完全不一样。&lt;/p&gt;

&lt;p&gt;如果你学过概率论，那么 Autopilot 车主出事的“后验概率”（&lt;a href=&quot;https://en.wikipedia.org/wiki/Posterior_probability&quot;&gt;posterior probability&lt;/a&gt;）会因为“Autopilot 引起一次车祸”的发生而大幅度提高，而如果是人开的汽车，那么它的后验概率基本不会因为另外一辆同型号车出事而提高。写成数学公式就是：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;P(其它 Autopilot 出车祸 | Autopilot 引起一次车祸)&lt;/p&gt;

  &lt;p&gt;远大于&lt;/p&gt;

  &lt;p&gt;P(其它非自动车出车祸 | 一辆非自动车出车祸)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;事故起因的随机性不同，后验概率也就随之不同。&lt;/p&gt;

&lt;p&gt;面对“Autopilot 有一定概率会要了你的命”这一事实，不管 Autopilot 的总体事故率有多低，甚至像 Elon Musk 说的低于全国车祸率，对于 Tesla 车主来说都是毫无意义的。一是因为“责任”：车主可以允许自己要了自己的命，却不允许 Autopilot 或者其他人要了自己的命，更不允许是因为别人（Autopilot）的愚蠢而要了自己的命。二是因为“个人风险”：不管全国的事故率是多少，自己开车的风险一般只跟自己开车的小心程度有关，也就是说自己开车出事的概率基本是独立于全国事故率的。而使用 Autopilot，自己的风险就受到 Autopilot 能力的影响，跟 Autopilot 的平均事故率差不多了。&lt;/p&gt;

&lt;h3 id=&quot;仔细看看统计数字&quot;&gt;仔细看看统计数字&lt;/h3&gt;

&lt;p&gt;Autopilot 的事故率真的低吗？你可以自己研究一下。如果你算对了数学，恐怕它的事故率并不低。举一个例子，普通人只计算了事故的数目与 Autopilot 导航的总里程的比例，却忽视了那些由于驾驶员及时接管而避免了的事故的数目。&lt;/p&gt;

&lt;p&gt;Autopilot 能不受打断的连续驾驶多少里程呢？按照现有的视觉技术，恐怕不会很远。聪明点的人都不会让 Autopilot 进入稍微复杂的局面，只用它进行“高速车道控制”，所以 Autopilot 事故率比较低的原因，很可能是因为大部分用户根本不在复杂的情况下使用它。所以虽然 Autopilot 统计数据看起来是“几十亿英里”，恐怕它从来没有在复杂的情况下做出过正确的反应。&lt;/p&gt;

&lt;p&gt;另外 Tesla 属于比较贵的车，买车的人属于对自己比较负责的人，所以事故率不应该跟所有车比，而应该跟同样年代的奔驰，保时捷之类的车比。&lt;/p&gt;

&lt;p&gt;我们来仔细看看汽车业的总体统计数字吧。美国 &lt;a href=&quot;https://en.wikipedia.org/wiki/Motor_vehicle_fatality_rate_in_U.S._by_year&quot;&gt;2017 年车祸死亡人数&lt;/a&gt;是 3.7 万人。看上去很多，可是按里程数的死亡率，每一亿英里平均只有 1.16 人。从 1975 年到 2017 年，每一亿英里死亡人数从 3.35 人降低到了 1.16 人，所以即使没有 Autopilot，开车也是越来越安全了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/car-accident-death-rate.jpg&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;

&lt;p&gt;对比一下&lt;a href=&quot;https://www.cdc.gov/nchs/fastats/deaths.htm&quot;&gt;其它死因&lt;/a&gt;吧。美国 2017 年总共死亡 281 万人，其中因心脏病死亡 64.7 万人，癌症 59.9 万，呼吸道疾病 16 万，中风 14.6 万，意外伤害死亡 16.9 万（包括车祸），糖尿病 8.3 万，流感 5.5 万，自杀 4.7 万。&lt;/p&gt;

&lt;p&gt;意外伤害死亡的 16.9 万里面包括了车祸的 3.7 万，所以另外 13.2 万人死于其它的意外。连自杀都有 4.7 万人。所以你可能意识到了，车祸死亡 3.7 万人并不是一个那么可怕的数字，而是相对来说最安全的领域之一了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/death-rate-2017.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;你知道车祸死掉的都是什么人吗？他们是怎么开的车？只要自己小心开车，我不觉得自己的风险会有那么高，可能比自杀的概率都要小。&lt;/p&gt;

&lt;p&gt;Elon Musk 在&lt;a href=&quot;https://youtu.be/dEv99vxKjVI&quot;&gt;采访&lt;/a&gt;中把汽车叫做“two-ton death machine”（两吨重的死亡机器），甚至说“难以置信我们居然允许人开车”，根本就是危言耸听。盲目的强调车祸死亡人数，号称可以降低事故率，就是自动驾驶领域常见的幌子。他们解决的并不是一个那么重要的问题，而且解决的方法根本就是&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/09/14/machine-vs-human&quot;&gt;不切实际的忽悠&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/musk-allowed-to-drive.jpg&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;

&lt;p&gt;所以 Tesla 不但在技术上无法实现自动驾驶，而且人品和诚信都很成问题。我还没有见过一个汽车公司如此急于推脱责任的，一般都是积极配合调查，勇于承担责任，及时整改，这样才可能得到公众的信任。&lt;/p&gt;

        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-09-30-autopilot-responsibility</guid>
<pubDate>Mon, 30 Sep 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>机器与人类视觉能力的差距（3）</title>
<link>https://henix.github.io/feeds/yinwang/2019-09-16-machine-vs-human-3.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/09/16/machine-vs-human-3&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;机器与人类视觉能力的差距（3）&lt;/h2&gt;
            &lt;blockquote&gt;
  &lt;p&gt;本文属于个人观点，跟本人在职公司的立场无关。由于最近 GitHub 服务器在国内访问速度严重变慢，虽然经过大幅度压缩尺寸，文中的图片仍然可能需要比较长时间才能加载。这篇文章揭示了 AI 领域重要的谬误和不实宣传，为了阻止愚昧的蔓延，我鼓励大家转发这篇文章和它的后续，转发时只需要注明作者和出处就行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这是这个系列文章的第三集，在这一集中，我想讲讲 AI 领域所谓的“超人类识别率”是怎么来的，以及由于对机器视觉的盲目信任所导致的灾难性后果。&lt;/p&gt;

&lt;h3 id=&quot;超人类准确率的迷雾&quot;&gt;“超人类准确率”的迷雾&lt;/h3&gt;

&lt;p&gt;我发现神经网络在测试数据的可靠性，准确率的计算方法上，都有严重的问题。&lt;/p&gt;

&lt;p&gt;神经网络进行图像识别，所谓“准确率”并不是通过实际数据测出来的，而是早就存在那里的，专用的测试数据。比如 ImageNet 里面有 120 万张图片，是从 Flickr 等照片网站下载过来的。反反复复都是那些，所以实际的准确率和识别效果值得怀疑。数据全都是网络上的照片，但网络上数据肯定是不全面的，拍照的角度和光线都无法概括现实的多样性。而且不管是训练还是测试的数据，他们选择的都是在理想环境下的照片，没有考虑各种自然现象：反光，折射，阴影等。&lt;/p&gt;

&lt;p&gt;比如下图就是图像识别常用的 ImageNet 和其它几个数据集的一小部分。你可以看到它们几乎全都是光线充足情况下拍的照片，训练和测试用的都是这样的照片，所以遇到现实的场景，光线不充足或者有阴影，准确率很可能就没有 paper 上那么高了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/imagenet-data.jpg&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;

&lt;p&gt;如此衡量“准确率”，有点像你做个编译器，却只针对很小一个 benchmark 进行优化跑分。一旦遇到实际的代码，别人可能就发现性能不行。但神经网络训练需要的硬件等条件比较昂贵，一般人可能也很少有机会进行完整的模型训练和实际的测试，所以大家只有任凭业内人士说“超人类准确率”，却无法验证它的实际效果。&lt;/p&gt;

&lt;h3 id=&quot;蹊跷的top-5-准确率&quot;&gt;蹊跷的“top-5 准确率”&lt;/h3&gt;

&lt;p&gt;不但测试数据的“通用性”值得怀疑，所谓“准确率”的计算标准也来的蹊跷。AI 领域向公众宣扬神经网络准确率的时候，总喜欢暗地里使用所谓“top-5 准确率”，也就是说每张图片给 5 次机会分类，只要其中一个对了就算正确，然后计算准确率。依据 top-5 准确率，他们得出的结论是，某些神经网络模型已经“超越了人类”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/top-5-error.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;

&lt;p&gt;如果他们提到“top-5”还算好的了，大部分时候他们只说“准确率”，而不提“top-5”几个字。在跟人比较的时候，总是说“超越了人类”，而绝口不提“top-5”，不解释是按照什么标准。我为什么对 top-5 有如此强烈的异议呢？现在我来解释一下。&lt;/p&gt;

&lt;p&gt;具体一点，“top-5”是什么意思呢？也就是说对于一张图片，你可以给出 5 个可能的分类，只要其中一个对了就算分类正确。比如图片上本来是汽车，我看到图片，说：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;“那是苹果？”&lt;/li&gt;
  &lt;li&gt;“哦不对，是杯子？”&lt;/li&gt;
  &lt;li&gt;“还是不对，那是马？”&lt;/li&gt;
  &lt;li&gt;“还是不对，所以是手机？”&lt;/li&gt;
  &lt;li&gt;“居然还是不对，那我最后猜它是汽车！”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;五次机会，我说出 5 个风马不及的词，其中一个对了，所以算我分类正确。荒谬吧？这样继续，给很多图片分类，然后统计你的“正确率”。&lt;/p&gt;

&lt;p&gt;为什么要给 5 次机会呢？ImageNet 比赛（&lt;a href=&quot;http://image-net.org/challenges/LSVRC/2015&quot;&gt;ILSVRC&lt;/a&gt;）对两种不同的比赛给出了两种不大一样的说法。一种说是为了让机器可以识别出图片上的多个物体，而不因为其中某个识别出的物体不是正确标签（ground truth）而被算作错误。另外一种说是为了避免输出意义相同的近义词，却不能完全匹配标签而被算作错误。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/top5-definition.jpg&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;

&lt;p&gt;看似合理？然而这却是模糊而错误的标准。这使得神经网络可以给出像上面那样风马不及的 5 个标签（苹果，杯子，马，手机，汽车），其中前四个都不是图片上的物体，却仍然被判为识别正确。&lt;/p&gt;

&lt;p&gt;不管你给出的其他四个分类有多离谱，只要你有一个对了就算分类正确，所以 top-5 准确率总是比 top-1 高很多。高多少呢？比如 ResNet-50 的 top-1 准确率只有 77.1%，而 top-5 准确率却有 93.3%。Top-1 准确率只能算“勉强能用”，换成 top-5 之后，忽然就可以宣称“超越人类”了。&lt;/p&gt;

&lt;p&gt;Kaggle（一个进行数据科学比赛的网站）在&lt;a href=&quot;https://www.kaggle.com/c/imagenet-object-localization-challenge&quot;&gt;对 ImageNet 的说明&lt;/a&gt;里提到，2010 至 2014 年期间，图像分类的错误率从 28.2% 下降到 6.7%，却也决口不提 top-5 这个字。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/kaggle-imagenet.jpg&quot; width=&quot;80%&quot;&gt;&lt;/p&gt;

&lt;p&gt;可能很多人还没意识到，top-5 比较方法对人是不公平的。图片上要是人见过的物体，几乎总是一次就能做对，根本不需要 5 次机会。使用“top-5 准确率”，就像考试的时候给差等生和优等生各自 5 次机会来做对题目。当然，这样你就分不清谁是差等生，谁是优等生了。“top-5 准确率”大大的模糊了好与坏之间的界线，最后看起来都差不多了，甚至差等生显得比优等生还要好。&lt;/p&gt;

&lt;p&gt;具体一点。假设一个人识别那些图片的时候，他的 top-5 错误率是 5.1% （就像他们给出的数字那样），那么他的 top-1 错误率大概也是 5.1%。因为人要是一次机会做不对，那他可能根本就没见过图片上的物体。如果他一次做不对，你给他 5 次机会，他也做不对，因为他根本就不知道那东西叫什么名字。&lt;/p&gt;

&lt;p&gt;现在某个神经网络的 top-5 错误率是 4.94%，它的 top-1 错误率是 20% 以上。你却只根据 top-5 得出结论，说神经网络超越了人类。是不是很荒谬？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/human-top5.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;退一万步讲，就算你可以用  top-5，像这种 4.94% 与 5.1% 的差别，也应该是忽略不计的。因为实验都是有误差，有随机性的，根据测试数据的不同也有差异，像这样的实验，0.2% 的差别根本不能说明问题。如果你仔细观察各个文献列出来识别率，就会发现它们列出的数字都不大一样。同样的模型，准确率差距可以有 3% 以上。但他们拿神经网络跟人比，却总是拿神经网络最好的那个数，跟人死扣那百分之零点几的“优势”，然后欢天喜地宣称已经“超人类”了。&lt;/p&gt;

&lt;p&gt;而且他们真的拿人做过公平的实验吗？为什么从来没有发布过“神经网络 vs 人类 top-1 对比结果”呢？5.1% 的“人类 top-5 准确率”数字是哪里来的呢？哪些人参加了这个测试，他们都是什么人？我唯一看到对人类表现的描述，是在 Andrej Karpathy 的主页上。他拿 ImageNet 测试了自己的识别准确率，发现好多东西根本没见过，不认识，所以他又看 ImageNet 的图片“训练”自己，再次进行测试，结果准确率大大提高。&lt;/p&gt;

&lt;p&gt;就那么一个人得出的“准确率”，就能代表全人类吗？而且你们知道 Andrej Karpathy 是谁吧。他是李飞飞的学生，目前是 Tesla 的 AI 主管，而李飞飞是 ImageNet 的发起者和创造者。让一个“内幕人士”拿自己来测试，这不像是公正和科学的实验方法。你见过有医学家，心理学家拿自己做个实验，就发表结果的吗？第一，人数太少，至少应该有几十个智商正常的人来做这个，然后数据平均一下吧？第二，这个人是个内幕人士，他的表现恐怕不具有客观性。&lt;/p&gt;

&lt;p&gt;别误会了，我并不否认 Andrej Karpathy 是个很聪明，说话挺耿直的人。我很欣赏他讲的斯坦福 cs231n 课程，通过他的讲述我第一次明白了神经网络到底是什么，明白了 back-propagation 到底如何工作。我也感谢李飞飞准备了这门课，并且把它无私地放在网上。但是这么大一个领域，这么多人，要提出“超越了人类视觉”这么大一个口号，居然只有研究者自己一个人挺身而出做了实验，你不觉得这有点不负责任吗？&lt;/p&gt;

&lt;p&gt;AI 领域对神经网络训练进行各种优化，甚至专门针对 top-5 进行优化，把机器的每一点性能每一点精度都想榨干了去，对于如何让人准确显示自己的识别能力，却漫不经心，没有组织过可靠的实验，准确率数字都不知道是怎么来的。对比一下生物，神经科学，医学，这些领域是如何拿人做实验，如何向大家汇报结果，AI 领域的做法像是科学的吗？&lt;/p&gt;

&lt;p&gt;这就是“AI 图像识别超越人类”这种说法来的来源。AI 业界所谓“超人类的识别率”，“90+% 的准确率”，全都是用“top-5 准确率”为标准的，而且用来比较的人类识别率的数字没有可靠的来源。等你用“top-1 准确率”来衡量它们，使用客观公正抽选的人类实验者的时候，恐怕就会发现机器的准确率远远不如人类。&lt;/p&gt;

&lt;p&gt;这么基础而重要的问题，AI 业界的解决方案如此幼稚，却被全世界研究者广泛接受。你们不觉得蹊跷吗？我觉得他们有自己的目的：top-5 使得神经网络的准确率显得很高，只有使用这个标准，神经网络才会看起来“超越了人类”。&lt;/p&gt;

&lt;h3 id=&quot;尴尬的-top-1-准确率&quot;&gt;尴尬的 top-1 准确率&lt;/h3&gt;

&lt;p&gt;我们来看看 top-1 准确率吧。业界最先进的模型之一 ResNet-152 的 top-1 准确率只有 77.6%。2017 年的 ImageNet 分类冠军 &lt;a href=&quot;https://github.com/hujie-frank/SENet&quot;&gt;SENet-154&lt;/a&gt;，top-1 准确率也只有 81.32%。当然这也没有考虑过任何实际的光线，阴影和扭曲问题，只是拿标准的，理想情况的 ImageNet “测试图片”来进行。遇到实际的情况，准确率肯定会更低。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/vision-accuracy.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;神经网络要想提高 top-1 准确率已经非常困难了，都在 80% 左右徘徊。有些算法工程师告诉我，识别率好像已经到了瓶颈，扩大模型的规模才能提高一点点。可是更大的模型具有更多的参数，也就需要更大规模的计算能力来训练。比如 SENet-154 尺寸是 ResNet-152 的 1.7 倍，ResNet-152 尺寸又是 ResNet-50 的 2.4  倍，top-1 准确率才提高一点点。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/senet-accuracy.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;我还有一个有趣的发现。如果你算一下 ResNet-50 和 ResNet-152 的差距，就会发现 ResNet-152 虽然模型大小是 ResNet-50 的 2.4 倍，它的 top-1 错误率绝对值却只降低了 1.03%。从 22.37% 降低到 21.34%，相对降低了 (22.37-21.24)/22.37 = 4.6%，很少。可是如果你看它的 top-5 错误率，就会觉得它好了不少，因为它从 6.36% 降低到了 5.54%，虽然绝对值只少了 0.82%，比 top-1 错误率的改进还小，可是相对值却降低了 (6.36-5.54)/6.36 = 12.9%，就显得改进了挺多。&lt;/p&gt;

&lt;p&gt;这也许就是为什么 AI 业界用 top-5 的第二个原因。因为它的错误率基数很小，所以你减小一点点，相对的“改进”就显得很多了。而如果你看 top-1 准确率，就会觉得几乎没有变化。模型虽然大了几倍，计算量大了那么多，准确率却几乎没有变。&lt;/p&gt;

&lt;p&gt;所以你又意识到，Hinton 在他的演讲中说到的“同样的数据，大的模型更好”，很可能并不是那样的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/hinton-big-model.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;模型里面有这么多的参数，说明我们并没有抓住问题的本质。科学家都知道，当我们需要越来越大，越来越复杂的模型才能概括自然规律的时候，那说明这个模型很可能是错的。这就是为什么爱因斯坦的相对论那么可贵，因为它简单地解释了许多复杂的模型都无法概括的自然规律。&lt;/p&gt;

&lt;h3 id=&quot;ai-业界的诚信问题和自动驾驶的闹剧&quot;&gt;AI 业界的诚信问题和自动驾驶的闹剧&lt;/h3&gt;

&lt;p&gt;准确率不够高其实问题不大，只要你承认它的局限性，把它用到能用的地方就行了。可是最严重的问题是人的诚信，AI 人士总是夸大图像识别的效果，把它推向超出自己能力的应用。AI 业界从来没有向公众说清楚他们所谓的“超人类识别率”是基于什么标准，反而在各种媒体宣称“AI 已经超越了人类视觉”。这完全是在欺骗和误导公众。上面  Geoffrey Hinton 的&lt;a href=&quot;https://www.youtube.com/watch?v=UTfQwTuri8Y&quot;&gt;采访视频&lt;/a&gt;中，主持人也提到“神经网络视觉超越了人类”，这位深度学习的先驱者对此没有任何说明，而是欣然接受，继续自豪地夸夸其谈。&lt;/p&gt;

&lt;p&gt;你可以给自动驾驶车 5 次机会来判断前面出现的是什么物体吗？你有几条命可以给它试验呢？Tesla 的 Autopilot 系统可能 top-5 正确率很高吧：“那是个白板…… 哦不对，那是辆&lt;a href=&quot;https://en.wikipedia.org/wiki/Tesla_Autopilot#Incidents&quot;&gt;卡车&lt;/a&gt;！” “那是块面包…… 哦不对，那是高速公路的&lt;a href=&quot;https://www.forbes.com/sites/alanohnsman/2019/05/01/tesla-sued-by-family-of-silicon-valley-driver-killed-in-model-x-autopilot-crash&quot;&gt;隔离带&lt;/a&gt;！”&lt;/p&gt;

&lt;p&gt;我不是开玩笑，你点击上面的“卡车”和“隔离带”两个链接，它们指向的是 Tesla Autopilot 引起的两次致命车祸。第一次车祸，Autopilot 把卡车识别为白板，直接从侧面撞上去，导致车主立即死亡。另一次，它开出车道，没能识别出高速公路中间的隔离带，完全没有减速，反而加速撞上去，导致车主死亡，并且着火爆炸。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/tesla-accident-2018-03.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;

&lt;p&gt;神经网络能把卡车识别为白板还算“top-5 分类正确”，Autopilot 根本没有视觉理解能力，这就是为什么会引起这样可怕的事故。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/whiteboard-truck.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;你可以在这里看到一个 &lt;a href=&quot;https://en.wikipedia.org/wiki/Tesla_Autopilot#Incidents&quot;&gt;Autopilot 导致的事故列表&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;出了挺多人命，可是“自动驾驶”的研究仍然在混沌中进行。2018 年 3 月，Uber 的自动驾驶车在亚利桑那州撞死一名推自行车过马路的女性。事故发生时的&lt;a href=&quot;%5B%E8%A7%86%E9%A2%91%5D(https://www.youtube.com/watch?v=ufNNuafuU7M)&quot;&gt;车载录像&lt;/a&gt;已经被公布到了网上。&lt;/p&gt;

&lt;p&gt;报告显示，Uber 的自动驾驶系统在出事前 6 秒钟检测到了这位女士，起初把她分类为“不明物体”，然后分类为“汽车”，最后分类为“自行车”，完全没有刹车，以每小时 40 英里的速度直接撞了上去…… 【&lt;a href=&quot;https://www.nytimes.com/2019/03/05/technology/uber-self-driving-car-arizona.html&quot;&gt;新闻链接&lt;/a&gt;】&lt;/p&gt;

&lt;p&gt;在此之前，Uber 被加州政府吊销了自动驾驶实验执照，后来他们转向了亚利桑那州，因为亚利桑那州长热情地给放宽政策，“拥抱高科技创新”。结果呢，搞出人命来了。美国人看到 Uber 自动车撞死人，都在评论说，要实验自动驾驶车就去亚利桑那州吧，因为那里的人命不值钱，撞死不用负责！&lt;/p&gt;

&lt;p&gt;据 2018 年 12 月&lt;a href=&quot;https://www.apnews.com/88b38deec8b946db98aa1fab29e00bbc&quot;&gt;消息&lt;/a&gt;，Uber 想要重新开始自动驾驶实验，这次是在宾夕法尼亚州的匹兹堡。他们想要在匹兹堡的闹市区进行自动驾驶实验，因为那里有狭窄的街道，列车铁轨，许多的行人…… 我觉得要是他们真去那里实验，可能有更好的戏看了。&lt;/p&gt;

&lt;p&gt;自动驾驶领域使用的视觉技术是根本不可靠的，给其它驾驶者和行人造成生命威胁，各个自动驾驶公司却吵着想让政府交通部门给他们大开绿灯。某些公司被美国政府拒绝批准牌照之后大吵大闹，骂政府监管部门不懂他们的“高科技”，太保守，跟不上时代。有的公司更是异想天开，想要政府批准他们的自动车上&lt;a href=&quot;https://www.theverge.com/2019/8/30/20840631/self-driving-carmakers-federal-safety-rules-nhtsa-steering-wheels-pedals-waymo-cruise&quot;&gt;不安装方向盘&lt;/a&gt;，油门和刹车，号称自己的车已经不需要人类驾驶员，甚至说“只有完全去掉了人类的控制，自动车才能安全运行。”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/self-driving-regulations.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;一出出的闹剧上演，演得好像自动驾驶就快实现了，大家都在拼命抢夺这个市场似的，催促政府放宽政策。很是有些我们当年大炼钢铁，超英赶美的架势。这些公司就跟小孩子耍脾气要买玩具一样，全都吵着要爸妈让他玩自动驾驶，各种蛮横要求，马上给我，不然你就是不懂高科技，你就是“反智”，“反 AI”，你就是阻碍历史进步！给监管机构扣各种帽子，却完全不理解里面的难度，伦理和责任。玩死了人，却又抬出各种借口，不想负责任。&lt;/p&gt;

&lt;p&gt;自动驾驶领域最著名，最不负责任的人，当属 Tesla 的 Elon Musk 先生了。他不但总是对 Tesla 的 Autopilot 进行夸大的宣传，让人误解它的能力而导致车祸，死了人之后还要在网上发话扭曲人们的逻辑和伦理，让明眼人恶心。Tesla 公司总是抓住“车主开车不专心”等各种借口，逃脱对事故的责任。&lt;/p&gt;

&lt;p&gt;几乎每次 Tesla Autopilot 判断错误撞死了人，Elon Musk 都会出来说：“自动驾驶的事故率还是远远低于人类驾驶员！” 很多书呆子极客会听信他的“事故率”，为他的所谓“高科技”欢呼而忽略死者，可是他们不明白，这些大范围的统计数字对于事故责任分析，对于伦理是没用的。&lt;/p&gt;

&lt;p&gt;他的说法就相当于在说：“我活了这么久，为这么多客户服务，没杀过其中任何一个，我杀人的概率非常低，低于全国的谋杀犯罪率，所以我现在杀了你不用负责。” 先不说 Autopilot 的事故率是否真的那么低。即使它事故率是很低，难道弄死了人就可以不负责，甚至不受谴责吗？&lt;/p&gt;

&lt;p&gt;到底 Tesla 有没有责任，我们可以使用因果关系的“反事实分析”（counterfactual）：如果驾驶员没有使用 Autopilot 而是自己开车，这次事故还会不会发生？如果不会发生，那么我们得到因果关系：Autopilot 导致了事故。不管其他人用 Autopilot 有没有出事故，事故占多大比例，面对这里的因果关系都是无关紧要的。因果关系==责任。&lt;/p&gt;

&lt;p&gt;如果是 Autopilot 导致了事故，即使总共只发生了一次事故，都该它的设计者 Tesla 公司负责。很多人都是混淆了“责任”，“伦理”和“事故率”，所以才会继续支持 Elon Musk 和 Tesla 的欺诈行为。很多人总是以为“自动驾驶可能会降低全国的车祸率”，所以我们应该支持这些研究，而不明白事故率跟责任和伦理是两码事。&lt;/p&gt;

&lt;p&gt;如果拿事故率说事，航空业的事故率远远低于汽车业了吧？可是为什么全世界几年才一次空难，却每一次都带来那么多的恐慌，进行那么严格的调查，追究责任呢？就是因为我之前分析的，责任和事故率完全是两回事。只要有死伤，肯定有人要被调查，被追究，要负责的。只要人为导致了事故，都是不会被放过的，不管他的总体“事故率”如何低都一样要被惩罚。&lt;/p&gt;

&lt;p&gt;Autopilot 的事故率真的低吗？你可以自己研究一下。如果你算对了数学，恐怕它的事故率并不低。举一个例子，普通人只计算了事故的数目与 Autopilot 导航的总里程的比例，却忽视了那些由于驾驶员及时接管而避免了的事故的数目。另外 Tesla 属于比较贵的车，买车的人属于对自己比较负责的人，所以事故率不应该跟所有车的事故率比，而应该跟没有安装自动驾驶技术的奔驰，保时捷一类的车的事故率对比。&lt;/p&gt;

&lt;p&gt;每一次 Autopilot 相关的事故，Tesla 公司都会在事后散布新闻说是驾驶者开车不认真，手没有在方向盘上，不是 Autopilot 的责任。他们是否认真在开车，人死了无所对证，但这些全都成为了 Tesla 公司推脱责任的借口。&lt;/p&gt;

&lt;p&gt;Tesla 对 Autopilot 功能的不实宣传，导致了很多人产生盲目的信任，随即导致了放松警惕，这一切都是由 Tesla 而起的。启动 Autopilot 的时候签个生死状，说手必须一直放在方向盘上准备随时接管，否则后果自负。到头来一旦找出你没有认真开车的迹象，就把责任推得一干二净。&lt;/p&gt;

&lt;p&gt;所以 Tesla 不但视觉技术不行，而且人品和诚信都很成问题。我还没有见过一个汽车公司如此急于推脱责任的，一般都是积极配合调查，勇于承担责任，及时整改，这样才可能得到公众的信任。&lt;/p&gt;

&lt;p&gt;虽然 Tesla 和 Uber 是应该被谴责的，但这里面的视觉问题不只是这两家公司的问题，整个自动驾驶的领域都建立在虚浮的基础上。我们应该清楚地认识到，现有的所谓 AI 根本没有像人类一样的视觉理解能力，它们只是非常粗糙的图像识别，识别率还远远达不到人类的水平，所以根本就不可能实现自动驾驶。&lt;/p&gt;

&lt;p&gt;什么 L1~L4 的自动驾驶分级，都是瞎扯。根本没法实现的东西，分了级又有什么用呢？只是拿给这些公司用来忽悠大家的口号，外加推脱责任的借口而已。出事故前拿来做宣传：“我们已经实现 L2 自动驾驶，目前在研究 L3 自动驾驶，成功之后我们向 L4 进军！” 出事故后拿来推脱责任：“我们只是 L2 自动驾驶，所以这次事故是理所当然，不可避免的！”&lt;/p&gt;

&lt;p&gt;如果没有视觉理解，依赖于图像识别技术的“自动驾驶车”，是不可能在复杂的情况下做出正确操作，保障人们安全的。机器人等一系列技术，也只能停留在固定场景，精确定位的“工业机器人”阶段，而不能在复杂的自然环境中行动。&lt;/p&gt;

&lt;p&gt;我认识一些工业机器人的研究者。他们告诉我，深度神经网络那些识别算法太不精确了，根本没法用于准确性要求很高的应用。工业机器人控制不精确是完全不可接受的，所以他们都不用深度神经网络来控制机器人。&lt;/p&gt;

&lt;h3 id=&quot;识别技术还是有意义的&quot;&gt;识别技术还是有意义的&lt;/h3&gt;

&lt;p&gt;要实现真正的语言理解和视觉理解是非常困难的，可以说是毫无头绪。一代又一代的神经学家，认知科学家，哲学家，为了弄明白人类“认知”和“理解”到底是怎么回事，已经付出了许多的努力。可是直到现在，对于人类认知和理解的认识都不足以让机器具有真正的理解能力。&lt;/p&gt;

&lt;p&gt;真正的 AI 其实没有起步，很多跟 AI 沾点边的人都忙着忽悠和布道，没人关心其中的本质，又何谈实现呢？除非真正有人关心到问题所在，去研究本质的问题，否则实现真的理解能力就只是空中楼阁。我只是提醒大家不要盲目乐观，不要被忽悠了。与其夸大其词，欺骗大众，说人工智能快要实现了，不如拿已有的识别技术来做一些有用的事情，诚实地面对这些严重的局限性。&lt;/p&gt;

&lt;p&gt;我并不是一味否定识别技术，我只是反对把“识别”夸大为“理解”，把它等同于“智能”，进行不实宣传，用于超出它能力的领域。诚实地使用识别技术还是有用的，而且蛮有趣。我们可以用这些东西来做一些很有用的工具，辅助我们进行一些事情。从语音识别，语音合成，图片搜索，内容推荐，商业金融数据分析，反洗钱，公安侦查，医学图像分析，疾病预测，网络攻击监测，各种娱乐性质的 app…… 它确实可以给我们带来挺多好处，实现我们以前做不到的一些事情。&lt;/p&gt;

&lt;p&gt;另外虽然各公司都在对他们的“AI 对话系统”进行夸大和不实宣传，可是如果我们放弃“真正的对话”，坦诚地承认它们并不是真正的在对话，并没有智能，那它们确实可以给人带来一些便利。现有的所谓对话系统，比如 Siri，Alexa，基本可以被看作是语音控制的命令行工具。你说一句话，机器就挑出其中的关键字，执行一条命令。这虽然不是有意义的对话，却可以提供一些方便。特别是在开车不方便看屏幕的时候，语音控制“下一首歌”，“空调风量小一点”，“导航到最近的加油站”之类的命令，还是有用的。&lt;/p&gt;

&lt;p&gt;但不要忘记，识别技术不是真的智能，它没有理解能力，不能用在自动驾驶，自动客服，送外卖，保洁阿姨，厨师，发型师，运动员等需要真正“视觉理解”或者“语言理解”能力的领域，更不能期望它们取代教师，程序员，科学家等需要高级知识的工作。机器也没有感情和创造力，不能取代艺术家，作家，电影导演。所有跟你说机器也能有“感情”或者“创造力”的都是忽悠，就像现在的对话系统一样，只是让人以为它们有那些功能，而其实根本就没有。&lt;/p&gt;

&lt;p&gt;你也许会发现，机器学习很适合用来做那些不直观，人看不透，或者看起来很累的领域，比如各种数据分析。实际上那些就是统计学一直以来想解决的问题。可是视觉这种人类和高等动物的日常功能，机器的确非常难以超越。如果机器学习领域放弃对“人类级别智能”的盲目追求，停止拿“超人类视觉”一类的幌子来愚弄大众，各种夸大，那么他们应该能在很多方向做出积极的贡献。&lt;/p&gt;

&lt;p&gt;（全文完）&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-09-16-machine-vs-human-3</guid>
<pubDate>Mon, 16 Sep 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>机器与人类视觉能力的差距（2）</title>
<link>https://henix.github.io/feeds/yinwang/2019-09-15-machine-vs-human-2.html</link>
<description>&lt;p&gt;&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/09/15/machine-vs-human-2&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;script&gt;
            if (/mobile/i.test(navigator.userAgent) || /android/i.test(navigator.userAgent))
            {
               document.body.classList.add(&#39;mobile&#39;);
            }
        &lt;/script&gt;&lt;div class=&quot;inner&quot;&gt;
            &lt;h2&gt;机器与人类视觉能力的差距（2）&lt;/h2&gt;
            &lt;blockquote&gt;
  &lt;p&gt;本文属于个人观点，跟本人在职公司的立场无关。由于最近 GitHub 服务器在国内访问速度严重变慢，虽然经过大幅度压缩尺寸，文中的图片仍然可能需要比较长时间才能加载。这篇文章揭示了 AI 领域重要的谬误和不实宣传，为了阻止愚昧的蔓延，我鼓励大家转发这篇文章和它的后续，转发时只需要注明作者和出处就行。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这是这个系列文章的第二集，在这一集中，我想详细分析一下 AI 领域到底理解多少人类神经系统的构造。&lt;/p&gt;

&lt;h3 id=&quot;神经网络为什么容易被欺骗&quot;&gt;神经网络为什么容易被欺骗&lt;/h3&gt;

&lt;p&gt;“神经网络”与人类神经系统的关系是是很肤浅的。等你理解了所谓“神经网络”，就会明白它跟神经系统几乎没有一点关系。“神经网络”只是一个误导性质的 marketing 名词，它出现的目的只是为了让外行产生不明觉厉的效果，以为它跟人类神经系统有相似之处，从而对所谓的“人工智能”信以为真。&lt;/p&gt;

&lt;p&gt;其实所谓“神经网络”应该被叫做“可求导编程”。说穿了，所谓“神经网络”，“机器学习”，“深度学习”，就是利用微积分，梯度下降法，用大量数据拟合出一个函数，所以它只能做拟合函数能做的那些事情。&lt;/p&gt;

&lt;p&gt;用了千万张图片和几个星期的计算，拟合出来的函数也不是那么可靠。人们已经发现用一些办法生成奇怪的图片，能让最先进的深度神经网络输出&lt;a href=&quot;http://www.evolvingai.org/fooling&quot;&gt;完全错误的结果&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/diversity_40_images_label.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;（图片来源：&lt;a href=&quot;http://www.evolvingai.org/fooling&quot;&gt;http://www.evolvingai.org/fooling&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;神经网络为什么会有这种缺陷呢？因为它只是拟合了一个“像素=&amp;gt;名字”的函数。这函数碰巧能区分训练集里的图片，却不能抓住物体的结构和本质。它只是像素级别的拟合，所以这里面有很多空子可以钻。&lt;/p&gt;

&lt;p&gt;深度神经网络经常因为一些像素，颜色，纹理匹配了物体的一部分，就认为图片上有这个物体。它无法像人类一样理解物体的结构和拓扑关系，所以才会被像素级别的肤浅假象所欺骗。&lt;/p&gt;

&lt;p&gt;比如下面两个奇怪的图片，被认为是一个菠萝蜜和一个遥控器，仅仅因为它们中间出现了相似的纹理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/dnn-fool.jpg&quot; width=&quot;70%&quot;&gt;&lt;/p&gt;

&lt;p&gt;另外，神经网络还无法区分位置关系，所以它会把一些位置错乱的图片也识别成某种物体。比如下面这个，被认为是一张人脸，却没发现五官都错位了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/kardashian-cnn.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;神经网络为什么会犯这种错误呢？因为它的目标只是把训练集里的图片正确分类，提高“识别率”。至于怎么分类，它可以是毫无原则的，它完全不理解物体的结构。它并没有看到“叶子”，“果皮”，“方盒子”，“按钮”，它看到的只是一堆像素纹理。因为训练集里面的图片，出现了类似纹理的都被标记为“菠萝蜜”和“遥控器”，没有出现这纹理的都被标记为其它物品。所以神经网络找到了区分它们的“分界点”，认为看到这样的纹理，就一定是菠萝蜜和遥控器。&lt;/p&gt;

&lt;p&gt;我试图从神经网络的本质，从统计学来解释这个问题。神经网络其实是拟合一个函数，试图把标签不同的样本分开。拟合出来的函数试图接近一个“真实分界线”。所谓“真实分界线”，是一个完全不会错的函数，也就是“现实”。&lt;/p&gt;

&lt;p&gt;数据量小的时候，函数特别粗糙。数据量大了，就逐渐逼近真实分界线。但不管数据量如何大，它都不可能得到完全准确的“解析解”，不可能正好抓住“现实”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/classification.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;

&lt;p&gt;除非现实函数特别简单，运气特别好，否则用数据拟合出来的函数，都会有很多小“缝隙”。以上的像素攻击方法，就是找到真实分界线附近，“缝隙”里面的样本，它们正好让拟合函数出现分类错误。&lt;/p&gt;

&lt;p&gt;人的视觉系统是完全不同的，人直接就看到了事物是什么，看到了“解析解”，看到了“现实”，而没有那个用数据逼近的过程，所以除非他累得头脑发麻或者喝了酒，你几乎不可能让他判断错误。&lt;/p&gt;

&lt;p&gt;退一步来看，图像识别所谓的“正确分类”都是人定义的。是人给了那些东西名字，是许多人一起标注了训练用的图片。所以这里所谓的“解析解”，“现实”，全都是人定义的。一定是某人看到了某个事物，他理解了它的结构和性质，然后给了它一个名字。所以别的人也可以通过理解同一个事物的结构，来知道它是什么。&lt;/p&gt;

&lt;p&gt;神经网络不能看到事物的结构，所以它们也就难以得到精确的分类，所以机器在图像识别方面是几乎不可能超越人类的。现在所谓的“超人类视觉”的深度学习模型，大部分都是欺骗和愚弄大众。使用没有普遍性的数据集，使用不公平的准确率标准来对比，所以才显得机器好像比人还厉害了。这是一个严重的问题，在后面我会详细分析。&lt;/p&gt;

&lt;h3 id=&quot;神经网络训练很像应试教育&quot;&gt;神经网络训练很像应试教育&lt;/h3&gt;

&lt;p&gt;神经网络就像应试教育训练出来的学生，他们的目标函数是“考高分”，为此他们不择手段。等毕业工作遇到现实的问题，他们就傻眼了，发现自己没学会什么东西。因为他们学习的时候只是在训练自己“从 ABCD 里区分出正确答案”。等到现实中没有 ABCD 的时候，他们就不知道怎么办了。&lt;/p&gt;

&lt;p&gt;深度学习训练出来的那些“参数”是不可解释的，因为它们存在的目的只是把数据拟合出来，把不同种类的图片分离开，而没有什么意义。AI 人士喜欢给这种“不可解释性”找借口，甚至有人说：“神经网络学到的数据虽然不可解释，但它却出人意料的有效。这些学习得到的模型参数，其实就是知识！”&lt;/p&gt;

&lt;p&gt;这些模型真的那么有效吗？那为什么能够被如此离谱的图片所欺骗呢？说“那就是知识”，这说法简直荒谬至极，严重玷污了“知识”这个词的意义。这些“学习”得到的参数根本就不是本质的东西，不是知识，真的就是一堆毫无道理可言的数字，只为了降低“误差”，能够把特征空间的图片区分开来，所以神经网络才能被这样钻空子。&lt;/p&gt;

&lt;p&gt;说这些参数是知识，就像在说考试猜答案的技巧是知识一样可笑。“另外几套题的第十题都是 B，所以这套题的第十题也选 B”…… 深度学习拟合函数，就像拿历年高考题和它们的答案来拟合函数一样，想要不上课，不理解科目知识就做出答案来。有些时候它确实可以蒙对答案，但遇到前所未见的题目，或者题目被换了一下顺序，就傻眼了。&lt;/p&gt;

&lt;p&gt;人为什么可以不受这种欺骗呢？因为人提取了高级的拓扑结构，不是瞎蒙的，所以人的判断不受像素的影响。因为提取了结构信息，人的观察是具有可解释性的。如果你问一个小孩，为什么你说这是一只猫而不是一只狗呢？她会告诉你：“因为它的耳朵是这样的，它的牙是那样的，它走路的姿势是那样的，它常常磨爪子，它用舌头舔自己……”&lt;/p&gt;

&lt;p&gt;做个实验好了，你可以问问你家孩子这是猫还是狗。如果是猫，为什么他们认为这是一只猫而不是一只狗？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/luoxiaohei.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;

&lt;p&gt;神经网络看到一堆像素，很多层处理之后也不知道是什么结构，分不清“眼睛”，“耳朵”和“嘴”，更不要说“走路”之类的动态概念了，所以它也就无法告诉你它认为这是猫的原因了。拟合的函数碰巧把这归成了猫，如果你要追究原因，很可能是肤浅的：图片上有一块像素匹配了图片库里某只猫的毛色纹理。&lt;/p&gt;

&lt;p&gt;有一些研究者把深度神经网络的各层参数拆出来，找到它们对应的图片中的像素和纹理，以此来证明神经网络里的参数是有意义的。咋一看好像有点道理，原来“学习”就能得到这么多好像设计过的滤镜啊！可是仔细一看，里面其实没有多少有意义的内容，因为它们学到的参数只是能把那些图片类别分离开。&lt;/p&gt;

&lt;p&gt;所以人的视觉系统很可能是跟深度神经网络原理完全不同的，或者只有最低级的部分有相似之处。&lt;/p&gt;

&lt;h3 id=&quot;神经网络与人类神经元的关系是肤浅的&quot;&gt;“神经网络”与人类神经元的关系是肤浅的&lt;/h3&gt;

&lt;p&gt;为什么 AI 人士总是认为视觉系统的高级功能都能通过“学习”得到呢？非常可能的事情是，人和动物视觉系统的“结构理解”，“3D建模”功能不是学来的，而是早就固化在基因里了。想一想你生下来之后，有任何时候看到世界是平面的，毫无关联的像素吗？&lt;/p&gt;

&lt;p&gt;所以我觉得，人和动物生下来就跟现有的机器不一样，结构理解所需的硬件在胚胎里就已经有了，只等发育和激活。人是有学习能力，可是人的学习是建立在结构理解之上，而不是无结构的像素。另外人的“学习”很可能处于比较高的层面，而不是神经元那么“底层”的。人的神经系统里面并没有机器学习那种 back-propagation。&lt;/p&gt;

&lt;p&gt;纵使你有再多的数据，再多的计算力，你能超越为期几十亿年的，地球规模的自然进化和选择吗？与其自己去“训练”或者“学习”，不如直接从人身上抄过来！但问题是，我们真的知道人的视觉系统是如何工作的吗？&lt;/p&gt;

&lt;p&gt;神经科学家们其实并没有完全搞明白人类视觉系统是如何工作的。就像所有的生物学领域一样，人们的理解仍然是很粗浅的。神经网络与人类视觉系统的关系是肤浅的。每当你质疑神经网络与人类视觉系统的关系，AI 研究者就会抬出 Hubel &amp;amp; Wiesel 在 1959 年拿猫做的那个&lt;a href=&quot;http://youtube.com/watch?v=8VdFf3egwfg&quot;&gt;实验&lt;/a&gt;：“有人已经证明了人类视觉系统就是那样工作的！” 如此的自信，不容置疑的样子。&lt;/p&gt;

&lt;p&gt;我问你啊，如果我们在 1959 年就已经知道人类视觉系统的工作原理细节，为什么现在还各种模型改来改去，训练来训练去呢？直接模仿过来不就行了？所以这些人的说法是自相矛盾的。&lt;/p&gt;

&lt;p&gt;你想过没有，为什么到了 2019 年，AI 人士还拿一个 60 年前的实验来说明问题？这 60 年来就没有新的发现了吗？而且从 H&amp;amp;W 的实验你可以看出来，它只说明了猫的视觉神经有什么样的底层功能（能够做“线检测”），却没有说那就是全部的构造，没说上层的功能都是那样够构造的。&lt;/p&gt;

&lt;p&gt;H&amp;amp;W 的实验只发现了最底层的“线检测”，却没有揭示这些底层神经元的信号到了上层是如何组合在一起的。“线检测”是图像处理的基础操作。一个能够识别拓扑结构的动物视觉系统，理所当然应该能做“线检测”，但它应该不止有这种低级功能。&lt;/p&gt;

&lt;p&gt;视觉系统应该还有更高级的结构，H&amp;amp;W 的实验并没能回答这个问题，它仍然是一个黑盒子。AI 研究者们却拿着 H&amp;amp;W 的结果大做文章，自信满满的声称已经破解了动物视觉系统的一切奥秘。&lt;/p&gt;

&lt;p&gt;那些说“我们已经完全搞明白了人类视觉是如何工作”的 AI 人士，应该来看看这个 2005 年的分析 Herman grid 幻觉现象的&lt;a href=&quot;http://web.mit.edu/bcs/schillerlab/research/A-Vision/A15-2.htm&quot;&gt;幻灯片&lt;/a&gt;。这些研究来自 Schiller Lab，MIT 的脑科学和认知科学实验室。通过一系列对 Herman grid 幻觉图案的改动实验，他们发现长久以来（从 1960 年代开始）对产生这种现象的理解是错误的：那些暗点不是来自视网膜的“边沿强化”功能。他们猜想，这是来自大脑的 V1 视觉皮层的 S1 “方向选择”细胞。接着，另一篇 2008 年的 &lt;a href=&quot;https://www.researchgate.net/publication/5246149_Straightness_as_the_main_factor_of_the_Hermann_grid_illusion&quot;&gt;paper&lt;/a&gt; 又说，Schiller 的结果是不对的，这种幻觉跟那些线条是直的有关系，因为你如果把那些白线弄弯，幻觉就消失了。然后他们提出了他们自己的，新的“猜想”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/herman-sine.jpg&quot; width=&quot;60%&quot;&gt;&lt;/p&gt;

&lt;p&gt;从这种研究的方式我们可以看出，即使是 MIT 这样高级的研究所，对视觉系统的研究还处于“猜”的阶段，把人脑作为黑盒子，拿一些图片来做“行为”级别的实验。他们并没有完全破解视觉系统，看到它的“线路”和“算法”具体如何工作，而是给它一些输入，测试它的输出。这就是“黑盒子”实验法。以至于很多关于人类视觉的理论都不是切实而确定的，很可能是错误的猜想。&lt;/p&gt;

&lt;p&gt;脑科学发展到今天也还是如此，AI 领域相对于脑科学的研究方式，又要低一个级别。2019 年了，仍然抬出神经科学家 1959 年的结果来说事。闭门造车，对人家的最新成果一点都不关心。现在的深度神经网络模型基本是瞎蒙出来的。把一堆像素操作叠在一起，然后对大量数据进行“训练”，以为这样就能得到所有的视觉功能。&lt;/p&gt;

&lt;p&gt;动物视觉系统里面真有“反向传导”（back-propagation）这东西吗？H&amp;amp;W 的实验里面并没有发现 back-propagation。实际上神经科学家们至今也没有发现神经系统里面有 back-propagation，因为神经元的信号传递机制不能进行“反向”的通信。很多神经科学家的结论是，人脑里面进行 back-propagation 不大可能。&lt;/p&gt;

&lt;p&gt;所以神经网络的各种做法恐怕没有受到 H&amp;amp;W 实验的多大启发。只是靠这么一个肤浅的相似之处来显得自己接近了“人类神经系统”。现在的所谓“神经网络”，其实只是一个普通的数学函数的表达式，里面唯一起作用的东西其实是微积分，所谓 back-propagation，就是微积分的求导操作。神经网络的“训练”，就是反复求导数，用梯度下降方法进行误差最小化，拟合一个函数。这一切都跟神经元的工作原理没什么关系，完全就是数学。&lt;/p&gt;

&lt;p&gt;为了消除无知带来的困惑，你可以像我一样，自己去了解一下人类神经系统的工作原理。我推荐你看看这个叫《&lt;a href=&quot;https://www.youtube.com/watch?v=NL1S0AhYyFw&amp;amp;list=PL25AE732D9E27096D&quot;&gt;Interactive Biology&lt;/a&gt;》的 YouTube 视频系列。你可以从中轻松地理解人类神经系统一些细节：神经元的工作原理，视觉系统的原理，眼睛，视网膜的结构，听觉系统的工作原理，等等。神经学家们对此研究到了如此细节的地步，神经传导信息过程的每一个细节都展示了出来。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/neuron-axon.jpg&quot; width=&quot;50%&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;ai-研究者并不知道人脑如何工作&quot;&gt;AI 研究者并不知道人脑如何工作&lt;/h3&gt;

&lt;p&gt;AI 领域真的理解人脑如何工作吗？你可以参考一下这个演讲：”&lt;a href=&quot;https://www.youtube.com/watch?v=VIRCybGgHts&quot;&gt;Can the brain do back-propagation?&lt;/a&gt;” （人脑能做 back-propagation 吗？）。演讲人是深度学习的鼻祖级人物 Geoffrey Hinton。他和其它两位研究者（Yoshua Bengio 和 Yann LeCun），因为对深度学习做出的贡献，获得了 2018 年的图灵奖。演讲一开头 Hinton 说，神经科学家们说人脑做 back-propagation 是不可能的，然后他开始证明这是可能的，依据神经元的工作原理，back-propagation 如何能用人脑神经元来实现。&lt;/p&gt;

&lt;p&gt;是的，如果你有能力让人脑按你的“算法”工作的话，神经元组成的系统也许真能做 back-propagation，可是人脑是你设计的吗？很可惜我们无法改变人脑，而只能去“发现”它到底是如何工作。这不是人脑“能不能”的问题，而是“做不做”的问题。研究人脑是一个科学发现工作，而不是一个工程设计工作。&lt;/p&gt;

&lt;p&gt;看了这个演讲，我觉得 AI 人士已经进入了一种“上了天”的状态。他们坚定的认为自己的模型（所谓的“神经网络”）就是终极答案，甚至试图把人脑也塞进这个模型，设想人脑神经元如何能实现他们所谓的“神经网络”。可是他们没有发现，人脑的方式也许比他们的做法巧妙很多，根本跟他们的“神经网络”不一样。&lt;/p&gt;

&lt;p&gt;从这个视频我们也可以看出，神经科学界并不支持 AI 领域的说法。AI 领域是自己在那里瞎猜。视频下面有一条评论我很欣赏，他用讽刺的口气说：“Geoff Hinton 确切地知道人脑是如何工作的，因为这是他第 52 次发现人脑工作的新方式。”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/hinton-comment.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;

&lt;h3 id=&quot;ai-人的盲目信仰&quot;&gt;AI 人的盲目信仰&lt;/h3&gt;

&lt;p&gt;AI 人士似乎总是有一种不切实际的“信仰”或者“信念”，他们坚信机器一定可以具有人类一样的智能，总有一天能够在所有方面战胜人类。总是显示出一副“人类没什么了不起”的心态，张口闭口拿“人类”说事，好像他们自己是另外一个物种，已经知道人类的一切能力，有资格评判所有人的智力似的。&lt;/p&gt;

&lt;p&gt;我不知道是什么导致了这种“AI 宗教”。有句话说得好：“我所有的自负都来自我的自卑，所有的英雄气概都来自于我内心的软弱，所有的振振有词都因为心中满是怀疑。” 似乎是某种隐藏很深的自卑和怨恨，导致了他们如此的坚定和自负。一定要搞出个超越所有人的机器才善罢甘休，却没发现人类智能的博大精深已经从日常生活的各种不起眼的小事透露出来。&lt;/p&gt;

&lt;p&gt;他们似乎看不到世界上有各种各样，五花八门的人类活动，每一种都显示出奇迹般的智能。连端茶倒水这么简单的事情，都包含了机器望尘莫及的智能，更不要说各种体育运动，音乐演奏，各种研究和创造活动了。就连比人类“低级”一点的动物，各种宠物，家畜家禽，飞鸟走兽，甚至昆虫，全都显示出足以让人敬畏的智能。他们对所有这些奇迹般的事物视而不见，不是去欣赏他们的精巧设计和卓越表现，而是坐井观天，念叨着“机器一定会超越人类”。&lt;/p&gt;

&lt;p&gt;他们似乎已经像科幻电影似的把机器当成了一个物种，像是保护“弱势群体”一样，要维护机器的“权益”和“尊严”。他们不允许其他人质疑这些机器，不允许你说它们恐怕没法实现人类一样的智能。总之机器在他们心理已经不再是工具，而是活的生命，甚至是比人还高级的生命。&lt;/p&gt;

&lt;p&gt;对此你可以参考另一个 Geoffrey Hinton 的&lt;a href=&quot;https://www.youtube.com/watch?v=UTfQwTuri8Y&quot;&gt;采访视频&lt;/a&gt;，录制于今年 5 月份的 Google 开发者大会（Google I/O ‘19）。&lt;/p&gt;

&lt;p&gt;从这个视频里面我看到了许多 AI 人士盲目信仰和各种没有根据的说法的来源，因为这些说法全都集中而强烈的体现在了 Hinton 的谈话中。他如此的坚信一些没有根据的说法，不容置疑地把它们像真理一样说出来，却没有任何证据。有时候主持人都不得不采用了有点怀疑的语气。&lt;/p&gt;

&lt;p&gt;Hinton 在采访中有以下说法：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;“神经网络被设计为像人脑的工作原理。”&lt;/li&gt;
  &lt;li&gt;“等神经网络能够跟人对话，我们就能用它来进行教育工作了。”&lt;/li&gt;
  &lt;li&gt;“神经网络终究会在所有事情上战胜人类。”&lt;/li&gt;
  &lt;li&gt;“我们不都是神经网络吗？” （先后强调了两次）&lt;/li&gt;
  &lt;li&gt;“…… 所以神经网络能够实现人类智能的一切功能。这包括感情，意识等。”&lt;/li&gt;
  &lt;li&gt;“人们曾经认为生命是一种特殊的力量，现在生物学解释了生命的一切。人们现在仍然认为意识是特殊的，可是神经网络将会说明，意识并没有什么特别。”&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/hinton-we-are-neuron-net.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/hinton-nn-win-everything.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;

&lt;p&gt;他的这些说法都是不准确，不科学，没有根据的。&lt;/p&gt;

&lt;p&gt;我发现每当主持人用稍微怀疑的语气问：“这真的可以实现吗？” Hinton 就会回答：“当然能。我们不都是神经网络吗？” 这里有一个严重的问题，那就是他所谓的“神经网络”，其实并不是人脑里面的神经元连成的网络。AI 领域的“神经网络”只是他们自己的数学模型，是他们自己给它起名叫“神经网络”而已。所以他的这种“证明”其实是在玩文字游戏：“因为我们都是神经网络，所以神经网络能够实现一切人类智能，感情，甚至意识本身！”&lt;/p&gt;

&lt;p&gt;前面的“神经网络”和后面的“神经网络”完全是两回事。我们是“神经网络”吗？我们的脑子里是有神经元，神经元貌似连成了一个网络，可是它的结构却跟 AI 领域所谓的“神经网络”是两回事，工作原理也非常不一样。Hinton 面对问题作出这样的回答，是非常不科学，不负责任的。&lt;/p&gt;

&lt;p&gt;最后关于生命，感情和意识的说法，我也很不认同。虽然生物学解释了生命体的各种构造和原理，可是人们为什么仍然没能从无生命的物质制造出有生命的事物呢？虽然人们懂得那么多生物学，生物化学，有机化学，甚至能合成出各种蛋白质，可是为什么没能把这些东西组装在一起，让它“活”起来呢？这就像你能造出一些机器零件，可是组装起来之后，发现这机器不转。你不觉得是因为少了点什么吗？生物学发展了这么久，我们连一个最简单的，可以说是“活”的东西都没造出来过，你还能说“生命没什么特别的”吗？&lt;/p&gt;

&lt;p&gt;这说明生物学家们虽然知道生命体的一些工作原理，却没有从根本上搞明白生命到底是什么。也就是说人们解决了一部分“how”问题（生命体如何工作），却不理解“what”和“why”（生命是什么，为什么会出现生命）。&lt;/p&gt;

&lt;p&gt;实际上生物学对生命体如何工作（how）的理解都还远远不够彻底，这就是为什么我们还有那么多病无法医治，甚至连一些小毛病都无法准确的根治，一直拖着，只是不会马上致命而已。“生命是什么”的 what 问题仍然是一个未解之谜，而不像 Hinton 说的，全都搞明白了，没什么特别的。&lt;/p&gt;

&lt;p&gt;也许生命就是一种特别的东西呢？也许只有从有生命的事物，才能产生有生命的事物呢？也许生命就是从外星球来的，也许就是由某种更高级的智慧设计出来的呢？这些都是有可能的。真正的科学家应该保持开放的心态，不应该有类似“人定胜天”这样的信仰。我们的一切结论都应该有证据，如果没有我们就不应该说“一定”或者“必然”，说得好像所有秘密全都解开了一样。&lt;/p&gt;

&lt;p&gt;对于智能和意识，我也是一样的态度。在我们没有从普通的物质制造出真正的智能和意识之前，不应该妄言理解了关于它们的一切。生命，智能和意识，比有些人想象的要奇妙得多。想要“人造”出这些东西，比 AI 人士的说法要困难许多。&lt;/p&gt;

&lt;p&gt;有心人仔细观察一下身边的小孩子，小动物，甚至观察一下自己，就会发现它们的“设计”是如此的精巧，简直不像是随机进化出来的，而是由某个伟大的设计者创造的。46 亿年的时间，真的够进化和自然选择出这样聪明的事物吗？&lt;/p&gt;

&lt;p&gt;别误会了，我是不信宗教的。我觉得宗教的圣经都是小人书，都是某些人吓编的。可是如果你坚定的相信人类和动物的这些精巧的结构都是“进化”来的，你坚定的相信它们不是什么更高级的智慧创造出来的，那不也是另外一种宗教吗？你没有证据。没有证据的东西都只是猜想，而不能坚信。&lt;/p&gt;

&lt;p&gt;好像扯远了……&lt;/p&gt;

&lt;p&gt;总之，深度学习的鼻祖级人物说出这样多信念性质的，没有根据的话，由此可见这个领域有多么混沌。另外你还可以从他的谈话中看出，他所谓的“AI”都是各种相对容易的识别问题（语音识别，图像识别）。他并没有看清楚机器要想达成“理解”有多困难。而“识别”与“理解”的区别，就是我的这篇文章想澄清的问题。&lt;/p&gt;

&lt;h3 id=&quot;炼丹师的工作方式&quot;&gt;炼丹师的工作方式&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://www.yinwang.org/images/alchemist.jpg&quot; width=&quot;30%&quot;&gt;&lt;/p&gt;

&lt;p&gt;设计神经网络的“算法工程师”，“数据科学家”，他们工作性质其实很像“炼丹师”（alchemist）。拿个模型这改改那改改，拿海量的图片来训练，“准确率”提高了，就发 paper。至于为什么效果会好一些，其中揭示了什么原理，模型里的某个节点是用来达到什么效果的，如果没有它会不会其实也行？不知道，不理解。甚至很多 paper 里的结果无法被别的研究者复现，存在作假的可能性。&lt;/p&gt;

&lt;p&gt;我很怀疑这样的研究方式能够带来什么质的突破，这不是科学的方法。如果你跟我一样，把神经网络看成是用“可求导编程语言”写出来的代码，那么现在这种设计模型的方法就很像“一百万只猴子敲键盘”，总有一只能敲出“Hello World！”&lt;/p&gt;

&lt;p&gt;许多数学家和统计学家都不认同 AI 领域的研究方式，对里面的很多做法表示不解和怀疑。为此斯坦福大学的统计学系还专门开了一堂课 &lt;a href=&quot;https://www.youtube.com/playlist?list=PLwUqqMt5en7fFLwSDa9V3JIkDam-WWgqy&quot;&gt;Stats 385&lt;/a&gt;，专门讨论这个问题。课堂上请来了一些老一辈的数学家，一起来分析深度学习模型里面的各种操作是用来达到什么目的。有一些操作很容易理解，可是另外一些没人知道是怎么回事，这些数学家都看不明白，连设计这些模型的炼丹师们自己都不明白。&lt;/p&gt;

&lt;p&gt;所以你也许看到了，AI 研究者并没能理解人类视觉系统的工作原理，许多的机器视觉研究都是在瞎猜。在接下来的续集中，我们会看到他们所谓的“超人类识别率”是如何来的。&lt;/p&gt;

&lt;p&gt;请看下一篇：&lt;a href=&quot;http://www.yinwang.org/blog-cn/2019/09/16/machine-vs-human-3&quot;&gt;机器与人类视觉能力的差距（3）&lt;/a&gt;&lt;/p&gt;


        &lt;/div&gt;</description>
<author>yinwang0</author>
<guid isPermaLink="false">2019-09-15-machine-vs-human-2</guid>
<pubDate>Sun, 15 Sep 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
