<div class="title-image"><img src="https://pic4.zhimg.com/v2-4c4238f4a49e23ae25b0d9f80cfc8164_b.jpg" alt=""></div>导语：非线性规划是一种应用场景甚广的优化问题，从神经网络训练的反向传播，到资产组合的配置优化，都离不开寻找非线性函数的极小点的技巧。本文将讲解无约束非线性规划的基本思路和一些简单的算法。<p>阅读本文需要掌握线性代数和多元微积分的知识。<br/>建议读者对<a href="https://link.zhihu.com/?target=https%3A//www.joinquant.com/post/3293%3Ff%3Dzh" class=" wrap external" target="_blank" rel="nofollow noreferrer">数学规划</a>有基本了解。<br/></p><h4>无约束非线性规划问题</h4><p>无约束的非线性规划问题具有以下形式</p><p><figure><noscript><img src="https://pic1.zhimg.com/v2-0e38dfa77e82ffd9e5b4a418415636c0_b.png" data-rawwidth="145" data-rawheight="62" class="content_image" width="145"/></noscript><img src="https://pic1.zhimg.com/v2-0e38dfa77e82ffd9e5b4a418415636c0_b.png" data-rawwidth="145" data-rawheight="62" class="content_image lazy" width="145" data-actualsrc="https://pic1.zhimg.com/v2-0e38dfa77e82ffd9e5b4a418415636c0_b.png"/></figure>这里 f:<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>→R 是一个连续函数。但只是单纯的连续函数是很难被优化的，我们一般会要求函数 f 满足 <img src="https://www.zhihu.com/equation?tex=C%5E%7B1%7D+" alt="C^{1} " eeimg="1"/> 或者 <img src="https://www.zhihu.com/equation?tex=C%5E%7B2%7D+" alt="C^{2} " eeimg="1"/> 的性质，这些性质在之后会有定义。</p><p>这种问题叫做“无约束”因为它的可取点是任何 x∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>；相对应的，有约束的规划问题一般有以下形式</p><p><figure><noscript><img src="https://pic4.zhimg.com/v2-6f259bd1e39950b316af83e59a6f42a7_b.png" data-rawwidth="423" data-rawheight="81" class="origin_image zh-lightbox-thumb" width="423" data-original="https://pic4.zhimg.com/v2-6f259bd1e39950b316af83e59a6f42a7_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-6f259bd1e39950b316af83e59a6f42a7_b.png" data-rawwidth="423" data-rawheight="81" class="origin_image zh-lightbox-thumb lazy" width="423" data-original="https://pic4.zhimg.com/v2-6f259bd1e39950b316af83e59a6f42a7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-6f259bd1e39950b316af83e59a6f42a7_b.png"/></figure>这里 g1,…,gm 是一些约束函数，而 b1,…,bm 是一些实数。</p><p>在解决非线性规划问题时，我们最想做的是计算出一个全局极小点。什么是全局极小点呢？<br/><br/>定义. 一个函数 f 的<em>全局极小点（global minimizer）</em> 是一个 x∗∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>，其满足对于任何一个 x∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>都有 f(x∗)≤f(x)。<br/><br/>但是找出函数的全局极小点只是我们的理想，这其实是非常困难的，而找出一个局部极小点是我们能做的最好的尝试。<br/><br/>定义. 一个函数 f 的<em>局部极小点（local minimizer）</em>是一个 x∗∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>，其满足，存在 <img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>中的一个开集 N∋x∗，对于所有 x∈N 有 f(x∗)≤f(x)。这个定义有时也称作弱局部极小点（weak local optimizer），因为它允许在局部内有和它取值相同的点。<br/><br/>定义. 一个函数 f 的<em>强局部极小点（strong local optimizer）</em>是一个 x∗∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>，其满足，存在 <img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/> 中的一个开集 N∋x∗，对于所有 x∈N 并且 x≠x∗ 有 f(x∗)&lt;f(x)。<br/><br/>不过我还有一个坏消息，其实局部极小点我们可能也找不到。嗯，不行。但是我们可以做到的，是找到一个足够接近局部极小点的向量，以至于 f 在该位置的取值非常接近实际的局部最小值。这就是本文中介绍的算法要达成的目标。</p><h4>定义和符号</h4><p>在一切开始之前，我们有必要讲清楚文中用到的数学符号和它们的定义。首先是关于微分的三个定义：</p><p>定义. 设 f:<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>→R 是一个连续函数，用 {x1,…,xn} 表示 <img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/> 的标准基。如果对于所有满足 l1+…,+ln=m 的 l1,…,ln≥0，m 阶导数 <figure><noscript><img src="https://pic1.zhimg.com/v2-288741800f50f2dd9db4c8b84c2734f8_b.png" data-rawwidth="110" data-rawheight="63" class="content_image" width="110"/></noscript><img src="https://pic1.zhimg.com/v2-288741800f50f2dd9db4c8b84c2734f8_b.png" data-rawwidth="110" data-rawheight="63" class="content_image lazy" width="110" data-actualsrc="https://pic1.zhimg.com/v2-288741800f50f2dd9db4c8b84c2734f8_b.png"/></figure>存在并且是连续的，我们说 f 是一个 <img src="https://www.zhihu.com/equation?tex=C%5E%7Bm%7D+" alt="C^{m} " eeimg="1"/> 函数。</p><p>定义. 设 f:<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>→R 是一个 <img src="https://www.zhihu.com/equation?tex=C%5E%7B1%7D+" alt="C^{1} " eeimg="1"/> 函数，那么 f 的\emph{梯度(gradiet)}是一个连续函数 ∇f:<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>→<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>，定义为</p><p><figure><noscript><img src="https://pic4.zhimg.com/v2-cbacaedabfc413299da89662630fea03_b.png" data-rawwidth="387" data-rawheight="227" class="content_image" width="387"/></noscript><img src="https://pic4.zhimg.com/v2-cbacaedabfc413299da89662630fea03_b.png" data-rawwidth="387" data-rawheight="227" class="content_image lazy" width="387" data-actualsrc="https://pic4.zhimg.com/v2-cbacaedabfc413299da89662630fea03_b.png"/></figure>高维函数的梯度和在一维函数的导数是同等的概念，它告诉我们一个函数在某点向各个方向的斜率是多少，通过这个信息我们可以寻找坡度向下的方向进行优化。<br/><br/>定义. 设 f:<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>→R 是一个 <img src="https://www.zhihu.com/equation?tex=C%5E%7B2%7D+" alt="C^{2} " eeimg="1"/> 函数，那么 f 的<em>海塞矩阵（Hessian matrix）</em>是一个连续函数 ∇2f:<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>→<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%2An%7D+" alt="R^{n*n} " eeimg="1"/>（有时也写作 Hf），定义为<br/></p><p><figure><noscript><img src="https://pic4.zhimg.com/v2-7a711bfc68fe1eabe1fd24c116424da7_b.png" data-rawwidth="884" data-rawheight="277" class="origin_image zh-lightbox-thumb" width="884" data-original="https://pic4.zhimg.com/v2-7a711bfc68fe1eabe1fd24c116424da7_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-7a711bfc68fe1eabe1fd24c116424da7_b.png" data-rawwidth="884" data-rawheight="277" class="origin_image zh-lightbox-thumb lazy" width="884" data-original="https://pic4.zhimg.com/v2-7a711bfc68fe1eabe1fd24c116424da7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-7a711bfc68fe1eabe1fd24c116424da7_b.png"/></figure>然后是线性代数中（半）正定矩阵的定义：<br/><br/>定义. 对于一个 n×n 矩阵 A∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%2An%7D+" alt="R^{n*n} " eeimg="1"/>，如果 A=AT 并且对于每一个向量 x∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/> 有 <img src="https://www.zhihu.com/equation?tex=x%5E%7BT%7D+" alt="x^{T} " eeimg="1"/>Ax≥0，我们说 A 是一个<em>半正定矩阵（positive semi-definite matrix）</em>。<br/><br/>定义. 对于一个 n×n 矩阵 A∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%2An%7D+" alt="R^{n*n} " eeimg="1"/>，如果 A=AT 并且对于每一个向量 x∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>∖{0} 有 <img src="https://www.zhihu.com/equation?tex=x%5E%7BT%7D+" alt="x^{T} " eeimg="1"/>Ax&gt;0，我们说 A 是一个<em>正定矩阵（positive definite matrix）</em>。<br/><br/>下面是两个至关重要的定理，读者也可以在微积分的教材里找到它们。<br/></p><h4>两个定理</h4><p>首先是一个在多元微积分中很常见的定理，它告诉我们如何判别一个函数的（强）局部极小点。<br/><br/>定理 1. 设 f:<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>→R 是一个<img src="https://www.zhihu.com/equation?tex=C%5E%7B2%7D+" alt="C^{2} " eeimg="1"/> 函数，并且设 x∗∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>。如果 ∇f(x∗)=0 并且 ∇2f(x∗) 是一个半正定矩阵，那么 x∗ 是 f 的一个局部极小点；如果 ∇f(x∗)=0 并且 ∇2f(x∗) 是一个正定矩阵，那么 x∗ 是 f 的一个强局部极小点。<br/><br/>你也许想到，啊哈！我们用这个定理的结论不就可以解决最小化问题了吗？为什么说不能找到极小点？</p><p>那是因为，找到一个函数（在这里就是梯度 ∇f）的零集其实也是千古难题，有时我们甚至会用非线性优化问题来估测一个函数等于零的位置。所以呐，还是老老实实地继续往下看吧。</p><p>下面的泰勒定理可以帮助我们分析函数在一个局部里的表现，方便我们找到极小点。<br/><br/>定理 2.（泰勒定理） 设 f:<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>→R 是一个 <img src="https://www.zhihu.com/equation?tex=C%5E%7B1%7D+" alt="C^{1} " eeimg="1"/> 函数，并设 p∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>。那么存在某个 t∈(0,1)，有</p><p><figure><noscript><img src="https://pic2.zhimg.com/v2-41f34ec46672099a890b804903de005d_b.png" data-rawwidth="407" data-rawheight="56" class="content_image" width="407"/></noscript><img src="https://pic2.zhimg.com/v2-41f34ec46672099a890b804903de005d_b.png" data-rawwidth="407" data-rawheight="56" class="content_image lazy" width="407" data-actualsrc="https://pic2.zhimg.com/v2-41f34ec46672099a890b804903de005d_b.png"/></figure>并且，如果 f 是 <img src="https://www.zhihu.com/equation?tex=C%5E%7B2%7D+" alt="C^{2} " eeimg="1"/> 函数，那么存在某个 t∈(0,1)，有<br/></p><p><figure><noscript><img src="https://pic1.zhimg.com/v2-6963cbd15016e25c02ea6b7be518f358_b.png" data-rawwidth="598" data-rawheight="71" class="origin_image zh-lightbox-thumb" width="598" data-original="https://pic1.zhimg.com/v2-6963cbd15016e25c02ea6b7be518f358_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-6963cbd15016e25c02ea6b7be518f358_b.png" data-rawwidth="598" data-rawheight="71" class="origin_image zh-lightbox-thumb lazy" width="598" data-original="https://pic1.zhimg.com/v2-6963cbd15016e25c02ea6b7be518f358_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-6963cbd15016e25c02ea6b7be518f358_b.png"/></figure>这个定理告诉我们，选定 <img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/> 上的一个点 x 和一个方向 p，函数 f 从 x 到 x+p 的变化等于 f 在某一点的梯度和向量 p 的点积，而这个点总能在 x 和 x+p 之间找到。</p><p>依照这个定理，我们还可以在仅知 f(x) 和 ∇f(x) 的情况下在局部内线性估测 f 的值。我们已知对于任何 p∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>，有</p><p><figure><noscript><img src="https://pic2.zhimg.com/v2-4097bc7c8b07e2615b8ce01071f823c5_b.png" data-rawwidth="402" data-rawheight="47" class="content_image" width="402"/></noscript><img src="https://pic2.zhimg.com/v2-4097bc7c8b07e2615b8ce01071f823c5_b.png" data-rawwidth="402" data-rawheight="47" class="content_image lazy" width="402" data-actualsrc="https://pic2.zhimg.com/v2-4097bc7c8b07e2615b8ce01071f823c5_b.png"/></figure>可是每一个 p 对应着不同的 t，用这个方法来计算 f(x+p) 并不方便。但是，如果我们允许一定误差的话，可以用 ∇f(x) 代替 ∇f(x+tp) 来进行估测，有<br/></p><p><figure><noscript><img src="https://pic4.zhimg.com/v2-69b4ca5359d96a0a18b58be582a0f1fb_b.png" data-rawwidth="351" data-rawheight="62" class="content_image" width="351"/></noscript><img src="https://pic4.zhimg.com/v2-69b4ca5359d96a0a18b58be582a0f1fb_b.png" data-rawwidth="351" data-rawheight="62" class="content_image lazy" width="351" data-actualsrc="https://pic4.zhimg.com/v2-69b4ca5359d96a0a18b58be582a0f1fb_b.png"/></figure>因为<br/></p><p><figure><noscript><img src="https://pic2.zhimg.com/v2-c2ad8241e23dc6cddc788e3c98812ae5_b.png" data-rawwidth="866" data-rawheight="62" class="origin_image zh-lightbox-thumb" width="866" data-original="https://pic2.zhimg.com/v2-c2ad8241e23dc6cddc788e3c98812ae5_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-c2ad8241e23dc6cddc788e3c98812ae5_b.png" data-rawwidth="866" data-rawheight="62" class="origin_image zh-lightbox-thumb lazy" width="866" data-original="https://pic2.zhimg.com/v2-c2ad8241e23dc6cddc788e3c98812ae5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-c2ad8241e23dc6cddc788e3c98812ae5_b.png"/></figure>所以如果 p 足够小，那么（由于 ∇f 是连续的）∇f(x)−∇f(x+tp) 的差也很小，以至于等式右侧的误差同样很小。</p><p>从基础微积分的知识中，我们知道如果 f 是一个一维函数，那么上面的做法是以 (x,f(x)) 为中心画出了一条切线（也就是一阶泰勒展开）</p><figure><noscript><img src="https://pic1.zhimg.com/v2-a186fdcf6da968ee985ff10f12ec3fb0_b.png" data-rawwidth="649" data-rawheight="400" class="origin_image zh-lightbox-thumb" width="649" data-original="https://pic1.zhimg.com/v2-a186fdcf6da968ee985ff10f12ec3fb0_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-a186fdcf6da968ee985ff10f12ec3fb0_b.png" data-rawwidth="649" data-rawheight="400" class="origin_image zh-lightbox-thumb lazy" width="649" data-original="https://pic1.zhimg.com/v2-a186fdcf6da968ee985ff10f12ec3fb0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-a186fdcf6da968ee985ff10f12ec3fb0_b.png"/></figure><p>这条切线在一定范围之内和实际函数非常接近。在更高维度中，函数 ϕ(p)=f(x)+∇f(x)Tp 的图像则构成以 (x,f(x)) 为中心的一个超切面。</p><p>泰勒定理中的第二个公式同样给了我们估测方法</p><p><figure><noscript><img src="https://pic1.zhimg.com/v2-ff4b6a168118b15377f9fa7d949db960_b.png" data-rawwidth="530" data-rawheight="64" class="origin_image zh-lightbox-thumb" width="530" data-original="https://pic1.zhimg.com/v2-ff4b6a168118b15377f9fa7d949db960_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-ff4b6a168118b15377f9fa7d949db960_b.png" data-rawwidth="530" data-rawheight="64" class="origin_image zh-lightbox-thumb lazy" width="530" data-original="https://pic1.zhimg.com/v2-ff4b6a168118b15377f9fa7d949db960_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-ff4b6a168118b15377f9fa7d949db960_b.png"/></figure>这个用到了二阶项的估值会比之前的估测更准确；使用这个估值的要求更高一点，需要 f 是 <img src="https://www.zhihu.com/equation?tex=C%5E%7B2%7D+" alt="C^{2} " eeimg="1"/> 函数（在实际应用中基本都可以满足）。<br/><br/>泰勒定理为我们提供了一种搜索一个函数极小点的方法。假设 x 不是函数 f 的局部极小点并且我们知道 f 在一个点 x 的取值 f(x) 以及梯度 ∇f(x)，如果选得一个方向 p∈R 满足 ∇f(x)Tp&lt;0，那么存在某个 α&gt;0，使得 f(x+αp)&lt;f(x)。如此重复，在第 k 次迭代是从 xk 开始，找到合适的 pk 和 αk 并更新 xk+1=xk+αkpk，这样，让 f(xk) 的取值越来越小，以收敛到一个局部极小点。</p><p>以上的方法叫做<em>线搜索（line search）</em>，是解决非线性规划问题的一类主要方法。线搜索的主要问题在于要寻找合适的<em>方向（direction）</em>pk 和<em>步长（step length）</em>αk。</p><h4>方向</h4><p>在线搜索的每一次迭代中，我们想要方向 pk 满足 ∇f<img src="https://www.zhihu.com/equation?tex=%28xk%29%5E%7BT%7D+" alt="(xk)^{T} " eeimg="1"/>pk&lt;0。我们知道向量 ∇f(xk) 和 pk 之间的角度 θk 满足</p><p><figure><noscript><img src="https://pic2.zhimg.com/v2-b82dc22b32ac33ab3236f8d0ea0188bd_b.png" data-rawwidth="329" data-rawheight="83" class="content_image" width="329"/></noscript><img src="https://pic2.zhimg.com/v2-b82dc22b32ac33ab3236f8d0ea0188bd_b.png" data-rawwidth="329" data-rawheight="83" class="content_image lazy" width="329" data-actualsrc="https://pic2.zhimg.com/v2-b82dc22b32ac33ab3236f8d0ea0188bd_b.png"/></figure>因此符合条件的 pk 也恰恰是和梯度 ∇f(xk) 的角度大于 90∘ 的向量。下面介绍选取方向 pk 的几个常用的方法。<br/><br/><b>梯度下降</b></p><p>最符合直觉的方向是选择 f 下降幅度最大的方向。也就是说，固定 p 的长度不变，我们想找到最小的 ∇f<img src="https://www.zhihu.com/equation?tex=%28xk%29%5E%7BT%7D+" alt="(xk)^{T} " eeimg="1"/>p。于是乎，我们解决一个简单的最小化问题</p><p><figure><noscript><img src="https://pic2.zhimg.com/v2-4f990d991b9e53c482e45c6471e2c821_b.png" data-rawwidth="256" data-rawheight="88" class="content_image" width="256"/></noscript><img src="https://pic2.zhimg.com/v2-4f990d991b9e53c482e45c6471e2c821_b.png" data-rawwidth="256" data-rawheight="88" class="content_image lazy" width="256" data-actualsrc="https://pic2.zhimg.com/v2-4f990d991b9e53c482e45c6471e2c821_b.png"/></figure>用 θ 表示 p 和 ∇f(xk) 之间的角度，有<br/></p><p><figure><noscript><img src="https://pic4.zhimg.com/v2-a7a7edf49bb963a1888b176426987ff3_b.png" data-rawwidth="597" data-rawheight="55" class="origin_image zh-lightbox-thumb" width="597" data-original="https://pic4.zhimg.com/v2-a7a7edf49bb963a1888b176426987ff3_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-a7a7edf49bb963a1888b176426987ff3_b.png" data-rawwidth="597" data-rawheight="55" class="origin_image zh-lightbox-thumb lazy" width="597" data-original="https://pic4.zhimg.com/v2-a7a7edf49bb963a1888b176426987ff3_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-a7a7edf49bb963a1888b176426987ff3_b.png"/></figure>很明显，当 cos⁡θ=−1 时这个值被最小化，对应向量 p⋆=−∇f(xk)/∥∇f(xk)∥。因此，作为最大下降的方向，可以选择 pk=−∇f(xk)，这个方向一般称为<em>梯度下降（gradient descent）</em>或者<em>最大下降（steepest descent）</em>。</p><p>最大下降方法的优点有两个。一是它保证全局的收敛性：不论我们的起始点 x0 在哪里，反复的迭代都会收敛向一个局部极小点。第二是它对函数 f 的要求很小，只需要 f 是连续可导的。但同时，它的缺点也很明显，那就是它在很多情况下的收敛速度很慢。最大下降的方向与 f 在 xk 的水平集 {x∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>:f(x)=f(xk)} 程垂直关系，因此根据起始点的位置，很可能在迭代中画出锯齿形的低效率路径。</p><p><figure><noscript><img src="https://pic4.zhimg.com/v2-ea994471c3cbbb2ffbe52b9512894ac7_b.png" data-rawwidth="711" data-rawheight="716" class="origin_image zh-lightbox-thumb" width="711" data-original="https://pic4.zhimg.com/v2-ea994471c3cbbb2ffbe52b9512894ac7_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-ea994471c3cbbb2ffbe52b9512894ac7_b.png" data-rawwidth="711" data-rawheight="716" class="origin_image zh-lightbox-thumb lazy" width="711" data-original="https://pic4.zhimg.com/v2-ea994471c3cbbb2ffbe52b9512894ac7_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ea994471c3cbbb2ffbe52b9512894ac7_b.png"/></figure>上图是一个二次函数 Q(x)=3<img src="https://www.zhihu.com/equation?tex=x_%7B1%7D+%5E%7B2%7D+" alt="x_{1} ^{2} " eeimg="1"/>+2<img src="https://www.zhihu.com/equation?tex=x_%7B1%7D+x_%7B2%7D+" alt="x_{1} x_{2} " eeimg="1"/>+<img src="https://www.zhihu.com/equation?tex=x_%7B2%7D%5E%7B2%7D+" alt="x_{2}^{2} " eeimg="1"/> 的热力图，它的最小值在 (0,0) 的位置。图中的三根曲线是从三个不同的点起始的梯度下降方法的搜寻路径，线段每变换一次代表一次迭代，其中每一条曲线都在 11 到 14 次迭代中找到了距离极小点误差小于 0.00001 的位置。当然了，二次函数一般都比较规整，梯度下降算法的表现还算不错，下面的例子就不太好了。<br/></p><figure><noscript><img src="https://pic3.zhimg.com/v2-303250b5d3b66526c01cbf269de5957a_b.png" data-rawwidth="971" data-rawheight="649" class="origin_image zh-lightbox-thumb" width="971" data-original="https://pic3.zhimg.com/v2-303250b5d3b66526c01cbf269de5957a_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-303250b5d3b66526c01cbf269de5957a_b.png" data-rawwidth="971" data-rawheight="649" class="origin_image zh-lightbox-thumb lazy" width="971" data-original="https://pic3.zhimg.com/v2-303250b5d3b66526c01cbf269de5957a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-303250b5d3b66526c01cbf269de5957a_b.png"/></figure><p>上图中的是 Rosenbrock 函数 R(x)=<img src="https://www.zhihu.com/equation?tex=%281-x_%7B1%7D+%29%5E%7B2%7D+" alt="(1-x_{1} )^{2} " eeimg="1"/>+100(<img src="https://www.zhihu.com/equation?tex=%28+x_%7B2%7D+-x_%7B1%7D%5E%7B2%7D%29%5E%7B2%7D+" alt="( x_{2} -x_{1}^{2})^{2} " eeimg="1"/>)，它的极小点在 (1,1)（图中红点）。可以在热力图上看出这个函数的表现不是那么友好，它挖了一个又弯又窄的沟，函数的梯度指向的不是沟的下游而是沟的中央，所以梯度下降法像在编小辫一样地反复地折返，用了 5491 次迭代才从 (−0.5,1)（图中绿点）达到与极小点的误差小于 0.00001 的位置。</p><p>我们放大算法在沟里搜索的路径，足以看出它多没有效率。</p><figure><noscript><img src="https://pic2.zhimg.com/v2-56d91258aebc724b5e444ba2d4fb79a5_b.png" data-rawwidth="1164" data-rawheight="791" class="origin_image zh-lightbox-thumb" width="1164" data-original="https://pic2.zhimg.com/v2-56d91258aebc724b5e444ba2d4fb79a5_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-56d91258aebc724b5e444ba2d4fb79a5_b.png" data-rawwidth="1164" data-rawheight="791" class="origin_image zh-lightbox-thumb lazy" width="1164" data-original="https://pic2.zhimg.com/v2-56d91258aebc724b5e444ba2d4fb79a5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-56d91258aebc724b5e444ba2d4fb79a5_b.png"/></figure><p>那碰上这种奇葩的函数我们该怎么办呢怎么办呢？往下看。<br/><br/><b>牛顿方法</b></p><p>另一个重要的衰减方向（可能可以说是最重要的）是<em>牛顿方向（Newton&#39;s direction）</em>。记得在泰勒定理中，二次可导的函数 f 在 xk 附近的二次泰勒估值是</p><p><figure><noscript><img src="https://pic3.zhimg.com/v2-4fb5151fd2b6c7ae76eb48a78358dc4a_b.png" data-rawwidth="609" data-rawheight="59" class="origin_image zh-lightbox-thumb" width="609" data-original="https://pic3.zhimg.com/v2-4fb5151fd2b6c7ae76eb48a78358dc4a_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-4fb5151fd2b6c7ae76eb48a78358dc4a_b.png" data-rawwidth="609" data-rawheight="59" class="origin_image zh-lightbox-thumb lazy" width="609" data-original="https://pic3.zhimg.com/v2-4fb5151fd2b6c7ae76eb48a78358dc4a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-4fb5151fd2b6c7ae76eb48a78358dc4a_b.png"/></figure>（这里 =:mk(p) 是指我们把 mk(p) 定义为等号左边的表达式）。暂时假设 ∇2f(xk) 是一个正定矩阵，我们想将二次估值 mk(p) 最小化，那么求一阶导数得到<br/></p><p><figure><noscript><img src="https://pic1.zhimg.com/v2-74473b50821309c00b5ee3353a8289b8_b.png" data-rawwidth="352" data-rawheight="60" class="content_image" width="352"/></noscript><img src="https://pic1.zhimg.com/v2-74473b50821309c00b5ee3353a8289b8_b.png" data-rawwidth="352" data-rawheight="60" class="content_image lazy" width="352" data-actualsrc="https://pic1.zhimg.com/v2-74473b50821309c00b5ee3353a8289b8_b.png"/></figure>将它设为 0，求得<br/></p><p><figure><noscript><img src="https://pic2.zhimg.com/v2-21a43ac8806a3d2bd205722fab010d09_b.png" data-rawwidth="283" data-rawheight="50" class="content_image" width="283"/></noscript><img src="https://pic2.zhimg.com/v2-21a43ac8806a3d2bd205722fab010d09_b.png" data-rawwidth="283" data-rawheight="50" class="content_image lazy" width="283" data-actualsrc="https://pic2.zhimg.com/v2-21a43ac8806a3d2bd205722fab010d09_b.png"/></figure>因为 mk 的二次导数是 ∇2f(xk)，在假设中是正定矩阵，根据定理 1.，上面所求 pk 确定是极小点。</p><p>理论上来说，牛顿方法的收敛速度要比梯度下降更快，因为梯度下降使用的是一次展开进行来估测方向，误差一般在 O(∥pk∥2)；而牛顿方法使用二阶展开，误差一般在 O(∥pk∥3)，所以选择的方向会更准确。可以理解为，牛顿方法选择的方法更有“远见”，因为梯度下降的方向虽然在眼前的下降速度是最快的，但是在跑出一定距离之后就会比牛顿方向的表现差，所以要重新选择方向的次数比牛顿方法更多。</p><p>当然，牛顿方法的缺点也比较明显：首先它需要函数 f 是二次可导的，并且需要在 xk 的海塞矩阵 ∇2f(xk) 是正定的（这一般需要起始点 x0 距离极小点足够近）。再者，即使 ∇2f(xk) 是正定的，方向 pk 也不一定是降低方向，有可能 f(xk+pk)&gt;f(xk)。而且计算 f的海塞矩阵再求它的逆矩阵有时也需要很大的计算量。牛顿方法的种种缺陷都有相应的解决方法，本文中不进行探索，可以期待未来的文章中的讲解。</p><p>一个比较直接地使用牛顿方向的算法就是在 ∇2f(xk) 为正定的时候使用牛顿方向，而不是的时候使用梯度下降，这样可以在足够接近局部极小点的时候以高效率收敛。判断正定矩阵的方法也并不困难，因为一个对称矩阵是正定的当且仅当它的所有特征值(eigenvalues)都是大于零的，这个用矩阵计算库（比如 Numpy）都可以计算出来。<br/><br/>我们来看牛顿方法在二次函数 Q(x) 上的计算。</p><p><figure><noscript><img src="https://pic3.zhimg.com/v2-11ee8b42d257f1fd7dad216a1544ae32_b.png" data-rawwidth="800" data-rawheight="807" class="origin_image zh-lightbox-thumb" width="800" data-original="https://pic3.zhimg.com/v2-11ee8b42d257f1fd7dad216a1544ae32_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-11ee8b42d257f1fd7dad216a1544ae32_b.png" data-rawwidth="800" data-rawheight="807" class="origin_image zh-lightbox-thumb lazy" width="800" data-original="https://pic3.zhimg.com/v2-11ee8b42d257f1fd7dad216a1544ae32_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-11ee8b42d257f1fd7dad216a1544ae32_b.png"/></figure>居然一次就找到了最优解！但是稍微仔细想一下，二次函数的二次泰勒展开并不只是在局部估测这个函数，而是完全就等于这个函数。所以当我们最小化了二次泰勒展开时，实际上已经找到问题的极小点。好，我们看一下创造困难的 Rosenbrock 函数。<br/></p><p><figure><noscript><img src="https://pic2.zhimg.com/v2-df110cdf0d2b4bb8a41079cda396a2d9_b.png" data-rawwidth="1169" data-rawheight="793" class="origin_image zh-lightbox-thumb" width="1169" data-original="https://pic2.zhimg.com/v2-df110cdf0d2b4bb8a41079cda396a2d9_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-df110cdf0d2b4bb8a41079cda396a2d9_b.png" data-rawwidth="1169" data-rawheight="793" class="origin_image zh-lightbox-thumb lazy" width="1169" data-original="https://pic2.zhimg.com/v2-df110cdf0d2b4bb8a41079cda396a2d9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-df110cdf0d2b4bb8a41079cda396a2d9_b.png"/></figure>牛顿方法用了 31 步就找到了误差小于 0.00001 的近似解，相比于梯度下降的 5491 步是一个巨大的进步。不得不说，函数的二次导数 ∇2R 中包含了这条壕沟的很多秘密。梯度下降和牛顿方法的表现差异在放大的图里一目了然。<br/></p><figure><noscript><img src="https://pic2.zhimg.com/v2-33cc03b231ee26f3cf26a705a1a282d5_b.png" data-rawwidth="1164" data-rawheight="793" class="origin_image zh-lightbox-thumb" width="1164" data-original="https://pic2.zhimg.com/v2-33cc03b231ee26f3cf26a705a1a282d5_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-33cc03b231ee26f3cf26a705a1a282d5_b.png" data-rawwidth="1164" data-rawheight="793" class="origin_image zh-lightbox-thumb lazy" width="1164" data-original="https://pic2.zhimg.com/v2-33cc03b231ee26f3cf26a705a1a282d5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-33cc03b231ee26f3cf26a705a1a282d5_b.png"/></figure><p>当然了，可以选择的搜索方向远远不止梯度和牛顿两种，不过它们两个是所有方向算法的始祖。至于一些更复杂的方法，我们以后再慢慢来讲。<br/></p><br/><h4>步长</h4><p>上面介绍了每一步从 xk 出发的方向 pk，但这还不够，我们还需要选择一个在这个方向上行走的步长 αk 以达到新的点 xk+αkpk。在选择每一次迭代的步长时，我们都面临一个权衡：如果一步过长了，那么可能会走得太远，跨过了极小点；如果一步太短，那么并不能将目标函数 f 的取值降低太多。<br/><br/><b>最大下降步长</b></p><p>一个最直接最“贪心”的步长选择是在射线 {xk+αpk:α≥0} 上 f 取值最小的点。也就是说，解决一个最小化问题</p><p><figure><noscript><img src="https://pic4.zhimg.com/v2-9468779cbccc08dd82d0156ffb47b27f_b.png" data-rawwidth="248" data-rawheight="57" class="content_image" width="248"/></noscript><img src="https://pic4.zhimg.com/v2-9468779cbccc08dd82d0156ffb47b27f_b.png" data-rawwidth="248" data-rawheight="57" class="content_image lazy" width="248" data-actualsrc="https://pic4.zhimg.com/v2-9468779cbccc08dd82d0156ffb47b27f_b.png"/></figure>并以求出的极小点作为步长 α。这是一个在一维切片上的优化问题，将导数设为 0，<br/></p><p><figure><noscript><img src="https://pic3.zhimg.com/v2-867c6a5a0ef908671883a2f83a047106_b.png" data-rawwidth="391" data-rawheight="169" class="content_image" width="391"/></noscript><img src="https://pic3.zhimg.com/v2-867c6a5a0ef908671883a2f83a047106_b.png" data-rawwidth="391" data-rawheight="169" class="content_image lazy" width="391" data-actualsrc="https://pic3.zhimg.com/v2-867c6a5a0ef908671883a2f83a047106_b.png"/></figure>（运算中使用了高维版的链式法 (f∘g)′(x)=∇f(g(x))Tg′(x)）可以看出极值出现在 f 的梯度和方向 pk 垂直的位置。不巧的是，等式右侧函数的零点可能并不好求，一般来说寻找一个“足够好”的步长会比计算上面的“最好”步长更有效率。那我们怎么判断一个步长是不是足够好呢？接下来的 Wolfe 条件是一个常用的辨别方法。<br/></p><p><b>Wolfe 条件</b></p><p>在 Wolfe 条件中有两个不等式，需要首先选取 c1∈(0,1) 以及 c2∈(c1,1) 作为参数。条件中的第一个不等式要求步长 α 满足</p><p><figure><noscript><img src="https://pic3.zhimg.com/v2-51f60e33dd0c69773a7d2f129f0eca8e_b.png" data-rawwidth="412" data-rawheight="61" class="content_image" width="412"/></noscript><img src="https://pic3.zhimg.com/v2-51f60e33dd0c69773a7d2f129f0eca8e_b.png" data-rawwidth="412" data-rawheight="61" class="content_image lazy" width="412" data-actualsrc="https://pic3.zhimg.com/v2-51f60e33dd0c69773a7d2f129f0eca8e_b.png"/></figure>也就是说，从 xk 到 xk+αpk 的移动对 f 的缩减应该和步长 α 以及方向导数 ∇f<img src="https://www.zhihu.com/equation?tex=%28xk%29%5E%7BT%7D+" alt="(xk)^{T} " eeimg="1"/>pk 达成一定以上的比例。这个条件叫做<em>足量衰减条件（sufficient decrease condition）</em> 或者<em> Armijo 条件（Armijo condition）</em>。<br/></p><figure><noscript><img src="https://pic4.zhimg.com/v2-806a06b78adcd25ddab19b491f4126ab_b.png" data-rawwidth="1169" data-rawheight="789" class="origin_image zh-lightbox-thumb" width="1169" data-original="https://pic4.zhimg.com/v2-806a06b78adcd25ddab19b491f4126ab_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-806a06b78adcd25ddab19b491f4126ab_b.png" data-rawwidth="1169" data-rawheight="789" class="origin_image zh-lightbox-thumb lazy" width="1169" data-original="https://pic4.zhimg.com/v2-806a06b78adcd25ddab19b491f4126ab_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-806a06b78adcd25ddab19b491f4126ab_b.png"/></figure><p>上图的横轴是 {xk+αpk:α∈R} 的一维直线，实线曲线是目标函数 f 在这个一维截面上的图像。我们从左侧的红点处出发，虚线表示的是直线 L(xk+αpk)=f(xk)+c1α∇f<img src="https://www.zhihu.com/equation?tex=%28xk%29%5E%7BT%7D+" alt="(xk)^{T} " eeimg="1"/>pk 的图像，如果 c1=1 的话那么虚线恰恰是曲线在 xk 的切线。稍经分析可以看出，所有曲线低于虚线的位置都满足第一个不等式。实际应用中的 c1一般设得很小，<img src="https://www.zhihu.com/equation?tex=10%5E%7B-4%7D+" alt="10^{-4} " eeimg="1"/>是一个比较常用的值，不过应随应用场景进行调整。</p><p>这里可以看出一个问题：起点的附近都小于虚线，没有被不等式排除；但我们并不想选过短的步长，因为这样离极小点依旧很远。因此，Wolfe 条件中有第二个不等式</p><p><figure><noscript><img src="https://pic4.zhimg.com/v2-9967827f3a809c19a7c11d554c5b8e13_b.png" data-rawwidth="365" data-rawheight="52" class="content_image" width="365"/></noscript><img src="https://pic4.zhimg.com/v2-9967827f3a809c19a7c11d554c5b8e13_b.png" data-rawwidth="365" data-rawheight="52" class="content_image lazy" width="365" data-actualsrc="https://pic4.zhimg.com/v2-9967827f3a809c19a7c11d554c5b8e13_b.png"/></figure>这个不等式叫做<em>曲率条件（curvature condition）</em>，它的意思是落脚点的梯度应该大于一定比例的起始点的梯度。我们使用的向量 pk 使得该方向的梯度 ∇f<img src="https://www.zhihu.com/equation?tex=%28xk%29%5E%7BT%7D+" alt="(xk)^{T} " eeimg="1"/>pk 小于 0，因此满足不等式的点的梯度应该更缓和（更接近 0），或者是向相反方向的（大于 0）。毕竟，在一个局部的极值点上，函数的梯度必定是 0，所以从一个梯度为负的点出发，我们应该要求梯度变得越来越大，才有可能找到极小点。<br/></p><p><figure><noscript><img src="https://pic3.zhimg.com/v2-aa854c471badb27605803f42af4dde6a_b.png" data-rawwidth="1160" data-rawheight="790" class="origin_image zh-lightbox-thumb" width="1160" data-original="https://pic3.zhimg.com/v2-aa854c471badb27605803f42af4dde6a_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-aa854c471badb27605803f42af4dde6a_b.png" data-rawwidth="1160" data-rawheight="790" class="origin_image zh-lightbox-thumb lazy" width="1160" data-original="https://pic3.zhimg.com/v2-aa854c471badb27605803f42af4dde6a_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-aa854c471badb27605803f42af4dde6a_b.png"/></figure>上图中，所有梯度大于起始点（也就是下降速度比起始点的下降速度慢）的区域都满足第二个不等式。在应用中，c2应根据选择 pk 的方法进行选择。在这个示例中，同时满足 Wolfe 条件中两个不等式的区域如下图所示。<br/></p><figure><noscript><img src="https://pic4.zhimg.com/v2-7df1f0fc1aefe515dae920f0c759601b_b.png" data-rawwidth="1161" data-rawheight="781" class="origin_image zh-lightbox-thumb" width="1161" data-original="https://pic4.zhimg.com/v2-7df1f0fc1aefe515dae920f0c759601b_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-7df1f0fc1aefe515dae920f0c759601b_b.png" data-rawwidth="1161" data-rawheight="781" class="origin_image zh-lightbox-thumb lazy" width="1161" data-original="https://pic4.zhimg.com/v2-7df1f0fc1aefe515dae920f0c759601b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-7df1f0fc1aefe515dae920f0c759601b_b.png"/></figure><p>寻找一个满足 Wolfe 条件的步长并不是很困难，但确实是比较麻烦。今天就假设我们又笨又懒，只想找一个简单粗暴有效可以完成任务的方法，往下看。<br/><br/><b>回溯算法</b></p><p>记得 Wolfe 条件中有两个不等式：足量下降条件和曲率条件，前者保证了落脚点的取值和局部最小值更接近，后者保证落脚点足够平缓。现在，我们要抛弃要求平缓落脚的曲率条件，只要求每步有足够的下降，这个很好满足，因为 pk 是一个下降方向并且 c1&lt;1，所以只要 α&gt;0 足够小，必定有 f(xk+αpk)≤f(xk)+c1α∇f<img src="https://www.zhihu.com/equation?tex=xk%5E%7BT%7D+" alt="xk^{T} " eeimg="1"/>pk。所以有以下<em>回溯算法（backtrack algorithm）</em>：</p><figure><noscript><img src="https://pic4.zhimg.com/v2-e6fdef8729e24f897bfe61d1b2f1dd9f_b.png" data-rawwidth="483" data-rawheight="206" class="origin_image zh-lightbox-thumb" width="483" data-original="https://pic4.zhimg.com/v2-e6fdef8729e24f897bfe61d1b2f1dd9f_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-e6fdef8729e24f897bfe61d1b2f1dd9f_b.png" data-rawwidth="483" data-rawheight="206" class="origin_image zh-lightbox-thumb lazy" width="483" data-original="https://pic4.zhimg.com/v2-e6fdef8729e24f897bfe61d1b2f1dd9f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-e6fdef8729e24f897bfe61d1b2f1dd9f_b.png"/></figure><p>这个算法输出的步长会有足够的下降，并且它保证了这一步不会太短：如果回溯的输出是 αk，那么我们知道 αk/ρ 作为一个步长是太长了，因为它不满足足量下降条件；所以虽然 αk 可能不够精确，但不会太短。</p><p>初始的 α¯ 决定了输出步长的上限，用梯度下降或者牛顿方法时都可以将它设为 α¯=1。另外一个参数 ρ 的选择面临一个权衡：如果 ρ 接近 0 的话，回溯算法会在更少的循环中完成，但整体的优化搜索收敛得会更慢；如果 ρ 接近 1 则正相反。一般来说，选择的方向越精确就可以把 ρ 设得越大。</p><p>回溯算法对于牛顿方向是一个非常好用的步长计算方法，但对于一些文中没有提到方向（比如拟牛顿(quasi-Newton)和共轭梯度(conjugate gradient)）就不太好用了，不过，这些都是以后的话题了。</p><h4>一个完整的线搜索算法</h4><p>正如之前所说，线搜索方法是由一个选择方向的算法和一个寻找步长的算法组成的，在有了这两个主要成分之后就可以拼凑起来。给定一个函数 f 和起始点 x0，我们计算从 x0 出发的下降方向 p0，再计算相对应的步长 α0，并得到一个新的点 x1=x0+α0p0。再从 x1 开始，计算 x2=x1+α1p1。如此重复，得到一个序列 x3,x4,x5,…，并且知道（或者不知道但假设）这个序列会收敛于一个局部极小点。那么问题来了，在绝大多数情况下我们不可能得到确切的极小点，那么我们该在什么时候终止迭代的过程呢？</p><p>一般用到的终止条件有两个：迭代次数和函数误差。迭代次数很好理解，我们设一个次数 K，那么在计算出 xK 后就终止计算，并且以 xK 作为极小点的估测；这是一个以运算时间作为标准的终止条件。与之相对的，函数误差是一个计算精度为标准的条件，我们设定一个 ε&gt;0，并且假设序列 x0,x1,… 收敛于某个点 x∗∈<img src="https://www.zhihu.com/equation?tex=R%5E%7Bn%7D+" alt="R^{n} " eeimg="1"/>，希望在 |f(xk)−f(x∗)|&lt;ε 时中止计算。不过这并不现实，虽然 f(x∗) 比 x∗ 更好找，但很多时候我们还是很难把它计算出来；一个可以替代的条件是，当出现 |f(xk)−f(xk−1)| 的情况时终止计算，并输出 xk。精度和时间的终止条件可以同时使用。</p><p>所有部件都已齐全，一个完整的线搜索的优化算法如下：<br/></p><figure><noscript><img src="https://pic2.zhimg.com/v2-74efee01465230cccae824f71e7622d9_b.png" data-rawwidth="596" data-rawheight="814" class="origin_image zh-lightbox-thumb" width="596" data-original="https://pic2.zhimg.com/v2-74efee01465230cccae824f71e7622d9_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-74efee01465230cccae824f71e7622d9_b.png" data-rawwidth="596" data-rawheight="814" class="origin_image zh-lightbox-thumb lazy" width="596" data-original="https://pic2.zhimg.com/v2-74efee01465230cccae824f71e7622d9_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-74efee01465230cccae824f71e7622d9_b.png"/></figure><p>Python 代码在后面的研究模块中可以找到。<br/></p><br/><h4>结语</h4><p>无约束的非线性规划问题有很多在实际场景中的应用，但更多的应用还是在于有约束的规划问题。比如在我们都非常关心的资产配置问题中，如果用 xi 代表在金融资产 i 上的分配权重，我们会要求资产配置的总和是 1，也就是 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bxi%3D1%7D+" alt="\sum_{i=1}^{n}{xi=1} " eeimg="1"/>；我们可能要求不能做空，那么有 xi≥0；也许又要求任何一种资产的权重都不超过总资产的三分之一，于是又有 xi≤1/3。这些要求让优化问题变得更困难但也更有趣，有时我们需要为一组特定的约束条件专门研究一套算法来解决。一切才刚刚开始，请读者期尽情待量化课堂未来的文章。</p><p>到JoinQuant查看策略并与作者交流讨论：<a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//www.joinquant.com/post/3361%3Ff%3Dzhzl%26m%3D23562000" target="_blank" rel="nofollow noreferrer">【量化课堂】无约束的非线性规划问题 -- 线搜索方法</a></p>