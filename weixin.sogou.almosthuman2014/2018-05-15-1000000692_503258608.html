<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>入门 | 关于神经网络：你需要知道这些</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1527195809&amp;src=3&amp;ver=1&amp;signature=Km2B6ZDo6eRtFM3ycPBIC80f6Hyg-gvlEZMvL3Y42q8qiHvyDtAzKjixYHRSjkD9-tQiIrXS1M440gQly2SCoPgznFoHcIXrk4BSX*0ZKg7K9-WFls0G7bFbap6tzWiSLNdvGaU1M4FZbjryGjPzZpGTt5y3WJYzlY4hMhabrfo=">原文</a></p>
<div id="js_top_ad_area" class="top_banner"></div><div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        
            <div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    入门 | 关于神经网络：你需要知道这些                                                                                </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <span class="rich_media_meta rich_media_meta_nickname" id="profileBt">
                      <a href="javascript:void(0);">
                        机器之心                      </a>
                      <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                          <div class="profile_inner">
                              <strong class="profile_nickname">机器之心</strong>
                              <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                              <p class="profile_meta">
                              <label class="profile_meta_label">微信号</label>
                              <span class="profile_meta_value">almosthuman2014</span>
                              </p>

                              <p class="profile_meta">
                              <label class="profile_meta_label">功能介绍</label>
                              <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                              </p>
                              
                          </div>
                          <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                              <i class="profile_arrow arrow_out"></i>
                              <i class="profile_arrow arrow_in"></i>
                          </span>
                      </div>
                    </span>

                    <em id="publish_time" class="rich_media_meta rich_media_meta_text">2018-05-15</em>

                                    </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><section style="white-space: normal;max-width: 100%;color: rgb(51, 51, 51);"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自towardsdatascience</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong><span style="max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">Suryansh S.</span></strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：</strong></span><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">白妤昕、路</span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(136, 136, 136);"><span style="font-size: 15px;text-align: justify;">神经网络（NN）几乎可以在每个领域帮助我们用创造性的方式解决问题。本文将介绍神经网络的相关知识。读后你将对神经网络有个大概了解，它是如何工作的？如何创建神经网络？</span></span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">本文涉及以下内容：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">神经网络的发展历史</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">什么是真正的神经网络？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">单元/神经元</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">权重/参数/连接</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">偏置项</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">超参数</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">激活函数</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">层</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">神经网络学习时发生了什么？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">实现细节（如何管理项目中的所有因素）</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">关于神经网络的更多信息（更多资源链接）</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">神经网络的发展历史</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">我们简单回顾一下神经网络的发展历程，如果你想了解更多关于其发展历程的信息，请看这篇维基百科的文章（https://en.wikipedia.org/wiki/Artificial_neural_network#History），它是本章节的基础。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">神经网络起源于 WarrenMcCulloch 和 Walter Pitts 于 1943 年首次建立的神经网络模型。他们的模型完全基于数学和算法，由于缺乏计算资源，模型无法测试。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">后来，在 1958 年，Frank Rosenblatt 创建了第一个可以进行模式识别的模型，改变了现状。即感知器。但是他只提出了 notation 和模型。实际的神经网络模型仍然无法测试，此前的相关研究也较少。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">第一批可以测试并具有多个层的神经网络于 1965 年由 Alexey Ivakhnenko 和 Lapa 创建。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">之后，由于机器学习模型具有很强可行性，神经网络的研究停滞不前。很多人认为这是因为 Marvin Minsky 和 Seymour Papert 在 1969 年完成的书《感知机》（Perceptrons）导致的。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">然而，这个停滞期相对较短。6 年后，即 1975 年，Paul Werbos 提出反向传播，解决了 XOR 问题，并且使神经网络的学习效率更高。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">1992 年，最大池化（max-pooling）被提出，这有助于 3D 目标识别，因为它具备平移不变性，对变形具备一定鲁棒性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">2009 年至 2012 年间，JürgenSchmidhuber 研究小组创建的循环神经网络和深度前馈神经网络获得了模式识别和机器学习领域 8 项国际竞赛的冠军。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">2011 年，深度学习神经网络开始将卷积层与最大池化层合并，然后将其输出传递给几个全连接层，再传递给输出层。这些被称为卷积神经网络。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在这之后还有更多的研究。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">什么是神经网络？</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">了解神经网络的一个好方法是将它看作复合函数。你输入一些数据，它会输出一些数据。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">3 个部分组成了神经网络的的基本架构：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">单元/神经元</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">连接/权重/参数</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">偏置项</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">你可以把它们看作建筑物的「砖块」。根据你希望建筑物拥有的功能来安排砖块的位置。水泥是权重。无论权重多大，如果没有足够的砖块，建筑物还是会倒塌。然而，你可以让建筑以最小的精度运行（使用最少的砖块），然后逐步构建架构来解决问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我将在后面的章节中更多地讨论权重、偏置项和单元。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">单元/神经元</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">作为神经网络架构三个部分中最不重要的部分，神经元是包含权重和偏置项的函数，等待数据传递给它们。接收数据后，它们执行一些计算，然后使用激活函数将数据限制在一个范围内（多数情况下）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">我们将这些单元想象成一个包含权重和偏置项的盒子。盒子从两端打开。一端接收数据，另一端输出修改后的数据。数据首先进入盒子中，将权重与数据相乘，再向相乘的数据添加偏置项。这是一个单元，也可以被认为是一个函数。该函数与下面这个直线方程类似：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.6147368421052631" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicd0IoYr8h2V5GNA4Q0S3MhHcy3qxsnyZA2uRluGCqFnab5J58hQ0ZmziaiaYS4Oib720IIsCzacsevQ/640?wx_fmt=png" data-type="png" data-w="475" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">想象一下有多个直线方程，超过 2 个可以促进神经网络中的非线性。从现在开始，你将为同一个数据点（输入）计算多个输出值。这些输出值将被发送到另一个单元，然后神经网络会计算出最终输出值。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">权重/参数/连接</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">作为神经网络最重要的部分，这些（和偏置项）是用神经网络解决问题时必须学习的数值。这就是你现在需要知道的。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">偏置项</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这些数字代表神经网络认为其在将权重与数据相乘之后应该添加的内容。当然，它们经常出错，但神经网络随后也学习到最佳偏置项。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">超参数</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">超参数必须手动设置。如果将神经网络看作一台机器，那么改变机器行为的 nob 就是神经网络的超参数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">你可以阅读我的另一篇文章（https://towardsdatascience.com/gas-and-nns-6a41f1e8146d），了解如何优化神经网络超参数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">激活函数</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">也称为映射函数（mapping function）。它们在 x 轴上输入数据，并在有限的范围内（大部分情况下）输出一个值。大多数情况下，它们被用于将单元的较大输出转换成较小的值。你选择的激活函数可以大幅提高或降低神经网络的性能。如果你喜欢，你可以为不同的单元选择不同的激活函数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">以下是一些常见的激活函数：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Sigmoid</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.665625" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicd0IoYr8h2V5GNA4Q0S3Mh9Z7QdY3aIsNmSicTcH3S3moqFwiczMqj6ialqTnzZt2BUVgd4SCic8sVhQ/640?wx_fmt=png" data-type="png" data-w="320" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">Sigmoid 函数</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Tanh</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5095729013254786" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicd0IoYr8h2V5GNA4Q0S3MhyKxmFQ77qRB1ddBjEcrviaNgicd4RwTOM6U1AcicGcplXtWt1tSAvFt4Q/640?wx_fmt=png" data-type="png" data-w="679" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">tanh 函数</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">ReLU：修正线性单元</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.7536945812807881" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicd0IoYr8h2V5GNA4Q0S3MhWoDxYu8EicPIusWCCgibkRm2QzQInd5qdFmwX4orVOArVu9leAooqOvw/640?wx_fmt=png" data-type="png" data-w="812" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">修正线性单元函数</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Leaky ReLU</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.6748251748251748" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicd0IoYr8h2V5GNA4Q0S3MhNKCaOLv2qRWI5Rqf5hWOpsHP1B4wxB4cRYW1Gm4dMlJMTGZglibhPaw/640?wx_fmt=png" data-type="png" data-w="572" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">Leaky ReLU 函数</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">层</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">这是神经网络在任何问题中都可获得复杂度的原因。增加层（具备单元）可增加神经网络输出的非线性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">每个层都包含一定数量的单元。大多数情况下单元的数量完全取决于创建者。但是，对于一个简单的任务而言，层数过多会增加不必要的复杂性，且在大多数情况下会降低其准确率。反之亦然。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">每个神经网络有两层：输入层和输出层。二者之间的层称为隐藏层。下图所示的神经网络包含一个输入层（8 个单元）、一个输出层（4 个单元）和 3 个隐藏层（每层包含 9 个单元）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5996309963099631" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicd0IoYr8h2V5GNA4Q0S3Mh5QZOhZN4yeHN6bgibho4dreDLQXU88Ebj7caYoB6sXJaicqYHILHwTpg/640?wx_fmt=png" data-type="png" data-w="542" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">深度神经网络</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">具有两个或更多隐藏层且每层包含大量单元的神经网络称为深度神经网络，它催生了深度学习这一新的学习领域。上图所示神经网络就是这样一个例子。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">神经网络学习时发生了什么？</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">教神经网络解决问题的最常见方式是使用梯度下降。梯度下降相关内容，参见：https://hackernoon.com/gradient-descent-aynk-7cbe95a778da。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">除梯度下降外，另一种常见的训练神经网络方法是使用反向传播。使用这种方法，神经网络输出层的误差会通过微积分中的链式规则向后传播。这对于没有微积分知识的初学者来说可能会难以理解，但也不要被吓倒，反向传播相关内容，推荐阅读：http://neuralnetworksanddeeplearning.com/chap2.html。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">训练神经网络有许多注意事项。但对于初学者来说，没有必要在一篇文章中了解全部。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">实现细节（如何管理项目中的所有因素）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了解释如何管理项目中的所有因素，我创建了一个 Jupyter Notebook，包含一个学习 XOR 逻辑门的小型神经网络。Jupyter Notebook 地址：https://github.com/Frixoe/xor-neural-network/blob/master/XOR-Net-Notebook.ipynb。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在查看并理解 Notebook 内容后，你应该对如何构建基础神经网络有一个大致的了解。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Notebook 创建的神经网络的训练数据以矩阵排列，这是常见的数据排列方式。不同项目中的矩阵维度可能会有所不同。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">大量数据通常分为两类：训练数据（60％）和测试数据（40％）。神经网络先使用训练数据，然后在测试数据上测试网络的准确率。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">关于神经网络的更多信息（更多资源链接）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">如果你仍然无法理解神经网络，那么推荐以下资源：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">YouTube：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">Siraj Raval (https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">3Blue1Brown (https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">The Coding Train (https://www.youtube.com/playlist?list=PLRqwX-V7Uu6aCibgK1PTWWu9by6XFdCfh)</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">Brandon Rohrer (https://www.youtube.com/channel/UCsBKTrp45lTfHa_p49I2AEQ)</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">giant_neural_network (https://www.youtube.com/channel/UCrBzGHKmGDcwLFnQGHJ3XYg)</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">Hugo Larochelle (https://www.youtube.com/channel/UCiDouKcxRmAdc5OeZdiRwAg)</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">Jabrils (https://www.youtube.com/channel/UCQALLeQPoZdZC4JNUboVEUg)</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">Luis Serrano (https://www.youtube.com/channel/UCgBncpylJ1kiVaPyP-PZauQ)</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">Coursera：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">Neural Networks for Machine Learning (https://www.coursera.org/learn/neural-networks) by University of Toronto</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">Deep Learning Specialization (https://www.coursera.org/specializations/deep-learning) by Andrew Ng</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;">Introduction to Deep Learning (https://www.coursera.org/learn/intro-to-deep-learning) by National Research University Higher School of Economics </span></p></li></ul><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="white-space: normal;"><span style="color: rgb(136, 136, 136);"><em><span style="text-align: justify;font-size: 12px;">原文链接：https://towardsdatascience.com/nns-aynk-c34efe37f15a</span></em></span></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：<strong style="max-width: 100%;color: rgb(62, 62, 62);font-size: 18px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">content</span></strong>@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="132474944" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg_new/winwx3db0db.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc appmsg_card_context" id="js_preview_reward" style="display:none;">
                    <div class="reward_inner">
                        <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                        <p>
                            <a class="reward_access" id="js_preview_reward_link" href="##">赞赏</a>
                        </p>
                    </div>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                                        
                                

                        
            <ul id="js_hotspot_area" class="article_extend_area"></ul>


            
                        <div class="rich_media_tool" id="js_toobar3">

                                            <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>
                <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">

                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>


        
        <div class="rich_media_area_extra">
            
            <div id="js_share_appmsg">
            </div>

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_container" style="display:none">
              
              <div class="discuss_mod" id="js_cmt_title" style="display:none">
                <div class="discuss_container">
                  <div class="weui-loadmore weui-loadmore_line mod_title_context_primary">
                    <span class="weui-loadmore__tips">留言</span>
                  </div>
                </div>
              </div>

              
              <div class="discuss_mod" id="js_friend_cmt_area" style="display:none">
                
                
                
              </div>

                            <div class="discuss_mod" id="js_cmt_area" style="display:none">
              </div>
                          </div>
        </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
