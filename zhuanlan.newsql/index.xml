<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Wed, 10 Jul 2019 11:20:52 +0800</lastBuildDate>
<item>
<title>TiKV 源码解析系列文章（十）Snapshot 的发送和接收</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-09-72880848.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/72880848&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-72226a1aa2a61516be7404c2f7b95548_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄梦龙&lt;/p&gt;&lt;h2&gt;背景知识&lt;/h2&gt;&lt;p&gt;TiKV 使用 Raft 算法来提供高可用且具有强一致性的存储服务。在 Raft 中，Snapshot 指的是整个 State Machine 数据的一份快照，大体上有以下这几种情况需要用到 Snapshot：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;正常情况下 leader 与 follower/learner 之间是通过 append log 的方式进行同步的，出于空间和效率的考虑，leader 会定期清理过老的 log。假如 follower/learner 出现宕机或者网络隔离，恢复以后可能所缺的 log 已经在 leader 节点被清理掉了，此时只能通过 Snapshot 的方式进行同步。&lt;/li&gt;&lt;li&gt;Raft 加入新的节点的，由于新节点没同步过任何日志，只能通过接收 Snapshot 的方式来同步。实际上这也可以认为是 1 的一种特殊情形。&lt;/li&gt;&lt;li&gt;出于备份/恢复等需求，应用层需要 dump 一份 State Machine 的完整数据。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;TiKV 涉及到的是 1 和 2 这两种情况。在我们的实现中，Snapshot 总是由 Region leader 所在的 TiKV 生成，通过网络发送给 Region follower/learner 所在的 TiKV。&lt;/p&gt;&lt;p&gt;理论上讲，我们完全可以把 Snapshot 当作普通的 &lt;code&gt;RaftMessage&lt;/code&gt; 来发送，但这样做实践上会产生一些问题，主要是因为 Snapshot 消息的尺寸远大于其他 &lt;code&gt;RaftMessage&lt;/code&gt;：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Snapshot 消息需要花费更长的时间来发送，如果共用网络连接容易导致网络拥塞，进而引起其他 Region 出现 Raft 选举超时等问题。&lt;/li&gt;&lt;li&gt;构建待发送 Snapshot 消息需要消耗更多的内存。&lt;/li&gt;&lt;li&gt;过大的消息可能导致超出 gRPC 的 Message Size 限制等问题。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;基于上面的原因，TiKV 对 Snapshot 的发送和接收进行了特殊处理，为每个 Snapshot 创建单独的网络连接，并将 Snapshot 拆分成 1M 大小的多个 Chunk 进行传输。&lt;/p&gt;&lt;h2&gt;源码解读&lt;/h2&gt;&lt;p&gt;下面我们分别从 RPC 协议、发送 Snapshot、收取 Snapshot 三个方面来解读相关源代码。本文的所有内容都基于 v3.0.0-rc.2 版本。&lt;/p&gt;&lt;h3&gt;Snapshot RPC call 的定义&lt;/h3&gt;&lt;p&gt;与普通的 raft message 类似，Snapshot 消息也是使用 gRPC 远程调用的方式来传输的。在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/kvproto&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pingcap/kvproto&lt;/a&gt; 项目中可以找到相关 RPC Call 的定义，具体在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/kvproto/blob/5cb23649b361013f929e0d46a166ae24848fbcbb/proto/tikvpb.proto%23L57&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tikvpb.proto&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/kvproto/blob/5cb23649b361013f929e0d46a166ae24848fbcbb/proto/raft_serverpb.proto%23L42-L47&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;raft_serverpb.proto&lt;/a&gt; 文件中。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;rpc Snapshot(stream raft_serverpb.SnapshotChunk) returns (raft_serverpb.Done) {}
...
message SnapshotChunk {
  RaftMessage message = 1;
  bytes data = 2;
}

message Done {}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看出，Snapshot 被定义成 client streaming 调用，即对于每个 Call，客户端依次向服务器发送多个相同类型的请求，服务器接收并处理完所有请求后，向客户端返回处理结果。具体在这里，每个请求的类型是 &lt;code&gt;SnapshotChunk&lt;/code&gt;，其中包含了 Snapshot 对应的 &lt;code&gt;RaftMessage&lt;/code&gt;，或者携带一段 Snapshot 数据；回复消息是一个简单的空消息 &lt;code&gt;Done&lt;/code&gt;，因为我们在这里实际不需要返回任何信息给客户端，只需要关闭对应的 stream。&lt;/p&gt;&lt;h3&gt;Snapshot 的发送流程&lt;/h3&gt;&lt;p&gt;Snapshot 的发送过程的处理比较简单粗暴，直接在将要发送 &lt;code&gt;RaftMessage&lt;/code&gt; 的地方截获 Snapshot 类型的消息，转而通过特殊的方式进行发送。相关代码可以在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/transport.rs%23L313-L344&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;server/transport.rs&lt;/a&gt; 中找到：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn write_data(&amp;amp;self, store_id: u64, addr: &amp;amp;str, msg: RaftMessage) {
  if msg.get_message().has_snapshot() {
      return self.send_snapshot_sock(addr, msg);
  }
  if let Err(e) = self.raft_client.wl().send(store_id, addr, msg) {
      error!(&amp;#34;send raft msg err&amp;#34;; &amp;#34;err&amp;#34; =&amp;gt; ?e);
  }
}

fn send_snapshot_sock(&amp;amp;self, addr: &amp;amp;str, msg: RaftMessage) {
  ...
  if let Err(e) = self.snap_scheduler.schedule(SnapTask::Send {
      addr: addr.to_owned(),
      msg,
      cb,
  }) {
      ...
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从代码中可以看出，这里简单地把对应的 &lt;code&gt;RaftMessage&lt;/code&gt; 包装成一个 &lt;code&gt;SnapTask::Send&lt;/code&gt; 任务，并将其交给独立的 &lt;code&gt;snap-worker&lt;/code&gt; 去处理。值得注意的是，这里的 &lt;code&gt;RaftMessage&lt;/code&gt; 只包含 Snapshot 的元信息，而不包括真正的快照数据。TiKV 中有一个单独的模块叫做 &lt;code&gt;SnapManager&lt;/code&gt; ，用来专门处理数据快照的生成与转存，稍后我们将会看到从 &lt;code&gt;SnapManager&lt;/code&gt; 模块读取 Snapshot 数据块并进行发送的相关代码。&lt;/p&gt;&lt;p&gt;我们不妨顺藤摸瓜来看看 &lt;code&gt;snap-worker&lt;/code&gt; 是如何处理这个任务的，相关代码在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/snap.rs%23L329-L398&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;server/snap.rs&lt;/a&gt;，精简掉非核心逻辑后的代码引用如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn run(&amp;amp;mut self, task: Task) {
  match task {
      Task::Recv { stream, sink } =&amp;gt; {
           ...
           let f = recv_snap(stream, sink, ...).then(move |result| {
               ...
           });
           self.pool.spawn(f).forget();
      }
      Task::Send { addr, msg, cb } =&amp;gt; {
          ...
          let f = future::result(send_snap(..., &amp;amp;addr, msg))
              .flatten()
              .then(move |res| {
                  ...
              });
          self.pool.spawn(f).forget();
      }
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;snap-worker&lt;/code&gt; 使用了 &lt;code&gt;future&lt;/code&gt; 来完成收发 Snapshot 任务：通过调用 &lt;code&gt;send_snap()&lt;/code&gt; 或 &lt;code&gt;recv_snap()&lt;/code&gt; 生成一个 future 对象，并将其交给 &lt;code&gt;FuturePool&lt;/code&gt; 异步执行。&lt;/p&gt;&lt;p&gt;现在我们暂且只关注 &lt;code&gt;send_snap()&lt;/code&gt; 的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/snap.rs%23L103-L175&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;实现&lt;/a&gt;：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn send_snap(
  ...
  addr: &amp;amp;str,
  msg: RaftMessage,
) -&amp;gt; Result&amp;lt;impl Future&amp;lt;Item = SendStat, Error = Error&amp;gt;&amp;gt; {
  ...
  let key = {
      let snap = msg.get_message().get_snapshot();
      SnapKey::from_snap(snap)?
  };
  ...
  let s = box_try!(mgr.get_snapshot_for_sending(&amp;amp;key));
  if !s.exists() {
      return Err(box_err!(&amp;#34;missing snap file: {:?}&amp;#34;, s.path()));
  }
  let total_size = s.total_size()?;
  let chunks = {
      let mut first_chunk = SnapshotChunk::new();
      first_chunk.set_message(msg);

      SnapChunk {
          first: Some(first_chunk),
          snap: s,
          remain_bytes: total_size as usize,
      }
  };

  let cb = ChannelBuilder::new(env);
  let channel = security_mgr.connect(cb, addr);
  let client = TikvClient::new(channel);
  let (sink, receiver) = client.snapshot()?;

  let send = chunks.forward(sink).map_err(Error::from);
  let send = send
      .and_then(|(s, _)| receiver.map_err(Error::from).map(|_| s))
      .then(move |result| {
          ...
      });
  Ok(send)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这一段流程还是比较清晰的：先是用 Snapshot 元信息从 &lt;code&gt;SnapManager&lt;/code&gt; 取到待发送的快照数据，然后将 &lt;code&gt;RaftMessage&lt;/code&gt; 和 &lt;code&gt;Snap&lt;/code&gt; 一起封装进 &lt;code&gt;SnapChunk&lt;/code&gt; 结构，最后创建全新的 gRPC 连接及一个 Snapshot stream 并将 &lt;code&gt;SnapChunk&lt;/code&gt; 写入。这里引入 &lt;code&gt;SnapChunk&lt;/code&gt; 是为了避免将整块 Snapshot 快照一次性加载进内存，它 impl 了 &lt;code&gt;futures::Stream&lt;/code&gt; 这个 trait 来达成按需加载流式发送的效果。如果感兴趣可以参考它的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/snap.rs%23L55-L92&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体实现&lt;/a&gt;，本文就暂不展开了。&lt;/p&gt;&lt;h3&gt;Snapshot 的收取流程&lt;/h3&gt;&lt;p&gt;最后我们来简单看一下 Snapshot 的收取流程，其实也就是 gRPC Call 的 server 端对应的处理，整个流程的入口我们可以在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/service/kv.rs%23L714-L729&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;server/service/kv.rs&lt;/a&gt; 中找到：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn snapshot(
  &amp;amp;mut self,
  ctx: RpcContext&amp;lt;&amp;#39;_&amp;gt;,
  stream: RequestStream&amp;lt;SnapshotChunk&amp;gt;,
  sink: ClientStreamingSink&amp;lt;Done&amp;gt;,
) {
  let task = SnapTask::Recv { stream, sink };
  if let Err(e) = self.snap_scheduler.schedule(task) {
      ...
  }
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;与发送过程类似，也是直接构建 &lt;code&gt;SnapTask::Recv&lt;/code&gt; 任务并转发给 &lt;code&gt;snap-worker&lt;/code&gt; 了，这里会调用上面出现过的 &lt;code&gt;recv_snap()&lt;/code&gt; 函数，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/tikv/tikv/blob/892c12039e0213989940d29c232bddee9cbe4686/src/server/snap.rs%23L237-L291&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;具体实现&lt;/a&gt; 如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;fn recv_snap&amp;lt;R: RaftStoreRouter + &amp;#39;static&amp;gt;(
  stream: RequestStream&amp;lt;SnapshotChunk&amp;gt;,
  sink: ClientStreamingSink&amp;lt;Done&amp;gt;,
  ...
) -&amp;gt; impl Future&amp;lt;Item = (), Error = Error&amp;gt; {
  ...
  let f = stream.into_future().map_err(|(e, _)| e).and_then(
      move |(head, chunks)| -&amp;gt; Box&amp;lt;dyn Future&amp;lt;Item = (), Error = Error&amp;gt; + Send&amp;gt; {
          let context = match RecvSnapContext::new(head, &amp;amp;snap_mgr) {
              Ok(context) =&amp;gt; context,
              Err(e) =&amp;gt; return Box::new(future::err(e)),
          };

          ...
          let recv_chunks = chunks.fold(context, |mut context, mut chunk| -&amp;gt; Result&amp;lt;_&amp;gt; {
              let data = chunk.take_data();
              ...
              if let Err(e) = context.file.as_mut().unwrap().write_all(&amp;amp;data) {
                  ...
              }
              Ok(context)
          });

          Box::new(
              recv_chunks
                  .and_then(move |context| context.finish(raft_router))
                  .then(move |r| {
                      snap_mgr.deregister(&amp;amp;context_key, &amp;amp;SnapEntry::Receiving);
                      r
                  }),
          )
      },
  );
  f.then(move |res| match res {
      ...
  })
  .map_err(Error::from)
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;值得留意的是 stream 中的第一个消息（其中包含有 &lt;code&gt;RaftMessage&lt;/code&gt;）被用来创建 &lt;code&gt;RecvSnapContext&lt;/code&gt; 对象，其后的每个 chunk 收取后都依次写入文件，最后调用 &lt;code&gt;context.finish()&lt;/code&gt; 把之前保存的 &lt;code&gt;RaftMessage&lt;/code&gt; 发送给 &lt;code&gt;raftstore&lt;/code&gt; 完成整个接收过程。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;以上就是 TiKV 发送和接收 Snapshot 相关的代码解析了。这是 TiKV 代码库中较小的一个模块，它很好地解决了由于 Snapshot 消息特殊性所带来的一系列问题，充分应用了 &lt;code&gt;grpc-rs&lt;/code&gt; 组件及 &lt;code&gt;futures&lt;/code&gt;/&lt;code&gt;FuturePool&lt;/code&gt; 模型，大家可以结合本系列文章的 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tikv-source-code-reading-7/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;第七篇&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tikv-source-code-reading-8/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;第八篇&lt;/a&gt; 进一步拓展学习。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tikv-source-code-reading-10/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章（十）Snapshot 的发送和接收&lt;/a&gt;&lt;p&gt;&lt;b&gt;更多 TiKV 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-09-72880848</guid>
<pubDate>Tue, 09 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 源码解析系列文章（九）Service 层处理流程解析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-08-72640867.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/72640867&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3505224dad3b818557ddf813893b94ba_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：周振靖&lt;/p&gt;&lt;p&gt;之前的 TiKV 源码解析系列文章介绍了 TiKV 依赖的周边库，从本篇文章开始，我们将开始介绍 TiKV 自身的代码。本文重点介绍 TiKV 最外面的一层——Service 层。&lt;/p&gt;&lt;p&gt;TiKV 的 Service 层的代码位于 &lt;code&gt;src/server&lt;/code&gt; 文件夹下，其职责包括提供 RPC 服务、将 store id 解析成地址、TiKV 之间的相互通信等。这一部分的代码并不是特别复杂。本篇将会简要地介绍 Service 层的整体结构和组成 Service 层的各个组件。&lt;/p&gt;&lt;h2&gt;整体结构&lt;/h2&gt;&lt;p&gt;位于 &lt;code&gt;src/server/server.rs&lt;/code&gt; 文件中的 &lt;code&gt;Server&lt;/code&gt; 是我们本次介绍的 Service 层的主体。它封装了 TiKV 在网络上提供服务和 Raft group 成员之间相互通信的逻辑。&lt;code&gt;Server&lt;/code&gt; 本身的代码比较简短，大部分代码都被分离到 &lt;code&gt;RaftClient&lt;/code&gt;，&lt;code&gt;Transport&lt;/code&gt;，&lt;code&gt;SnapRunner&lt;/code&gt; 和几个 gRPC service 中。上述组件的层次关系如下图所示：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1288&quot; data-rawheight=&quot;618&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1288&quot; data-original=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1288&quot; data-rawheight=&quot;618&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1288&quot; data-original=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;接下来，我们将详细介绍这些组件。&lt;/p&gt;&lt;h2&gt;Resolver&lt;/h2&gt;&lt;p&gt;在一个集群中，每个 TiKV 实例都由一个唯一的 store id 进行标识。Resolver 的功能是将 store id 解析成 TiKV 的地址和端口，用于建立网络通信。&lt;/p&gt;&lt;p&gt;Resolver 是一个很简单的组件，其接口仅包含一个函数：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;pub trait StoreAddrResolver: Send + Clone {
   fn resolve(&amp;amp;self, store_id: u64, cb: Callback) -&amp;gt; Result&amp;lt;()&amp;gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中 &lt;code&gt;Callback&lt;/code&gt; 用于异步地返回结果。&lt;code&gt;PdStoreAddrResolver&lt;/code&gt; 实现了该 trait，它的 &lt;code&gt;resolve&lt;/code&gt; 方法的实现则是简单地将查询任务通过其 &lt;code&gt;sched&lt;/code&gt; 成员发送给 &lt;code&gt;Runner&lt;/code&gt;。而 &lt;code&gt;Runner&lt;/code&gt; 则实现了 &lt;code&gt;Runnable&amp;lt;Task&amp;gt;&lt;/code&gt;，其意义是 &lt;code&gt;Runner&lt;/code&gt; 可以在自己的一个线程里运行，外界将会向 &lt;code&gt;Runner&lt;/code&gt; 发送 &lt;code&gt;Task&lt;/code&gt; 类型的消息，&lt;code&gt;Runner&lt;/code&gt; 将对收到的 &lt;code&gt;Task&lt;/code&gt; 进行处理。 这里使用了由 TiKV 的 util 提供的一个单线程 worker 框架，在 TiKV 的很多处代码中都有应用。&lt;code&gt;Runner&lt;/code&gt; 的 &lt;code&gt;store_addrs&lt;/code&gt; 字段是个 cache，它在执行任务时首先尝试在这个 cache 中找，找不到则向 PD 发送 RPC 请求来进行查询，并将查询结果添加进 cache 里。&lt;/p&gt;&lt;h2&gt;RaftClient&lt;/h2&gt;&lt;p&gt;TiKV 是一个 Multi Raft 的结构，Region 的副本之间，即 Raft group 的成员之间需要相互通信，&lt;code&gt;RaftClient&lt;/code&gt; 的作用便是管理 TiKV 之间的连接，并用于向其它 TiKV 节点发送 Raft 消息。&lt;code&gt;RaftClient&lt;/code&gt; 可以和另一个节点建立多个连接，并把不同 Region 的请求均摊到这些连接上。这部分代码的主要的复杂性就在于连接的建立，也就是 &lt;code&gt;Conn::new&lt;/code&gt; 这个函数。建立连接的代码的关键部分如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;let client1 = TikvClient::new(channel);

let (tx, rx) = batch::unbounded::&amp;lt;RaftMessage&amp;gt;(RAFT_MSG_NOTIFY_SIZE);
let rx = batch::BatchReceiver::new(rx, RAFT_MSG_MAX_BATCH_SIZE, Vec::new, |v, e| v.push(e));
let rx1 = Arc::new(Mutex::new(rx));

let (batch_sink, batch_receiver) = client1.batch_raft().unwrap();
let batch_send_or_fallback = batch_sink
   .send_all(Reusable(rx1).map(move |v| {
       let mut batch_msgs = BatchRaftMessage::new();
       batch_msgs.set_msgs(RepeatedField::from(v));
       (batch_msgs, WriteFlags::default().buffer_hint(false))
   })).then(/*...*/);

client1.spawn(batch_send_or_fallback.map_err(/*...*/));&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;上述代码向指定地址调用了 &lt;code&gt;batch_raft&lt;/code&gt; 这个 gRPC 接口。&lt;code&gt;batch_raft&lt;/code&gt; 和 &lt;code&gt;raft&lt;/code&gt; 都是 stream 接口。对 &lt;code&gt;RaftClient&lt;/code&gt; 调用 &lt;code&gt;send&lt;/code&gt; 方法会将消息发送到对应的 &lt;code&gt;Conn&lt;/code&gt; 的 &lt;code&gt;stream&lt;/code&gt; 成员，即上述代码的 &lt;code&gt;tx&lt;/code&gt; 中，而在 gRPC 的线程中则会从 &lt;code&gt;rx&lt;/code&gt; 中取出这些消息（这些消息被 &lt;code&gt;BatchReceiver&lt;/code&gt; 这一层 batch 起来以提升性能），并通过网络发送出去。&lt;/p&gt;&lt;p&gt;如果对方不支持 batch，则会 fallback 到 &lt;code&gt;raft&lt;/code&gt; 接口。这种情况通常仅在从旧版本升级的过程中发生。&lt;/p&gt;&lt;h2&gt;RaftStoreRouter 与 Transport&lt;/h2&gt;&lt;p&gt;&lt;code&gt;RaftStoreRouter&lt;/code&gt; 负责将收到的 Raft 消息转发给 raftstore 中对应的 Region，而 &lt;code&gt;Transport&lt;/code&gt; 负责将 Raft 消息发送到指定的 store。&lt;/p&gt;&lt;p&gt;&lt;code&gt;ServerRaftStoreRouter&lt;/code&gt; 是在 TiKV 实际运行时将会使用的 &lt;code&gt;RaftStoreRouter&lt;/code&gt; 的实现，它包含一个内层的、由 raftstore 提供的 &lt;code&gt;RaftRouter&lt;/code&gt; 对象和一个 &lt;code&gt;LocalReader&lt;/code&gt; 对象。收到的请求如果是一个只读的请求，则会由 &lt;code&gt;LocalReader&lt;/code&gt; 处理；其它情况则是交给内层的 router 来处理。&lt;/p&gt;&lt;p&gt;&lt;code&gt;ServerTransport&lt;/code&gt; 则是 TiKV 实际运行时使用的 &lt;code&gt;Transport&lt;/code&gt; 的实现（&lt;code&gt;Transport&lt;/code&gt; trait 的定义在 raftstore 中），其内部包含一个 &lt;code&gt;RaftClient&lt;/code&gt; 用于进行 RPC 通信。发送消息时，&lt;code&gt;ServerTransport&lt;/code&gt; 通过上面说到的 Resolver 将消息中的 store id 解析为地址，并将解析的结果存入 &lt;code&gt;raft_client.addrs&lt;/code&gt; 中；下次向同一个 store 发送消息时便不再需要再次解析。接下来，再通过 &lt;code&gt;RaftClient&lt;/code&gt; 进行 RPC 请求，将消息发送出去。&lt;/p&gt;&lt;h2&gt;Node&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Node&lt;/code&gt; 可以认为是将 raftstore 的复杂的创建、启动和停止逻辑进行封装的一层，其内部的 &lt;code&gt;RaftBatchSystem&lt;/code&gt; 便是 raftstore 的核心。在启动过程中（即 &lt;code&gt;Node&lt;/code&gt; 的 &lt;code&gt;start&lt;/code&gt; 函数中），如果该节点是一个新建的节点，那么会进行 bootstrap 的过程，包括分配 store id、分配第一个 Region 等操作。&lt;/p&gt;&lt;p&gt;&lt;code&gt;Node&lt;/code&gt; 并没有直接包含在 &lt;code&gt;Server&lt;/code&gt; 之内，但是 raftstore 的运行需要有用于向其它 TiKV 发送消息的 &lt;code&gt;Transport&lt;/code&gt;，而 &lt;code&gt;Transport&lt;/code&gt; 作为提供网络通信功能的一部分，则是包含在 &lt;code&gt;Server&lt;/code&gt; 内。所以我们可以看到，在 &lt;code&gt;src/binutil/server.rs&lt;/code&gt;文件的 &lt;code&gt;run_raft_server&lt;/code&gt; 中（被 tikv-server 的 &lt;code&gt;main&lt;/code&gt; 函数调用），启动过程中需要先创建 &lt;code&gt;Server&lt;/code&gt;，然后创建并启动 &lt;code&gt;Node&lt;/code&gt; 并把 &lt;code&gt;Server&lt;/code&gt; 所创建的 &lt;code&gt;Transport&lt;/code&gt; 传给 &lt;code&gt;Node&lt;/code&gt;，最后再启动 &lt;code&gt;Node&lt;/code&gt;。&lt;/p&gt;&lt;h2&gt;Service&lt;/h2&gt;&lt;p&gt;TiKV 包含多个 gRPC service。其中，最重要的一个是 &lt;code&gt;KvService&lt;/code&gt;，位于 &lt;code&gt;src/server/service/kv.rs&lt;/code&gt; 文件中。&lt;/p&gt;&lt;p&gt;&lt;code&gt;KvService&lt;/code&gt; 定义了 TiKV 的 &lt;code&gt;kv_get&lt;/code&gt;，&lt;code&gt;kv_scan&lt;/code&gt;，&lt;code&gt;kv_prewrite&lt;/code&gt;，&lt;code&gt;kv_commit&lt;/code&gt; 等事务操作的 API，用于执行 TiDB 下推下来的复杂查询和计算的 &lt;code&gt;coprocessor&lt;/code&gt; API，以及 &lt;code&gt;raw_get&lt;/code&gt;，&lt;code&gt;raw_put&lt;/code&gt; 等 Raw KV API。&lt;code&gt;batch_commands&lt;/code&gt; 接口则是用于将上述的接口 batch 起来，以优化高吞吐量的场景。当我们要为 TiKV 添加一个新的 API 时，首先就要在 kvproto 项目中添加相关消息体的定义，并在这里添加相关代码。另外，TiKV 的 Raft group 各成员之间通信用到的 &lt;code&gt;raft&lt;/code&gt; 和 &lt;code&gt;batch_raft&lt;/code&gt; 接口也是在这里提供的。&lt;/p&gt;&lt;p&gt;下面以 &lt;code&gt;kv_prewrite&lt;/code&gt; 为例，介绍 TiKV 处理一个请求的流程。首先，无论是直接调用还是通过 &lt;code&gt;batch_commands&lt;/code&gt; 接口调用，都会调用 &lt;code&gt;future_prewrite&lt;/code&gt; 函数，并在该函数返回的 future 附加上根据结果发送响应的操作，再将得到的 future spawn 到 &lt;code&gt;RpcContext&lt;/code&gt;，也就是一个线程池里。&lt;code&gt;future_prewrite&lt;/code&gt; 的逻辑如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;// 从请求体中取出调用 prewrite 所需的参数

let (cb, f) = paired_future_callback();
let res = storage.async_prewrite(/*其它参数*/, cb);

AndThenWith::new(res, f.map_err(Error::from)).map(|v| {
   let mut resp = PrewriteResponse::new();
   if let Some(err) = extract_region_error(&amp;amp;v) {
       resp.set_region_error(err);
   } else {
       resp.set_errors(RepeatedField::from_vec(extract_key_errors(v)));
   }
   resp
})&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里的 &lt;code&gt;paired_future_callback&lt;/code&gt; 是一个 util 函数，它返回一个闭包 &lt;code&gt;cb&lt;/code&gt; 和一个 future &lt;code&gt;f&lt;/code&gt;，当 &lt;code&gt;cb&lt;/code&gt; 被调用时 &lt;code&gt;f&lt;/code&gt; 就会返回被传入 &lt;code&gt;cb&lt;/code&gt; 的值。上述代码会立刻返回，但 future 中的逻辑在 &lt;code&gt;async_prewrite&lt;/code&gt; 中的异步操作完成之后才会执行。一旦 prewrite 操作完成，&lt;code&gt;cb&lt;/code&gt; 便会被调用，将结果传给 &lt;code&gt;f&lt;/code&gt;，接下来，我们写在 &lt;code&gt;future&lt;/code&gt; 中的创建和发送 Response 的逻辑便会继续执行。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;以上就是 TiKV 的 Service 层的代码解析。大家可以看到这些代码大量使用 trait 和泛型，这是为了方便将其中一些组件替换成另外一些实现，方便编写测试代码。另外，在 &lt;code&gt;src/server/snap.rs&lt;/code&gt; 中，我们还有一个专门用于处理 Snapshot 的模块，由于 Snapshot 消息的特殊性，在其它模块中也有一些针对 snapshot 的代码。关于 Snapshot，我们将在另一篇文章里进行详细讲解，敬请期待。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tikv-source-code-reading-9/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/blog-cn/tik&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;v-source-code-reading-9/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多 TiKV 源码阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-08-72640867</guid>
<pubDate>Mon, 08 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Binlog 源码阅读系列文章（二）初识 TiDB Binlog 源码</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-05-72306986.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/72306986&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7e9b57ad1e4c1ebaae5bfabfa6d28ffb_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：satoru&lt;/p&gt;&lt;h2&gt;TiDB Binlog 架构简介&lt;/h2&gt;&lt;p&gt;TiDB Binlog 主要由 Pump 和 Drainer 两部分组成，其中 Pump 负责存储 TiDB 产生的 binlog 并向 Drainer 提供按时间戳查询和读取 binlog 的服务，Drainer 负责将获取后的 binlog 合并排序再以合适的格式保存到对接的下游组件。&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;402&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;402&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-57c2652749ce0db78f97e527819d1c36_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;br/&gt;&lt;b&gt;在《&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog 架构演进与实现原理&lt;/a&gt;》一文中，我们对 TiDB Binlog 整体架构有更详细的说明，建议先行阅读该文。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;相关源码仓库&lt;/h2&gt;&lt;p&gt;TiDB Binlog 的实现主要分布在 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb-tools&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;tidb-binlog&lt;/a&gt; 两个源码仓库中，我们先介绍一下这两个源码仓库中的关键目录。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. tidb-tools&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Repo: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-tools/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;这个仓库除了 TiDB Binlog 还有其他工具的组件，在这里与 TiDB Binlog 关系最密切的是 &lt;code&gt;tidb-binlog/pump_client&lt;/code&gt; 这个 package。&lt;code&gt;pump_client&lt;/code&gt; 实现了 Pump 的客户端接口，当 binlog 功能开启时，TiDB 使用它来给 pump &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/v3.0.0-rc.3/tidb-binlog/pump_client/client.go%23L242&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;发送 binlog&lt;/a&gt; 。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. tidb-binlog&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Repo: &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tidb&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;-binlog&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;TiDB-Binlog 的核心组件都在这个仓库，下面是各个关键目录：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;cmd&lt;/code&gt;：包含 &lt;code&gt;pump&lt;/code&gt;，&lt;code&gt;drainer&lt;/code&gt;，&lt;code&gt;binlogctl&lt;/code&gt;，&lt;code&gt;reparo&lt;/code&gt;，&lt;code&gt;arbiter&lt;/code&gt; 等 5 个子目录，分别对应 5 个同名命令行工具。这些子目录下面的 &lt;code&gt;main.go&lt;/code&gt; 是对应命令行工具的入口，而主要功能的实现则依赖下面将介绍到的各个同名 packages。&lt;/li&gt;&lt;li&gt;&lt;code&gt;pump&lt;/code&gt;：Pump 源码，主要入口是 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/pump/server.go%23L103&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pump.NewServer&lt;/a&gt;&lt;/code&gt; 和 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/pump/server.go%23L313&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Server.Start&lt;/a&gt;&lt;/code&gt;；服务启动后，主要的功能是 &lt;code&gt;WriteBinlog&lt;/code&gt;（面向 &lt;code&gt;TiDB/pump_client&lt;/code&gt;） 和 &lt;code&gt;PullBinlogs&lt;/code&gt;（面向 &lt;code&gt;Drainer&lt;/code&gt;）。&lt;/li&gt;&lt;li&gt;&lt;code&gt;drainer&lt;/code&gt;：Drainer 源码，主要入口是 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/drainer/server.go%23L88&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;drainer.NewServer&lt;/a&gt;&lt;/code&gt; 和 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/drainer/server.go%23L238&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Server.Start&lt;/a&gt;&lt;/code&gt;；服务启动后，Drainer 会先找到所有 Pump 节点，然后调用 Pump 节点的 &lt;code&gt;PullBinlogs&lt;/code&gt; 接口同步 binlog 到下游。目前支持的下游有：mysql/tidb，file（文件增量备份），kafka 。&lt;/li&gt;&lt;li&gt;&lt;code&gt;binlogctl&lt;/code&gt;：Binlogctl 源码，实现一些常用的 Binlog 运维操作，例如用 &lt;code&gt;-cmd pumps&lt;/code&gt; 参数可以查看当前注册的各个 Pump 节点信息，相应的实现就是 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/binlogctl/nodes.go%23L37&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;QueryNodesByKind&lt;/a&gt;&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code&gt;reparo&lt;/code&gt;：Reparo 源码，实现从备份文件（Drainer 选择 file 下游时保存的文件）恢复数据到指定数据库的功能。&lt;/li&gt;&lt;li&gt;&lt;code&gt;arbiter&lt;/code&gt;：Arbiter 源码，实现从 Kafka 消息队列中读取 binlog 同步到指定数据库的功能，binlog 在消息中以 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-tools/blob/v3.0.0-rc.3/tidb-binlog/slave_binlog_proto/proto/binlog.proto%23L85&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Protobuf&lt;/a&gt;&lt;/code&gt; 格式编码。&lt;/li&gt;&lt;li&gt;&lt;code&gt;pkg&lt;/code&gt;：各个工具公用的一些辅助类的 packages，例如 &lt;code&gt;pkg/util&lt;/code&gt; 下面有用于重试函数执行的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/pkg/util/util.go%23L145&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RetryOnError&lt;/a&gt;&lt;/code&gt;，pkg/version 下面有用于打印版本信息的 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/pkg/version/version.go%23L45&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;PrintVersionInfo&lt;/a&gt;&lt;/code&gt;。&lt;/li&gt;&lt;li&gt;&lt;code&gt;tests&lt;/code&gt;：集成测试。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;启动测试集群&lt;/h2&gt;&lt;p&gt;上个小节提到的 &lt;code&gt;tests&lt;/code&gt; 目录里有一个名为 &lt;code&gt;run.sh&lt;/code&gt; 脚本，我们一般会使用 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog/blob/v3.0.0-rc.3/Makefile%23L68&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;make integration_test&lt;/a&gt;&lt;/code&gt; 命令，通过该脚本执行一次完整的集成测试，不过现在我们先介绍如何用它来启动一个测试集群。&lt;br/&gt;启动测试集群前，需要在 &lt;code&gt;bin&lt;/code&gt; 目录下准备好相关组件的可执行文件：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;pd-server：下载链接（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/pd-master-linux-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Linux&lt;/a&gt; / &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/pd-master-darwin-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;macOS&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;tikv-server：下载链接（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/tikv-master-linux-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Linux&lt;/a&gt; / &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/tikv-master-darwin-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;macOS&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;tidb-server：下载链接（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/tidb-master-linux-amd64.tar.g&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Linux&lt;/a&gt; / &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//download.pingcap.org/tidb-master-darwin-amd64.tar.gz&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;macOS&lt;/a&gt;）&lt;/li&gt;&lt;li&gt;&lt;code&gt;pump&lt;/code&gt;, &lt;code&gt;drainer&lt;/code&gt;, &lt;code&gt;binlogctl&lt;/code&gt;：在 tidb-binlog 目录执行 &lt;code&gt;make build&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;脚本依赖 MySQL 命令行客户端来确定 TiDB 已经成功启动，所以我们还需要安装一个 MySQL 客户端。&lt;/p&gt;&lt;p&gt;准备好以上依赖，运行 &lt;code&gt;tests/run.sh --debug&lt;/code&gt;，就可以启动一个测试集群。启动过程中会输出一些进度信息，看到以下提示就说明已成功启动：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;Starting Drainer... 
You may now debug from another terminal. Press [ENTER] to continue.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;测试集群包含以下服务：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;2 个作为上游的 TiDB 实例，分别使用端口 4000 和 4001&lt;/li&gt;&lt;li&gt;1 个作为下游的 TiDB 实例， 使用端口 3306&lt;/li&gt;&lt;li&gt;PD 实例，使用端口 2379&lt;/li&gt;&lt;li&gt;TiKV，使用端口 20160&lt;/li&gt;&lt;li&gt;Pump ，使用端口 8250&lt;/li&gt;&lt;li&gt;Drainer，使用端口 8249&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;使用 MySQL 客户端连接任意一个上游 TiDB，可以用 &lt;code&gt;SHOW PUMP STATUS&lt;/code&gt; 和 &lt;code&gt;SHOW DRAINER STATUS&lt;/code&gt; 查询对应工具的运行状态，例如：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;350&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;350&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-97f89c164e7fbb354e60b91e05cc20f1_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;br/&gt;通过 &lt;code&gt;binlogctl&lt;/code&gt; 也可以查询到同样的信息，例如：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;$ bin/binlogctl -pd-urls=localhost:2379 -cmd pumps
[2019/06/26 14:36:29.158 +08:00] [INFO] [nodes.go:49] [&amp;#34;query node&amp;#34;] [type=pump] [node=&amp;#34;{NodeID: pump:8250, Addr: 127.0.0.1:8250, State: online, MaxCommitTS: 409345979065827329, UpdateTime: 2019-06-26 14:36:27 +0800 CST}&amp;#34;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;br/&gt;接下来我们可以用 MySQL 客户端连接上端口为 4000 或 4001 的 TiDB 数据库，插入一些测试数据。&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;295&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;295&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a01bc8e8c6aaff1836551d65bf7db725_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;br/&gt;上图的演示中创建了一个叫 &lt;code&gt;hello_binlog&lt;/code&gt; 的 database，在里面新建了 &lt;code&gt;user_info&lt;/code&gt; 表并插入了两行数据。完成上述操作后，就可以连接到端口为 3306 的下游数据库验证同步是否成功：&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;252&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;252&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-b1d08efefc8e02fb49c2342dd8a00d73_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;h2&gt;&lt;br/&gt;小结&lt;/h2&gt;&lt;p&gt;本文简单介绍了 tidb-tools 和 tidb-binlog 及其中的目录，并且展示了如何启动测试集群。有了这些准备，大家就可以进一步了解各个功能的源码实现。下篇文章将会介绍 &lt;code&gt;pump_client&lt;/code&gt; 如何将一条 binlog 发送往 Pump Server。&lt;/p&gt;&lt;p&gt;&lt;br/&gt;原文链接：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-binlog-source-code-reading-2/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/blog-cn/tid&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;b-binlog-source-code-reading-2/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;延展阅读：&lt;/p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/69587196&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic3.zhimg.com/v2-c3da67191753fc9376f711e1c9dbec9a_180x120.jpg&quot; data-image-width=&quot;1280&quot; data-image-height=&quot;593&quot; class=&quot;internal&quot;&gt;ZoeyZhai：TiDB Binlog 源码阅读系列文章（一）序&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-05-72306986</guid>
<pubDate>Fri, 05 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>暑期特别企划 | 快来接收 PingCAP Talent Plan 的小惊喜！</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-07-03-71982747.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/71982747&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-ce5e8ae80f8104765d9371489e4fcc25_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;PingCAP Talent Plan 学习通道自开通以来，收获了海内外小伙伴的密切关注，有 100 余名小伙伴参与到线上课程的学习中，第二期线下课程也于 5 月中旬圆满落幕。结合大家的意见，我们对 Talent Plan 的课程做了一些优化，并推出 Talent Plan 暑期特别企划，线上课程和线下课程都增加了一些新的元素～大家快来接收这一波“小惊喜”吧！&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;线上课程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;1. Practical Networked Applications in Rust 全面开放&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们发现很多开发者都愿意参与 TiKV 的研发，但通常都会遇到两个困难，第一是不会 Rust 语言，因为这门语言的门槛实在太高了，第二是没有分布式数据库相关的理论知识，不知道如何用 Rust 写一个分布式高性能服务。虽然现在市面上有很多的 Rust 教程，但大多数是集中在语言本身的教学上面，所以我们决定在它们的基础上，专门推出一套新的 Rust 培训课。基于这方面的考虑，&lt;b&gt;Rust 核心作者 Brian Anderson 对 Rust 课程进行重新设计，推出&lt;/b&gt; &lt;b&gt;Practical Networked Applications in Rust&lt;/b&gt;（&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/talent-plan/tree/master/rust&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/tale&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;nt-plan/tree/master/rust&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;），并向社区小伙伴全面开放。&lt;/p&gt;&lt;p&gt;通过这门课程，大家不仅能学到 Rust 的基本知识，还能使用 Rust 来构建自己的存储引擎和网络框架，学习如何写高性能的并发程序，从而真正进入使用 Rust 来进行分布式系统开发的大门。&lt;/p&gt;&lt;p&gt;&lt;b&gt;温馨提示：对该课程提出改进意见的小伙伴，我们会结合意见及改进情况给予额外的加分哦！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 线上作业提交方式变更：由集中打包提交改为分批次提交&lt;/b&gt;&lt;/p&gt;&lt;p&gt;线上课程开放之初，作业提交采用的是集中打包的方式，这么做的目的是为了使作业更具连贯性，在进行作业评估的时候，也能够更全面的了解大家对于线上课程的掌握程度。但是运行了一段时间之后，我们发现，大部分小伙伴基于学业及工作方面的考虑，学习课程的时间相对分散，于是我们将线上课程提交方式改为分批次提交，一方面是为了更好地适应大家的学习节奏，另一方面也可以通过作业提交情况了解大家的学习进度以及在学习中遇到的问题，以便针对性地对课程进行调整并组织集中答疑。更新后的线上作业提交方式如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;发送邮件至&lt;/b&gt; &lt;b&gt;ts-team@pingcap.com&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;邮件主题&lt;/b&gt;：【PingCAP Talent Plan】申请线上课程作业评估+申请人+联系方式。&lt;/li&gt;&lt;li&gt;&lt;b&gt;正文&lt;/b&gt;：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;请简单介绍自己（包括姓名、GitHub ID、常用联系方式等）。&lt;/li&gt;&lt;li&gt;在校学生需注明所在高校、年级和专业等信息；非在校学生需注明当前就职公司、是否能 full-time 参与 4 周线下课程等。&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;以附件形式提交作业。&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;线上作业提交通道每周六 0:00 开启，至周日 24:00 关闭，持续 48h 开放。&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;作业可以「完成多少就提交多少」，但要以周为单位&lt;/b&gt;（如果某一周的作业只完成了一部分，可以放到下个提交通道开启时提交）。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;线下课程&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;完成线上课程并通过考核的同学将有机会参加线下课程。第三期线下课程正值暑期，为了帮助同学们充分利用暑假时间，更好地参与和熟悉开源社区，我们对第三期线下课程做了大量调整。调整后的线下课程包括 1 周的集中授课阶段以及 3 周的实战演练阶段。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. 集中授课阶段（Week 1）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;集中授课阶段将针对 Rust 语言、Go 语言、TiKV/TiDB 基础架构、SQL 优化与执行等基础知识进行重点讲解，除此之外，我们还为大家准备了三重惊喜。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5173304e0e53895107d45c2a98d2d5ef_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;983&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-5173304e0e53895107d45c2a98d2d5ef_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5173304e0e53895107d45c2a98d2d5ef_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;983&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-5173304e0e53895107d45c2a98d2d5ef_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-5173304e0e53895107d45c2a98d2d5ef_b.jpg&quot;/&gt;&lt;figcaption&gt;Week 1 时间安排表&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;惊喜 1：在大家熟悉的校园环境中进行集中授课&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为了给同学们营造更加舒适的学习氛围，我们将第一周集中授课地点选在了&lt;b&gt;华中科技大学&lt;/b&gt;。在前两期 Talent Plan 的实践过程中，我们结识了华中科技大学的老师和同学们，华科的同学们无论是从报名人数上还是学习的积极性上，都给我们留下了深刻的印象，我们深切地感受到了他们对于计算机科研的热情和专注，在此也要特别感谢华中科技大学的老师和同学们给予的支持和帮助。&lt;/p&gt;&lt;p&gt;&lt;b&gt;惊喜 2：增设公开课程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;不少小伙伴表示想要对 TiDB 开源社区以及如何成为社区 Contributor 有更加深入的了解，对于 TiKV、TiDB 等工程实践也有着浓厚的兴趣，于是我们增设了公开课程。不只有 Deep Dive into TiKV/TiDB/Cloud TiDB/Columnstore for TiDB，还有 Rust Language 课程专场讲授。更有负责 TiDB 开源社区运营的小姐姐为大家分享 TiDB 开源社区的现状以及如何成为 TiDB Contributor。&lt;/p&gt;&lt;p&gt;&lt;b&gt;* 公开课报名通道：&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0da8bbbd8b74146de56559b854e36afe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;198&quot; data-rawheight=&quot;198&quot; class=&quot;content_image&quot; width=&quot;198&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0da8bbbd8b74146de56559b854e36afe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;198&quot; data-rawheight=&quot;198&quot; class=&quot;content_image lazy&quot; width=&quot;198&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-0da8bbbd8b74146de56559b854e36afe_b.jpg&quot;/&gt;&lt;figcaption&gt;扫描上方二维码报名线下公开课&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;惊喜 3：TiDB TechDay 2019 武汉站邀请函&lt;/b&gt;&lt;/p&gt;&lt;p&gt;线下课程第一周周六（7 月 20 日）恰逢 &lt;u&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247489051%26idx%3D1%26sn%3D3b54c8c83492c730594bc1eda36e5235%26chksm%3Deb163171dc61b8670fe9c58ad8fcc11d48c4902d4693dcd529042f2a86e95d3a7924342a09f2%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB TechDay 2019&lt;/a&gt;&lt;/u&gt; 武汉站，TechDay 上不仅有 TiDB 最新的 OLAP 架构、云原生 TiDB demo、TiKV 性能大幅提升等技术分享，用户伙伴也会一起交流分享 TiDB 实践经验，还有关于开源社区运营的新想法，对于小伙伴来说是一次难得的学习机会。所以在第一周的周六，我们会邀请线下的小伙伴一起参与 TechDay 武汉站，与社区小伙伴进行近距离交流，感受开源社区的魅力！ &lt;/p&gt;&lt;p&gt;&lt;b&gt;2. 实战演练阶段（Week 2-4）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于大多数热爱开源、热爱 Coding 的小伙伴来说，能够亲身参与到开源项目的开发，是一件非常值得兴奋的事情。在参与开源项目的过程中，你会不自觉地对自己的代码规范和代码质量进行严格要求，你的代码甚至有可能在世界范围内被使用，听起来就很酷！TiDB 作为世界级开源项目，深入参与其开源实践，能够帮助小伙伴们了解开源世界，提升工程实践能力。&lt;/p&gt;&lt;p&gt;所以，在第一周集中授课之后，我们会邀请大家回到 PingCAP 北京总部，开启为期 3 周的实战演练阶段。实战演练阶段将重点培养大家的动手实践能力，同学们可以自由组队，深度参与 TiKV、TiDB 工程实践。&lt;/p&gt;&lt;p&gt;&lt;b&gt;可选项目&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. TiKV 方向：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;可插入式的 PD 调度器&lt;/li&gt;&lt;li&gt;PD 调度 simulater&lt;/li&gt;&lt;li&gt;LSM：减少 TiKV 写放大&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2. TiDB 方向：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;SQL Index Advisor&lt;/li&gt;&lt;li&gt;Full Vectorized Expression Evaluation&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;第三期线下课程将于 7 月 15 日正式开始，第一周为集中授课阶段，第二周至第四周实战演练阶段，整个课程将持续 1 个月，目前线下课程学员已集结 90%。&lt;b&gt;在 7 月 7 日之前完成线上课程学习的小伙伴依然有机会参与第三期的线下课程哦！&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;本期课程大纲&lt;/b&gt;&lt;/h2&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0b790b1b657cf5d6dcb9fa58496f0bbe_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1033&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-0b790b1b657cf5d6dcb9fa58496f0bbe_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0b790b1b657cf5d6dcb9fa58496f0bbe_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;1033&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-0b790b1b657cf5d6dcb9fa58496f0bbe_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-0b790b1b657cf5d6dcb9fa58496f0bbe_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;详细课程大纲：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//docs.google.com/document/d/1vZJWMWd_83VHAqMjOIyUIJLyCCo9y4QrELFgbqXwSHc/edit%3Fts%3D5d1085a6&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;docs.google.com/documen&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;t/d/1vZJWMWd_83VHAqMjOIyUIJLyCCo9y4QrELFgbqXwSHc/edit?ts=5d1085a6&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;h2&gt;PingCAP Talent Plan&lt;/h2&gt;&lt;p&gt;PingCAP Talent Plan 是 PingCAP 为 TiDB 开源社区小伙伴提供的进阶式学习计划。课程设置上分为两个方向，分别是面向 SQL 引擎的 TiDB 方向和面向大规模、一致性的分布式存储的 TiKV 方向。每个方向的课程都包含线上和线下两部分，线上课程侧重于对基础知识的讲解，对社区所有小伙伴们开放，时间上比较灵活。线下课程在夯实基础知识的基础上，注重实操能力的培养。&lt;/p&gt;&lt;p&gt;完成线上课程并通过线上考核的小伙伴可以获得线上课程结业证书，表现优秀的还将有机会拿到 PingCAP 校招/实习免笔试绿色通道，而且有机会参与半年内 PingCAP 组织的任意一期线下课程；&lt;b&gt;完成线下课程的小伙伴&lt;/b&gt;可以获得专属 PingCAP Talent Plan 结业证书，表现优秀的还将有机会拿到 PingCAP 校招/实习免面试绿色通道/Special Offer、 PingCAP/TiDB 全球 Meetup 的邀请函等。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-07-03-71982747</guid>
<pubDate>Wed, 03 Jul 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 3.0 GA Release Notes</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-06-29-71488780.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/71488780&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5d7d3ffcfc32f80e26b317e2084566af_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;发版日期：2019 年 6 月 28 日&lt;/p&gt;&lt;p&gt;TiDB 版本：3.0.0&lt;/p&gt;&lt;p&gt;TiDB Ansible 版本：3.0.0&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Overview&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2019 年 6 月 28 日，TiDB 发布 3.0 GA 版本，对应的 TiDB Ansible 版本为 3.0.0。 相比于 V2.1，V3.0.0 版本在以下方面有重要改进：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;稳定性方面，显著提升了大规模集群的稳定性，集群支持 150+ 存储节点，300+ TB 存储容量长期稳定运行。&lt;/li&gt;&lt;li&gt;易用性方面有显著的提升，降低用户运维成本，例如：标准化慢查询日志，制定日志文件输出规范，新增 &lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt;，SQL Trace 功能方便排查问题等。&lt;/li&gt;&lt;li&gt;性能方面，与 2.1 相比，TPC-C 性能提升约 4.5 倍，Sysbench 性能提升 50%+。 因支持 View，TPC-H 50G Q15 可正常运行。&lt;/li&gt;&lt;li&gt;新功能方面增加了窗口函数、视图（&lt;b&gt;实验特性&lt;/b&gt;）、分区表、插件系统、悲观锁（&lt;b&gt;实验特性&lt;/b&gt;）、&lt;code&gt;SQL Plan Management&lt;/code&gt; 等特性。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiDB&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;新功能&lt;/li&gt;&lt;ul&gt;&lt;li&gt;新增 Window Function，支持所有 MySQL 8.0 中的窗口函数，包括 &lt;code&gt;NTILE&lt;/code&gt;，&lt;code&gt;LEAD&lt;/code&gt;，&lt;code&gt;LAG&lt;/code&gt;、&lt;code&gt;PERCENT_RANK&lt;/code&gt;、&lt;code&gt;NTH_VALUE&lt;/code&gt;、&lt;code&gt;CUME_DIST&lt;/code&gt;、&lt;code&gt;FIRST_VALUE&lt;/code&gt;、&lt;code&gt;LAST_VALUE&lt;/code&gt;、&lt;code&gt;RANK&lt;/code&gt;、&lt;code&gt;DENSE_RANK&lt;/code&gt;、&lt;code&gt;ROW_NUMBER&lt;/code&gt; 函数&lt;/li&gt;&lt;li&gt;新增 View 功能（&lt;b&gt;实验特性&lt;/b&gt;）&lt;/li&gt;&lt;li&gt;完善 Table Partition 功能：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Range Partition&lt;/li&gt;&lt;li&gt;Hash Partition&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;新增插件系统，官方提供 IP 白名单（&lt;b&gt;企业版特性&lt;/b&gt;），审记日志（&lt;b&gt;企业版特性&lt;/b&gt;）等插件&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;SQL Plan Management&lt;/code&gt; 功能，通过绑定 SQL 执行计划确保查询的稳定性（&lt;b&gt;实验特性&lt;/b&gt;）&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;SQL 优化器&lt;/li&gt;&lt;ul&gt;&lt;li&gt;优化&lt;code&gt;NOT EXISTS&lt;/code&gt; 子查询，转化为 Anti Semi Join 提升性能&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;Outer Join&lt;/code&gt; 常量传播，新增 &lt;code&gt;Outer Join&lt;/code&gt; 消除优化规则，避免无效计算，提升性能&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;IN&lt;/code&gt; 子查询，先聚合后执行 &lt;code&gt;Inner Join&lt;/code&gt;，提升性能&lt;/li&gt;&lt;li&gt;优化 Index Join，适应更多的场景，提升性能&lt;/li&gt;&lt;li&gt;优化 Range Partition 的 Partition Pruning 优化规则，提升性能&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;_tidb_rowid&lt;/code&gt; 查询逻辑，避免全表扫描，提升性能&lt;/li&gt;&lt;li&gt;当过滤条件中包含相关列时，在抽取复合索引的访问条件时尽可能多地匹配索引的前缀列，提升性能&lt;/li&gt;&lt;li&gt;利用列之间的顺序相关性，提升代价估算准确度&lt;/li&gt;&lt;li&gt;基于统计信息的贪心算法及动态规划算法改进了 &lt;code&gt;Join Order&lt;/code&gt;，提升多表关联的执行速度&lt;/li&gt;&lt;li&gt;新增 Skyline Pruning，利用规则防止执行计划过于依赖统计信息，提升查询的稳定性&lt;/li&gt;&lt;li&gt;提升单列索引上值为 NULL 时行数估算准确度&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;FAST ANALYZE&lt;/code&gt;，通过在各个 Region 中随机采样避免全表扫描的方式提升统计信息收集性能&lt;/li&gt;&lt;li&gt;新增单调递增的索引列增量 &lt;code&gt;Analyze&lt;/code&gt; 功能，提升统计信息收集性能&lt;/li&gt;&lt;li&gt;支持 &lt;code&gt;DO&lt;/code&gt; 语句中使用子查询&lt;/li&gt;&lt;li&gt;支持在事务中使用 Index Join&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;prepare&lt;/code&gt;/&lt;code&gt;execute&lt;/code&gt;，支持不带参数的 DDL 语句&lt;/li&gt;&lt;li&gt;修改变量 &lt;code&gt;stats-lease&lt;/code&gt; 值为 0 时系统的行为，使其自动加载统计&lt;/li&gt;&lt;li&gt;新增导出历史统计信息功能&lt;/li&gt;&lt;li&gt;新增导入导出列的关联性信息功能&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;SQL 执行引擎&lt;/li&gt;&lt;ul&gt;&lt;li&gt;优化日志输出，&lt;code&gt;EXECUTE&lt;/code&gt; 语句输出用户变量，&lt;code&gt;COMMIT&lt;/code&gt; 语句输出慢查询日志，方便排查问题&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt; 功能，提升SQL 调优易用性&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;admin show next_row_id&lt;/code&gt; 功能，方便获取下一行 ID&lt;/li&gt;&lt;li&gt;新增&lt;code&gt;JSON_QUOTE&lt;/code&gt;、&lt;code&gt;JSON_ARRAY_APPEND&lt;/code&gt;、&lt;code&gt;JSON_MERGE_PRESERVE&lt;/code&gt;、&lt;code&gt;BENCHMARK&lt;/code&gt;、&lt;code&gt;COALESCE&lt;/code&gt;、&lt;code&gt;NAME_CONST&lt;/code&gt; 6 个内建函数&lt;/li&gt;&lt;li&gt;优化 Chunk 大小控制逻辑，根据查询上下文件动态调整，降低 SQL 执行时间和资源消耗，提升性能&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;TableReader&lt;/code&gt;、&lt;code&gt;IndexReader&lt;/code&gt; 和 &lt;code&gt;IndexLookupReader&lt;/code&gt; 算子内存追踪控制&lt;/li&gt;&lt;li&gt;优化 Merge Join 算子，使其支持空的 &lt;code&gt;ON&lt;/code&gt; 条件&lt;/li&gt;&lt;li&gt;优化单个表列较多时写入性能，提升数倍性能&lt;/li&gt;&lt;li&gt;通过支持逆序扫数据提升 &lt;code&gt;admin show ddl jobs&lt;/code&gt; 的性能&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;split table region&lt;/code&gt; 语句，手动分裂表的 Region，缓解热点问题&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;split index region&lt;/code&gt; 语句，手动分裂索引的 Region 缓解热点问题&lt;/li&gt;&lt;li&gt;新增黑名单禁止下推表达式到 Coprocessor 功能&lt;/li&gt;&lt;li&gt;优化 Expensive Query 日志，在日志中打印执行时间或者使用内存超过阈值的 SQL 查询&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;DDL&lt;/li&gt;&lt;ul&gt;&lt;li&gt;支持字符集从 &lt;code&gt;utf8&lt;/code&gt; 转换到 &lt;code&gt;utf8mb4&lt;/code&gt; 的功能&lt;/li&gt;&lt;li&gt;修改默认字符集从 &lt;code&gt;utf8&lt;/code&gt; 变为 &lt;code&gt;utf8mb4&lt;/code&gt;&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;alter schema&lt;/code&gt; 语句修改数据库 charset 和 collation 功能&lt;/li&gt;&lt;li&gt;新增 ALTER ALGORITHM &lt;code&gt;INPLACE&lt;/code&gt;/&lt;code&gt;INSTANT&lt;/code&gt; 功能&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;SHOW CREATE VIEW&lt;/code&gt; 功能&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;SHOW CREATE USER&lt;/code&gt; 功能&lt;/li&gt;&lt;li&gt;新增快速恢复误删除的表功能&lt;/li&gt;&lt;li&gt;新增动态调整 &lt;code&gt;ADD INDEX&lt;/code&gt; 的并发数功能&lt;/li&gt;&lt;li&gt;新增 pre_split_regions 选项，在 &lt;code&gt;CREATE TABLE&lt;/code&gt; 时预先分配 Region，缓解建表后大量写入造成的写热点问题&lt;/li&gt;&lt;li&gt;新增通过 SQL 语句指定表的索引及范围分裂 Region，缓解热点问题&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;ddl_error_count_limit&lt;/code&gt; 全局变量，控制 DDL 任务重次数&lt;/li&gt;&lt;li&gt;新增列属性包含 &lt;code&gt;AUTO_INCREMENT&lt;/code&gt; 时利用 &lt;code&gt;SHARD_ROW_ID_BITS&lt;/code&gt; 打散行 ID 功能，缓解热点问题&lt;/li&gt;&lt;li&gt;优化无效 DDL 元信息存活时间，使集群升级后一段时间 DDL 操作比较慢的情况变短&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;事务&lt;/li&gt;&lt;ul&gt;&lt;li&gt;新增悲观事务模型（&lt;b&gt;实验特性&lt;/b&gt;）&lt;/li&gt;&lt;li&gt;优化事务处理逻辑，适应更多场景，具体如下：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;code&gt;tidb_disable_txn_auto_retry&lt;/code&gt; 的默认值为 &lt;code&gt;on&lt;/code&gt;，即不会重试非自动提交的事务&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;tidb_batch_commit&lt;/code&gt; 系统变量控制将事务拆分成多个事务并发执行&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;tidb_low_resolution_tso&lt;/code&gt; 系统变量控制批量获取 &lt;code&gt;tso&lt;/code&gt; 个数，减少事务获取 &lt;code&gt;tso&lt;/code&gt;的次数以适应某些数据一致性要求较低的场景&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;tidb_skip_isolation_level_check&lt;/code&gt; 变量控制事务检查隔离级别设置为 SERIALIZABLE 时是否报错&lt;/li&gt;&lt;li&gt;修改 &lt;code&gt;tidb_disable_txn_auto_retry&lt;/code&gt; 系统变量的行为，修改为影响所有的可重试错误&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;li&gt;权限管理 &lt;/li&gt;&lt;ul&gt;&lt;li&gt;对 &lt;code&gt;ANALYZE&lt;/code&gt;、&lt;code&gt;USE&lt;/code&gt;、&lt;code&gt;SET GLOBAL&lt;/code&gt;、&lt;code&gt;SHOW PROCESSLIST&lt;/code&gt; 语句进行权限检查 &lt;/li&gt;&lt;li&gt;新增基于角色的权限访问控制功能 (RBAC)（&lt;b&gt;实验特性&lt;/b&gt;）&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Server&lt;/li&gt;&lt;ul&gt;&lt;li&gt;优化慢查询日志，具体包括：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;重构慢查询日志格式&lt;/li&gt;&lt;li&gt;优化慢查询日志内容&lt;/li&gt;&lt;li&gt;优化查询慢查询日志的方法，通过内存表 &lt;code&gt;INFORMATION_SCHEMA.SLOW_QUERY&lt;/code&gt;，&lt;code&gt;ADMIN SHOW SLOW&lt;/code&gt; 语句查询慢查询日志&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;制定日志格式规范，重构日志系统，方便工具收集分析&lt;/li&gt;&lt;li&gt;新增 SQL 语句管理 TiDB Binlog 服务功能，包括查询状态，开启 TiDB Binlog，维护发送 TiDB Binlog 策略&lt;/li&gt;&lt;li&gt;新增通过 &lt;code&gt;unix_socket&lt;/code&gt; 方式连接数据库&lt;/li&gt;&lt;li&gt;新增 SQL 语句 &lt;code&gt;Trace&lt;/code&gt; 功能&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;/debug/zip&lt;/code&gt; HTTP 接口，获取 TiDB 实例的信息，方便排查问题&lt;/li&gt;&lt;li&gt;优化监控项，方便排查问题，如下：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;新增 &lt;code&gt;high_error_rate_feedback_total&lt;/code&gt; 监控项，监控真实数据量与统计信息估算数据量之间的差距&lt;/li&gt;&lt;li&gt;新增 Database 维度的 QPS 监控项&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;优化系统初始化流程，仅允许 DDL Owner 执行初始化操作，缩短初始化或升级过程中的启动时间&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;kill query&lt;/code&gt; 语句执行逻辑，提升性能，确保资源正确释放&lt;/li&gt;&lt;li&gt;新增启动选项 &lt;code&gt;config-check&lt;/code&gt; 检查配置文件合法性&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;tidb_back_off_weight&lt;/code&gt; 系统变量，控制内部出错重试的退避时间&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;wait_timeout&lt;/code&gt;、&lt;code&gt;interactive_timeout&lt;/code&gt; 系统变量，控制连接空闲超过变量的值，系统自动断开连接。&lt;/li&gt;&lt;li&gt;新增连接 TiKV 的连接池，减少连接创建时间&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;兼容性&lt;/li&gt;&lt;ul&gt;&lt;li&gt;支持 &lt;code&gt;ALLOW_INVALID_DATES&lt;/code&gt; SQL mode&lt;/li&gt;&lt;li&gt;支持 MySQL 320 握手协议&lt;/li&gt;&lt;li&gt;支持将 unsigned bigint 列声明为自增列&lt;/li&gt;&lt;li&gt;支持 &lt;code&gt;SHOW CREATE DATABASE IF NOT EXISTS&lt;/code&gt; 语法&lt;/li&gt;&lt;li&gt;优化 load data 对 CSV 文件的容错&lt;/li&gt;&lt;li&gt;过滤条件中包含用户变量时谓词不下推，兼容 MySQL Window Function 中使用用户变量行为&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;PD&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;新增从单个节点重建集群的功能&lt;/li&gt;&lt;li&gt;将 Region 元信息从 etcd 移到 go-leveldb 存储引擎，解决大规模集群 etcd 存储瓶颈问题&lt;/li&gt;&lt;li&gt;API&lt;/li&gt;&lt;ul&gt;&lt;li&gt;新增 &lt;code&gt;remove-tombstone&lt;/code&gt; 接口，用于清理 Tombstone Store&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;ScanRegions&lt;/code&gt; 接口，用于批量查询 Region 信息&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;GetOperator&lt;/code&gt; 接口，用于查询运行中的 Operator&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;GetStores&lt;/code&gt; 接口的性能&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;配置&lt;/li&gt;&lt;ul&gt;&lt;li&gt;优化配置检查逻辑，防止配置项错误&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;enable-two-way-merge&lt;/code&gt;，用于控制 Region merge 的方向&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;hot-region-schedule-limit&lt;/code&gt;，用于控制热点 Region 调度速度&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;hot-region-cache-hits-threshold&lt;/code&gt;，连续命中阀值用于判断热点&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;store-balance-rate&lt;/code&gt; 配置，用于控制每分钟产生 balance Region Operator 数量的上限&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;调度器优化&lt;/li&gt;&lt;ul&gt;&lt;li&gt;添加 Store Limit 机制限制调度速度，使得速度限制适用于不同规模的集群&lt;/li&gt;&lt;li&gt;添加 &lt;code&gt;waitingOperator&lt;/code&gt; 队列，用于优化不同调度器之间资源竞争的问题&lt;/li&gt;&lt;li&gt;支持调度限速功能，主动向 TiKV 下发调度操作，限制单节点同时执行调度任务的个数，提升调度速度&lt;/li&gt;&lt;li&gt;Region Scatter 调度不再受 limit 机制限制，提升调度的速度&lt;/li&gt;&lt;li&gt;新增 &lt;code&gt;shuffle-hot-region&lt;/code&gt; 调度器，解决稳定性测试易用性问题&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;模拟器&lt;/li&gt;&lt;ul&gt;&lt;li&gt;新增数据导入场景模拟&lt;/li&gt;&lt;li&gt;新增为 Store 设置不同的心跳间隔的功能&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;其他&lt;/li&gt;&lt;ul&gt;&lt;li&gt;升级 etcd，解决输出日志格式不一致，prevote 时选举不出 Leader，Lease 死锁等问题&lt;/li&gt;&lt;li&gt;制定日志格式规范，重构日志系统，方便工具收集分析&lt;/li&gt;&lt;li&gt;新增调度参数，集群 Label 信息，PD 处理 TSO 请求的耗时，Store ID 与地址信息等监控指标&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiKV&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;新增分布式 GC 以及并行 Resolve Lock 功能，提升 GC 的性能&lt;/li&gt;&lt;li&gt;新增逆向 &lt;code&gt;raw_scan&lt;/code&gt; 和 &lt;code&gt;raw_batch_scan&lt;/code&gt; 功能&lt;/li&gt;&lt;li&gt;新增多线程 Raftstore 和 Apply 功能，提升单节点内可扩展性，提升单节点内并发处理能力，提升单节点的资源利用率，降低延时，同等压力情况下性能提升 70%&lt;/li&gt;&lt;li&gt;新增批量接收和发送 Raft 消息功能，写入密集的场景 TPS 提升 7%&lt;/li&gt;&lt;li&gt;新增 Apply snapshot 之前检查 RocksDB level 0 文件的优化，避免产生 Write stall&lt;/li&gt;&lt;li&gt;新增 Titan 存储引擎插件，提升 Value 超过 1KiB 时系统的性能，一定程度上缓解写放大问题（&lt;b&gt;实验特性&lt;/b&gt;）&lt;/li&gt;&lt;li&gt;新增悲观事务模型（&lt;b&gt;实验特性&lt;/b&gt;）&lt;/li&gt;&lt;li&gt;新增通过 HTTP 方式获取监控信息功能&lt;/li&gt;&lt;li&gt;修改 Insert 语义，仅在 Key 不存在的时候 Prewrite 才成功&lt;/li&gt;&lt;li&gt;制定日志格式规范，重构日志系统，方便工具收集分析&lt;/li&gt;&lt;li&gt;新增配置信息，Key 越界相关的性能监控指标&lt;/li&gt;&lt;li&gt;RawKV 使用 Local Reader，提升性能&lt;/li&gt;&lt;li&gt;Engine&lt;/li&gt;&lt;ul&gt;&lt;li&gt;优化内存管理，减少 &lt;code&gt;Iterator Key Bound Option&lt;/code&gt; 的内存分配和拷贝，提升性能&lt;/li&gt;&lt;li&gt;支持多个 column family 共享 block cache，提升资源的利用率&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Server&lt;/li&gt;&lt;ul&gt;&lt;li&gt;优化 &lt;code&gt;batch commands&lt;/code&gt; 的上下文切换开销，提升性能&lt;/li&gt;&lt;li&gt;删除 txn scheduler&lt;/li&gt;&lt;li&gt;新增 read index，GC worker 相关监控项&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;RaftStore&lt;/li&gt;&lt;ul&gt;&lt;li&gt;新增 hibernate Regions 功能，优化 RaftStore CPU 的消耗（&lt;b&gt;实验特性&lt;/b&gt;）&lt;/li&gt;&lt;li&gt;删除 local reader 线程&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Coprocessor&lt;/li&gt;&lt;ul&gt;&lt;li&gt;重构计算框架，实现向量化算子、向量化表达式计算、向量化聚合，提升性能&lt;/li&gt;&lt;li&gt;支持为 TiDB &lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt; 语句提供算子执行详情&lt;/li&gt;&lt;li&gt;改用 work-stealing 线程池模型，减少上下文切换&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Tools&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;TiDB Lightning&lt;/li&gt;&lt;ul&gt;&lt;li&gt;支持数据表重定向同步功能&lt;/li&gt;&lt;li&gt;新增导入 CSV 文件功能&lt;/li&gt;&lt;li&gt;提升 SQL 转 KV 对的性能&lt;/li&gt;&lt;li&gt;单表支持批量导入功能，提升单表导入的性能&lt;/li&gt;&lt;li&gt;支持将大表的数据和索引分别导入，提升 &lt;code&gt;TiKV-Importer&lt;/code&gt; 导入数据性能&lt;/li&gt;&lt;li&gt;支持对新增文件中缺少 Column 数据时使用 row id 或者列的默认值填充缺少的 column 数据&lt;/li&gt;&lt;li&gt;&lt;code&gt;TiKV-Importer&lt;/code&gt; 支持对 upload SST 到 TiKV 限速功能&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;TiDB Binlog&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Drainer 新增 &lt;code&gt;advertise-addr&lt;/code&gt; 配置，支持容器环境中使用桥接模式&lt;/li&gt;&lt;li&gt;Pump 使用 TiKV GetMvccByKey 接口加快事务状态查询&lt;/li&gt;&lt;li&gt;新增组件之间通讯数据压缩功能，减少网络资源消耗&lt;/li&gt;&lt;li&gt;新增 Arbiter 工具支持从 Kafka 读取 binlog 并同步到 MySQL 功能&lt;/li&gt;&lt;li&gt;Reparo 支持过滤不需要被同步的文件的功能&lt;/li&gt;&lt;li&gt;新增同步 Generated column 功能&lt;/li&gt;&lt;li&gt;新增 syncer.sql-mode 配置项，支持采用不同的 SQL mode 解析 DDL&lt;/li&gt;&lt;li&gt;新增 syncer.ignore-table 配置项，过滤不需要被同步的表&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;sync-diff-inspector&lt;/li&gt;&lt;ul&gt;&lt;li&gt;新增 checkpoint 功能，支持从断点继续校验的功能&lt;/li&gt;&lt;li&gt;新增 only-use-checksum 配置项，控制仅通过计算 checksum 校验数据的一致性&lt;/li&gt;&lt;li&gt;新增采用 TiDB 统计信息以及使用多个 Column 划分 Chunk 的功能，适应更多的场景&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;TiDB Ansible&lt;/b&gt;&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;升级监控组件版本到安全的版本&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Prometheus 从 2.2.1 升级到 2.8.1 版本&lt;/li&gt;&lt;li&gt;Pushgateway 从 0.4.0 升级到 0.7.0 版本&lt;/li&gt;&lt;li&gt;Node_exporter 从 0.15.2 升级到 0.17.0 版本&lt;/li&gt;&lt;li&gt;Alertmanager 从 0.14.0 升级到 0.17.0 版本&lt;/li&gt;&lt;li&gt;Grafana 从 4.6.3 升级到 6.1.6 版本&lt;/li&gt;&lt;li&gt;Ansible 从 2.5.14 升级到 2.7.11 版本&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;新增 TiKV summary 监控面板，方便查看集群状态&lt;/li&gt;&lt;li&gt;新增 TiKV trouble_shooting 监控面板，删除重复项，方便排查问题&lt;/li&gt;&lt;li&gt;新增 TiKV details 监控面板，方便调试排查问题&lt;/li&gt;&lt;li&gt;新增滚动升级并发检测版本是否一致功能，提升滚动升级性能&lt;/li&gt;&lt;li&gt;新增 lightning 部署运维功能&lt;/li&gt;&lt;li&gt;优化 &lt;code&gt;table-regions.py&lt;/code&gt; 脚本，新增按表显示 leader 分布功能&lt;/li&gt;&lt;li&gt;优化 TiDB 监控，新增以 SQL 类别显示延迟的监控项&lt;/li&gt;&lt;li&gt;修改操作系统版本限制，仅支持 CentOS 7.0 及以上，Red Hat 7.0 及以上版本的操作系统&lt;/li&gt;&lt;li&gt;新增预测集群最大 QPS 的监控项，默认隐藏&lt;/li&gt;&lt;/ul&gt;&lt;p class=&quot;ztext-empty-paragraph&quot;&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;TiDB  源码地址：&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-06-29-71488780</guid>
<pubDate>Sat, 29 Jun 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 3.0 GA，稳定性和性能大幅提升</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-06-29-71488654.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/71488654&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f48f1a3e0f71f72a3b8f24786866da8f_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;TiDB 是 PingCAP 自主研发的开源分布式关系型数据库，具备商业级数据库的数据可靠性，可用性，安全性等特性，支持在线弹性水平扩展，兼容 MySQL 协议及生态，创新性实现 OLTP 及 OLAP 融合。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB 3.0 版本显著提升了大规模集群的稳定性，集群支持 150+ 存储节点，300+TB 存储容量长期稳定运行。易用性方面引入大量降低用户运维成本的优化，包括引入 Information_Schema 中的多个实用系统视图、EXPLAIN ANALYZE、SQL Trace 等。在性能方面，特别是 OLTP 性能方面，3.0 比 2.1 也有大幅提升，其中 TPC-C 性能提升约 4.5 倍，Sysbench 性能提升约 1.5 倍，OLAP 方面，TPC-H 50G Q15 因实现 View 可以执行，至此 TPC-H 22 个 Query 均可正常运行。新功能方面增加了窗口函数、视图（实验特性）、分区表、插件系统、悲观锁（实验特性）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;截止本文发稿时 TiDB 已在 500+ 用户的生产环境中长期稳定运行，涵盖金融、保险、制造，互联网，游戏等领域，涉及交易、数据中台、历史库等多个业务场景。不同业务场景对关系型数据库的诉求可用 “百花齐放”来形容，但对关系数据库最根本的诉求未发生任何变化，如数据可靠性，系统稳定性，可扩展性，安全性，易用性等。请跟随我们的脚步梳理 TiDB 3.0 有什么样的惊喜。 &lt;/p&gt;&lt;h2&gt;&lt;b&gt;一、提升大规模集群稳定性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;3.0 与 2.1 版本相比，显著提升了大规模集群的稳定性，支持单集群 150+ 存储节点，300+TB 存储容量长期稳定运行，主要的优化点如下：&lt;/p&gt;&lt;p&gt;1. 优化 Raft 副本之间的心跳机制，按照 Region 的活跃程度调整心跳频率，减小冷数据对集群的负担。&lt;/p&gt;&lt;p&gt;2. 热点调度策略支持更多参数配置，采用更高优先级，并提升热点调度的准确性。&lt;/p&gt;&lt;p&gt;3. 优化 PD 调度流程，提供调度限流机制，提升系统稳定性。&lt;/p&gt;&lt;p&gt;4. 新增分布式 GC 功能，提升 GC 的性能，降低大集群 GC 时间，提升系统稳定性。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、提升查询计划的稳定性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;众所周知，数据库查询计划的稳定性对业务至关重要，TiDB 3.0 版本采用多种优化手段提升查询计划的稳定性，如下：&lt;/p&gt;&lt;p&gt;1. 新增 Fast Analyze 功能，提升收集统计信息的速度，降低集群资源的消耗及对业务的影响。&lt;/p&gt;&lt;p&gt;2. 新增 Incremental Analyze 功能，提升收集单调递增的索引统计信息的速度，降低集群资源的消耗及对业务的影响。&lt;/p&gt;&lt;p&gt;3. 在 CM-Sketch 中新增 TopN 的统计信息，缓解 CM-Sketch 哈希冲突导致估算偏大，提升代价估算的准确性，提升查询计划的稳定性。&lt;/p&gt;&lt;p&gt;4. 引入 Skyline Pruning 框架，利用规则防止查询计划过度依赖统计信息，缓解因统计信息滞后导致选择的查询计划不是最优的情况，提升查询计划的稳定性。&lt;/p&gt;&lt;p&gt;5. 新增 SQL Plan Management 功能，支持在查询计划不准确时手动绑定查询计划，提升查询计划的稳定性。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、提升系统性能，TPC-C 性能提升约 4.5 倍，Sysbench 性能提升约 1.5 倍&lt;/b&gt;&lt;/h2&gt;&lt;h2&gt;&lt;b&gt;1. OLTP&lt;/b&gt;&lt;/h2&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8804ddce00336ae8ae6156656c4e83d6_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;750&quot; data-rawheight=&quot;468&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;750&quot; data-original=&quot;https://pic3.zhimg.com/v2-8804ddce00336ae8ae6156656c4e83d6_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8804ddce00336ae8ae6156656c4e83d6_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;750&quot; data-rawheight=&quot;468&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;750&quot; data-original=&quot;https://pic3.zhimg.com/v2-8804ddce00336ae8ae6156656c4e83d6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-8804ddce00336ae8ae6156656c4e83d6_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6016623afb17a1425fabddbbb6c77df5_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;743&quot; data-rawheight=&quot;481&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;743&quot; data-original=&quot;https://pic2.zhimg.com/v2-6016623afb17a1425fabddbbb6c77df5_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6016623afb17a1425fabddbbb6c77df5_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;743&quot; data-rawheight=&quot;481&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;743&quot; data-original=&quot;https://pic2.zhimg.com/v2-6016623afb17a1425fabddbbb6c77df5_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-6016623afb17a1425fabddbbb6c77df5_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7acb8d68b833982d0530639240164a56_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;756&quot; data-rawheight=&quot;470&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;756&quot; data-original=&quot;https://pic3.zhimg.com/v2-7acb8d68b833982d0530639240164a56_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-7acb8d68b833982d0530639240164a56_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;756&quot; data-rawheight=&quot;470&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;756&quot; data-original=&quot;https://pic3.zhimg.com/v2-7acb8d68b833982d0530639240164a56_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-7acb8d68b833982d0530639240164a56_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2273d661e37fe7c0bf29f7551f81dbc9_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;737&quot; data-rawheight=&quot;456&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;737&quot; data-original=&quot;https://pic2.zhimg.com/v2-2273d661e37fe7c0bf29f7551f81dbc9_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2273d661e37fe7c0bf29f7551f81dbc9_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;737&quot; data-rawheight=&quot;456&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;737&quot; data-original=&quot;https://pic2.zhimg.com/v2-2273d661e37fe7c0bf29f7551f81dbc9_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-2273d661e37fe7c0bf29f7551f81dbc9_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;3.0 与 2.1 版本相比 Sysbench 的 Point Select，Update Index，Update Non-Index 均提升约 1.5 倍，TPC-C 性能提升约 4.5 倍。主要的优化点如下：&lt;/p&gt;&lt;p&gt;1. TiDB 持续优化 SQL 执行器，包括：优化 NOT EXISTS 子查询转化为 Anti Semi Join，优化多表 Join 时 Join 顺序选择等。&lt;/p&gt;&lt;p&gt;2. 优化 Index Join 逻辑，扩大 Index Join 算子的适用场景并提升代价估算的准确性。&lt;/p&gt;&lt;p&gt;3. TiKV 批量接收和发送消息功能，提升写入密集的场景的 TPS 约 7%，读密集的场景提升约 30%。&lt;/p&gt;&lt;p&gt;4. TiKV 优化内存管理，减少 Iterator Key Bound Option 的内存分配和拷贝，多个 Column Families 共享 block cache 提升 cache 命中率等手段大幅提升性能。&lt;/p&gt;&lt;p&gt;5. 引入 Titan 存储引擎插件，提升 Value 值超过 1KB 时性能，缓解 RocksDB 写放大问题，减少磁盘 IO 的占用。&lt;/p&gt;&lt;p&gt;6. TiKV 新增多线程 Raftstore 和 Apply 功能，提升单节点内可扩展性，进而提升单节点内并发处理能力和资源利用率，降低延时，大幅提升集群写入能力。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2. TiDB Lightning&lt;/b&gt;&lt;/h2&gt;&lt;p class=&quot;ztext-empty-paragraph&quot;&gt;&lt;br/&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7dd5e129f2faf5f80b49b763b8ef9585_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;639&quot; data-rawheight=&quot;395&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;639&quot; data-original=&quot;https://pic2.zhimg.com/v2-7dd5e129f2faf5f80b49b763b8ef9585_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-7dd5e129f2faf5f80b49b763b8ef9585_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;639&quot; data-rawheight=&quot;395&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;639&quot; data-original=&quot;https://pic2.zhimg.com/v2-7dd5e129f2faf5f80b49b763b8ef9585_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-7dd5e129f2faf5f80b49b763b8ef9585_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p class=&quot;ztext-empty-paragraph&quot;&gt;&lt;br/&gt;&lt;/p&gt;&lt;p&gt;TiDB Lightning 性能与 2019 年年初相比提升 3 倍，从 100GB/h 提升到 300GB/h，即 28MB/s 提升到 85MB/s，优化点，如下：&lt;/p&gt;&lt;p&gt;1. 提升 SQL 转化成 KV Pairs 的性能，减少不必要的开销。&lt;/p&gt;&lt;p&gt;2. 提升单表导入性能，单表支持批量导入。&lt;/p&gt;&lt;p&gt;3. 提升 TiKV-Importer 导入数据性能，支持将数据和索引分别导入。&lt;/p&gt;&lt;p&gt;4. TiKV-Importer 支持上传 SST 文件限速功能。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四、提升系统安全性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;RBAC（Role-Based Access Control，基于角色的权限访问控制）&lt;/b&gt;是商业系统中最常见的权限管理技术之一，通过 RBAC 思想可以构建最简单“用户-角色-权限”的访问权限控制模型。RBAC 中用户与角色关联，权限与角色关联，角色与权限之间一般是多对多的关系，用户通过成为什么样的角色获取该角色所拥有的权限，达到简化权限管理的目的，通过此版本的迭代 RBAC 功能开发完成。&lt;/p&gt;&lt;p&gt;&lt;b&gt;IP 白名单功能（企业版特性）&lt;/b&gt;：TiDB 提供基于 IP 白名单实现网络安全访问控制，用户可根据实际情况配置相关的访问策略。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Audit log 功能（企业版特性）&lt;/b&gt;：Audit log 记录用户对数据库所执行的操作，通过记录 Audit log 用户可以对数据库进行故障分析，行为分析，安全审计等，帮助用户获取数据执行情况。&lt;/p&gt;&lt;p&gt;&lt;b&gt;加密存储（企业版特性）&lt;/b&gt;：TiDB 利用 RocksDB 自身加密功能，实现加密存储的功能，保证所有写入到磁盘的数据都经过加密，降低数据泄露的风险。&lt;/p&gt;&lt;p&gt;&lt;b&gt;完善权限语句的权限检查&lt;/b&gt;，新增 ANALYZE，USE，SET GLOBAL，SHOW PROCESSLIST 语句权限检查。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;五、提升系统易用性&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;1. 新增 SQL 方式查询慢查询，丰富 TiDB 慢查询日志内容，如：Coprocessor 任务数，平均/最长/90% 执行/等待时间，执行/等待时间最长的 TiKV 地址，简化慢查询定位工作，提高排查慢查询问题效率，提升产品易用性。&lt;/p&gt;&lt;p&gt;2. 新增系统配置项合法性检查，优化系统监控项等，提升产品易用性。&lt;/p&gt;&lt;p&gt;3. 新增对 TableReader、IndexReader 和 IndexLookupReader 算子内存使用情况统计信息，提高 Query 内存使用统计的准确性，提升处理内存消耗较大语句的效率。&lt;/p&gt;&lt;p&gt;4. 制定日志规范，重构日志系统，统一日志格式，方便用户理解日志内容，有助于通过工具对日志进行定量分析。&lt;/p&gt;&lt;p&gt;5. 新增 EXPLAIN ANALYZE 功能，提升SQL 调优的易用性。&lt;/p&gt;&lt;p&gt;6. 新增 SQL 语句 Trace 功能，方便排查问题。&lt;/p&gt;&lt;p&gt;7. 新增通过 unix_socket 方式连接数据库。&lt;/p&gt;&lt;p&gt;8. 新增快速恢复被删除表功能，当误删除数据时可通过此功能快速恢复数据。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;六、增强 HTAP 能力&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;TiDB 3.0 新增 TiFlash  组件，解决复杂分析及 HTAP 场景。TiFlash 是列式存储系统，与行存储系统实时同步，具备低延时，高性能，事务一致性读等特性。&lt;/b&gt;通过 Raft 协议从 TiKV 中实时同步行存数据并转化成列存储格式持久化到一组独立的节点，解决行列混合存储以及资源隔离性问题。TiFlash 可用作行存储系统（TiKV）实时镜像，实时镜像可独立于行存储系统，将行存储及列存储从物理隔离开，提供完善的资源隔离方案，HTAP 场景最优推荐方案；亦可用作行存储表的索引，配合行存储对外提供智能的 OLAP 服务，提升约 10 倍复杂的混合查询的性能。&lt;/p&gt;&lt;p&gt;TiFlash 目前处于 Beta 阶段，计划 2019 年 12 月 31 日之前 GA，欢迎大家申请试用。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;七、未来规划&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;未来我们会继续投入到系统稳定性，易用性，性能，弹性扩展方面，向用户提供极致的弹性伸缩能力，极致的性能体验，极致的用户体验。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;稳定性方面 V4.0 版本将继续完善 V3.0 未 GA 的重大特性，例如：悲观事务模型，View，Table Partition，Titan 行存储引擎，TiFlash 列存储引擎；引入近似物理备份恢复解决分布数据库备份恢复难题；优化 PD 调度功能等。&lt;/p&gt;&lt;p&gt;性能方面 V4.0 版本将继续优化事务处理流程，减少事务资源消耗，提升性能，例如：1PC，省去获取 commit ts 操作等。&lt;/p&gt;&lt;p&gt;弹性扩展方面，PD 将提供弹性扩展所需的元信息供外部系统调用，外部系统可根据元信息及负载情况动态伸缩集群规模，达成节省成本的目标。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;八、社区概况&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们相信战胜“未知”最好的武器就是社区的力量，基础软件需要坚定地走开源路线。截止发稿我们已经完成 41 篇源码阅读文章。TiDB 开源社区总计 265 位 Contributor，6 位 Committer，在这里我们对社区贡献者表示由衷的感谢，希望更多志同道合的人能加入进来，也希望大家在 TiDB 这个开源社区能够有所收获。&lt;/p&gt;&lt;p&gt;&lt;i&gt;TiDB 3.0 GA Release Notes：&lt;u&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/docs-cn/v3.0/releases/3.0-ga/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;pingcap.com/docs-cn/v3.&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;0/releases/3.0-ga/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/u&gt;&lt;/i&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-06-29-71488654</guid>
<pubDate>Sat, 29 Jun 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在知乎万亿量级业务数据下的实践和挑战</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-06-27-71023604.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/71023604&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-33fac149fd45265dadc71fa84d2af5ca_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;br/&gt;孙晓光，知乎搜索后端负责人，目前承担知乎搜索后端架构设计以及工程团队的管理工作。曾多年从事私有云相关产品开发工作关注云原生技术，TiKV 项目 Committer。&lt;/blockquote&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-44acdd0bdd6978684885c75952131a50_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1010&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1010&quot; data-original=&quot;https://pic1.zhimg.com/v2-44acdd0bdd6978684885c75952131a50_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-44acdd0bdd6978684885c75952131a50_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1010&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1010&quot; data-original=&quot;https://pic1.zhimg.com/v2-44acdd0bdd6978684885c75952131a50_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-44acdd0bdd6978684885c75952131a50_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;本文根据孙晓光老师在 TiDB TechDay 2019 北京站上的演讲整理。&lt;/p&gt;&lt;p&gt;本次分享首先将从宏观的角度介绍知乎已读服务的业务场景中的挑战、架构设计思路，然后将从微观的角度介绍其中的关键组件的实现，最后分享在整个过程中 TiDB 帮助我们解决了什么样的问题，以及 TiDB 是如何帮助我们将庞大的系统全面云化，并推进到一个非常理想的状态的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;一、业务场景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;知乎从问答起步，在过去的 8 年中逐步成长为一个大规模的综合性知识内容平台，目前，知乎上有多达 3000 万个问题，共收获了超过 1.3 亿个回答，同时知乎还沉淀了数量众多的文章、电子书以及其他付费内容，目前注册用户数是 2.2 亿，这几个数字还是蛮惊人的。我们有 1.3 亿个回答，还有更多的专栏文章，所以如何高效的把用户最感兴趣的优质内容分发他们，就是非常重要的问题。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-82100eaa41cce64800dedcbb4ae3a8ac_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-82100eaa41cce64800dedcbb4ae3a8ac_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-82100eaa41cce64800dedcbb4ae3a8ac_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-82100eaa41cce64800dedcbb4ae3a8ac_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-82100eaa41cce64800dedcbb4ae3a8ac_b.jpg&quot;/&gt;&lt;figcaption&gt;图 1&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;知乎首页是解决流量分发的一个关键的入口，而已读服务想要帮助知乎首页解决的问题是，如何在首页中给用户推荐感兴趣的内容，同时避免给用户推荐曾经看过的内容。已读服务会将所有知乎站上用户深入阅读或快速掠过的内容记录下来长期保存，并将这些数据应用于首页推荐信息流和个性化推送的已读过滤。图 2 是一个典型的流程：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-49b41dad1c0a8689a4aa19722ec1a79f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-49b41dad1c0a8689a4aa19722ec1a79f_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-49b41dad1c0a8689a4aa19722ec1a79f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-49b41dad1c0a8689a4aa19722ec1a79f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-49b41dad1c0a8689a4aa19722ec1a79f_b.jpg&quot;/&gt;&lt;figcaption&gt;图 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当用户打开知乎进入推荐页的时候，系统向首页服务发起请求拉取“用户感兴趣的新内容”，首页根据用户画像，去多个召回队列召回新的候选内容，这些召回的新内容中可能有部分是用户曾经看到过的，所以在分发给用户之前，首页会先把这些内容发给已读服务过滤，然后做进一步加工并最终返回给客户端，其实这个业务流程是非常简单的。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cdfd56e43cc5512e7a7b16d203940daa_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-cdfd56e43cc5512e7a7b16d203940daa_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-cdfd56e43cc5512e7a7b16d203940daa_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-cdfd56e43cc5512e7a7b16d203940daa_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-cdfd56e43cc5512e7a7b16d203940daa_b.jpg&quot;/&gt;&lt;figcaption&gt;图 3&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;这个业务第一个的特点是可用性要求非常高，因为首页可能是知乎最重要的流量分发渠道。第二个特点是写入量非常大，峰值每秒写入 40k+ 条记录，每日新增记录近 30 亿条。并且我们保存数据的时间比较长，按照现在产品设计需要保存三年。整个产品迭代到现在，已经保存了约一万三千亿条记录，按照每月近一千亿条的记录增长速度，大概两年之后，可能要膨胀到三万亿的数据规模。&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e31709d9ae97661bae72c44e72ca3ab9_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-e31709d9ae97661bae72c44e72ca3ab9_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e31709d9ae97661bae72c44e72ca3ab9_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-e31709d9ae97661bae72c44e72ca3ab9_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-e31709d9ae97661bae72c44e72ca3ab9_b.jpg&quot;/&gt;&lt;figcaption&gt;图 4&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这个业务的查询端要求也很高。首先，产品吞吐高。用户在线上每次刷新首页，至少要查一次，并且因为有多个召回源和并发的存在，查询吞吐量还可能放大。&lt;b&gt;峰值时间首页每秒大概产生 3 万次独立的已读查询，每次查询平均要查 400 个文档，长尾部分大概 1000 个文档，也就是说，整个系统峰值平均每秒大概处理 1200 万份文档的已读查询。在这样一个吞吐量级下，要求的响应时间还比较严格，要求整个查询响应时间（端到端超时）是 90ms，也就意味着最慢的长尾查询都不能超过 90ms。&lt;/b&gt;还有一个特点是，它可以容忍 false positive，意味着有些内容被我们过滤掉了，但是系统仍然能为用户召回足够多的他们可能感兴趣的内容，只要 false positive rate 被控制在可接受的范围就可以了。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、架构设计&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;由于知乎首页的重要性，我们在设计这个系统的时候，考虑了三个设计目标：高可用、高性能、易扩展。首先，如果用户打开知乎首页刷到大量已经看过的内容，这肯定不可接受，所以对已读服务的第一个要求是「高可用」。第二个要求是「性能高」，因为业务吞吐高，并且对响应时间要求也非常高。第三点是这个系统在不断演进和发展，业务也在不断的更新迭代，所以系统的「扩展性」非常重要，不能说今天能支撑，明天就支撑不下来了，这是没法接受的。&lt;/p&gt;&lt;p&gt;接下来从这三个方面来介绍我们具体是如何设计系统架构的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 高可用&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d13ebba4c75ada261b2c11e00a69d19_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5d13ebba4c75ada261b2c11e00a69d19_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d13ebba4c75ada261b2c11e00a69d19_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-5d13ebba4c75ada261b2c11e00a69d19_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5d13ebba4c75ada261b2c11e00a69d19_b.jpg&quot;/&gt;&lt;figcaption&gt;图 5&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当我们讨论高可用的时候，也意味着我们已经意识到故障是无时无刻都在发生的，想让系统做到高可用，首先就要有系统化的故障探测机制，检测组件的健康状况，然后设计好每一个组件的自愈机制，让它们在故障发生之后可以自动恢复，无需人工干预。最后我们希望用一定的机制把这些故障所产生的变化隔离起来，让业务侧尽可能对故障的发生和恢复无感知。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2 高性能&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0245f5bf18634302db5bf1bd5845b4a6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-0245f5bf18634302db5bf1bd5845b4a6_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0245f5bf18634302db5bf1bd5845b4a6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-0245f5bf18634302db5bf1bd5845b4a6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-0245f5bf18634302db5bf1bd5845b4a6_b.jpg&quot;/&gt;&lt;figcaption&gt;图 6&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;对常见的系统来说，越核心的组件往往状态越重扩展的代价也越大，层层拦截快速降低需要深入到核心组件的请求量对提高性能是非常有效的手段。首先我们通过缓冲分 Slot 的方式来扩展集群所能缓冲的数据规模。接着进一步在 Slot 内通过多副本的方式提升单个 Slot 缓冲数据集的读取吞吐，将大量的请求拦截在系统的缓冲层进行消化。如果请求不可避免的走到了最终的数据库组件上，我们还可以利用效率较高的压缩来继续降低落到物理设备上的 I/O 压力。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3 易扩展&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c322ebb4206b236919a5484226c67614_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-c322ebb4206b236919a5484226c67614_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c322ebb4206b236919a5484226c67614_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-c322ebb4206b236919a5484226c67614_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-c322ebb4206b236919a5484226c67614_b.jpg&quot;/&gt;&lt;figcaption&gt;图 7&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;提升系统扩展性的关键在于减少有状态组件的范围。在路由和服务发现组件的帮助下，系统中的无状态组件可以非常轻松的扩展扩容，所以通过扩大无状态服务的范围，收缩重状态服务的比例，可以显著的帮助我们提升整个系统的可扩展性。除此之外，如果我们能够设计一些可以从外部系统恢复状态的弱状态服务，部分替代重状态组件，这样可以压缩重状态组件的比例。随着弱状态组件的扩大和重状态组件的收缩，整个系统的可扩展性可以得到进一步的提升。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.4 已读服务最终架构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在高可用、高性能和易扩展的设计理念下，我们设计实现了已读服务的架构，图 8 是已读服务的最终架构。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2f54ae366c145f5d1850cb21bd5bda3d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-2f54ae366c145f5d1850cb21bd5bda3d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2f54ae366c145f5d1850cb21bd5bda3d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-2f54ae366c145f5d1850cb21bd5bda3d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-2f54ae366c145f5d1850cb21bd5bda3d_b.jpg&quot;/&gt;&lt;figcaption&gt;图 8&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;首先，上层的客户端 API 和 Proxy 是完全无状态可随时扩展的组件。最底层是存储全部状态数据的 TiDB，中间这些组件都是弱状态的组件，主体是分层的 Redis 缓冲。除了 Redis 缓冲之外，我们还有一些其他外部组件配合 Redis 保证 Cache 的一致性，这里面的细节会在下一章详述。&lt;/p&gt;&lt;p&gt;从整个系统来看，TiDB 这层自身已经拥有了高可用的能力，它是可以自愈的，系统中无状态的组件非常容易扩展，而有状态的组件中弱状态的部分可以通过 TiDB 中保存的数据恢复，出现故障时也是可以自愈的。此外系统中还有一些组件负责维护缓冲一致性，但它们自身是没有状态的。所以在系统所有组件拥有自愈能力和全局故障监测的前提下，我们使用 Kubernetes 来管理整个系统，从而在机制上确保整个服务的高可用。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;三、关键组件&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;3.1 Proxy&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0a9f0d3695493f0b1a643d655a2daf1b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-0a9f0d3695493f0b1a643d655a2daf1b_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0a9f0d3695493f0b1a643d655a2daf1b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-0a9f0d3695493f0b1a643d655a2daf1b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-0a9f0d3695493f0b1a643d655a2daf1b_b.jpg&quot;/&gt;&lt;figcaption&gt;图 9&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Proxy 层是无状态的，设计同常见的 Redis 代理相似，从实现角度看也非常简单。首先我们会基于用户纬度将缓冲拆分成若干 Slot，每个 Slot 里有多个 Cache 的副本，这些多副本一方面可以提升我们整个系统的可用性，另外一方面也可以分摊同一批数据的读取压力。&lt;b&gt;这里面也有一个问题，就是 Cache 的副本一致性的如何保证？我们在这里选择的是「会话一致性」，也就是一个用户在一段时间内从同一个入口进来，就会绑定在这一个 Slot 里面的某个副本上，只要没有发生故障，这个会话会维持在上面。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如果一个 Slot 内的某个副本发生故障，Proxy 首先挑这个 Slot 内的其他的副本继续提供服务。更极端的情况下，比如这个 Slot 内所有副本都发生故障，Proxy 可以牺牲系统的性能，把请求打到另外一个完全不相干的一个 Slot 上，这个 Slot 上面没有当前请求对应数据的缓存，而且拿到结果后也不会缓存相应的结果。我们付出这样的性能代价获得的收益是系统可用性变得更高，即便 Slot 里的所有的副本同时发生故障，依旧不影响系统的可用性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.2 Cache&lt;/b&gt;&lt;/p&gt;&lt;p&gt;对于缓冲来说，非常重要的一点就是如何提升缓冲利用率。 &lt;/p&gt;&lt;p&gt;第一点是如何用同样的资源缓冲更大量的数据。在由「用户」和「内容类型」和「内容」所组成的空间中，由于「用户」维度和「内容」维度的基数非常高，都在数亿级别，即使记录数在万亿这样的数量级下，数据在整个三维空间内的分布依然非常稀疏。如图 10 左半部分所示。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5cd5c74a5d6d3fb6e5bec81c5b3fae13_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-5cd5c74a5d6d3fb6e5bec81c5b3fae13_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-5cd5c74a5d6d3fb6e5bec81c5b3fae13_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-5cd5c74a5d6d3fb6e5bec81c5b3fae13_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-5cd5c74a5d6d3fb6e5bec81c5b3fae13_b.jpg&quot;/&gt;&lt;figcaption&gt;图 10&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;考虑到目前知乎站上沉淀的内容量级巨大，我们可以容忍 false positive 但依旧为用户召回到足够多可能会感兴趣的内容。基于这样的业务特点，我们将数据库中存储的原始数据转化为更加致密的 BloomFilter 缓冲起来，这极大的降低了内存的消耗在相同的资源状况下可以缓冲更多的数据，提高缓存的命中率。&lt;/p&gt;&lt;p&gt;提升缓存命中率的方式有很多种，除了前面提到的提升缓存数据密度增加可缓冲的数据量级之外，我们还可以通过避免不必要的缓存失效来进一步的提升缓存的效率。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-48b8838b042413cb5e25fc31da89b3fa_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-48b8838b042413cb5e25fc31da89b3fa_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-48b8838b042413cb5e25fc31da89b3fa_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-48b8838b042413cb5e25fc31da89b3fa_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-48b8838b042413cb5e25fc31da89b3fa_b.jpg&quot;/&gt;&lt;figcaption&gt;图 11&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;一方面我们将缓存设计为 write through cache 使用原地更新缓存的方式来避免 invalidate cache 操作，再配合数据变更订阅我们可以在不失效缓冲的情况下确保同一份数据的多个缓冲副本能在很短的时间内达成最终一致。&lt;/p&gt;&lt;p&gt;另一方面得益于 read through 的设计，我们可以将对同一份数据的多个并发查询请求转化成一次 cache miss 加多次缓冲读取（图 11 右半部分），进一步提升缓存的命中率降低穿透到底层数据库系统的压力。&lt;/p&gt;&lt;p&gt;接下来再分享一些不单纯和缓冲利用率相关的事情。众所周知，缓冲特别怕冷，一旦冷了， 大量的请求瞬间穿透回数据库，数据库很大概率都会挂掉。在系统扩容或者迭代的情况下，往往需要加入新的缓冲节点，那么如何把新的缓冲节点热起来呢？如果是类似扩容或者滚动升级这种可以控制速度的情况，我们可以控制开放流量的速度，让新的缓冲节点热起来，但当系统发生故障的时候，我们就希望这个节点非常快速的热起来。 所以在我们这个系统和其他的缓冲系统不大一样的是，当一个新节点启动起来，Cache 是冷的，它会马上从旁边的 Peer 那边 transfer 一份正在活跃的缓存状态过来，这样就可以非常快的速度热起来，以一个热身的状态去提供线上的服务（如图 12）。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_b.jpg&quot;/&gt;&lt;figcaption&gt;图 12&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;另外，我们可以设计分层的缓冲，每一层缓冲可以设计不同的策略，分别应对不同层面的问题，如图 13 所示，可以通过 L1 和 L2 分别去解决空间层面的数据热度问题和时间层面的热度问题，通过多层的 Cache 可以逐层的降低穿透到下一层请求的数量，尤其是当我们发生跨数据中心部署时，对带宽和时延要求非常高，如果有分层的设计，就可以在跨数据中心之间再放一层 Cache，减少在穿透到另外一个数据中心的请求数量。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-24b2d5a9de1eb3c99adb85a00356062f_b.jpg&quot;/&gt;&lt;figcaption&gt;图 13&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;为了让业务之间不互相影响并且针对不同业务的数据访问特征选择不同的缓冲策略，我们还进一步提供了 Cache 标签隔离的机制来隔离离线写入和多个不同的业务租户的查询。刚刚说的知乎已读服务数据，在后期已经不只是给首页提供服务了，还同时为个性化推送提供服务。个性化推送是一个典型的离线任务，在推送内容前去过滤一下用户是否看过。虽然这两个业务访问的数据是一样的，但是它们的访问特征和热点是完全不一样的，相应的缓冲策略也不一样的。于是我们在做分组隔离机制（如图 14），缓冲节点以标签的方式做隔离，不同的业务使用不同的缓冲节点，不同缓冲节点搭配不同的缓冲策略，达到更高的投入产出比，同时也能隔离各个不同的租户，防止他们之间互相产生影响。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f613b6bf7549f6fe767caa173540f293_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-f613b6bf7549f6fe767caa173540f293_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f613b6bf7549f6fe767caa173540f293_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-f613b6bf7549f6fe767caa173540f293_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-f613b6bf7549f6fe767caa173540f293_b.jpg&quot;/&gt;&lt;figcaption&gt;图 14&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;3.3 Storage &lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-456df3890be7bbaa9a9e47f4091ce3a8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;529&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-456df3890be7bbaa9a9e47f4091ce3a8_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-456df3890be7bbaa9a9e47f4091ce3a8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;529&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-456df3890be7bbaa9a9e47f4091ce3a8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-456df3890be7bbaa9a9e47f4091ce3a8_b.jpg&quot;/&gt;&lt;figcaption&gt;图 15&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;存储方面，我们最初用的是 MySQL，显然这么大量的数据单机是搞不定的，所以我们使用了分库分表 + MHA 机制来提升系统的性能并保障系统的高可用，在流量不太大的时候还能忍受，但是在当每月新增一千亿数据的情况下，我们心里的不安与日俱增，所以一直在思考怎样让系统可持续发展、可维护，并且开始选择替代方案。这时我们发现 TiDB 兼容了 MySQL，这对我们来说是非常好的一个特点，风险非常小，于是我们开始做迁移工作。迁移完成后，整个系统最弱的“扩展性”短板就被补齐了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3.4 性能指标&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4cc47ebc14248fec77ac4aaf8095597a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-4cc47ebc14248fec77ac4aaf8095597a_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-4cc47ebc14248fec77ac4aaf8095597a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-4cc47ebc14248fec77ac4aaf8095597a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-4cc47ebc14248fec77ac4aaf8095597a_b.jpg&quot;/&gt;&lt;figcaption&gt;图 16&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;现在整个系统都是高可用的，随时可以扩展，而且性能变得更好。图 16 是前两天我取出来的性能指标数据，目前已读服务的流量已达每秒 4 万行记录写入， 3 万独立查询和 1200 万个文档判读，在这样的压力下已读服务响应时间的 P99 和 P999 仍然稳定的维持在 25ms 和 50ms，其实平均时间是远低于这个数据的。这个意义在于已读服务对长尾部分非常敏感，响应时间要非常稳定，因为不能牺牲任何一位用户的体验，对一位用户来说来说超时了就是超时了。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;四、All about TiDB &lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最后分享一下我们从 MySQL 迁移到 TiDB 的过程中遇到的困难、如何去解决的，以及 TiDB 3.0 发布以后我们在这个快速迭代的产品上，收获了什么样的红利。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1 MySQL to TiDB&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e028c1a8b3ed6073d953255c0d95dbc6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-e028c1a8b3ed6073d953255c0d95dbc6_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e028c1a8b3ed6073d953255c0d95dbc6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-e028c1a8b3ed6073d953255c0d95dbc6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-e028c1a8b3ed6073d953255c0d95dbc6_b.jpg&quot;/&gt;&lt;figcaption&gt;图 17&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;现在其实整个 TiDB 的数据迁移的生态工具已经很完善，我们打开 TiDB DM 收集 MySQL 的增量 binlog 先存起来，接着用 TiDB Lightning 快速把历史数据导入到 TiDB 中，当时应该是一万一千亿左右的记录，导入总共用时四天。这个时间还是非常震撼的，因为如果用逻辑写入的方式至少要花一个月。当然四天也不是不可缩短，那时我们的硬件资源不是特别充足，选了一批机器，一批数据导完了再导下一批，如果硬件资源够的话，可以导入更快，也就是所谓“高投入高产出”，如果大家有更多的资源，那么应该可以达到更好的效果。在历史数据全部导入完成之后，就需要开启 TiDB DM 的增量同步机制，自动把刚才存下来的历史增量数据和实时增量数据同步到 TiDB 中，并近实时的维持 TiDB 和 MySQL 数据的一致。&lt;/p&gt;&lt;p&gt;在迁移完成之后，我们就开始小流量的读测试，刚上线的时候其实发现是有问题的，Latency 无法满足要求，刚才介绍了这个业务对 Latency 特别敏感，稍微慢一点就会超时。这时 PingCAP 伙伴们和我们一起不停去调优、适配，解决 Latency 上的问题。图 18 是我们总结的比较关键的经验。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fe4001d4419f4a25967bbd02bba66aac_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-fe4001d4419f4a25967bbd02bba66aac_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-fe4001d4419f4a25967bbd02bba66aac_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-fe4001d4419f4a25967bbd02bba66aac_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-fe4001d4419f4a25967bbd02bba66aac_b.jpg&quot;/&gt;&lt;figcaption&gt;图 18&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;第一，我们把对 Latency 敏感的部分 Query 布了一个独立的 TiDB 隔离开，防止特别大的查询在同一个 TiDB 上影响那些对 Latency 敏感的的 Query。第二，有些 Query 的执行计划选择不是特别理想，我们也做了一些 SQL Hint，帮助执行引擎选择一个更加合理的执行计划。除此之外，我们还做了一些更微观的优化，比如说使用低精度的 TSO，还有包括复用 Prepared Statement 进一步减少网络上的 roundtrip，最后达到了很好的效果。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-99552264fa3fde7a8f8eb5e2c6839605_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-99552264fa3fde7a8f8eb5e2c6839605_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-99552264fa3fde7a8f8eb5e2c6839605_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-99552264fa3fde7a8f8eb5e2c6839605_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-99552264fa3fde7a8f8eb5e2c6839605_b.jpg&quot;/&gt;&lt;figcaption&gt;图 19&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这个过程中我们还做了一些开发的工作，比如 binlog 之间的适配。因为这套系统是靠 binlog 变更下推来维持缓冲副本之间的一致性，所以 binlog 尤为重要。我们需要把原来 MySQL 的 binlog 改成 TiDB 的 binlog，但是过程中遇到了一些问题，因为 TiDB 作为一个数据库产品，它的 binlog 要维持全局的有序性的排列，然而在我们之前的业务中由于分库分表，我们不关心这个事情，所以我们做了些调整工作，把之前的 binlog 改成可以用 database 或者 table 来拆分的 binlog，减轻了全局有序的负担，binlog 的吞吐也能满足我们要求了。同时，PingCAP 伙伴们也做了很多 Drainer 上的优化，目前 Drainer 应该比一两个月前的状态好很多，不论是吞吐还是 Latency 都能满足我们现在线上的要求。&lt;/p&gt;&lt;p&gt;最后一点经验是关于资源评估，因为这一点可能是我们当时做得不是特别好的地方。最开始我们没有特别仔细地想到底要多少资源才能支撑同样的数据。最初用 MySQL 的时候，为了减少运维负担和成本，我们选择了“1 主 1 从”方式部署 ，而 TiDB 用的 Raft 协议要求至少三个副本，所以资源要做更大的准备，不能指望用同样的资源来支撑同样的业务，一定要提前准备好对应的机器资源。另外，我们的业务模式是一个非常大的联合主键，这个联合主键在 TiDB 上非聚簇索引，又会导致数据更加庞大，也需要对应准备出更多的机器资源。最后，因为 TiDB 是存储与计算分离的架构，所以网络环境一定要准备好。当这些资源准备好，最后的收益是非常明显的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.2 TiDB 3.0&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在知乎内部采用与已读服务相同的技术架构我们还支撑了一套用于反作弊的风控类业务。与已读服务极端的历史数据规模不同，反作弊业务有着更加极端的写入吞吐但只需在线查询最近 48 小时入库的数据（详细对比见图 20）。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-12c30e58e06170aee0ae35ff4c4bbbee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-12c30e58e06170aee0ae35ff4c4bbbee_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-12c30e58e06170aee0ae35ff4c4bbbee_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-12c30e58e06170aee0ae35ff4c4bbbee_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-12c30e58e06170aee0ae35ff4c4bbbee_b.jpg&quot;/&gt;&lt;figcaption&gt;图 20&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;那么 TiDB 3.0 的发布为我们这两个业务，尤其是为反作弊这个业务，带来什么样的可能呢？&lt;/p&gt;&lt;p&gt;首先我们来看看已读服务。已读服务写读吞吐也不算小，大概 40k+，TiDB 3.0 的 gRPC Batch Message 和多线程 Raft store，能在这件事情上起到很大的帮助。另外，Latency 这块，我刚才提到了，就是我们写了非常多 SQL Hint 保证 Query 选到最优的执行计划，TiDB 3.0 有 Plan Management 之后，我们再遇到执行计划相关的问题就无需调整代码上线，直接利用 Plan Management 进行调整就可以生效了，这是一个非常好用的 feature。&lt;/p&gt;&lt;p&gt;刚才马晓宇老师详细介绍了 TiFlash，在 &lt;u&gt;&lt;a href=&quot;https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487986%26idx%3D1%26sn%3Dcc0d28d9776bc50ede7a9fc4aa403208%26chksm%3Deb163698dc61bf8e602fe61d12376c5d951a71c1568b3cd253e0f4d410bf7918c5c2fadf01ce%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB DevCon 2019&lt;/a&gt;&lt;/u&gt; 上第一次听到这个产品的时候就觉得特别震撼，大家可以想象一下，一万多亿条的数据能挖掘出多少价值， 但是在以往这种高吞吐的写入和庞大的全量数据规模用传统的 ETL 方式是难以在可行的成本下将数据每日同步到 Hadoop 上进行分析的。而当我们有 TiFlash，一切就变得有可能了。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d2e4607dc0852743a046547ebcc296f6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-d2e4607dc0852743a046547ebcc296f6_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-d2e4607dc0852743a046547ebcc296f6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-d2e4607dc0852743a046547ebcc296f6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-d2e4607dc0852743a046547ebcc296f6_b.jpg&quot;/&gt;&lt;figcaption&gt;图 21&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;再来看看反作弊业务，它的写入更极端，这时 TiDB 3.0 的 Batch message 和多线程 Raft Store 两个特性可以让我们在更低的硬件配置情况下，达到之前同样的效果。&lt;b&gt;另外反作弊业务写的记录偏大，TiDB 3.0 中包含的新的存储引擎 Titan，就是来解决这个问题的，我们从 TiDB 3.0.0- rc1 开始就在反作弊业务上将 TiDB 3.0 引入到了生产环境，并在 rc2 发布不久之后开启了 Titan 存储引擎，下图右半部分可以看到 Titan 开启前后的写入/查询 Latency 对比，当时我们看到这个图的时候都非常非常震撼，这是一个质的变化。&lt;/b&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f3a4295f93010f060b1bbb537148d347_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-f3a4295f93010f060b1bbb537148d347_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f3a4295f93010f060b1bbb537148d347_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;525&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-f3a4295f93010f060b1bbb537148d347_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-f3a4295f93010f060b1bbb537148d347_b.jpg&quot;/&gt;&lt;figcaption&gt;图 22&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;另外，我们也使用了 TiDB 3.0 中 Table Partition 这个特性。通过在时间维度拆分  Table Partition，可以控制查询落到最近的 Partition 上，这对查询的时效提升非常明显。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;五、总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最后简单总结一下我们开发这套系统以及在迁移到 TiDB 过程中的收获和思考。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a05b2f43c7be33f8e30c47b863a4b0bd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-a05b2f43c7be33f8e30c47b863a4b0bd_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a05b2f43c7be33f8e30c47b863a4b0bd_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;527&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-a05b2f43c7be33f8e30c47b863a4b0bd_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a05b2f43c7be33f8e30c47b863a4b0bd_b.jpg&quot;/&gt;&lt;figcaption&gt;图 23&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;首先开发任何系统前一定先要理解这个业务特点，对应设计更好的可持续支撑的方案，同时希望这个架构具有普适性，就像已读服务的架构，除了支撑知乎首页，还可以同时支持反作弊的业务。&lt;/p&gt;&lt;p&gt;另外，我们大量应用了开源软件，不仅一直使用，还会参与一定程度的开发，在这个过程中我们也学到了很多东西。所以我们应该不仅以用户的身份参与社区，甚至还可以为社区做更多贡献，一起把 TiDB 做的更好、更强。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后一点，我们业务系统的设计可能看上去有点过于复杂，但站在今天 Cloud Native 的时代角度，即便是业务系统，我们也希望它能像 Cloud Native 产品一样，原生的支持高可用、高性能、易扩展，我们做业务系统也要以开放的心态去拥抱新技术，Cloud Native from Ground Up。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;更多 TiDB 用户实践：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-06-27-71023604</guid>
<pubDate>Thu, 27 Jun 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>DM 源码阅读系列文章（八）Online Schema Change 同步支持</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-06-19-69849157.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/69849157&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c57c79c148273959defa990675b8eea9_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：lan&lt;/p&gt;&lt;p&gt;本文为 DM 源码阅读系列文章的第八篇，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/dm-source-code-reading-7/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;上篇文章&lt;/a&gt; 对 DM 中的定制化数据同步功能进行详细的讲解，包括库表路由（Table routing）、黑白名单（Black &amp;amp; white table lists）、列值转化（Column mapping）、binlog 过滤（Binlog event filter）四个主要功能的实现。&lt;/p&gt;&lt;p&gt;本篇文章将会以 gh-ost 为例，详细地介绍 DM 是如何支持一些 MySQL 上的第三方 online schema change 方案同步，内容包括 online schema change 方案的简单介绍，online schema change 同步方案，以及同步实现细节。&lt;/p&gt;&lt;h2&gt;MySQL 的 Online Schema Change 方案&lt;/h2&gt;&lt;p&gt;目前有一些第三方工具支持在 MySQL 上面进行 Online Schema Change，比较主流的包括 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.percona.com/doc/percona-toolkit/LATEST/pt-online-schema-change.html&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pt-online-schema-change&lt;/a&gt; 和 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/github/gh-ost&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;gh-ost&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;这些工具的实现原理比较类似，本文会以 gh-ost 为例来进行分析讲解。&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bf6622c323dacdadcb395afdf12a1463_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;420&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-bf6622c323dacdadcb395afdf12a1463_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-bf6622c323dacdadcb395afdf12a1463_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;420&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-bf6622c323dacdadcb395afdf12a1463_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-bf6622c323dacdadcb395afdf12a1463_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;从上图可以大致了解到 gh-ost 的逻辑处理流程：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;在操作目标数据库上使用 &lt;code&gt;create table ghost table like origin table&lt;/code&gt; 来创建 ghost 表；&lt;/li&gt;&lt;li&gt;按照需求变更表结构，比如 &lt;code&gt;add column/index&lt;/code&gt;；&lt;/li&gt;&lt;li&gt;gh-ost 自身变为 MySQL replica slave，将原表的全量数据和 binlog 增量变更数据同步到 ghost 表；&lt;/li&gt;&lt;li&gt;数据同步完成之后执行 &lt;code&gt;rename origin table to table_del, table_gho to origin table&lt;/code&gt; 完成 ghost 表和原始表的切换&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;pt-online-schema-change 通过 trigger 的方式来实现数据同步，剩余流程类似。&lt;/p&gt;&lt;p&gt;在 DM 的 task 配置中可以通过设置 &lt;code&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/dm/config/task.go%23L244&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;online-ddl-scheme&lt;/a&gt;&lt;/code&gt; 来配置的 online schema change 方案，目前仅支持 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/dm/config/task.go%23L32&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;gh-ost/pt&lt;/a&gt; 两个配置选项。&lt;/p&gt;&lt;h2&gt;DM Online Schema Change 同步方案&lt;/h2&gt;&lt;p&gt;根据上个章节介绍的流程，pt 和 gh-ost 除了 replicate 数据的方式不一样之外，其他流程都类似，并且这种 native 的模式可以使得 binlog replication 几乎不需要修改就可以同步数据。但是 DM 为了减少同步的数据量，简化一些场景（如 shard tables merge）下的处理流程，并做了额外的优化，即，不同步 ghost 表的数据。&lt;/p&gt;&lt;p&gt;继续分析 online schema change 的流程，从数据同步的角度看有下面这些需要关注的点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;原始表的增量数据同步模式有没有变化&lt;/li&gt;&lt;li&gt;ghost 表会产生跟原始表几乎一样的冗余 binlog events&lt;/li&gt;&lt;li&gt;通过  &lt;code&gt;rename origin table to table_del, table_gho to origin table&lt;/code&gt; 完成 ghost 表和原始表的切换&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果使用 ghost 表的 &lt;code&gt;alter DDL&lt;/code&gt; 替换掉  &lt;code&gt;rename origin table to table_del, table_gho to origin table&lt;/code&gt; ，那么就可以实现我们的不同步 ghost 表数据的目的。&lt;/p&gt;&lt;h2&gt;DM Online Schema Change 同步实现细节&lt;/h2&gt;&lt;p&gt;Online schema change 模块代码实现如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ghost.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;gh-ost 同步代码实现&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/pt_osc.go&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;pt-online-schema-change 同步代码实现&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;DM 将 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/online_ddl.go%23L62&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;同步的表分为三类&lt;/a&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;real table - 原始表&lt;/li&gt;&lt;li&gt;trash table - online schema change 过程中产生的非关键数据表，比如以 &lt;code&gt;_ghc&lt;/code&gt;, &lt;code&gt;_del&lt;/code&gt; 为后缀的表&lt;/li&gt;&lt;li&gt;ghost table - 与原始表对应的经过 DDL 变更的数据表，比如以 &lt;code&gt;_gho&lt;/code&gt; 为后缀的表&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;当 DM 遇到 DDL 的时候，都会 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ddl.go%23L210&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;调用 online schema change 模块的代码进行处理&lt;/a&gt;，首先判断表的类型，接着针对不同类型作出不同的处理：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;real table - &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ghost.go%23L55&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;对 rename table statement 进行模式检查，直接返回执行&lt;/a&gt;&lt;/li&gt;&lt;li&gt;trash table - &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ghost.go%23L70&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;对 rename table statement 做一些模式检查，直接忽略同步&lt;/a&gt;&lt;/li&gt;&lt;li&gt;ghost table&lt;/li&gt;&lt;ul&gt;&lt;li&gt;如果 DDL 是 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ghost.go%23L86&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;create/drop table statement&lt;/a&gt; ，则 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ghost.go%23L87&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;清空内存中的残余信息后忽略这个 DDL 继续同步&lt;/a&gt;&lt;/li&gt;&lt;li&gt;如果 DDL 是 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ghost.go%23L96&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;rename table statement&lt;/a&gt; ，则 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ghost.go%23L103&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;返回内存中保存的 ghost table 的 DDLs&lt;/a&gt;&lt;/li&gt;&lt;li&gt;如果是其他类型 DDL，&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/dm/blob/25f95ee08d008fb6469f0b172e432270aaa6be52/syncer/ghost.go%23L119&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;则把这些 DDL 保存在内存中&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;p&gt;下面是一个执行示例，方便大家对照着来理解上面的代码逻辑：&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-28d5afa0816aadb045ecc52ce5f1fc7d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-28d5afa0816aadb045ecc52ce5f1fc7d_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-28d5afa0816aadb045ecc52ce5f1fc7d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-28d5afa0816aadb045ecc52ce5f1fc7d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-28d5afa0816aadb045ecc52ce5f1fc7d_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ol&gt;&lt;li&gt;Section 1： 使用 create table like statement 创建 ghost table，DM 会清空内存中 &lt;code&gt;online_ddl&lt;/code&gt;.&lt;code&gt;_t2_gho&lt;/code&gt; 对应的 DDL 信息&lt;/li&gt;&lt;li&gt;Section 2： 执行 alter table statement，DM 会保存 DDL 到内存中&lt;/li&gt;&lt;li&gt;Section 3：trash table 的 DDLs 会被忽略&lt;/li&gt;&lt;li&gt;Section 4：遇到 ghost table 的 rename table statement 会替换成 Section 2 的 DDL, 并且将该 DDL 的 table name 更换成对应 real table name 去执行&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;注意： rename table statement 模式检查主要是为了确保在 online schema change 变更过程中除了  &lt;code&gt;rename origin table to table_del, table_gho to origin table&lt;/code&gt; 之外没有其他 rename table statement，避免同步状态的复杂化。&lt;/p&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;本篇文章详细地介绍 DM 对 online schema change 方案的同步支持，内容包含 online schema change 方案的简单介绍， online schema change 同步方案，以及同步实现细节。下一章会对 DM 的 shard DDL merge 方案进行详细的讲解，敬请期待。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文阅读：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/dm-source-code-reading-8/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;DM 源码阅读系列文章（八）Online Schema Change 同步支持&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-06-19-69849157</guid>
<pubDate>Wed, 19 Jun 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB Binlog 源码阅读系列文章（一）序</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-06-18-69587196.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/69587196&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-c3da67191753fc9376f711e1c9dbec9a_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者：黄佳豪&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog&lt;/a&gt; 组件用于收集 TiDB 的 binlog，并准实时同步给下游，如 TiDB、MySQL 等。该组件在功能上类似于 MySQL 的主从复制，会收集各个 TiDB 实例产生的 binlog，并按事务提交的时间排序，全局有序的将数据同步至下游。利用 TiDB Binlog 可以实现数据准实时同步到其他数据库，以及 TiDB 数据准实时的备份与恢复。随着大家使用的广泛和深入，我们遇到了不少由于对 TiDB Binlog 原理不理解而错误使用的情况，也发现了一些 TiDB Binlog 支持并不完善的场景和可以改进的设计。&lt;/p&gt;&lt;p&gt;在这样的背景下，我们开展 TiDB Binlog 源码阅读分享活动，通过对 TiDB Binlog 代码的分析和设计原理的解读，帮助大家理解 TiDB Binlog 的实现原理，和大家进行更深入的交流，同时也有助于社区参与 TiDB Binlog 的设计、开发和测试。&lt;/p&gt;&lt;h2&gt;背景知识&lt;/h2&gt;&lt;p&gt;本系列文章会聚焦 TiDB Binlog 本身，读者需要有一些基本的知识，包括但不限于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Go 语言，TiDB Binlog 由 Go 语言实现，有一定的 Go 语言基础有助于快速理解代码。&lt;/li&gt;&lt;li&gt;数据库基础知识，包括 MySQL、TiDB 的功能、配置和使用等；了解基本的 DDL、DML 语句和事务的基本常识。&lt;/li&gt;&lt;li&gt;了解 Kafka 的基本原理。&lt;/li&gt;&lt;li&gt;基本的后端服务知识，比如后台服务进程管理、RPC 工作原理等。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总体而言，读者需要有一定 MySQL/TiDB/Kafka 的使用经验，以及可以读懂 Go 语言程序。在阅读 TiDB Binlog 源码之前，可以先从阅读 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/tidb-ecosystem-tools-1/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《TiDB Binlog 架构演进与实现原理》&lt;/a&gt; 入手。&lt;/p&gt;&lt;h2&gt;内容概要&lt;/h2&gt;&lt;p&gt;本篇作为《TiDB Binlog 源码阅读系列文章》的序篇，会简单的给大家讲一下后续会讲哪些部分以及逻辑顺序，方便大家对本系列文章有整体的了解。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;初识 TiDB Binlog 源码：整体介绍一下 TiDB Binlog 以及源码，包括 TiDB Binlog 主要有哪些组件与模块，以及如何在本地利用集成测试框架快速启动一个集群，方便大家体验 Binlog 同步功能与后续可能修改代码的测试。&lt;/li&gt;&lt;li&gt;pump client 介绍：介绍 pump client 同时让大家了解 TiDB 是如何生成 binlog 的。&lt;/li&gt;&lt;li&gt;pump server 介绍：介绍 pump 启动的主要流程，包括状态维护，定时触发 gc 与生成 fake binlog 驱动下游。&lt;/li&gt;&lt;li&gt;pump storage 模块：storage 是 pump 的主要模块，主要负载 binlog 的存储，读取与排序, 可能分多篇讲解。&lt;/li&gt;&lt;li&gt;drainer server 介绍：drainer 启动的主要流程，包括状态维护，如何获取全局 binlog 数据以及 Schema 信息。&lt;/li&gt;&lt;li&gt;drainer loader package 介绍：loader packge 是负责实时同步数据到 mysql 的模块，在 TiDB Binlog 里多处用到。&lt;/li&gt;&lt;li&gt;drainer sync 模块介绍：以同步 mysql 为例介绍 drainer 是如何同步到不同下游系统。&lt;/li&gt;&lt;li&gt;slave binlog 介绍：介绍 drainer 如何转换与输出 binlog 数据到 Kafka。&lt;/li&gt;&lt;li&gt;arbiter 介绍：同步 Kafka 中的数据到下游，通过了解 arbiter，大家可以了解如何同步数据到其他下游系统，比如更新 Cache，全文索引系统等。&lt;/li&gt;&lt;li&gt;reparo 介绍：通过了解 reparo，大家可以将 drainer 的增量备份文件恢复到 TiDB 中。&lt;/li&gt;&lt;/ol&gt;&lt;h2&gt;小结&lt;/h2&gt;&lt;p&gt;本篇文章主要介绍了 TiDB Binlog 源码阅读系列文章的目的和规划。下一篇文章我们会从 TiDB Binlog 的整体架构切入，然后分别讲解各个组件和关键设计点。更多的源码内容会在后续文章中逐步展开，敬请期待。&lt;/p&gt;&lt;p&gt;最后欢迎大家参与 &lt;a href=&quot;https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb-binlog&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog&lt;/a&gt; 的开发。&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-binlog-source-code-reading-1/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB Binlog 源码阅读系列文章（一）序&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-06-18-69587196</guid>
<pubDate>Tue, 18 Jun 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>六城一起 High！TiDB TechDay 2019 巡讲启动</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-06-13-69023375.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/69023375&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-fa41bd457e35dd58ddf95bfb059c3b14_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8586cd2684f54a840fa8d06bfa302c5c_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1212&quot; data-rawheight=&quot;800&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1212&quot; data-original=&quot;https://pic1.zhimg.com/v2-8586cd2684f54a840fa8d06bfa302c5c_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-8586cd2684f54a840fa8d06bfa302c5c_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1212&quot; data-rawheight=&quot;800&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1212&quot; data-original=&quot;https://pic1.zhimg.com/v2-8586cd2684f54a840fa8d06bfa302c5c_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-8586cd2684f54a840fa8d06bfa302c5c_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;TiDB 的设计灵魂，是让优雅灵活的架构充满无限可能性。从大规模业务场景中稳定使用的 TiDB 2.0 版本，到这一次备受关注的 3.0，我们持续地在倾听、修正、尝试，并获得一次又一次验证。距离年初公布 TiDB 3.0 beta 版本，已经过去了大半年，这期间我们对各方面进行了测试和优化，也看到有第一梯队用户在业务中体验了 3.0 的新特性。很多小伙伴好奇我们当时承诺的 TiDB 3.0 稳定性 / 易用性 / 高性能 / 新功能，都兑现得怎么样了？&lt;/p&gt;&lt;p&gt;今天正式剧透一波：TiDB 3.0 GA 版本将在本月底正式发布！借此机会，为了让更多的社区伙伴能够近距离与我们展开交流，并快速 Get 3.0 GA 的技术细节和正确使用姿势，&lt;b&gt;我们启动「TiDB TechDay 2019 全国巡讲」，打破「一年一城」的传统，巡回北京、上海、成都、深圳、武汉、杭州 6 座城市&lt;/b&gt;，用一整天的时间为当地朋友深入拆解 TiDB 3.0 以及展示今年技术层面的各个大招：从 TiDB 最新的 OLAP 架构，到云原生 TiDB demo、TiKV 性能大幅提升等等。各地用户伙伴也会一起交流分享 TiDB 实践经验，另外关于全球开源社区运营，我们又有了新的想法，也将与各地的社区伙伴们聊聊。当然 TechDay 2019 特别设计的 T-Shirt &amp;amp; 贴纸也会有的！&lt;/p&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-38fe0e5fb7c2864b9b0542a433724f00_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;333&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-38fe0e5fb7c2864b9b0542a433724f00_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-38fe0e5fb7c2864b9b0542a433724f00_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;333&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-38fe0e5fb7c2864b9b0542a433724f00_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-38fe0e5fb7c2864b9b0542a433724f00_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;相信在社区伙伴们的力量加持下，我们可以接着做更多做不到的事情。期待与大家见面～&lt;/p&gt;&lt;h2&gt;&lt;b&gt;北京站日程&lt;/b&gt;&lt;/h2&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f546abf33d0cab8b1a4bdfdec1a08f05_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;692&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f546abf33d0cab8b1a4bdfdec1a08f05_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f546abf33d0cab8b1a4bdfdec1a08f05_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;692&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f546abf33d0cab8b1a4bdfdec1a08f05_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f546abf33d0cab8b1a4bdfdec1a08f05_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;时间：2019-06-23 周日 10:00 - 16:50 &lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;地点：北京市-朝阳区-西大望路-地铁 14 号线平乐园站 B 口-灿空间&lt;/b&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;扫描下方二维码报名【北京站】&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;报名截止时间：6 月 22 日晚 18:00&lt;/li&gt;&lt;li&gt;请大家认真填写表单信息（T-Shirt 尺码别忘填啦）&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2ced3e37da357cef5f7b17faf0cf986e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;256&quot; data-rawheight=&quot;256&quot; class=&quot;content_image&quot; width=&quot;256&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2ced3e37da357cef5f7b17faf0cf986e_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;256&quot; data-rawheight=&quot;256&quot; class=&quot;content_image lazy&quot; width=&quot;256&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-2ced3e37da357cef5f7b17faf0cf986e_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;交通提示&lt;/li&gt;&lt;ul&gt;&lt;li&gt;公共交通：地铁 14 号线平乐园站 B 口出，步行 300 米即到。&lt;/li&gt;&lt;li&gt;驾车导航“地铁 14 号线平乐园站 B 口”，行驶西大望路至南磨房路口北 200 米，进入停车场（路东）。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;figure data-size=&quot;normal&quot;&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f67f151b82447ff53c8609cd5886a465_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;578&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f67f151b82447ff53c8609cd5886a465_r.jpg&quot;/&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f67f151b82447ff53c8609cd5886a465_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;578&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f67f151b82447ff53c8609cd5886a465_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f67f151b82447ff53c8609cd5886a465_b.jpg&quot;/&gt;&lt;/figure&gt;&lt;p&gt;* 最终日程信息以 TiDB TechDay 2019 官网显示为准&lt;/p&gt;&lt;p&gt;&lt;b&gt;点击进入 TiDB TechDay 2019 官网查询其余五站报名信息：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//pingcap.com/community-cn/techday2019/%3Futm_source%3Dwechat%26utm_medium%3Dpingcap%26utm_campaign%3Dtechday%2520190613&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TechDay 2019 | PingCAP&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-06-13-69023375</guid>
<pubDate>Thu, 13 Jun 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
