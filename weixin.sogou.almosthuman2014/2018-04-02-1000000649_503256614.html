<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>资源 | 学到了！UC Berkeley  CS 294深度强化学习课程（附视频与PPT）</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1523489533&amp;src=3&amp;ver=1&amp;signature=DceMtFD8-qHjoR-Cgo60qJcNfZG95jNMDq1gpBdkGcsW6lkvXzblweZqDbTKDbpFIgf19053F9z9xfPTsxClihu4HT8VKm3PaLLeySxTyisvIcHEssj*vxJp5-HwmuZEywsHdgJu40HMKtQQxoV53G86ejOAJ4ndldlBrkeCn0E=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    资源 | 学到了！UC Berkeley  CS 294深度强化学习课程（附视频与PPT）                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-02</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="font-size: 14px;text-align: justify;">UC Berkeley</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;font-size: 16px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心整理</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p><span style="font-size: 14px;text-align: justify;"><br></span></p><blockquote><p><span style="font-size: 14px;text-align: justify;color: rgb(136, 136, 136);">CS294 深度强化学习 2017 年秋季课程的所有资源已经放出。该课程为各位读者提供了强化学习的进阶资源，且广泛涉及深度强化学习的基本理论与前沿挑战。本文介绍了该课程主要讨论的强化学习主题，读者可根据兴趣爱好与背景知识选择不同部分的课程。请注意，UC Berkeley 的 CS 294 并未被归类为在线开放课程，所有视频的使用权仅限个人学习。</span><br></p></blockquote><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">课程主页：http://rll.berkeley.edu/deeprlcourse/</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">所有视频的链接：https://www.youtube.com/playlist?list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">知识背景</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本课程要求具有 CS 189 或同等学力。本课程将假定你已了解强化学习、数值优化和机器学习的相关背景知识。本课程所需的背景资料已在下表列出。在课程中，授课人会回顾这些资料的内容，但会非常简略。</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">强化学习和 MDP</span></p></li><ul class=" list-paddingleft-2" style="list-style-type: square;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">MDP 的定义</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">精确算法：策略与价值迭代</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">搜索算法</span></p></li></ul><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">数值优化</span></p></li><ul class=" list-paddingleft-2" style="list-style-type: square;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">梯度下降、随机梯度下降</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">反向传播算法</span></p></li></ul><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">机器学习</span></p></li><ul class=" list-paddingleft-2" style="list-style-type: square;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">分类和回归问题：使用哪些损失函数，如何拟合线性和非线性模型</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">训练/测试错误、过拟合</span></p></li></ul></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">有关强化学习与 MDP 的介绍资料：</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">CS188 EdX 课程，从马尔可夫决策过程 I 开始：http://ai.berkeley.edu/home.html</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Richard S. Sutton 与 Andrew G. Barto 的《强化学习导论》，第三章和第四章：http://incompleteideas.net/book/the-book-2nd.html</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">有关 MDP 的介绍，请参阅吴恩达的论文《Shaping and policy search in Reinforcement learning》：http://rll.berkeley.edu/deeprlcourse/docs/ng-thesis.pdf</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">David Silver 的课程：http://rll.berkeley.edu/deeprlcourse/#related-materials</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">有关机器学习和神经网络的介绍性资料，请参阅：</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Andrej Karpathy 的课程：http://cs231n.github.io/</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Geoff Hinton 的 Coursera 课程：https://www.coursera.org/learn/neural-networks</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">吴恩达的 Coursera 课程：https://www.coursera.org/learn/machine-learning/</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Yaser Abu-Mostafa 的课程：https://work.caltech.edu/telecourse.html</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">以下是 CS 294 深度强化学习 2017 年秋季课程的主要内容概要，所有的授课文档与视频都已经发布且展示在课程主页中。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">8 月 23 日：课程简介（Levine）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-copyright="0" data-ratio="0.5591286307053942" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskotsybHpne8SmsWMia47SszGEAcb47vpVAvytk4OzCJCxDR8Mx4Fx02w/640?wx_fmt=png" data-type="png" data-w="964" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">该课程第一节课主要是课程介绍和对强化学习基本概念的介绍。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">该课程教学大纲中包含以下内容：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 从监督学习到决策</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 基础强化学习：Q 学习和策略梯度</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3. 高级模型学习和预测、distillation、奖励学习</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">4. 高级深度强化学习：置信域策略梯度、actor-critic 方法、探索</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">5. 开放性问题、学术讲座、特邀报告</span></p><p><br></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">8 月 28 日：监督学习和模仿学习（Levine）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.570694087403599" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskWWa7ELvEloIYSRuvHgCx04k1iaCGXB2EtOnzm3ibksDqETYRp8yuVyNg/640?wx_fmt=png" data-type="png" data-w="1556" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍监督学习，主要内容包括：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 序列决策问题的定义</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 模仿学习：使用监督学习进行决策</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3.（深度）模仿学习近期研究案例分析</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">4. 模仿学习的缺点</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课目标：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解监督学习定义和符号；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解基础模仿学习算法；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解模仿学习算法的优劣势。</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">模仿学习：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.5095628415300546" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskSSxUEFn97TBawqSZToAIOqTDYpoAbR71YPeyHw0gzdTiciaa7VF4RjBA/640?wx_fmt=png" data-type="png" data-w="1464" style=""></p><p><br></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">8 月 30：强化学习简介（Levine）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5661375661375662" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskcC1icCxRRWyCWTffn9gkHicnibCbFw7Ig7JYHtvVPArEktnyCQbfyqz9A/640?wx_fmt=png" data-type="png" data-w="1512" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍强化学习，主要内容包括：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 马尔可夫决策过程的定义</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 强化学习问题的定义</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3. 强化学习算法解析</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">4. 简要介绍强化学习算法类型</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课目标：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解强化学习定义和符号；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解强化学习的目标；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽可能了解所有强化学习算法。</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">马尔可夫链定义：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5411764705882353" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskfvNe07ibcWR53G7OT3rQA9R0XWiclW2zKTCiaAvjomnJyAYNFhaDibKr2Q/640?wx_fmt=png" data-type="png" data-w="1530" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">马尔可夫决策过程定义：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5689655172413793" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskIuxzppvbAwzUuY5nsccMW1FnXm4BtXvpK2ia4fjLxJEz1EL7JSINNbg/640?wx_fmt=png" data-type="png" data-w="1508" style=""></p><p><img class="" data-ratio="0.5652741514360313" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskvHhXs73swZ5u3TiaibvIv7AqkIwZAiaeza0XyyibzvmRDY0qqDu6iaA6Frw/640?wx_fmt=png" data-type="png" data-w="1532" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">强化学习算法类型：</span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.549367088607595" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBsk86WGhTBicLqLvcSDsWvPK0bBcict4lvCsWYMseWaaAL22sXL07HibTcYg/640?wx_fmt=png" data-type="png" data-w="1580" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">9 月 6 日：策略梯度简介（Levine）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.6046511627906976" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBsk9JINwsJAkzdzHhrUrwaTzE65RAicf9L7zHNTibEgLYkYibuicK6dWvzYDQ/640?wx_fmt=png" data-type="png" data-w="1548" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍了策略梯度，主要内容包括：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 策略梯度算法</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 策略梯度的作用</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3. 基础方差缩减：因果性（causality）</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">4. 基础方差缩减：基线</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">5. 策略梯度示例</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课的目标：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解策略梯度强化学习；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解使用策略梯度时需要实际考虑的事情。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.5056179775280899" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskRlW40aySDTMum7GmsCBGiaiaAN4gibribibqfTDUibBd6Bj4smfERs7gJknA/640?wx_fmt=png" data-type="png" data-w="1602" style=""></p><p><img class="" data-ratio="0.4157014157014157" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskX4V7MYXwZ5TSvY2WZaGjWDn5AJteRKDHmNw5NKOoSV8YZOV8tZ2oQA/640?wx_fmt=png" data-type="png" data-w="1554" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.48186528497409326" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskIkKcXNflzNtOgfXoYTbJdg8BdO4k8bZ6ZfdAt21MIMLyADewiayKTSQ/640?wx_fmt=png" data-type="png" data-w="1544" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">9 月 8 日：神经网络概述（Achiam）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课全面介绍了神经网络，主要内容包括：自动微分、TensorFlow 基础知识、构建高级计算图、log 和 debug，以及计算图库、TensorFlow 的其他 API／封装器。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">9 月 11 日：actor-critic 算法简介（Levine）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5366492146596858" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskJWahtD21y7HxqFHBN6n6fQrdPkSyv4JPalGm1I033x8sicMLwqwic66w/640?wx_fmt=png" data-type="png" data-w="1528" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍了 actor-critic 算法，主要内容包括：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 改进具备 critic 的策略梯度</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 策略评估问题</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3. 折现因子</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">4. actor-critic 算法</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课目标：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解策略评估与策略梯度如何拟合；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解 actor-critic 算法的工作原理。</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">actor-critic 算法：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="1.095890410958904" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBsk073FYP3ovrgWPwc8p9CZkB1fHkzpksKfqBfLxTLtkmBlVlfFnvFYHA/640?wx_fmt=png" data-type="png" data-w="1168" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">架构设计：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5659050966608085" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBsklDuq2RI864hyfTicFIVTrk6Zibzodg1aaU1OicIzYraxbpQP0x6aQibSyA/640?wx_fmt=png" data-type="png" data-w="1138" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">9 月 13 日：价值函数介绍（Levine）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p><img class="" data-ratio="0.5119825708061002" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskycZRNHEEhWNNRGprJJfVDsldnA8N3pSO1K9O8K31tqIdlDnsEzibMVw/640?wx_fmt=png" data-type="png" data-w="918" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍价值函数的应用，包括从价值函数提取策略，如何用价值函数优化策略，Q-学习算法的介绍、实际应用和扩展等。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.4670138888888889" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskkjwBr0vaWMicjaG7kMm4nuj1ibxXYPY2T9ibia9N5kib7Sf1xTHBOicPG1gQ/640?wx_fmt=png" data-type="png" data-w="1152" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">其中强调了聚焦于价值函数而不是策略本身的重要性，这有助于简化问题；并介绍了 Q-学习的多种模式，如离线模式、在线模式等。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">9 月 18 日：高级 Q-学习算法（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍 Q-学习算法的扩展，包括如何与深度学习结合、广义的 Q-学习算法、Q-学习算法的实际应用以及连续性 Q 学习算法。重点是理解在复杂函数逼近中实现 Q-学习，以及如何将 Q-学习扩展到连续动作。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5925297113752123" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskvR7Tb4QTTdcc1FHnabbQfyQZ85kricLfJUJc31J9qyDl0ia2GraGbCIA/640?wx_fmt=png" data-type="png" data-w="1178" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">深度 Q-学习算法的典型定义。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5448275862068965" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskyP92dgnoAZpk90zmJyib97OX7ljtV10ygOwAywHLWghM2Btt8nEibAibA/640?wx_fmt=png" data-type="png" data-w="1160" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">广义的 Q-学习算法：数据收集—目标更新—Q-函数回归。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">9 月 20 日：最优控制和规划（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.46379310344827585" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBsk8TUO5OoAzErkkwibfbXoWyytzmibia5zxlXPzVTAic52ahP8QxFQepKiaYQ/640?wx_fmt=png" data-type="png" data-w="1160" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍了无模型和基于模型的强化学习的差别，以及在建模过程中对转换动力学的先验知识的重要性；然后介绍了多种优化方法，包括随机优化（连续型）、蒙特卡洛树搜索（离散型）和轨迹优化。重点是理解如何结合离散或连续空间的已知系统动力学知识来执行规划。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5797598627787307" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskAUyPqh65zMph2smMdFALVIDX0L0zLQNfXUMFHXS2S7DufJGiaZsvkmg/640?wx_fmt=png" data-type="png" data-w="1166" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">知道强化学习问题的动力学知识会通常来说使问题更加简单，围棋、汽车、机器人、视频游戏等的动力学知识都是比较容易获取的。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">9 月 25 日：从数据中学习动力学系统（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.38420107719928187" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskrLIEQY75DTVVfB5oszNvtSOESVPDJGx4AdLTXwDpNTzGXuZNhogVkA/640?wx_fmt=png" data-type="png" data-w="1114" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">上节课中介绍了当知道系统的动力学知识之后，如何对问题进行建模。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.57" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskwsJdTa5kzb5Dgdia81vOF6rjjBFYxvxfIQxTia9lu6KMZ5xbw6s0H7oA/640?wx_fmt=png" data-type="png" data-w="1200" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课将介绍当系统动力学知识未知时的解决方案，包括拟合全局动力学模型（基于模型的强化学习）以及拟合局域动力学模型。重点是理解基于模型强化学习的术语和形式，可选的模型类型，以及模型学习中的实际考虑。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">9 月 27 日：利用模仿优化控制器学习策略（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.4290657439446367" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskRv1q9IK96rYKaQyowfKO6Cl5Via9HNBQELxTq35E8SRB5ZFxBZG54Gw/640?wx_fmt=png" data-type="png" data-w="1156" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">上节课中介绍了当系统动力学知识未知时的解决方案，包括全局方法（基于模型的强化学习）以及局域方法（基于模型并添加约束）。但当需要系统生成策略的时候，该怎么办呢？生成策略可以更快地评估智能体的动作，并且泛化潜力更好。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5242214532871973" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskkfIDib0Qg9jBHhqDESicfhqgG5ajuRqScQZVbU0kJ4jib7Sv9lsqeQlSQ/640?wx_fmt=png" data-type="png" data-w="1156" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课将介绍如何利用反向传播算法来学习策略，它和模仿优化控制的关系，然后介绍了引导策略搜索算法，最后介绍了如何权衡基于模型和无模型强化学习的选择。本节课的重点在于理解用优化控制训练策略的过程，以及多种不同方法的权衡过程。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 2 日：高级强化学习和图像处理应用（客座演讲：Chelsea Finn）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><img class="" data-ratio="0.5797373358348968" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskpNuLPmcbQzQcqIgVMCicibQ8nIe2BFhZiaTVzWVaFgmKZWKKyswY1Jp3g/640?wx_fmt=png" data-type="png" data-w="1066" style="white-space: normal;"></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍多种高级的模型学习方法，并以图像应用为例分别展示了隐空间学习、图像空间学习、逆模型学习和预测替代数量。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><br></p><p><img class="" data-ratio="0.37122557726465366" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBsktITCrjfK2PudOf8Lqs8SASWibhvTErp94EztOicjic9ZibTT0ibDtEUAreA/640?wx_fmt=png" data-type="png" data-w="1126" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">其中强调了学习特征的重要性，以及在利用观测模型时，需要考虑奖励函数和目标函数的设置。</span></p><p><br></p><p><img class="" data-ratio="0.5525773195876289" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBsk1qqAjRj5d63eLUMo0tmHr2tAxF0unvnNfmwWwRYialJIOmHswB902Kw/640?wx_fmt=png" data-type="png" data-w="970" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">基于模型方法和无模型方法的优缺点对比。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 4 日：推断和控制之间的联系（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这一课程的主要目的是理解推断和控制之间的关系，以及理解具体的强化学习算法在框架下如何实例化。最优的控制其实可以作为拟合人类行为的模型，但如果数据达不到最优，那有如何拟合人类行为？我们还是可以将强化学习作为图模型中的推断而实现控制，其中价值函数为反向信息，且最大化奖励和信息熵以训练模型。其它方法还有 Soft Q-learning 和基于信息熵正则化的策略梯度等。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如下展示了一种制定决策或控制的概率图模型：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.563963963963964" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskNmAYgEsdmic2ZAC2NRzgicGUcd8ibo7KFXHHuYdhmIwicSwtzHzkCVMKdw/640?wx_fmt=png" data-type="png" data-w="1110" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 9 日：逆向强化学习（Levine）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5701438848920863" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskU7ylBT5fRvnBWkZbOqy6TibMFH83fXWicfbpyM0BmWy0Up6ibIbfS98GQ/640?wx_fmt=png" data-type="png" data-w="1112" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍逆向强化学习，主要内容包括：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 手动设计奖励函数来定义一个任务</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 当我们想从观察专家过程中学习奖励函数，然后使用强化学习时会发生什么？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3. 使用上节课的近似最优模型，学习奖励函数。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课目标：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解逆向强化学习的定义；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理解如何使用行为概率模型推导出逆向强化学习算法；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">了解我们实践中使用的逆向强化学习算法。</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">逆向强化学习：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.5132275132275133" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskQ1BYG2pwRwcMqsqDcRG6FcvkruzpEwFcvw8OsTIXY1ez94D4iaBYz0w/640?wx_fmt=png" data-type="png" data-w="1134" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">以下是这一章节的总结：</span></p><p><br></p><p><img class="" data-ratio="0.49040139616055844" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskJTsl55EHuxq9lmn9ibQ3PG7BduaLR08bjsozV2QDfxXKX1UvYc1yKBw/640?wx_fmt=png" data-type="png" data-w="1146" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 11 日：高级策略梯度（自然梯度、重要性采样）（Achiam）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.6697247706422018" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskqkIDbgf3K9LGDnmb0ibkoC0dn322ek1p0v87KmZKr7ibhbMklbqFQozQ/640?wx_fmt=png" data-type="png" data-w="654" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本节课介绍高级策略梯度方法，主要内容包括：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">理论：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">策略梯度方法的问题</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">策略性能边界</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">单调改进理论（Monotonic Improvement Theory）</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">算法：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">自然策略梯度</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">置信域策略优化</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">近端策略优化</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">自然策略梯度：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.6902654867256637" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskia121CWmtDN97dhW2KlbDsk00ZqPrcAZic9vnTIe5a8JPHjOBqVZy9EA/640?wx_fmt=png" data-type="png" data-w="1130" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">置信域策略优化：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.7240143369175627" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskn0tLSbPIRbWSicODItCKdibC8o5riaObLnb2yibhenP728zqQyE0icLCHag/640?wx_fmt=png" data-type="png" data-w="1116" style=""></p><p><img class="" data-ratio="0.6958855098389982" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskMHibbMBaUNdicSiaicjSqoIK1uYfj4C9R4u27fgvjClKQPMuAc626PEmkg/640?wx_fmt=png" data-type="png" data-w="1118" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 16 日：探索（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这一章节主要介绍了什么是探索（exploration），以及为什么它在强化学习中非常重要。一般来说探索分为基于乐观探索、基于后验匹配的探索和基于信息理论的探索。探索和利用（exploitation）的均衡在强化学习中非常重要，也是非常难以解决的问题。以下展示了探索与利用之间的基本区别：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.6088709677419355" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskUeu5JxROm7EhNakPQ3FSAXBbzfxzVUb1WItAXVUfanWkVueicvVaX8Q/640?wx_fmt=png" data-type="png" data-w="992" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">随后 Levine 详细展开介绍了为什么探索是非常困难的，包括摇臂赌博机问题等，而后重点介绍了乐观探索（Optimistic exploration）、概率匹配与后验采样，以及信息增益等探索方法。以下展示了一种探索算法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p><img class="" data-ratio="0.5673758865248227" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskAQKUKzHaO2mHicAXmSRQJ5ib66raw6VomTlEfBWPd2hjPo5DZ9bgEugA/640?wx_fmt=png" data-type="png" data-w="1128" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 18 日：探索（第二部分）和迁移学习（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这一章节首先复习了上堂课介绍的乐观探索、Thompson 采样风格的算法和信息增益风格的算法，然后介绍了这三类算法的近似论证。最后，讲师 Levine 还给出了一系列的延伸阅读以加强我们对探索的理解。</span></p><p><br></p><p><img class="" data-ratio="0.5876106194690266" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskUqJOXlnuicgSAOFNlp49uH4G4FV20l9N6IOnRYIaOUCEUBfDh6OZaMQ/640?wx_fmt=png" data-type="png" data-w="1130" style=""></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">该课程后一部分介绍了元学习与迁移学习，以下展示了迁移学习中的一种架构： 渐进神经网络。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p><img class="" data-ratio="0.5870736086175943" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskSfMdy2XQw5VULUicKaqmzvpPibQAwwnicXGM3tzdpqSO87e5Usr6L6STA/640?wx_fmt=png" data-type="png" data-w="1114" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 23 日：多任务学习与迁移（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">该课程主要介绍了多任务学习与迁移学习。说到如何解决迁移学习的问题，没有一个特定的解决方案，所以此课主要对近期（当时）的各种演讲论文进行了介绍。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5553539019963702" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskNEahXzYWxInq2yAianbuPFViagmAhFek6W4evg6FshDI8qSpcCxEG5dw/640?wx_fmt=png" data-type="png" data-w="1102" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1.「前向」迁移：在一个任务上训练，迁移到新任务</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">只是试试，希望有好结果</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">架构迁移：渐进网络</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在新任务上微调</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">随机化源任务域</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 多任务迁移：在多种任务上训练，迁移到一个新任务上</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p><span style="font-size: 14px;text-align: justify;">基于模型的强化学习</span><br></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">模型精炼</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">情境策略</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">模块化策略网络</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3. 多任务元学习：学习从多种任务上学习</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">基于 RNN 的元学习</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">基于梯度的元学习</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 25 日：元学习和并行化（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先元学习是一种学习如何学习的方法，它在实践中与多任务学习非常相近，一般而言元学习可以分为学习一种优化器、学习一个 RNN 以捕捉经验信息和学习一种表征。如果元学习是一种快速的强化学习器，那么我们就能快速学习新任务。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一般而言元学习可以通过监督学习或强化学习构建，它可以返回优秀的表征而加速学习也可以用来构建对经验的记忆等。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><img class="" data-ratio="0.5596491228070175" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskH8vbA24szCWGqOskkunV14GkSlia0EgXIib57EGFBQxYGicGmh1pWkBpg/640?wx_fmt=png" data-type="png" data-w="1140" style="white-space: normal;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">该章节的后一部分介绍了强化学习中的并行化，包括强化学习到底哪一部分需要并行，如何并行以及最终怎样实现等。以下展示了我们最终需要并行的部分。</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p><img class="" data-ratio="0.4665523156089194" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskySfibemGoKHJ1B53SF8pibHvIS6yaUwCr7An5NvFD1vj8xfQSlfoeEKw/640?wx_fmt=png" data-type="png" data-w="1166" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">10 月 30 日：进阶模仿学习和开放性问题（Levine）</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们知道模仿学习的目标是通过监督学习在给定观察下求得行动的概率分布，而强化学习是给定环境和状态下求得行动的概率分布。模仿学习要求预先的演示且必须解决分布迁移问题，它的优点在于可以通过简单稳定的监督学习实现。而强化学习需要奖励函数且必须解决模型的探索问题，它虽然可能会无法收敛，但却能够实现任意好的性能。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这一章节介绍了结合模仿学习的监督方式和强化学习的方法：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5026548672566372" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBskc6XxXwLoDtbMKCcV4BJticBicQ2OfgDmza2FBNS3UEm4VUulSuV6fJhg/640?wx_fmt=png" data-type="png" data-w="1130" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">后一部分介绍了深度强化学习的挑战，包括超参数调整、样本复杂度、泛化性能和 shenwuxu 生物学启示等。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-ratio="0.5711835334476844" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9fkCwPkjalbGEPQGcHqBsk6F6xvZSGiaFnwNnSnCcLbZNtiaTmZVH20aRNOuZTiaDVK8tsAaibJvclPA/640?wx_fmt=png" data-type="png" data-w="1166" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">该课程后面还有很多受邀嘉宾与它们所授的课程，包括 OpenAI 的 Igor Mordatch、谷歌的 Mohammad Norouz、伯克利和 OpenAI 的 Pieter Abbeel、伯克利的 Aviv Tamar 和 OpenAI 的 John Schulman。他们并没有提供对应的授课文档，但演讲视频在 YouTube 上都已经放出来了。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">相关学习材料</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">机器学习夏季课程中 John 的视频：</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">视频 1：https://www.youtube.com/watch?v=aUrX-rP_ss4</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">视频 2：https://www.youtube.com/watch?v=oPGVsoBonLM</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">视频 3：https://www.youtube.com/watch?v=rO7Dx8pSJQw</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">视频 4：https://www.youtube.com/watch?v=gb5Q2XL5c8A</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">课程：</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">David Silver 的强化学习课程：http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Nando de Freita 的机器学习课程：https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Andrej Karpathy 的神经网络课程：http://cs231n.github.io/</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">相关书籍：</span></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">深度学习：http://www.deeplearningbook.org/</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Sutton 和 Barto 合著的 Reinforcement Learning: An Introduction (http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html)：http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Szepesvari 的Algorithms for Reinforcement Learning：https://sites.ualberta.ca/~szepesva/RLBook.html</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Dynamic Programming and Optimal Control：http://www.athenasc.com/dpbook.html</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Markov Decision Processes: Discrete Stochastic Dynamic Programming：https://www.wiley.com/en-us/Markov+Decision+Processes%3A+Discrete+Stochastic+Dynamic+Programming-p-9780471727828</span></p></li><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">Approximate Dynamic Programming：http://adp.princeton.edu/<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" width="48px" style="font-size: 14px;text-align: justify;white-space: normal;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;width: 48px !important;visibility: visible !important;"></span></p></li></ul><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;"></span></em></span></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心整理，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="2090253345" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
