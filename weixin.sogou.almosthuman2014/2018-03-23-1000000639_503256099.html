<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>深度 | DeepMind提出神经元删除法：通过理解每个神经元来理解深度学习</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1522642230&amp;src=3&amp;ver=1&amp;signature=UFMi71EhKKQivqykvIjB2ksPFldzU3sgcajRinkrMNrYZFR*grW894fakrR4vkPk31gFgKkOdwBg1inum120OgNGxmYw35aRe-IOeW-8MAtrXUlW89rlPsR449o7qBHC1LsOJQvA9MSOm59f8FHooy43R*xaIS8W*JtZcxFLRAY=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    深度 | DeepMind提出神经元删除法：通过理解每个神经元来理解深度学习                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-23</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自DeepMind</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 14px;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></span></p><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">近日，DeepMind 发表博客介绍其对神经网络可解释性的最新研究成果。受神经科学启发，他们通过删除神经元来探索其对网络性能的影响。研究发现，和过去的经验直觉相反，选择性神经元（如「猫神经元」）对于网络的泛化能力并不重要。而某些行为难以理解的非选择性神经元却是不可或缺的。此外，作者还对比了泛化好和记忆好的网络对删除操作的响应行为。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">深度神经网络由很多独立的神经元组成，这些神经元以一种复杂而反直觉的方式结合，从而完成一系列的挑战性任务。这一复杂性保证了神经网络的效力，但也使其成为了一个令人困惑且不透明的黑箱。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">理解深度神经网络的工作原理对于解释其决策、构建更强大的系统来说至关重要。比如，想象一下，如果不了解每个齿轮之间的协作原理，那么制造一个钟表该有多么困难。探索独立神经元的作用，尤其是那些可以轻松解释的神经元，可以帮助我们理解神经科学和深度学习中的神经网络。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">我们的论文 </span><span style="font-size: 14px;">On the importance of single directions for generalization 将很快出现在 ICLR 2018 上，它使用一种受到数十年神经科学实验成果启发的方法来决定深度神经网络中小批神经元的重要性，以及更易解释的神经元对网络计算是否更重要，从而<span style="font-size: 14px;text-align: justify;">探索破坏带来的影响。</span></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">我们通过删除单个神经元和神经元集群来测量破坏网络造成的性能影响。实验得出了两个出人意料的结果:</span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽管许多早先的研究集中探讨容易解释的单个神经元 (如「猫神经元」或深度网络中只对猫的图像有反应的神经元)，但我们发现这些可解释的神经元并不比激活行为难以解释的困惑神经元更重要。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">与只能对以前看过的图像进行分类的网络相比，能对未看过的图像进行正确分类的网络在神经元删除时表现出了更强的适应性。换句话说，泛化良好的网络比记忆良好的网络对单方向的依赖要小得多。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">「猫神经元」也许更易解释，但并不重要</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">在神经科学和深度学习中，人们已经广泛分析了只对单一输入类别的图像（比如狗）作出积极回应的易于解释的神经元（「选择性」神经元）。在深度学习中，这导致了对猫神经元、情感神经元和括号神经元的重要性强调；在神经科学中则是 Jennifer Aniston 神经元，等等。然而，相比于具有低选择性、高度困惑性和难以解释的行为的绝大多数神经元，这些少量的高选择性神经元的相对重要性依然不得而知。</span></p><p><img class="" data-copyright="0" data-ratio="0.556" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibqDBHA2aqQk3TF23MTejgAbiaRqiaveicIWibeST7uXZq1n4PP9MmxLByJIAMw2y87XLia59J9O5ryLMg/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><p><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">相对于那些对图像集做出似乎随机性的积极和消极回应的令人困惑的神经元，带有清晰回应模式（比如只对狗积极回应，对其他一切消极回应）的神经元更易解释。<br></span></em></span></p><p><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">为了评估神经元的重要性，我们测量了当删除神经元时神经网络在图像分类任务上的表现是如何改变的。如果一个神经元非常重要，删除它的后果应该很严重，并导致网络性能锐减；而当删除一个不重要的神经元时则影响较小。神经科学家也惯常地执行类似的实验，尽管他们无法达到实验所需的细粒度精确度，但应用于人工神经网络则毫无难度。</span></p><p><img class="" data-copyright="0" data-ratio="0.8294930875576036" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibqDBHA2aqQk3TF23MTejgA8RDbRYADnaLST4iccpicul1RjDlMj4G7zib3Uyw1NYrWicqDGtemodKs2A/640?wx_fmt=png" data-type="png" data-w="1302" style=""></p><p><span style="font-size: 14px;">删除操作对简单神经网络的影响的概念图。颜色越深，表明神经元越活跃。尝试单击隐藏层神经元对它们进行删除，并查看输出神经元活跃度的变化（原网页）。请注意，仅删除一个或两个神经元对输出的影响很小，而删除大多数神经元则影响很大，并且某些神经元的重要度高于其他神经元！</span></p><p><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">出人意料的是，我们发现选择性和重要性之间没有什么关系。换句话说，「猫神经元」并不比困惑神经元更重要。这一发现与神经科学最近的研究成果相呼应，后者已经证明，困惑神经元实际上可以提供相当多的信息，除了那些最容易解释的神经元之外，我们还应研究其他神经元，只有这样才能理解深度神经网络。</span></p><p><br></p><p><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="0.8307254623044097" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibqDBHA2aqQk3TF23MTejgA0IhPw4ib1YdDHFwPKmbiaqffnqkvwuuTOuib9cCbv9DqshFTCibP0VSOjw/640?wx_fmt=png" data-type="png" data-w="1406" style=""></p><p><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;"></span></em></span><em style="color: rgb(136, 136, 136);"><span style="font-size: 12px;">虽然「猫神经元」可能更具可解释性，但它们相对于没有明显偏好的困惑神经元并没有更强的重要性。可以尝试点击上图（原网页）来查看重要性和可解释性的几种可能关系（正相关、负相关或不相关）。</span></em></p><p><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">尽管可解释的神经元在直觉上更易理解（「它喜欢狗」），但它们并不比没有明显偏好的令人困惑的神经元重要。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">泛化能力更好的网络更不容易崩坏</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">只有当系统能泛化到新的场景中时，该系统才能称得上是智能的。例如，一个图像分类网络仅能分类它见过的特定的狗的图像，而对于同一只狗的不同图像则无能为力，该网络就是无用的。近期一篇由 Google Brain、Berkeley 和 DeepMind 合作的论文《Understanding deep learning requires rethinking generalization》表明深度网络可以简单地记住训练过的每张图像，而不是像人类一样学习（例如，理解「狗」的抽象概念）。（参见：<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650727205&amp;idx=4&amp;sn=2f1fa918d951a198bf0c0d20572473a3&amp;chksm=871b275bb06cae4d3d226b1c68b3244876b6461b514a12a32af7092c564f987200ba8ff72ec7&amp;scene=21#wechat_redirect" target="_blank">解读 | ICLR-17 最佳论文：理解深度学习需要重新思考泛化问题</a>）</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">然而，关于神经网络学习到的解的泛化能力是由什么因素造成的，至今仍未得到清晰的解答。通过持续删除越来越大的神经元集群，我们发现泛化能力更好的网络对于删除操作更具鲁棒性（相对于仅在训练过程中记忆图像的网络而言）。换句话说，泛化能力更好的网络的性能更不容易崩坏（虽然仍可能遭遇崩坏）。</span></p><p><iframe class="video_iframe" data-vidtype="2" allowfullscreen="" frameborder="0" data-ratio="1.875" data-w="480" data-src="https://v.qq.com/iframe/preview.html?vid=m1335y4fat6&amp;width=500&amp;height=375&amp;auto=0"></iframe><br></p><p><span style="color: rgb(136, 136, 136);"><em><span style="font-size: 12px;">随着被删除的神经元集群越来越大，泛化能力好的网络的性能下降显著慢于在训练中记忆的网络。</span></em></span></p><p><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">通过这种方式来测量网络的鲁棒性，我们可以评估网络是否使用记忆能力在「作弊」。理解网络记忆过程中的变化，可以帮助我们建立泛化能力更好、更不依赖于记忆的网络。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">神经科学启发的分析方法</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">综上，这些发现表明使用实验神经科学启发的技术可以帮助我们理解人工神经网络。通过这些方法，我们发现高度选择性的独立神经元并不比非选择性的神经元更加重要，而泛化能力更好的网络相对于简单地记忆的网络，对独立神经元的依赖性更小。这些结果暗示我们，独立神经元的重要性可能小得多。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">通过解释所有神经元在任务中的角色，而不仅仅是那些更好解释的神经元，我们希望能更好地理解神经网络的内部工作原理，并通过这种理解构建更智能和更通用的系统。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 14px;">论文：ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(123, 12, 0);"></span></p><p><img class="" data-copyright="0" data-ratio="0.24661246612466126" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWibqDBHA2aqQk3TF23MTejgA78sn4pPP3tHqP0NgGRUpvI3u643G1WF2EvUia5NwcrEB2J6mPptXibCw/640?wx_fmt=png" data-type="png" data-w="738" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="color: rgb(123, 12, 0);font-size: 14px;">论文链接：https://arxiv.org/abs/1803.06959Despite</span><br><span style="font-size: 14px;color: rgb(123, 12, 0);"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">尽管有能力记忆大规模的数据集，深度神经网络通常也能获得良好的泛化性能。然而，关于神经网络学习到的解的泛化能力是由什么因素造成的，至今仍未得到清晰的解答。此外，人们曾强调过探索单个方向的微调属性（被定义为一个单元或多个单元的线性组合的激活值对一些输入的反应）的方法，但其重要性也未被评估过。在本文中，我们将这些探究方向连接起来，并证明网络对单个方向的依赖性可以很好地预测其泛化性能（通过让网络在不同比例的损坏标签的数据集上训练，让网络在未修改标签的数据集上训练并集成，进行不同的超参数试验以及多项训练试验）。dropout 仅能在一定程度上将这个量正则化，批量归一化却隐含地会减弱单一方向的依赖性，这部分是因为减少了独立单元的类别选择性。最后，我们发现类别选择性并不能很好地预测任务重要性。这不仅意味着网络可通过减少选择性来最小化对独立单元的依赖性，从而提高泛化能力；还表明独立地选择的单元对于强大的网络性能可能不是必须的。<img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);font-size: 16px;text-align: justify;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;width: 44px !important;visibility: visible !important;" width="44px"></span></p><p><span style="color: rgb(136, 136, 136);text-decoration: none;"><em><span style="text-decoration: none;color: rgb(136, 136, 136);font-size: 12px;">原文链接：</span></em></span><span style="color: rgb(136, 136, 136);"><em><span style="color: rgb(136, 136, 136);font-size: 12px;">https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/</span></em><em><span style="color: rgb(136, 136, 136);font-size: 12px;"></span></em><em><span style="color: rgb(136, 136, 136);font-size: 12px;"></span></em></span></p><p><br></p><p><br></p><p style="margin-bottom: 20px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="433742007" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
