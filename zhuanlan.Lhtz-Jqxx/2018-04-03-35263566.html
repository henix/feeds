<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>比较13种算法在165个数据集上的表现，你猜哪个最好？</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/35263566">原文</a></p>
<div class="title-image"><img src="https://pic3.zhimg.com/v2-3ccc7edf53b6d8026e3c2f61196993ba_r.jpg" alt=""></div><p>原文：</p><a href="https://mp.weixin.qq.com/s/VflRgUx563AVnMsEmf36FA" data-draft-node="block" data-draft-type="link-card" data-image="v2-93ed78ce63d4ac6a675d8313d790b2a9" data-image-width="888" data-image-height="500" data-image-size="180x120">比较13种算法在165个数据集上的表现，你猜哪个最好？</a><p>作者 |  Jason Brownlee </p><p>编译 | 公众号编辑部</p><p><br></p><p><b>你应该使用哪种机器学习算法？</b></p><p><br></p><p>这是应用机器学习中的一个让大家很捉急的问题。</p><p>在Randal Olson和其他人最近的一篇论文中，他们试图去回答它，并给出一个指导关于算法和参数。</p><p>在这篇文章中，你将展开一项研究和评估许多机器学习算法通过大量的机器学习数据集。并且得到对这项研究的一些意见。</p><p><br></p><h2><b>论文</b></h2><p>2017年，Randal Olson等人发表了一篇标题为<b><i>“Data-driven Advice for Applying Machine Learning to Bioinformatics Problems” </i></b>的论文。</p><p>论文下载地址：<i>https://arxiv.org/abs/1708.05070</i></p><p>他们的工作目标是解决每个从业人员在开始预测建模问题时所面临的问题，即：</p><p><br></p><p><b>我应该使用什么算法？</b></p><p><br></p><p>作者将此问题描述为choice overload，如下所示：</p><blockquote><i>Although having several readily-available ML algorithm implementations is advantageous to bioinformatics researchers seeking to move beyond simple statistics, many researchers experience “choice overload” and find difficulty in selecting the right ML algorithm for their problem at hand.</i></blockquote><p>他们通过在大量机器学习数据集的样本上运行其算法样本来解决这个问题，以了解通常哪些算法和参数最适合。</p><p>论文描述为：</p><blockquote><i>… a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers</i></blockquote><p><br></p><h2><b>机器学习算法</b></h2><p>A total of 13 different algorithms were chosen for the study.Algorithms were chosen to provide a mix of types or underlying assumptions.The goal was to represent the most common classes of algorithms used in the literature, as well as recent state-of-the-art algorithms The complete list of algorithms is provided below.</p><p>下面提供了完整的13种算法列表：</p><ul><li>Gaussian Naive Bayes (GNB)</li><li>Bernoulli Naive Bayes (BNB)</li><li>Multinomial Naive Bayes (MNB)</li><li>Logistic Regression (LR)</li><li>Stochastic Gradient Descent (SGD)</li><li>Passive Aggressive Classifier (PAC)</li><li>Support Vector Classifier (SVC)</li><li>K-Nearest Neighbor (KNN)</li><li>Decision Tree (DT)</li><li>Random Forest (RF)</li><li>Extra Trees Classifier (ERF)</li><li>AdaBoost (AB)</li><li>Gradient Tree Boosting (GTB)</li></ul><p>scikit-learn库被用来实现这些算法。</p><p>每个算法具有零个或多个参数，并且针对每个算法执行合理参数值的网格搜索。</p><p>对于每种算法，使用固定的网格搜索来调整超参数。</p><p>下面列出了算法和超参数评估表：</p><img src="https://pic2.zhimg.com/v2-8e5827d860c739e80e73521765186785_r.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="1606" data-watermark="" data-original-src="" data-watermark-src="" data-private-watermark-src=""><p>使用10倍交叉验证和平衡准确性度量来评估算法。</p><p>交叉验证没有重复，可能会在结果中引入一些统计噪音。</p><p><br></p><h2><b>机器学习数据集</b></h2><p>研究选择了165种标准机器学习问题。</p><p>许多问题来自生物信息学领域，尽管并非所有数据集都属于这一研究领域。</p><p>所有的预测问题都是两类或更多类的分类问题。</p><blockquote><i>The algorithms were compared on 165 supervised classification datasets from the Penn Machine Learning Benchmark (PMLB). […] PMLB is a collection of publicly available classification problems that have been standardized to the same format and collected in a central location with easy access via Python.</i></blockquote><p>数据集来自Penn机器学习基准（PMLB）集合，你可以在GitHub项目中了解关于此数据集的更多信息。地址：<i>https://github.com/EpistasisLab/penn-ml-benchmarks</i></p><p>在拟合模型之前，所有数据集均已标准化。</p><p><br></p><h2><b>结果分析</b></h2><blockquote><i>The entire experimental design consisted of over 5.5 million ML algorithm and parameter evaluations in total, resulting in a rich set of data that is analyzed from several viewpoints…</i></blockquote><p>对每个数据集对算法性能进行排名，然后计算每个算法的平均排名。</p><p>这提供了一个粗略和容易理解每一种算法在平均情况下好或不好活的方法。</p><p>结果表明，<b>梯度提升</b>（Gradient boosting）和<b>随机森林</b>（random forest ）的排名最低（<b>表现最好</b>），<b>朴素贝叶斯</b>（Naive Bayes）平均得分最高（<b>表现最差</b>）。</p><blockquote><i>The post-hoc test underlines the impressive performance of Gradient Tree Boosting, which significantly outperforms every algorithm except Random Forest at the p &lt; 0.01 level.</i></blockquote><p>通过这张图，展示了所有算法的结果，摘自论文。</p><img src="https://pic1.zhimg.com/v2-fd89f383b60e8e51dc96657237ce775d_r.jpg" data-caption="" data-size="normal" data-rawwidth="1024" data-rawheight="481" data-watermark="" data-original-src="" data-watermark-src="" data-private-watermark-src=""><p>没有单一的算法表现最好或最差。</p><p>这是机器学习实践者所熟知的，但对于该领域的初学者来说很难掌握。</p><p>你必须在一个给定的数据集上测试一套算法，看看什么效果最好。</p><blockquote><i>… it is worth noting that no one ML algorithm performs best across all 165 datasets. For example, there are 9 datasets for which Multinomial NB performs as well as or better than Gradient Tree Boosting, despite being the overall worst- and best-ranked algorithms, respectively. Therefore, it is still important to consider different ML algorithms when applying ML to new datasets.</i></blockquote><p>此外，选择正确的算法是不够的。你还必须为数据集选择正确的算法配置。</p><blockquote><i>选择正确的ML算法并调整其参数对于大多数问题是至关重要的。</i></blockquote><p>结果发现，根据算法和数据集的不同，调整算法可将该方法的性能从提高至3%——50％。</p><blockquote><i>The results demonstrate why it is unwise to use default ML algorithm hyperparameters: tuning often improves an algorithm’s accuracy by 3-5%, depending on the algorithm. In some cases, parameter tuning led to CV accuracy improvements of 50%.</i></blockquote><p>本图表展示了参数调整对每种算法的改进情况。</p><img src="https://pic4.zhimg.com/v2-c7d8e5b6e06560538d230ca1f7bf6497_r.jpg" data-caption="" data-size="normal" data-rawwidth="1078" data-rawheight="1086" data-watermark="" data-original-src="" data-watermark-src="" data-private-watermark-src=""><p>并非所有算法都是必需的。</p><p>结果发现，在165个测试数据集中的106个中，五种算法和特定参数的性能达到Top1％。</p><p>推荐这五种算法:</p><ul><li>Gradient Boosting</li><li>Random Forest</li><li>Support Vector Classifier</li><li>Extra Trees</li><li>Logistic Regression</li></ul><blockquote><i>The paper provides a table of these algorithms, including the recommend parameter settings and the number of datasets covered, e.g. where the algorithm and configuration achieved top 1% performance.</i></blockquote><img src="https://pic3.zhimg.com/v2-a70cb366b0e862f1f3ba24b8251a93e2_r.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="811" data-watermark="" data-original-src="" data-watermark-src="" data-private-watermark-src=""><p><br></p><h2><b>实际结果</b></h2><p>本文有两个重要的发现对于从业者是有价值的，尤其是对那些刚开始学习机器学习算法或者对此有困惑的人。</p><p>1、使用Ensemble Trees</p><p>The analysis demonstrates the strength of state-of-the-art, tree-based ensemble algorithms, while also showing the problem-dependent nature of ML algorithm performance.</p><p>2、Spot Check and Tune</p><p>没有人可以看到你的问题，并告诉你使用什么算法。</p><p>你必须为每种算法测试一套参数，以查看哪些方法更适合你的特定问题。</p><blockquote><i>In addition, the analysis shows that selecting the right ML algorithm and thoroughly tuning its parameters can lead to a significant improvement in predictive accuracy on most problems, and is there a critical step in every ML application.</i></blockquote><p>这个问题的讨论请查看这个链接：<i>https://machinelearningmastery.com/a-data-driven-approach-to-machine-learning/</i></p><p><br></p><h2><b>进一步阅读</b></h2><p>如果你希望深入了解类似问题，这里提供了有关该主题的更多资源：</p><ul><li>Data-driven Advice for Applying Machine Learning to Bioinformatics Problems（<i>https://arxiv.org/abs/1708.05070</i>）</li><li>scikit-learn benchmarks on GitHub（<i>https://github.com/rhiever/sklearn-benchmarks</i>）</li><li>Penn Machine Learning Benchmarks（<i>https://github.com/EpistasisLab/penn-ml-benchmarks</i>）</li><li>Quantitative comparison of scikit-learn’s predictive models on a large number of machine learning datasets: A good start（<i>https://crossinvalidation.com/2017/08/22/quantitative-comparison-of-scikit-learns-predictive-models-on-a-large-number-of-machine-learning-datasets-a-good-start/</i>）</li><li>Use Random Forest: Testing 179 Classifiers on 121 Datasets（<i>https://machinelearningmastery.com/use-random-forest-testing-179-classifiers-121-datasets/</i>）</li></ul><p>原文：<i>https://machinelearningmastery.com/start-with-gradient-boosting/</i></p><b><img src="https://pic1.zhimg.com/v2-4cf3f189e3fb128dc0be0314b7e07019_r.jpg" data-caption="" data-size="normal" data-rawwidth="1080" data-rawheight="367" data-watermark="" data-original-src="" data-watermark-src="" data-private-watermark-src=""></b><p></p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
