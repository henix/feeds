<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>学界 | CVPR 2018接收论文公布，上海交通大学6篇论文简介</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1520536826&amp;src=3&amp;ver=1&amp;signature=S8RBhf4P0jwxTWqb60cgUIQhKwVqoUGevRxN2rzgrX9S5I5vou5v-2aAYmaOFS9mSSXvXVOarJ4yeh7PqYOqgGRX*FS8BZHHGlEC5OcCpMbXd36OSe2wCb8kt2Wah1zfdWDfAgjLMBT2yKiBuLpHMBNri8y5bszVD3Mv9OttzBI=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    学界 | CVPR 2018接收论文公布，上海交通大学6篇论文简介                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-02-27</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="white-space: normal;max-width: 100%;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="text-align: center;margin-top: -1.2em;color: rgb(62, 62, 62);max-width: 100%;min-height: 1em;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;text-align: justify;">机器之心报道</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="text-align: center;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(127, 127, 127);"><span style="font-size: 12px;"><strong>作者：吴欣</strong></span></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;">不久之前，CVPR 2018 论文接收列表公布。据机器之心了解，上海交通大学电子系人工智能实验室倪冰冰教授课题组有 6 篇论文入选，本文对这几篇论文做了简介，更多详细内容可通过论文网盘链接下载查看。</span></span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">CVPR 2018 论文接收列表：http://cvpr2018.thecvf.com/files/cvpr_2018_final_accept_list.txt</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">Paper 1：《Fine-grained Video Captioning for Sports Narrative》</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">细粒度视频描述——体育视频自动解说</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">网盘链接：https://pan.baidu.com/s/1miUzoCC</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">视频描述方向的研究在近段时间取得了较大的进展，但是一直都停留在粗略的视频内容讲述上，没有做到对于人物交互关系和动作细节的细致描述，而这些恰恰都是体育视频中非常重要的部分。在这篇 CVPR 工作中，作者提出了一个全新的细粒度视频描述课题，做出了一个对应的体育视频细粒度描述数据集，并用一个完善的视频描述网络解决了该问题，实现了国际首次人工智能体育视频自动解说。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.8360655737704918" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8OBanjzMZ60w1mOiccbQM15Np6v2Rz9MibNb9yrjm0hncOoKPuOtPo8boxxLToWxuUI8LxrmVF5soA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">体育视频自动解说效果展示。所有球员的角色、关系和位置，及球的位置都实现了准确的理解识别。相比较传统的「一群人在打篮球」的粗略描述，这篇文章实现的描述更加细致、准确，可以全面真实地反映运动场上的实际情况。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在文章中，作者通过一个时域-空域定位子网络来进行动作片段的分割和球员角色的定位；通过一个引入骨骼信息的细粒度动作识别子网络来精确地识别球员在高速运动中做出的细小动作；再通过一个群体交互子网络来构建球员间的交互关系。通过以上三个子网络捕获充足全面的视频特征，从而输出准确的细粒度视频描述语句。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">此外，文章中提出的细粒度体育视频自动解说数据集（FSN dataset）也将会在不久后公开供科研使用，以促进细粒度视频描述领域的技术发展。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">Paper 2：《Structure Preserving Video Prediction》</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">面向保持结构一致性的视频预测模型</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">网盘链接：https://pan.baidu.com/s/1kWUb4c3</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">像素级的视频生成一直是计算机视觉领域的热点问题。过去的方法一直试图解决所预测的视频中存在的运动模糊问题。这个问题一方面由随时间增加所带来的累计误差引起，另一方面由于像素级的视频生成的解空间非常巨大。这里将像素级的视频生成一直存在的两个问题总结如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">静态结构预测损失 这个问题来源于预测具有固定结构的场景，城市景观中的交通标志、树木等任务。这些静态结构的运动常常是由相机运动引起的。现有方法的预测结构大多不能保持原始对象结构，例如物体的边缘结构信息。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">动态结构预测损失 虽然最近的一些工作可以预测一般粗粒度运动。但是一般不能精确预测精细的局部运动，如人体关节运动。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.3707440100882724" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8OBanjzMZ60w1mOiccbQM15094v02K29lKWib0osL9NPbXGf6I7F4nZazNsq1Sf14Pv0PxrTVyaf8A/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">论文提出的视频预测模型图示。该架构使用论文提出的多频带分析和时变卷积核技术，能够更大限度的利用输入信息和更灵活的应对视频内容的动态变化，使得更精确地预测像素级的视频内容成为可能。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在这篇文章中，作者提出基于多频带分析和时变卷积核的视频预测模型来解决上述两个问题。一方面作者将输入视频分解为多个频率分量分别进行处理，以求从原始视频中获取尽可能多的物体静态结构信息，称之为多频带分析；另一方面作者利用输入视频帧来生成最终预测模型中的卷积核，以求能够更灵活的应对动态结构预测任务，称之为时变卷集核。两个方法分别较好地解决了上述两个问题，显著提高了视频预测精度。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">Paper 3：《Multiple Granularity Group Interaction Prediction》</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">基于多粒度的群体交互预测</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">网盘链接：https://pan.baidu.com/s/1i6HovGh</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);"></span></p><p><img class="" data-copyright="0" data-ratio="0.40226986128625475" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8OBanjzMZ60w1mOiccbQM15d5ibarujPxEibjMtkZx6YMzdBg8HYtOPNfmRc1sOTVGnibVOHOSxzAMcQ/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">多粒度群体交互预测框架。首先我们把骨架序列处理成两个不同粒度的信息，分别代表整体轨迹运动和肢体细节运动。基于 seq2seq 预测结构，我们设计了同粒度间信息交互子网络来考虑群体之间的相互影响，以及不同粒度间的信息交互子网络来促进两个粒度上信息的交互，更准确地预测结果。最后整合预测出的两个粒度信息，展示群体交互预测结果。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">当前大多数人体活动分析（识别或预测）的工作仅关注于单个粒度，例如在粗粒度层次对整体运动进行建模（轨迹预测）；或者在细粒度层次对肢体细节运动进行建模（骨骼动作预测）。相反，在这项工作中，作者提出了多粒度群体交互预测网络，能够同时考虑两个粒度上的信息（整体轨迹和细节动作）。首先对于每个人的骨架运动序列，作者把它处理成能够分别代表轨迹运动和肢体运动的两个粒度上的运动序列。对于每个粒度上的信息，基于 seq2seq 网络结构，作者设计了同粒度间的交互网络，在预测每个人这个粒度上的运动信息时能够考虑其他人的运动信息。同时，基于双向 LSTM，作者设计了不同粒度间的信息交互网络，来促进每个人的不同粒度上信息的交互，更准确地预测未来轨迹和细节动作。最后作者把两个粒度上的信息整合在一起，多景观式地展示预测的群体交互。此方法在 SBU 和 Choi's New Dataset 数据集上都取得了目前最佳效果。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">Paper 4：《pose transferrable person re-identification》</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">姿态可迁移行人再识别</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">网盘链接：https://pan.baidu.com/s/1nwFetDZ</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">行人再识别旨在解决跨时空匹配行人的问题，其在智能安防领域有极大的应用价值。由于行人姿态、外观、光照、遮挡等因素的影响，行人再识别仍然是一项极具挑战性的任务。为了解决行人姿态变化丰富导致模型难以在有限训练集下获得良好性能的问题，作者在这篇 CVPR 的工作中提出了一个姿态可迁移的行人再识别模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.6305170239596469" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8OBanjzMZ60w1mOiccbQM154vibjHx2H3cfuQVkkg2097fOkgf1AF28Cic33JvhVicSmmKgt2e3V8VPA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">姿态可迁移行人再识别框架。首先训练姿态迁移模块，其中我们引入了「向导」子模块来提升生成器的性能。然后使用训练好的生成器实现行人姿态由源数据集到目标数据集的迁移，进而提升行人再识别在目标数据集上的性能。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">假设给定大小两个数据集，分别称为 A 和 B。其中数据集 A 中覆盖的行人姿态少，而 B 中的样本包含丰富的姿态。作者提出将数据集 A 中的图片样本与数据集 B 中样本的骨架进行配对，并通过 Skeleton-to-Image 模型生成与 A 中样本共享身份信息且与 B 中样本共享姿态的新样本集 C。为了提高生成样本的质量，作者提出了一个与对抗生成网络中的判别器平行的向导模块。向导模块是一个预训练好的行人再识别模型，用于指导生成器生成包含更丰富身份信息、更适应行人再识别任务的样本。在得到了生成样本集 C 后，作者将其与数据集 A 混合并通过平滑机制分配样本权重后训练模型。实验结果表明此方法能够与其他高性能方法 (包括特征学习、度量学习类方法) 结合并进一步提升它们的性能。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">Paper 5：《Crowd Counting via Adversarial Cross-Scale Consistency Pursuit》</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">基于对抗跨尺度一致性追求的人群计数方法</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">网盘链接：https://pan.baidu.com/s/1mjPpKqG</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">作者提出了一个新的人群计数的网络结构 Adversarial Cross-Scale Consistency Pursuit Network，在四个公开的人群计数数据集上刷新了目前国际最佳的计数精度。人群计数任务是一个极具挑战的任务，原因在于其存在场景变化跨度大、目标空间尺度变化大、人群之间存在严重遮挡等困难。现有的方法存在以下两个缺陷：一、由不同大小的卷积子构成的多通道卷积网络融合得到的图像多尺度特征，再经传统的欧式损失（L1/L2）回归用来计数的人群密度图会导致密度图模糊，同时由于在网络中使用池化层，大大降低了密度图的分辨率，给最终的计数带来误差；二、输入一张图像计算得出的人数与将此图像分割成 4 份分别输入得到的总人数存在差异，此即为跨尺度统计不一致带来的误差。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5838587641866331" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8OBanjzMZ60w1mOiccbQM15R13Ykfw9g1ah0pcynPYibHlVU31BOMlAa4SHBA5tFEbyAWBLZFqn3QQ/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">基于对抗跨尺度一致性追求网络的结构图，两种尺度的生成器 G/判别器 D 利用跨尺度一致性损失实现联合训练。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">针对以上两点，作者提出了基于生成对抗网络的跨尺度结构模型，其中对抗损失的引入使得生成的密度图更加尖锐，U-net 结构的生成网络保证了密度图的高分辨率，同时跨尺度一致性正则子约束了图像间的跨尺度误差。因此，该提出的模型最终能生成质量好分辨率高的人群密度图，从而获得更高的人群计数精度。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">Paper 6：《Scale-Transferrable Object Detection》</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">基于尺度变换模块的物体检测器</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">网盘链接：https://pan.baidu.com/s/1i6Yjvpz</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);"></span></p><p><img class="" data-copyright="0" data-ratio="0.4968474148802018" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8OBanjzMZ60w1mOiccbQM15WKCphF5KePicGlsFGBqyK22EYrMe3JGFVJ9SaibXLHiaO7jw6QQsvkN2w/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">基于尺度转移模块的物体检测。尺度转移模块由简单的 mean pooling 层和尺度转移层构成，被嵌入到 DenseNet 网络的最后一个模块中，从而得到不同分辨率的被用来做物体检测的特征图。尺度转移层可以有效地减少输入特征图的通道数，同时扩大其分辨率，不增加额外的计算开销。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">作者构建了一个类似于 SSD 的一阶段物体检测器，称之为 STDN（尺度转移检测网络）。与 SSD 物体检测方法相比主要有两点不同，一是基础网络使用的是 DenseNet，二是作者使用了一个尺度转移模块来获得不同分辨率的特征图，这些特征图被用来做物体检测。这个尺度转移模块由 mean pooling 层和像素重排层构成。Mean pooling 层用来获得低分辨率的特征图；像素重排层通过压缩特征图的通道数来扩大特征图的分辨率，没有额外的计算开销。尺度转移模块可以直接嵌入到 DenseNet 网络中，不需要在 DenseNet 网络之后添加额外的层就能获得多尺度的特征图。而且像素重排层可以有效地压缩 DenseNet 网络特征图的通道数，从而减少之后卷积层的参数数量。作者在 pascal voc07 和 coco 数据集上取得了不错的检测性能，对构建开销较小的物体检测器具有一定的启发意义。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">注：上海交通大学电子系人工智能实验室由倪冰冰教授、徐奕教授领衔，杨小康教授、张文军教授指导 </span><img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWib0JZxz5ovG7gL67yGQmoN7cKRk8EfHpuibXtdTP5lajgBIicEgEnWYzibP1BnWtKCOmZEibzX09iaSXrw/640?wx_fmt=png" data-type="png" data-w="73" width="50px" style="color: rgb(62, 62, 62);font-size: 16px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;width: 50px !important;visibility: visible !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心报道，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="1788211803" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
