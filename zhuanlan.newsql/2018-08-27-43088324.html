<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>TiDB 源码阅读系列文章（十七）DDL 源码解析</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/43088324">原文</a></p>
<div class="title-image"><img src="https://pic4.zhimg.com/v2-eef68c3b98ddf29ee7a0311e6259636c_r.jpg" alt=""></div><blockquote>作者：陈霜</blockquote><p><br></p><p>DDL 是数据库非常核心的组件，其正确性和稳定性是整个 SQL 引擎的基石，在分布式数据库中，如何在保证数据一致性的前提下实现无锁的 DDL 操作是一件有挑战的事情。</p><p>本文首先会介绍 TiDB DDL 组件的总体设计，以及如何在分布式场景下支持无锁 shema 变更，并描述这套算法的大致流程，然后详细介绍一些常见的 DDL 语句的源码实现，包括 <code class="inline">create table</code>、<code class="inline">add index</code>、<code class="inline">drop column</code>、<code class="inline">drop table</code> 这四种。</p><h2><b>DDL in TiDB</b></h2><p>TiDB 的 DDL 通过实现 Google F1 的在线异步 schema 变更算法，来完成在分布式场景下的无锁，在线 schema 变更。为了简化设计，TiDB 在同一时刻，只允许一个节点执行 DDL 操作。用户可以把多个 DDL 请求发给任何 TiDB 节点，但是所有的 DDL 请求在 TiDB 内部是由 <b>owner</b> 节点的 <b>worker </b>串行执行的。</p><ul><li>worker：每个节点都有一个 worker 用来处理 DDL 操作。</li><li>owner：整个集群中只有一个节点能当选 owner，每个节点都可能当选这个角色。当选 owner 后的节点 worker 才有处理 DDL 操作的权利。owner 节点的产生是用 Etcd 的选举功能从多个 TiDB 节点选举出 owner 节点。owner 是有任期的，owner 会主动维护自己的任期，即续约。当 owner 节点宕机后，其他节点可以通过 Etcd 感知到并且选举出新的 owner。</li></ul><p>这里只是简单概述了 TiDB 的 DDL 设计，下两篇文章详细介绍了 TiDB DDL 的设计实现以及优化，推荐阅读：</p><ul><li><a href="https://github.com/ngaut/builddatabase/blob/master/f1/schema-change-implement.md">TiDB 的异步 schema 变更实现  </a></li><li><a href="http://zimulala.github.io/2017/12/24/optimize/">TiDB 的异步 schema 变更优化</a></li></ul><p>下图描述了一个 DDL 请求在 TiDB 中的简单处理流程：</p><img src="https://pic3.zhimg.com/v2-b50b7dac9057fcc661c11624a87f5eb7_r.jpg" data-caption="图 1：TiDB 中 DDL SQL 的处理流程" data-size="normal" data-rawwidth="1088" data-rawheight="730" data-watermark="watermark" data-original-src="v2-b50b7dac9057fcc661c11624a87f5eb7" data-watermark-src="v2-ec295f4a6baf2e4d815cefcca89fcacb" data-private-watermark-src=""><p>TiDB 的 DDL 组件相关代码存放在源码目录的 <code class="inline">ddl</code> 目录下。</p><img src="https://pic2.zhimg.com/v2-830cd4ddaaaf3919c05a60248d81cc42_r.jpg" data-caption="" data-size="normal" data-rawwidth="635" data-rawheight="276" data-watermark="watermark" data-original-src="v2-830cd4ddaaaf3919c05a60248d81cc42" data-watermark-src="v2-de4c148aff1c3ed3f886bcce72f5ebc5" data-private-watermark-src=""><p><code class="inline">ddl owner</code> 相关的代码单独放在 <code class="inline">owner</code> 目录下，实现了 owner 选举等功能。</p><p>另外，<code class="inline">ddl job queue</code> 和 <code class="inline">history ddl job queue</code> 这两个队列都是持久化到 TiKV 中的。<code class="inline">structure</code> 目录下有 list，<code class="inline">hash</code> 等数据结构在 TiKV 上的实现。</p><p><b>本文接下来按照 TiDB 源码的</b> <b><a href="https://github.com/pingcap/tidb/tree/source-code">origin/source-code</a></b> <b>分支讲解，最新的 master 分支和 source-code 分支代码会稍有一些差异。</b></p><h2><b>Create table</b></h2><p><code class="inline">create table</code> 需要把 table 的元信息（<a href="https://github.com/pingcap/tidb/blob/source-code/model/model.go#L95">TableInfo</a>）从 SQL 中解析出来，做一些检查，然后把 table 的元信息持久化保存到 TiKV 中。具体流程如下：</p><ol><li>语法解析：<a href="https://github.com/pingcap/tidb/blob/source-code/session.go#L790">ParseSQL</a> 解析成抽象语法树 <a href="https://github.com/pingcap/tidb/blob/source-code/ast/ddl.go#L393">CreateTableStmt</a>。</li><li>编译生成 Plan：<a href="https://github.com/pingcap/tidb/blob/source-code/session.go#L805">Compile</a> 生成 DDL plan , 并 check 权限等。</li><li>生成执行器：<a href="https://github.com/pingcap/tidb/blob/source-code/executor/adapter.go#L227">buildExecutor</a> 生成 <a href="https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L33"> DDLExec</a> 执行器。TiDB 的执行器是火山模型。</li><li>执行器调用 <a href="https://github.com/pingcap/tidb/blob/source-code/executor/adapter.go#L300">e.Next</a> 开始执行，即 <a href="https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L42">DDLExec.Next</a> 方法，判断 DDL 类型后执行 <a href="https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L68">executeCreateTable</a> , 其实质是调用 <code class="inline">ddl_api.go</code> 的 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L739">CreateTable</a> 函数。</li><li><a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L739">CreateTable</a> 方法是主要流程如下：</li></ol><ul><li>会先 check 一些限制，比如 table name 是否已经存在，table 名是否太长，是否有重复定义的列等等限制。</li><li><a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L775">buildTableInfo</a> 获取 global table ID，生成 <code class="inline">tableInfo</code> , 即 table 的元信息，然后封装成一个 DDL job，这个 job 包含了 <code class="inline">table ID</code> 和 <code class="inline">tableInfo</code>，并将这个 job 的 type 标记为 <code class="inline">ActionCreateTable</code>。</li><li><a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L793">d.doDDLJob(ctx, job)</a> 函数中的 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L423">d.addDDLJob(ctx, job)</a> 会先给 job 获取一个 global job ID 然后放到 job queue 中去。</li><li>DDL 组件启动后，在 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L318">start</a> 函数中会启动一个 <code class="inline">ddl_worker</code> 协程运行 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L37">onDDLWorker</a> 函数（最新 Master 分支函数名已重命名为 start），每隔一段时间调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L193">handleDDLJobQueu</a> 函数去尝试处理 DDL job 队列里的 job，<code class="inline">ddl_worker</code> 会先 check 自己是不是 owner，如果不是 owner，就什么也不做，然后返回；如果是 owner，就调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L212">getFirstDDLJob</a> 函数获取 DDL 队列中的第一个 job，然后调 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L236">runDDLJob</a> 函数执行 job。</li><ul><li><a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L275">runDDLJob</a> 函数里面会根据 job 的类型，然后调用对应的执行函数，对于 <code class="inline">create table</code> 类型的 job，会调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L31">onCreateTable</a> 函数，然后做一些 check 后，会调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L56">t.CreateTable</a> 函数，将 <code class="inline">db_ID</code> 和 <code class="inline">table_ID</code> 映射为 <code class="inline">key</code>，<code class="inline">tableInfo</code> 作为 value 存到 TiKV 里面去，并更新 job 的状态。</li></ul><li><a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152">finishDDLJob</a> 函数将 job 从 DDL job 队列中移除，然后加入 history ddl job 队列中去。</li><li><a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl.go#L451">doDDLJob</a> 函数中检测到 history DDL job 队列中有对应的 job 后，返回。</li></ul><h2><b>Add index</b></h2><p><code class="inline">add index</code> 主要做 2 件事：</p><ul><li>修改 table 的元信息，把 <code class="inline">indexInfo</code> 加入到 table 的元信息中去。</li><li>把 table 中已有了的数据行，把 <code class="inline">index columns</code> 的值全部回填到 <code class="inline">index record</code> 中去。</li></ul><p>具体执行流程的前部分的 SQL 解析、Compile 等流程，和 <code class="inline">create table</code> 一样，可以直接从 <a href="https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L42">DDLExec.Next</a> 开始看，然后调用 <code class="inline">alter</code> 语句的 <a href="https://github.com/pingcap/tidb/blob/source-code/executor/ddl.go#L78">e.executeAlterTable(x)</a> 函数，其实质调 ddl 的 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L862">AlterTable</a> 函数，然后调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L1536">CreateIndex</a> 函数，开始执行 add index 的主要工作，具体流程如下：</p><ol><li>Check 一些限制，比如 table 是否存在，索引是否已经存在，索引名是否太长等。</li><li>封装成一个 job，包含了索引名，索引列等，并将 job 的 type 标记为 <code class="inline">ActionAddIndex</code>。</li><li>给 job 获取一个 global job ID 然后放到 DDL job 队列中去。</li><li><code class="inline">owner ddl worker</code> 从 DDL job 队列中取出 job，根据 job 的类型调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L177">onCreateIndex</a> 函数。</li></ol><ul><li><code class="inline">buildIndexInfo</code> 生成 <code class="inline">indexInfo</code>，然后更新 <code class="inline">tableInfo</code> 中的 <code class="inline">Indices</code>，持久化到 TiKV 中去。</li><li>这里引入了 online schema change 的几个步骤，<a href="https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L237">需要留意 indexInfo 的状态变化</a>：<code class="inline">none -&gt; delete only -&gt; write only -&gt; reorganization -&gt;  public</code>。在 <code class="inline">reorganization -&gt; public</code> 时，首先调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L147">getReorgInfo</a> 获取 <code class="inline">reorgInfo</code>，主要包含需要 <code class="inline">reorganization</code> 的 range，即从表的第一行一直到最后一行数据都需要回填到 <code class="inline">index record</code> 中。然后调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L72">runReorgJob</a> , <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L554">addTableIndex</a>函数开始填充数据到 <code class="inline">index record</code>中去。<a href="https://github.com/pingcap/tidb/blob/source-code/ddl/reorg.go#L112">runReorgJob</a> 函数会定期保存回填数据的进度到 TiKV。<a href="https://github.com/pingcap/tidb/blob/source-code/ddl/index.go#L566">addTableIndex</a> 的流程如下：</li><ul><li>启动多个 <code class="inline">worker</code> 用于并发回填数据到 <code class="inline">index record</code>。</li><li>把 <code class="inline">reorgInfo</code> 中需要 <code class="inline">reorganization</code> 分裂成多个 range。扫描的默认范围是 <code class="inline">[startHandle , endHandle]</code>，然后默认以 128 为间隔分裂成多个 range，之后并行扫描对应数据行。在 master 分支中，range 范围信息是从 PD 中获取。</li><li>把 range 包装成多个 task，发给 <code class="inline">worker</code> 并行回填 <code class="inline">index record</code>。</li><li>等待所有 <code class="inline">worker</code> 完成后，更新 <code class="inline">reorg</code> 进度，然后持续第 3 步直到所有的 task 都做完。</li></ul></ul><p>5. 后续执行 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152">finishDDLJob</a>，检测 history ddl job 流程和 <code class="inline">create table</code> 类似。</p><h2><b>Drop Column</b></h2><p><code class="inline">drop Column</code> 只要修改 table 的元信息，把 table 元信息中对应的要删除的 column 删除。<code class="inline">drop Column</code> 不会删除原有 table 数据行中的对应的 Column 数据，在 decode 一行数据时，会根据 table 的元信息来 decode。</p><p>具体执行流程的前部分都类似，直接跳到 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_api.go#L1093">DropColumn</a> 函数开始，具体执行流程如下：</p><ol><li>Check table 是否存在，要 drop 的 column 是否存在等。</li><li>封装成一个 job, 将 job 类型标记为 <code class="inline">ActionDropColumn</code>，然后放到 DDL job 队列中去</li><li><code class="inline">owner ddl worker</code> 从 DDL job 队列中取出 job，根据 job 的类型调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/column.go#L174">onDropColumn</a> 函数：</li></ol><ul><li>这里 <code class="inline">column info</code> 的状态变化和 <code class="inline">add index</code> 时的变化几乎相反：<code class="inline">public -&gt; write only -&gt; delete only -&gt; reorganization -&gt; absent</code>。</li><li><a href="https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L362">updateVersionAndTableInfo</a> 更新 table 元信息中的 Columns。</li></ul><p>4. 后续执行 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152">finishDDLJob</a>，检测 history ddl job 流程和 <code class="inline">create table</code> 类似。</p><h2><b>Drop table</b></h2><p><code class="inline">drop table</code> 需要删除 table 的元信息和 table 中的数据。</p><p>具体执行流程的前部分都类似，<code class="inline">owner ddl worker</code> 从 DDL job 队列中取出 job 后执行 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/table.go#L76">onDropTable</a> 函数：</p><ol><li><code class="inline">tableInfo</code> 的状态变化是：<code class="inline">public -&gt; write only -&gt; delete only -&gt; none</code>。</li><li><code class="inline">tableInfo</code> 的状态变为 <code class="inline">none</code> 之后，会调用 <a href="https://github.com/pingcap/tidb/blob/source-code/meta/meta.go#L306"> DropTable</a> 将 table 的元信息从 TiKV 上删除。</li></ol><p>至于删除 table 中的数据，后面在调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L152">finishDDLJob</a> 函数将 job 从 job queue 中移除，加入 history ddl job queue 前，会调用 <a href="https://github.com/pingcap/tidb/blob/source-code/ddl/ddl_worker.go#L160">delRangeManager.addDelRangeJob(job)</a>，将要删除的 table 数据范围插入到表 <code class="inline">gc_delete_range</code> 中，然后由 <a href="https://github.com/pingcap/tidb/blob/source-code/store/tikv/gcworker/gc_worker.go">GC worker</a> 根据 <code class="inline">gc_delete_range</code> 中的信息在 GC 过程中做真正的删除数据操作。</p><h2><b>New Parallel DDL</b></h2><p>目前 TiDB 最新的 Master 分支的 DDL 引入了并行 DDL，用来加速多个 DDL 语句的执行速度。因为串行执行 DDL 时，<code class="inline">add index</code> 操作需要把 table 中已有的数据回填到 <code class="inline">index record</code> 中，如果 table 中的数据较多，回填数据的耗时较长，就会阻塞后面 DDL 的操作。目前并行 DDL 的设计是将 <code class="inline">add index job</code> 放到新增的 <code class="inline">add index job queue</code> 中去，其它类型的 DDL job 还是放在原来的 job queue。相应的，也增加一个 <code class="inline">add index worker</code> 来处理 <code class="inline">add index job queue</code> 中的 job。</p><img src="https://pic3.zhimg.com/v2-c2a566a46f5c077f474a4e17ccce2b6b_r.jpg" data-caption="图 2：并行 DDL 处理流程" data-size="normal" data-rawwidth="1121" data-rawheight="676" data-watermark="watermark" data-original-src="v2-c2a566a46f5c077f474a4e17ccce2b6b" data-watermark-src="v2-6b88067473e2e6254b3b8cf922b0de11" data-private-watermark-src=""><p>并行 DDL 同时也引入了 job 依赖的问题。job 依赖是指同一 table 的 DDL job，job ID 小的需要先执行。因为对于同一个 table 的 DDL 操作必须是顺序执行的。比如说，<code class="inline">add column a</code>，然后 <code class="inline">add index on column a</code>, 如果 <code class="inline">add index</code> 先执行，而 <code class="inline">add column</code> 的 DDL 假设还在排队未执行，这时 <code class="inline">add index on column a</code> 就会报错说找不到 <code class="inline">column a</code>。所以当 <code class="inline">add index job queue</code> 中的 job2 执行前，需要检测 job queue 是否有同一 table 的 job1 还未执行，通过对比 job 的 job ID 大小来判断。执行 job queue 中的 job 时也需要检查 <code class="inline">add index job queue</code> 中是否有依赖的 job 还未执行。</p><h2><b>End</b></h2><p>TiDB 目前一共支持 <a href="https://github.com/pingcap/tidb/blob/source-code/model/ddl.go#L32">十多种 DDL</a>，具体以及和 MySQL 兼容性对比可以看 <a href="https://github.com/pingcap/docs-cn/blob/master/sql/ddl.md">这里</a>。剩余其它类型的 DDL 源码实现读者可以自行阅读，流程和上述几种 DDL 类似。</p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
