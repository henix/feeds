<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>业界 | 谷歌发布MobileNetV2：可做语义分割的下一代移动端计算机视觉架构</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1523652429&amp;src=3&amp;ver=1&amp;signature=9HzPNuPk1vaKEKQylnFLk1Y4XZ9ajILeqG-lWxl4t*toeU2RfTpx5Bj5z6PLJurVB4d4AQagDhTTcrdpePUhsiEokH6HiGAThJMvTTRDPNrpPC5fXkyb9i5xPJmOFNrHH53Fa2ZZcGzdwkjROI8tHND9SitRnlUx5PAK3ZH764U=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    业界 | 谷歌发布MobileNetV2：可做语义分割的下一代移动端计算机视觉架构                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-04</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><section style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自Google Blog</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：Mark Sandler、Andrew Howard</strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：</strong><strong>黄小天、思源</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><blockquote><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(136, 136, 136);">深度学习在手机等移动端设备上的应用是机器学习未来的重要发展方向。2017 年 4 月，谷歌发布了 MobileNet——一个面向有限计算资源环境的轻量级神经网络。近日，谷歌将这一技术的第二代产品开源，开发者称，新一代 MobileNet 的模型更小，速度更快，同时还可以实现更高的准确度。</span><br></p></blockquote><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(123, 12, 0);">项目链接：https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</span></p><p style="margin-bottom: 20px;"><span style="font-size: 14px;text-align: justify;">谷歌 2017 年推出了 MobileNetV1，它是一种为移动设备设计的通用计算机视觉神经网络，因此它也能支持图像分类和检测等。一般在个人移动设备上运行深度网络能提升用户体验、提高访问的灵活性，以及在安全、隐私和能耗上获得额外的优势。此外，随着新应用的出现，用户可以与真实世界进行实时交互，因此我们对更高效的神经网络有着很大的需求。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">今天，谷歌很高兴地宣布下一代移动视觉应用 MobileNetV2 已经发布。MobileNetV2 在 MobileNetV1 的基础上获得了显著的提升，并推动了移动视觉识别技术的有效发展，包括分类、目标检测和语义分割。MobileNetV2 作为 TensorFlow-Slim 图像分类库的一部分而推出，读者也可以在 Colaboratory 中立即探索 MobileNetV2。此外，我们也可以下载代码到本地，并在 Jupyter Notebook 中探索。MobileNetV2 在 TF-Hub 中会作为模块使用，且预训练保存点可在以下地址中找到。</span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;margin-bottom: 5px;"><span style="font-size: 14px;">Colaboratory 试验地址：https://colab.research.google.com/github/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb</span></p></li><li><p style="text-align: left;line-height: 1.75em;margin-bottom: 5px;"><span style="font-size: 14px;">MobileNetV2 本地实验地址：https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_example.ipynb</span></p></li><li><p style="text-align: left;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">预训练模型下载：https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</span></p></li></ul><p style="margin-bottom: 20px;"><span style="font-size: 14px;text-align: justify;">MobileNetV2 基于 MobileNetV1[1] 的基本概念构建，并使用在深度上可分离的卷积作为高效的构建块。此外，MobileNetV2 引入了两种新的架构特性：1）层之间的线性瓶颈层；2）瓶颈层之间的连接捷径。MobileNetV2 的基本架构展示如下：</span><br></p><p style="margin-bottom: 20px;"><span style="font-size: 14px;text-align: justify;"></span></p><p style="text-align: center;margin-bottom: 20px;"><img class="" data-ratio="0.9515951595159516" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZfwWeFfzWpXcF4lEkXyhSctJ5PtzgFOibSmMtVGj729TJJKlPzdpSVlic54FL8DFnJEWLswyiaXJoA/640?wx_fmt=png" data-type="png" data-w="909" style="width: 408px;height: 388px;"></p><p style="text-align: left;margin-bottom: 20px;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em><span style="color: rgb(136, 136, 136);text-align: justify;">MobileNetV2 的架构概览，蓝色块如上所示为复合卷积构建块。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">我们可以直观理解为，瓶颈层对模型的中间输入与输出进行编码，而内层封装了模型从像素等低级概念到图像类别等高级概念的转换能力。最后，与传统的残差连接一样，捷径能快速训练并获得更优精确度。读者可查阅文末的 MobileNetV2 论文了解更多的详情。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">V2 与第一代的 MobileNet 相比有什么区别？总体而言，MobileNetV2 模型在整体延迟范围内上实现相同的准确度要更快。特别是，目前新模型减少了两倍 operations 的数量，且只需要原来 70% 的参数，在 Google Pixel 手机上的测试表明 V2 要比 MobileNetV1 快 30% 到 40%，同时还能实现更高的准确度。</span></p><p style="margin-bottom: 20px;"><span style="font-size: 14px;text-align: justify;"></span></p><p style="margin-bottom: 20px;"><img class="" data-ratio="0.5684596577017115" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZfwWeFfzWpXcF4lEkXyhSNRwynCAO3HUPMMhJ7gMBfxQ9c5Qyhn55R1lhXatJvibGcuGJxXNPDTw/640?wx_fmt=png" data-type="png" data-w="818" style=""></p><p style="text-align: left;margin-bottom: 20px;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);text-align: justify;">MobileNetV2 不仅速度更快（降低延迟），还刷新了 ImageNet Top 1 准确度。</span></em></span><br></p><p style="margin-bottom: 20px;"><span style="font-size: 14px;text-align: justify;">MobileNetV2 是一个用于目标检测和分割的非常有效的特征提取器。比如在检测方面，当 MobileNetV2 搭配上全新的 SSDLite [2]，在取得相同准确度的情况下速度比 MobileNetV1 提升了 35%。我们已通过 Tensorflow Object Detection API [4] 开源了该模型。</span><br></p><p style="margin-bottom: 20px;"><img class="" data-ratio="0.13247011952191234" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZfwWeFfzWpXcF4lEkXyhSJGDTOiciaejvKO8icnpRPdIQdybLhUCjBf3cBXBiaoQyOuPI9XjcicA85RA/640?wx_fmt=png" data-type="png" data-w="1004" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">为实现实时语义分割，我们借助简化版 DeepLabv3 [3] 把 MobileNetV2 用作特征提取器，这将稍后公布。在语义分割基准 PASCAL VOC 2012 上，MobileNetV1 与 MobileNetV2 作为特征提取器表现相当，但是后者所需的参数量减少了 5.3 倍，在 Multiply-Adds 方面 operations 也减少了 5.2 倍。</span></p><p style="margin-bottom: 20px;"><span style="font-size: 14px;text-align: justify;"></span></p><p style="margin-bottom: 20px;"><img class="" data-ratio="0.1383084577114428" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZfwWeFfzWpXcF4lEkXyhS3zr1AlmR0HwPfYymdRlXiczl9duOc9d1SmGBkTyicvSFVF2OCFCqE66Q/640?wx_fmt=png" data-type="png" data-w="1005" style=""></p><p style="margin-bottom: 20px;"><span style="font-size: 14px;text-align: justify;">正如我们所看到的，MobileV2 面向移动端提供了一个非常高效的模型，它能处理许多基本的视觉识别任务。最后，谷歌也希望能与广泛的学术社区和开源社区分享这个新模型，并期待它有新的提升与应用。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 5px;"><strong><span style="font-size: 14px;">论文：MobileNetV2: Inverted Residuals and Linear Bottlenecks</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(123, 12, 0);"></span></p><p style="margin-bottom: 5px;"><img class="" data-ratio="0.2813141683778234" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8ZfwWeFfzWpXcF4lEkXyhSAUOnODu6XcLpVEgOdheRwGULbpdicjT3m7ic3RlZQTe4xIbkqp3OOuGQ/640?wx_fmt=png" data-type="png" data-w="974" style=""></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1801.04381</span><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">本文中我们介绍了一种新的移动端架构——MobileNetV2，其在多任务和基准以及不同模型大小的范围上进一步刷新了移动端模型的当前最佳性能。我们还介绍了如何通过全新框架 SSDLite 将这些模型高效应用于目标检测。此外，我们也展示了通过简化版 DeepLabv3（我们称之为 Mobile DeepLabv3）构建移动端的语义分割方法。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">MobileNetV2 架构基于反向残差结构，其中残差块的输入和输出是较短的瓶颈层，这与在输入中使用扩展表征的传统残差模型正相反。MobileNetV2 使用轻量级深度卷积过滤中间扩展层的特征。此外，我们发现为了保持表征能力，移除短层中的非线性很重要，这提升了性能，并带来了催生该设计的直观想法。最后，我们的方法允许将输入/输出域与转换的表现性分开，从而为未来的分析提供一个简便的框架。我们在 ImageNet 分类、COCO 目标检测、VOC 图像分割上测试了 MobileNetV2 的性能，同时也评估准确度、operations 数量（通过 MAdd 测量）以及参数量之间的权衡。<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" width="48px" style="font-size: 14px;text-align: justify;white-space: normal;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;"></span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 14px;">参考文献：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">1. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H, arXiv:1704.04861, 2017.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">2. MobileNetV2: Inverted Residuals and Linear Bottlenecks, Sandler M, Howard A, Zhu M, Zhmoginov A, Chen LC. arXiv preprint. arXiv:1801.04381, 2018.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">3. Rethinking Atrous Convolution for Semantic Image Segmentation, Chen LC, Papandreou G, Schroff F, Adam H. arXiv:1706.05587, 2017.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">4. Speed/accuracy trade-offs for modern convolutional object detectors, Huang J, Rathod V, Sun C, Zhu M, Korattikara A, Fathi A, Fischer I, Wojna Z, Song Y, Guadarrama S, Murphy K, CVPR 2017.</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;">5. Deep Residual Learning for Image Recognition, He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. arXiv:1512.03385,2015</span></p><p><br></p><p style="text-align: left;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>原文地址：https://research.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html</em></span></p><p><br></p><p><br></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="1741105584" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                <p class="media_tool_meta tips_global meta_primary article_modify_tag">修改于<span id="js_modify_time"></span></p>
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                <p class="media_tool_meta tips_global meta_primary article_modify_tag">修改于<span id="js_modify_time"></span></p>
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
