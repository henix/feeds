<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CVPR 2018 | Spotlight 论文：非参数化方法实现的极端无监督特征学习</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1527560022&amp;src=3&amp;ver=1&amp;signature=sytEABRup*qUL*qtJCY0ykbaRqumaomKmOZ-9T9fjmbPCzucXgHiWI2mT5nNWwu-LRrNYua7Ce6TUbrC24a-5AAIKs7uPH-UlCtBPjAFX0mdOSpBxgcYEuU9ieLij*NSAQzDp0WgicIs7KQLej*MxQIoHxlDyGBxDeEFCb3B9cI=">原文</a></p>
<div id="js_top_ad_area" class="top_banner"></div><div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        
            <div id="img-content">

                
                <h2 class="rich_media_title" id="activity-name">
                    
                    <script nonce="281907118" type="text/javascript">
                        if(/(iPhone|iPad|iPod|iOS)/i.test(navigator.userAgent)){
                            document.write("<span class='rich_media_title_ios'>CVPR 2018 | Spotlight 论文：非参数化方法实现的极端无监督特征学习");
                        }else{
                            document.write("CVPR 2018 | Spotlight 论文：非参数化方法实现的极端无监督特征学习");
                        }
                    </script>

                                                                                </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                                                <span class="rich_media_meta rich_media_meta_nickname" id="profileBt">
                      <a href="javascript:void(0);">
                        机器之心                      </a>
                      <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                          <div class="profile_inner">
                              <strong class="profile_nickname">机器之心</strong>
                              <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                              <p class="profile_meta">
                              <label class="profile_meta_label">微信号</label>
                              <span class="profile_meta_value">almosthuman2014</span>
                              </p>

                              <p class="profile_meta">
                              <label class="profile_meta_label">功能介绍</label>
                              <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                              </p>
                              
                          </div>
                          <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                              <i class="profile_arrow arrow_out"></i>
                              <i class="profile_arrow arrow_in"></i>
                          </span>
                      </div>
                    </span>


                    <em id="publish_time" class="rich_media_meta rich_media_meta_text"></em>





                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " lang='="en"' id="js_content">
                    

                    

                    
                    
                    <section style="white-space: normal;max-width: 100%;color: rgb(51, 51, 51);"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(255, 255, 255);max-width: 100%;background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自arXiv</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color: rgb(136, 136, 136);max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：乾树、刘晓坤</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><span style="max-width: 100%;font-size: 15px;box-sizing: border-box !important;word-wrap: break-word !important;"></span></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 15px;text-align: justify;">本研究受监督学习中的输出排序的启发，指出数据本身的表面相似性而非语义标签，使得某些类比其他类更加接近。研究者据此提出了一种极端化的无监督学习方法，主要特点是非参数化训练、实例级判别（一个实例视为一个类）。在 ImageNet 上的实验结果表明，该方法在图像分类方面远超过最先进的无监督方法。若有更多的训练数据和更好的网络架构，该算法会持续提高测试结果。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">深度神经网络，特别是卷积神经网络（CNN）的兴起，在计算机视觉领域取得了若干突破。大多数成功的模型都是通过监督学习进行训练的，而这需要大量的依任务类型而定的特定标注数据集。但是，在某些情况下，获取标注数据通常代价昂贵甚至不可行。近年来，无监督学习受到学界越来越多的关注 [5,2]。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">研究者在本文中提出的无监督学习的创新方法源于对监督学习物体识别结果的一些观察。在 ImageNet 上，top-5 分类误差远低于 top-1 误差 [18]，并且图像在 softmax 层输出中的预测值排第二的响应类更可能与真实类有视觉关联。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">如图 1 所示，包含猎豹（leopard）的图像被识别成美洲豹（jaguar）的概率比识别成书柜（bookcase）高很多 [11]。这一观察表明，经典的判别式学习方法在没有干预时可以自动发现语义类别之间的表面（明显的）相似性。换句话说，明显的相似性不是来自语义注释，而是来自图像本身。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.8036211699164345" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8wrfCUekygN4z5B9c1wm7532yDrK9EIcS5zKxZE7mZNjRXIHRJOqOIGnWwbZVmfiazDNBF0GFqlXA/640?wx_fmt=png" data-type="png" data-w="718" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 1：激励研究者提出无监督方法的有监督学习效果图。以猎豹图片为例，网络输出的几个最高响应类都是视觉相关的，例如美洲豹和猎豹。数据本身的表面相似性而非语义标签，使得某些类比其他类更加接近。该无监督方法将类监督发展到极致，并学习了辨别各个单独实例的特征表示。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">研究者将类监督发展到极端的实例监督，并提出这样的问题：我们是否可以通过纯粹的判别学习来学到反映实例间表面相似性的度量？图像本身具有鲜明的特征，并且每幅图像与相同语义类别中的其他图像都可能有很大差异 [23]。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">如果我们在没有语义信息的情况下学习区分单独实例，那么我们最终可能会得到一个可以捕获实例间的表面相似性的特征表示，就像类监督学习在类别间仍然保留表面相似性那样。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">无监督学习作为实例级别的判别形式在技术上也引人入胜，因为它可以受益于监督学习判别网络的最新进展，例如，新的网络架构。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">然而，现在我们还面临着一个重大挑战，即现在「类别」的数量就是整个训练集的大小。对于 ImageNet 来说，「类别」将是 120 万而不是 1000 个类。简单将 softmax 扩展到更多的类是不可行的。研究者通过使用噪声对比估计（NCE）[9] 逼近的 softmax 分布并采用近端正则化方法 [29] 以稳定训练过程来解决这个挑战。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了评估无监督学习的有效性，过去的工作如 [2,31] 依赖于线性分类器（例如，支持向量机（SVM）），在测试时将学习到的特征与类别信息结合以便进行分类。但是，我们不清楚未知的测试任务为什么可以将训练学习到的特征线性分离。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">研究者提倡在训练和测试时都采用非参数化方法。他们将实例级别的分类看作度量学习问题，其中实例之间的距离（相似度）是以非参数方式直接从特征中计算得到的。也就是说，每个实例的特征都存储在离散的内存块中，而不是网络中的权重。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在测试阶段，使用基于学习度量的 k-近邻（kNN）进行分类。因为模型的学习和评估都与图像间的相同的度量空间有关，所以其训练和测试是一致的。研究者总结了与 SVM 和 kNN 的准确率对比实验结果。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">实验结果表明，在无监督领域，该方法在图像分类方面远超过最先进的方法。具体地，在 ImageNet 1K [1] 上的 top-1 准确率为 46.5％，Places 205 [41] 为 41.6％。若有更多的训练数据和更好的网络架构，该算法会持续提高测试结果。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">通过微调学习到的特征，可进一步获得半监督学习和物体检测任务的具竞争性的结果。最后，该非参数化模型非常紧凑：每张图片有 128 个特征，存储一百万张图像仅需 600MB，从而在运行时实现快速最近邻检索。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.23151347615756737" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8wrfCUekygN4z5B9c1wm751PvfKQ4Zgkd2RpLeCiaKk20jI4icasc3R8XEpunoOqxo4pgEZ0srONvA/640?wx_fmt=png" data-type="png" data-w="1447" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 2：本文提出的无监督特征学习方法的工作流图。研究者使用骨干 CNN 将每个图像编码为 128 维空间并进行 L2 归一化的特征向量。最佳特征嵌入过程是通过实例级判别器学习的，该判别器尝试将训练样本的特征最大程度地散布在 128 维的单位球上。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="0.4307692307692308" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8wrfCUekygN4z5B9c1wm75mFVQnSZ6KzotnmmibE2RIV2ia9HnGiaTWFCzOrjHG1uKWn3gkkaeKI4WA/640?wx_fmt=png" data-type="png" data-w="650" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 1：通过在学习到的特征上应用线性 SVM 或 kNN 分类器在 CIFAR10 的 Top-1 准确率。本文提出的非参数化的 softmax 优于参数化的 softmax，并且用 NCE 方法 得到的准确率随 m 单调递增。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">图像分类</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">研究者在 ImageNet ILSVRC [34] 上学习特征表示，并将他们的方法与代表性的无监督学习方法进行比较。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">实验设置。研究者通过经验验证来选择并设计参数。具体来说，他们设定 τ= 0.07，并使用 m = 4,096 的 NCE 来平衡性能和计算成本。该模型使用带 momentum 的 SGD 训练 200 个 epoch。批量大小为 256，学习率初始化为 0.03，在训练 120 个 epoch 后每 40 个 epoch 乘一次 0.1。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">对比实验。研究者将他们的方法与随机初始化的网络（作为下界）及各种无监督学习方法进行了比较，包括自监督学习 [2,47,27,48]、对抗学习 [4] 和 Exemplar CNN [3]。split-brain 自编码器 [48] 提供代表当前最佳水平的强大基线。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在他们的初版论文中，他们的实验网络都基于 AlexNet 架构 [18]，除了 exemplar CNN [5]，其基于 ResNet-101 [3]。由于网络架构对性能有很大影响，研究者考虑了一些经典的架构：AlexNet [18]、VGG16 [36]、ResNet-18 和 ResNet-50 [10]。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">研究者使用两种不同的标准评估性能：（1）对从 conv1 到 conv5 的中间特征运行线性 SVM。注意，VGG16 和 ResNet 中也有对应层 [36,10]。（2）对输出特征运行 kNN。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.7662517289073306" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8wrfCUekygN4z5B9c1wm75bych1IPibQzGqYhSUmI5DAlgGCLDoPdEHrIxELCfVRUQL9OiaEIpWBwA/640?wx_fmt=png" data-type="png" data-w="723" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">表 2：在 ImageNet 上的 Top-1 分类准确率。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="0.7198335644937587" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8wrfCUekygN4z5B9c1wm75SdAXKmtpbEdFLvaTlE1k8P0exVHibHic3jiamvl75PLdd0MKHibo7G5lTg/640?wx_fmt=png" data-type="png" data-w="721" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 3：直接基于在 ImageNet 上学习特征的、没有微调的在 Places 上的 Top-1 分类准确率。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="0.6551020408163265" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8wrfCUekygN4z5B9c1wm75DpWDsQjvic0JXBknzX1icQYKVWicstdLsyxL7JYicCiaUACOTd87sGE34nA/640?wx_fmt=png" data-type="png" data-w="980" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 5：查询示例的检索结果。左列是验证集的查询，右列是训练集中检索到的 10 个最接近的实例。上半部分展示了最好的表现。下半部分展示了最差的表现。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: left;line-height: 1.75em;"><strong><span style="font-size: 15px;">论文：Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination</span></strong></p><p style="text-align: left;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p style="text-align: left;line-height: 1.75em;"><strong><span style="font-size: 15px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="0.15166548547129696" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8wrfCUekygN4z5B9c1wm75xIRNG45icTpyn4F9ViceMUlI7boV7jG9E7L55KekAcqlyh53OFia83Mfw/640?wx_fmt=png" data-type="png" data-w="1411" style=""></p><p style="text-align: left;line-height: 1.75em;"><strong><span style="font-size: 15px;"></span></strong><br></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文地址：https://arxiv.org/abs/1805.01978</span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">开源代码：http://github. com/zhirongw/lemniscate.pytorch</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">摘要：</span></strong><span style="font-size: 15px;">在标注数据集上训练过的神经网络分类器无须人为干预就可以在各个类别间捕捉明显的视觉相似性。我们研究了这一行为是否可以扩展到传统的监督学习领域之外：我们是否可以仅通过获取可区分单独实例的特征来学习一个可以很好捕捉实例间而非类间明显相似性的特征表示？</span><span style="font-size: 15px;">我们将该思路看做实例级的非参数化分类问题，并使用噪声对比估计来解决大量实例类带来的计算挑战。我们的实验结果表明，在无监督学习条件下，我们的算法性能远超 ImageNet 分类问题上最先进的算法。</span><span style="font-size: 15px;">若有更多的训练数据和更好的网络架构，我们的算法会持续提高测试结果。通过微调学习到的特征，我们进一步获得了半监督学习和物体检测任务的有竞争力的结果。我们的非参数化模型非常紧凑：每张图片有 128 个特征，我们的方法存储一百万张图像仅需 600MB，从而在运行时实现快速最近邻检索。</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" width="48px" style="font-size: 15px;color: rgb(51, 51, 51);width: 38px;height: 16px;"></p><p style="white-space: normal;"><br></p><p style="white-space: normal;"><br></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：<strong style="max-width: 100%;color: rgb(62, 62, 62);font-size: 18px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">content</span></strong>@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p>
                </div>
                <script nonce="281907118" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg_new/winwx3de35e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc appmsg_card_context" id="js_preview_reward" style="display:none;">
                    <div class="reward_inner">
                        <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                        <p>
                            <a class="reward_access" id="js_preview_reward_link" href="##">赞赏</a>
                        </p>
                    </div>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                                        
                                

                        
            <ul id="js_hotspot_area" class="article_extend_area"></ul>


            
                        <div class="rich_media_tool" id="js_toobar3">

                                            <div id="js_read_area3" class="media_tool_meta tips_global_primary meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_extra meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">

                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>


        
        <div class="rich_media_area_extra">
            
            <div id="js_share_appmsg">
            </div>

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_container" style="display:none">
              

              
              <div class="discuss_mod" id="js_friend_cmt_area" style="display:none">
                
                
                
              </div>

                            <div class="discuss_mod" id="js_cmt_area" style="display:none">
              </div>
                          </div>
        </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
