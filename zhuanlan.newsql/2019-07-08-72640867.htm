<div class="title-image"><img src="https://pic1.zhimg.com/v2-3505224dad3b818557ddf813893b94ba_b.jpg" alt=""></div><p>作者：周振靖</p><p>之前的 TiKV 源码解析系列文章介绍了 TiKV 依赖的周边库，从本篇文章开始，我们将开始介绍 TiKV 自身的代码。本文重点介绍 TiKV 最外面的一层——Service 层。</p><p>TiKV 的 Service 层的代码位于 <code>src/server</code> 文件夹下，其职责包括提供 RPC 服务、将 store id 解析成地址、TiKV 之间的相互通信等。这一部分的代码并不是特别复杂。本篇将会简要地介绍 Service 层的整体结构和组成 Service 层的各个组件。</p><h2>整体结构</h2><p>位于 <code>src/server/server.rs</code> 文件中的 <code>Server</code> 是我们本次介绍的 Service 层的主体。它封装了 TiKV 在网络上提供服务和 Raft group 成员之间相互通信的逻辑。<code>Server</code> 本身的代码比较简短，大部分代码都被分离到 <code>RaftClient</code>，<code>Transport</code>，<code>SnapRunner</code> 和几个 gRPC service 中。上述组件的层次关系如下图所示：</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg" data-caption="" data-size="normal" data-rawwidth="1288" data-rawheight="618" class="origin_image zh-lightbox-thumb" width="1288" data-original="https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg" data-caption="" data-size="normal" data-rawwidth="1288" data-rawheight="618" class="origin_image zh-lightbox-thumb lazy" width="1288" data-original="https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-845da5f93f6e1ce8015f4f61dfde6101_b.jpg"/></figure><p>接下来，我们将详细介绍这些组件。</p><h2>Resolver</h2><p>在一个集群中，每个 TiKV 实例都由一个唯一的 store id 进行标识。Resolver 的功能是将 store id 解析成 TiKV 的地址和端口，用于建立网络通信。</p><p>Resolver 是一个很简单的组件，其接口仅包含一个函数：</p><div class="highlight"><pre><code class="language-text">pub trait StoreAddrResolver: Send + Clone {
   fn resolve(&amp;self, store_id: u64, cb: Callback) -&gt; Result&lt;()&gt;;
}</code></pre></div><p>其中 <code>Callback</code> 用于异步地返回结果。<code>PdStoreAddrResolver</code> 实现了该 trait，它的 <code>resolve</code> 方法的实现则是简单地将查询任务通过其 <code>sched</code> 成员发送给 <code>Runner</code>。而 <code>Runner</code> 则实现了 <code>Runnable&lt;Task&gt;</code>，其意义是 <code>Runner</code> 可以在自己的一个线程里运行，外界将会向 <code>Runner</code> 发送 <code>Task</code> 类型的消息，<code>Runner</code> 将对收到的 <code>Task</code> 进行处理。 这里使用了由 TiKV 的 util 提供的一个单线程 worker 框架，在 TiKV 的很多处代码中都有应用。<code>Runner</code> 的 <code>store_addrs</code> 字段是个 cache，它在执行任务时首先尝试在这个 cache 中找，找不到则向 PD 发送 RPC 请求来进行查询，并将查询结果添加进 cache 里。</p><h2>RaftClient</h2><p>TiKV 是一个 Multi Raft 的结构，Region 的副本之间，即 Raft group 的成员之间需要相互通信，<code>RaftClient</code> 的作用便是管理 TiKV 之间的连接，并用于向其它 TiKV 节点发送 Raft 消息。<code>RaftClient</code> 可以和另一个节点建立多个连接，并把不同 Region 的请求均摊到这些连接上。这部分代码的主要的复杂性就在于连接的建立，也就是 <code>Conn::new</code> 这个函数。建立连接的代码的关键部分如下：</p><div class="highlight"><pre><code class="language-text">let client1 = TikvClient::new(channel);

let (tx, rx) = batch::unbounded::&lt;RaftMessage&gt;(RAFT_MSG_NOTIFY_SIZE);
let rx = batch::BatchReceiver::new(rx, RAFT_MSG_MAX_BATCH_SIZE, Vec::new, |v, e| v.push(e));
let rx1 = Arc::new(Mutex::new(rx));

let (batch_sink, batch_receiver) = client1.batch_raft().unwrap();
let batch_send_or_fallback = batch_sink
   .send_all(Reusable(rx1).map(move |v| {
       let mut batch_msgs = BatchRaftMessage::new();
       batch_msgs.set_msgs(RepeatedField::from(v));
       (batch_msgs, WriteFlags::default().buffer_hint(false))
   })).then(/*...*/);

client1.spawn(batch_send_or_fallback.map_err(/*...*/));</code></pre></div><p>上述代码向指定地址调用了 <code>batch_raft</code> 这个 gRPC 接口。<code>batch_raft</code> 和 <code>raft</code> 都是 stream 接口。对 <code>RaftClient</code> 调用 <code>send</code> 方法会将消息发送到对应的 <code>Conn</code> 的 <code>stream</code> 成员，即上述代码的 <code>tx</code> 中，而在 gRPC 的线程中则会从 <code>rx</code> 中取出这些消息（这些消息被 <code>BatchReceiver</code> 这一层 batch 起来以提升性能），并通过网络发送出去。</p><p>如果对方不支持 batch，则会 fallback 到 <code>raft</code> 接口。这种情况通常仅在从旧版本升级的过程中发生。</p><h2>RaftStoreRouter 与 Transport</h2><p><code>RaftStoreRouter</code> 负责将收到的 Raft 消息转发给 raftstore 中对应的 Region，而 <code>Transport</code> 负责将 Raft 消息发送到指定的 store。</p><p><code>ServerRaftStoreRouter</code> 是在 TiKV 实际运行时将会使用的 <code>RaftStoreRouter</code> 的实现，它包含一个内层的、由 raftstore 提供的 <code>RaftRouter</code> 对象和一个 <code>LocalReader</code> 对象。收到的请求如果是一个只读的请求，则会由 <code>LocalReader</code> 处理；其它情况则是交给内层的 router 来处理。</p><p><code>ServerTransport</code> 则是 TiKV 实际运行时使用的 <code>Transport</code> 的实现（<code>Transport</code> trait 的定义在 raftstore 中），其内部包含一个 <code>RaftClient</code> 用于进行 RPC 通信。发送消息时，<code>ServerTransport</code> 通过上面说到的 Resolver 将消息中的 store id 解析为地址，并将解析的结果存入 <code>raft_client.addrs</code> 中；下次向同一个 store 发送消息时便不再需要再次解析。接下来，再通过 <code>RaftClient</code> 进行 RPC 请求，将消息发送出去。</p><h2>Node</h2><p><code>Node</code> 可以认为是将 raftstore 的复杂的创建、启动和停止逻辑进行封装的一层，其内部的 <code>RaftBatchSystem</code> 便是 raftstore 的核心。在启动过程中（即 <code>Node</code> 的 <code>start</code> 函数中），如果该节点是一个新建的节点，那么会进行 bootstrap 的过程，包括分配 store id、分配第一个 Region 等操作。</p><p><code>Node</code> 并没有直接包含在 <code>Server</code> 之内，但是 raftstore 的运行需要有用于向其它 TiKV 发送消息的 <code>Transport</code>，而 <code>Transport</code> 作为提供网络通信功能的一部分，则是包含在 <code>Server</code> 内。所以我们可以看到，在 <code>src/binutil/server.rs</code>文件的 <code>run_raft_server</code> 中（被 tikv-server 的 <code>main</code> 函数调用），启动过程中需要先创建 <code>Server</code>，然后创建并启动 <code>Node</code> 并把 <code>Server</code> 所创建的 <code>Transport</code> 传给 <code>Node</code>，最后再启动 <code>Node</code>。</p><h2>Service</h2><p>TiKV 包含多个 gRPC service。其中，最重要的一个是 <code>KvService</code>，位于 <code>src/server/service/kv.rs</code> 文件中。</p><p><code>KvService</code> 定义了 TiKV 的 <code>kv_get</code>，<code>kv_scan</code>，<code>kv_prewrite</code>，<code>kv_commit</code> 等事务操作的 API，用于执行 TiDB 下推下来的复杂查询和计算的 <code>coprocessor</code> API，以及 <code>raw_get</code>，<code>raw_put</code> 等 Raw KV API。<code>batch_commands</code> 接口则是用于将上述的接口 batch 起来，以优化高吞吐量的场景。当我们要为 TiKV 添加一个新的 API 时，首先就要在 kvproto 项目中添加相关消息体的定义，并在这里添加相关代码。另外，TiKV 的 Raft group 各成员之间通信用到的 <code>raft</code> 和 <code>batch_raft</code> 接口也是在这里提供的。</p><p>下面以 <code>kv_prewrite</code> 为例，介绍 TiKV 处理一个请求的流程。首先，无论是直接调用还是通过 <code>batch_commands</code> 接口调用，都会调用 <code>future_prewrite</code> 函数，并在该函数返回的 future 附加上根据结果发送响应的操作，再将得到的 future spawn 到 <code>RpcContext</code>，也就是一个线程池里。<code>future_prewrite</code> 的逻辑如下：</p><div class="highlight"><pre><code class="language-text">// 从请求体中取出调用 prewrite 所需的参数

let (cb, f) = paired_future_callback();
let res = storage.async_prewrite(/*其它参数*/, cb);

AndThenWith::new(res, f.map_err(Error::from)).map(|v| {
   let mut resp = PrewriteResponse::new();
   if let Some(err) = extract_region_error(&amp;v) {
       resp.set_region_error(err);
   } else {
       resp.set_errors(RepeatedField::from_vec(extract_key_errors(v)));
   }
   resp
})</code></pre></div><p>这里的 <code>paired_future_callback</code> 是一个 util 函数，它返回一个闭包 <code>cb</code> 和一个 future <code>f</code>，当 <code>cb</code> 被调用时 <code>f</code> 就会返回被传入 <code>cb</code> 的值。上述代码会立刻返回，但 future 中的逻辑在 <code>async_prewrite</code> 中的异步操作完成之后才会执行。一旦 prewrite 操作完成，<code>cb</code> 便会被调用，将结果传给 <code>f</code>，接下来，我们写在 <code>future</code> 中的创建和发送 Response 的逻辑便会继续执行。</p><h2>总结</h2><p>以上就是 TiKV 的 Service 层的代码解析。大家可以看到这些代码大量使用 trait 和泛型，这是为了方便将其中一些组件替换成另外一些实现，方便编写测试代码。另外，在 <code>src/server/snap.rs</code> 中，我们还有一个专门用于处理 Snapshot 的模块，由于 Snapshot 消息的特殊性，在其它模块中也有一些针对 snapshot 的代码。关于 Snapshot，我们将在另一篇文章里进行详细讲解，敬请期待。</p><p><b>原文阅读：</b><a href="https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tikv-source-code-reading-9/" class=" external" target="_blank" rel="nofollow noreferrer"><span class="invisible">https://www.</span><span class="visible">pingcap.com/blog-cn/tik</span><span class="invisible">v-source-code-reading-9/</span><span class="ellipsis"></span></a></p><p><b>更多 TiKV 源码阅读：</b></p><a href="https://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590" data-draft-node="block" data-draft-type="link-card" data-image="https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg" data-image-width="1200" data-image-height="1200" class=" wrap external" target="_blank" rel="nofollow noreferrer">博客 | PingCAP</a><p></p>