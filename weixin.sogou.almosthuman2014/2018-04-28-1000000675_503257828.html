<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>专访 | 五一出游赏花，如何优雅地解释百度细粒度识别方案</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1525750003&amp;src=3&amp;ver=1&amp;signature=DQAX*cUGN0BZgdOVLmP8-20O4Bp6d58IK1ze2iQ-Kw-Feur01E2fN2A-CgiwAFEi3uLMS27*MRERZuAtntp9*3fw0X46nmmH5pcrInk4b8LPsVnsZusSF5Ga-Rday*rFIY18OqdoljgztflXDNCkIiEGGsuMblZlCgc6pKULXEM=">原文</a></p>
<div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        <div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    专访 | 五一出游赏花，如何优雅地解释百度细粒度识别方案                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                        <span id="copyright_logo" class="rich_media_meta meta_original_tag">原创</span>
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-28</em>

                                        <em class="rich_media_meta rich_media_meta_text">思源</em>
                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;caret-color: rgb(62, 62, 62);font-size: 16px;white-space: normal;line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color:#ffffff;"><span style="background-color: rgb(117, 117, 118);">机器之心原创</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：思源</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="color: rgb(136, 136, 136);font-size: 14px;">近日，百度发布了用于花卉识别的移动端应用，这种基于全卷积注意力网络的细粒度识别方法在计算和准确度上都有非常强大的优势。在百度主任研发架构师陈凯和资深研发工程师胡翔宇的解释下，本文首先将介绍什么是细粒度识别，以及一般的细粒度识别方法，然后重点解析百度基于强化学习和全卷积注意力网络的细粒度识别模型</span><span style="color: rgb(136, 136, 136);font-size: 14px;">。五一出游赏花，为女朋友解释解释细粒度识别也是极好的。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">细粒度识别的概念其实非常简单，即模型需要识别非常精细的子类别。例如百度的花卉识别应用，模型不仅需要如一般识别问题那样检测出物体是不是花，同时还需要检测出物体具体属于哪一品种的花。而通常属于不同子类别的物体是非常相似的，例如樱花和桃花等，我们不仅需要花的整体信息来识别它是「花」，同时还需要局部信息来确定「花」的品种。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">从这一观点出发，细粒度识别基本上就是同时使用全局信息和局部信息的分类任务。对于花的细粒度识别来说，全局信息就是用户拍摄的整张图像，而局部信息则是图像中的花或花的重要部位。这两部分信息都包括在整张图像中，我们希望模型根据整张图像预测出具体的细分类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">百度借助了知识图谱对世界上的花卉名字进行科学的科、属、种划分，建立了一个非常专业的花卉类别库。而对于其它巨量优质的花卉图像，标注人员通过权威样本库中的文字描述，并在中科院老师的帮助下，根据花卉的叶子、形状、颜色等微观特征进行挑选与标注。此外，百度还进行了标注质量的检查，标注准确率在 95% 以上。最后，这些巨量的标注数据包含了花卉的整体图像和对应的精细品种。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其实整体图像隐含地包括了全局信息与局部信息，但一般来说，局部信息同样也需要标注，而利用这些局部标注的方法即基于强监督信息的细粒度识别。不过这种局部标注是非常昂贵的，因此很多模型尝试自己学习重要的局部区域，这一类方法即基于弱监督信息的细粒度识别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">百度所采用的全卷积注意力网络是一种基于弱监督信息的细粒度识别方法，它不需要标注局部特征，并通过下文描述的全卷积注意力网络和马尔科夫决策过程自动学习重要的区域。</span></p><p style="text-align: left;"><span style="font-size: 15px;"><strong>细粒度识别</strong></span></p><p style="text-align: center;"><strong><br></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">如上所述，细粒度识别可分为强监督和弱监督两种。最开始优秀的模型一般都是基于强监督的，它们在测试集上有更好的性能。但基于强监督的方法标注成本非常高，且局部标注在遇到遮挡等情况时就会完全失去作用，因此更多的研究者开始探讨基于弱监督的细粒度识别方法。以下简单地介绍了这两种方法的实例，即基于强监督的 Part-based R-CNN 和弱监督的 Bilinear CNN。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">基于强监督信息的细粒度识别通常需要使用边界框和局部标注信息，例如 2014 年提出的 Part-based R-CNN 利用自底向上的候选区域（region proposals）计算深度卷积特征而实现细粒度识别。这种方法会学习建模局部外观，并加强局部信息之间的几何约束。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5384615384615384" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAlBX352c8hkibwza4vXWL8yLqX7K7EaAyZHKpUmyY37u6MnjVU8XBOxA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">如上所示，局部区域将从自底向上的候选区域开始（左上角），我们将基于深度卷积特征同时训练目标和局部区域。在推断阶段，所有的窗口都会由检测器评分（中间），且我们可以应用非参几何约束（底部）重新评估窗口并选择最优的目标和局部检测（右上角）。最后的步骤就是抽取局部语义信息来用于细粒度识别，并训练一个分类器预测最终类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">而基于弱监督信息的细粒度识别也有很多方法，例如 15 年提出的 Bilinear CNN，它由两个特征提取器组成，它们在每个图像位置的输出使用外积相乘，并池化以获得一个图像描述子。这种架构能以转译不变的方式对局部成对特征的交互进行建模，这对于细粒度识别非常重要。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">Bilinear CNN 所使用的外积捕捉了特征通道之间的成对相关性，因此可以建模局部特征的交互，即如果一个网络是局部检测器，那么另一个网络就是局部特征抽取器。以下展示了用于细粒度分类的 Bilinear CNN 模型，在推断过程中，一张图像将传入两个卷积神经网络 A 和 B，然后对应位置的外积相乘得出双线性向量，并传入 Softmax 层以完成分类。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img data-copyright="0" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAC0iasIBNdwvVa3IIPKpkpajaqk0Vices3rdrjnnqrIUXtiaCqbj6fo6Kw/640?wx_fmt=png" data-type="png" style="" class="" data-ratio="0.5044136191677175" data-w="793"></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span><span style="font-size: 15px;">百度所采用的细粒度识别方法也是一种使用弱监督的策略，它通过全卷积注意力网络实现类别预测，并基于强化学习调整需要注意的局部区域。</span></p><p style="text-align: center;"><strong>全卷积注意力网络</strong></p><p style="text-align: center;"><strong><br></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">尽管最近研究社区将目光投向弱监督的细粒度识别方法，但它的效果和实践上都有一些差距。百度的细粒度模型通过利用强化学习选择注意区域而大大减少了强监督所需要的图像标注量，且还能以非常高的准确度部署到应用中。他们构建一种全卷积注意网络，并根据马尔科夫决策过程确定哪些局部图像块对最终预测有帮助，因此图像的细粒度分类将同时利用全局图像和那些重要的局部图像完成预测。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">百度提出的这种架构首先基于弱监督强化学习而不需要昂贵的标注，其次它所采用的全卷积网络可以加速训练和推断过程，最后贪婪的奖励策略可以加速学习的收敛，这三点也是百度细粒度模型的显著特点。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">胡翔宇表示：「这种网络其实和人类的视觉系统非常像，在我们识别细分类别时，首先会查看整体特征而确定大概的类别，然后再仔细观察有区分度的特征确定细分类别。」百度的全卷积注意力网络（FCANs）同样首先会抽取整张图的特征进行分类，然后截取一小块特征图（Feature Map）作为当前网络注意的区域，当这样的区域是具有区分度的特征时，模型就更能正确预测出细分类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 15px;">模型架构</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">该模型主要的架构如下图所示可以分为特征抽取阶段、局部特征注意区域和分类过程。总体而言模型会先使用预训练卷积神经网络抽取图像特征，然后再结合整体图像信息和局部重要信息预测最终类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5825977301387137" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPACbkibNvAOPtJnVvZMyf4sK4EmayBxQ4k4xut74ayeDClxldLXYT2vgw/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">如上所示，在下部分的训练过程中，抽取的特征图会在所有通道上做一些截取，截取的部分就相当于模型关注的局部原始图像。如果说这个截取在最后的分类阶段中提升了预测效果，那么它就是值得注意的区域。在上部分的推断过程中，模型将利用输入图像的全局特征和所有重要的局部图像预测精细类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 15px;">特征图的抽取</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">特征图会通过全卷积神经网络进行抽取，一般我们可以选择 VGG 或 ResNet 等流行的网络架构，并在 ImageNet 数据集上预训练而能抽取一般图像的特征。胡翔宇表示：「百度很早就开始重视数据方面的建设，借助搜索方面的经验与对网页数据的理解，我们可以获取很多优质的样本。抽取特征的全卷积网络会在百度内部的数据集上预训练，它的规模要比 ImageNet 大很多，特征抽取的效果也要好一些。」</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在训练过程中，特征抽取只会运行一次，因此一张输入图像最后只会输出一组特征图。这组特征图首先可以直接用于预测图像类别，其次我们可以截取特征图的一个小部分（包含所有通道）作为模型可能需要注意的区域。这样的截取其实就近似于在输入图像上截取一个部分（感受视野），只不过直接截取特征图不需要重新抽取特征，因此会极大地减少计算量。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在推断过程中，特征图的抽取需要执行很多次，但它们都能并行地处理。直观而言，训练学习到的注意力区域其实就相当于一种掩码，它表示该区域的特征对最终预测有很重要的作用。百度并行地从输入图像抽取各个重要部分和全局的特征图，并结合它们的信息预测精细类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 15px;">注意力区域</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在抽取特征后，模型需要学习哪些局部区域对最终分类是重要的，而确定重要性的标准即局部区域对最终预测是否有帮助。在这一阶段中，注意力网络会将基本的局部卷积特征图生成一张评分图或置信图，即通过叠加的两个卷积层将特征图转换为通道数为 1 的评分图。一般第一个卷积层可使用 64 个 3×3 的卷积核，而第二个卷积层可使用 1 个 3×3 的卷积核将通道数降为 1，这一张置信图展示了模型关注的兴趣点。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">胡翔宇说：「兴趣点是网络自己学到的，而裁剪的大小是我们给定的。我们首先会给一个 8×8 的较大裁剪窗口，相当于关注较大的区域。随着迭代的进行，我们会慢慢减小裁剪窗口，这相当于关注更小的细节。」裁剪后的特征图一般需要馈送到 Softmax 层以将置信图转换为概率图。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其实在训练或推断中直接截取输入图像作为注意的局部特征有最好的效果，但这种方法在训练中计算量太大，且因为分类网络的时序预测而不能并行处理。不过在推断中可以直接使用图像的局部区域，因为后面的分类网络允许并行计算。此外，百度通过强化学习确定注意力区域，这一点将在后面讨论。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 15px;">分类网络</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">最后的分类网络将结合全局图像信息和局部特征信息预测最终的精细类别，训练阶段和推断阶段的分类网络架构也不一样。这种架构上的区别主要在于训练阶段需要根据分类结果动态地调整注意力区域，而推断过程直接使用重要的注意力区域联合预测最终类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">胡翔宇表示：「在训练阶段中，前向传播的过程有点类似循环神经网络。首先模型会根据全局特征图预测图像类别，然后再结合第一个抽取的局部特征图重新预测类别，这里的方法可以简单地将前一次的 Softmax 结果与后一次的 Softmax 结果求平均。」最后，考虑加入第一个注意力区域后是否提升预测效果，我们可以确定该区域是否重要。这样继续添加新的局部注意力区域可以搜索所有重要的局部区域。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在推断中，我们可以并行地使用全卷积神经网络抽取全局特征图和所有重要局部特征图，然后馈送到 Softmax 中以分别计算出对应的分类概率。最后，我们可以采用简单的求均值方法对所有的分类器做集成处理，因此最终的分类将结合了所有的重要信息而提升精细类别的预测结果。</span></p><p style="text-align: center;"><strong> 模型训练</strong></p><p style="text-align: center;"><strong><br></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">模型中最重要的部分就是选取重要的局部区域，这也称为注意力问题。百度在这一部分采取了马尔科夫决策过程，在每一个时间步中，上文所述的全卷积注意力网络可作为智能体，它将基于观察采取行动并收到奖励。在百度的模型中，全卷积注意力网络观察到的信息就是输入图像与根据注意力区域裁剪的图像，而行动对应于选择新的注意力区域。根据行动是否提升分类准确度，我们可以调整奖励来确定最优的行动或注意力区域。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在前面全卷积注意力网络的架构中，训练过程中的分类网络非常类似于循环网络，即时间步 t 的分类结果将集成之前所有时间步的分类分数。如下所示为计算最终分类分数的表达式，我们可以简单地对所有时间步上的分类分数求平均：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.22095959595959597" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAcZrY6cWoiaLZJFqgMz46VV8IcOmapkyObicwWDG4ubDYYic2LqzbbCt7A/640?wx_fmt=png" data-type="png" data-w="792" style="width: 436px;height: 96px;"></p><p style="text-align: center;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其中 S_t 为时间步 t 以内所有预测分数的均值，</span><img class="" data-ratio="0.20935960591133004" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPA9P1kl3RSZVoVbmhicttsDwcrAyMvO6PGX7xfQyyIe6jEpumSDoQia5iag/640?wx_fmt=png" data-type="png" data-w="406" style="width: 78px;height: 17px;"> <span style="font-size: 15px;">表示在给定分类网络的参数 θ^τ 和第 τ 个注意力区域的特征图下，在时间步τ的预测分数。智能体 FCANs 在采取了动作（选择 l^t 作为注意力区域）后可获得奖励 r^t，一般 r^t 会通过度量预测分数 S_t 和真实标注 y 之间的匹配度获得。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">陈凯表示：「在技术上，百度的细粒度识别能做到在线学习以根据用户反馈实时更新参数，但在产品价值观上，细粒度识别并不会使用用户数据在线调整模型。」因此，以下的模型训练都是在百度服务器中进行的，训练好的模型直接部署到服务器并实现云端推断。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在训练过程中，我们没有标注哪些局部图像或特征比较重要，且奖励函数不可微，因此百度采用了强化学习方法来训练注意力区域。若给定一组训练样本，我们希望能同时优化特征抽取网络的参数 θ^f、注意力网络的参数 θ^l 和分类网络的参数 θ^c，并最大化以下目标函数：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.12121212121212122" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPALvOsAMkusxYK9hAbSjMrfPX7PKoJzWI9RwO29VaLDN9Lqm6oKoAuVg/640?wx_fmt=png" data-type="png" data-w="792" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其中 L(θ_f, θ_c) 为平均交叉熵损失函数，它度量了 N 个训练样本和 T 个时间步上的平均分类损失，而最大化负的交叉熵函数即最小化分类损失。此外，给定注意力区域，分类损失将只和特征抽取网络的与分类网络的参数相关。R(θ_f, θ_l) 表示 N 个训练完本和 T 个时间步上的平均期望奖励，最大化 R 将确定最重要的注意力区域。简单来说，该函数度量了选择注意力区域 l 时获得的期望奖励 E[r]，在每一个时间步上都选择最好的注意力区域将获得最大的期望奖励。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在最大化上述目标函数的过程中，奖励策略非常重要，因为不同的奖励 r 将直接影响到注意力区域的选取，从而进一步影响分类损失。奖励策略主要体现在选择确定 r 值上，一般直观的奖励策略可以将最终分类结果作为度量整体注意力区域选择策略的标准，即如果 t=T 且 </span><img class="" data-ratio="0.12267080745341614" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPA12AiaSUic7zdc2UmjPJXWmchuiaWYm4SSvrnR0IU2MR3YhZFlKMqg8few/640?wx_fmt=png" data-type="png" data-w="644" style="width: 136px;height: 20px;"><span style="font-size: 15px;">，则 r^t =1，否则 r^t 都等于 0。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">尽管使用以上朴素的奖励策略可以通过循环的方式学习，但它可能会造成不同时间步上选择区域的困难，且导致收敛问题。因此，百度提出了一种新型奖励策略：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.16287878787878787" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAVwGokZYIEgb6jL0JCeRiaQ9FCrTew7dibBVtkTHQ21LDmODPqMjBRlicA/640?wx_fmt=png" data-type="png" data-w="792" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其中 </span><img class="" data-ratio="0.9710144927536232" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAEyELfPkwmnPOKLj8icgH7Bk5mZO9TuMMNpkicaJ5hkkj6r0sbX2pkXUA/640?wx_fmt=png" data-type="png" data-w="69" style="width: 17px;height: 17px;"> <span style="font-size: 15px;">表示第 n 个样本在第 t 个时间步上的分类损失。如果图像在第一个时间步就分类正确，那么它能立即收到一个奖励 1，这相当于在仅使用全图特征的情况下能正确分类。当我们奖励不同的注意力区域时，我们要求当前时间步需要分类正确，且同时分类损失相比上一时间步有降低。如果不满足这两个条件，我们将不奖励注意力区域。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">胡翔宇解释说：「对于细粒度分类问题，裁剪出来的区域是好是坏比较容易确定。例如裁剪出一个局部细节，且它已经足够证明图像是怎样的细分类，那么我们就可以确定这个区域是有价值的。其实自然图像的信息冗余度非常大，甚至只需要 1 到 2 个细节就能帮助我们识别花的具体品种，那么我们也认为这样的策略是优秀的。」</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">最后，结合上述所有过程就可构建整个模型的前向传播与反向传播过程。如下图所示，在前向传播的过程中，我们先使用全卷积神经网络 φ 抽取输入图像的特征图，然后使用注意力网络将多通道的特征图变换到单通道的置信图π。在依据置信图对完整特征图进行裁剪后，我们就得到了注意力区域 l。将注意力区域投入到分类网络就能计算出分类结果，并且当前时间步的结果和上一时间步的结果相结合就能给出对该注意力区域的奖励。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="1.2459016393442623" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAibQdztnibUOYBcHprc6gvLrC9l2CJVTecdeO8NRRLh8H8toNZ37XX0vA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在反向传播中，上文（2）式的目标函数需要对参数求梯度以将误差向前传，并更新参数。其中 L 是经典交叉熵损失函数，它的梯度很容易计算，而奖励的平均期望 R 是不可微的，所以百度采用了蒙特卡洛方法来逼近期望奖励 E[r] 的梯度。因此在上图（b）中，分类结果 s 将按常规实现反向传播，而期望奖励的梯度可直接调整注意力网络的参数，并向前传递调整特征卷积网络的参数。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">百度基于全卷积注意力网络和强化学习构建的细粒度识别应用目前已经部署到了云端，模型的实现都是通过 PaddlePaddle 架构完成的。胡翔宇表示：「该应用目前部署在云端，但随着移动端硬件的发展，我们会在一些场景中将网络部署到更靠近用户的地方。但即使部署在云端，考虑到推断的性价比，我们也会采用一些模型压缩方法。这些压缩方法一般可以分为两大类，首先是采用半精度（FP16）等和硬件相关的方法，其次就是将批归一化的参数压缩到卷积层等和硬件无关的方法。」</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">说到移动端的部署，百度其实有开源 PaddlePaddle Mobile 框架，陈凯表示：「PaddlePaddle 的移动端框架其实也在和移动硬件的厂商合作，它后续会加强对 NPU 等硬件的优化，包括内存管理和功耗控制等。」<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);text-align: justify;white-space: normal;font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></span></p><p><br></p><p style="max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心原创，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="871998185" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                        
            <ul id="js_hotspot_area" class="article_extend_area"></ul>


            
                        <div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>

        
        <div class="rich_media_area_extra">

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_friend_cmt_area" style="display:none">
              
              
              
            </div>

                        <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_area" style="display:none">
            </div>
                    </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div><div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    专访 | 五一出游赏花，如何优雅地解释百度细粒度识别方案                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                        <span id="copyright_logo" class="rich_media_meta meta_original_tag">原创</span>
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-28</em>

                                        <em class="rich_media_meta rich_media_meta_text">思源</em>
                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;caret-color: rgb(62, 62, 62);font-size: 16px;white-space: normal;line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color:#ffffff;"><span style="background-color: rgb(117, 117, 118);">机器之心原创</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：思源</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="color: rgb(136, 136, 136);font-size: 14px;">近日，百度发布了用于花卉识别的移动端应用，这种基于全卷积注意力网络的细粒度识别方法在计算和准确度上都有非常强大的优势。在百度主任研发架构师陈凯和资深研发工程师胡翔宇的解释下，本文首先将介绍什么是细粒度识别，以及一般的细粒度识别方法，然后重点解析百度基于强化学习和全卷积注意力网络的细粒度识别模型</span><span style="color: rgb(136, 136, 136);font-size: 14px;">。五一出游赏花，为女朋友解释解释细粒度识别也是极好的。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">细粒度识别的概念其实非常简单，即模型需要识别非常精细的子类别。例如百度的花卉识别应用，模型不仅需要如一般识别问题那样检测出物体是不是花，同时还需要检测出物体具体属于哪一品种的花。而通常属于不同子类别的物体是非常相似的，例如樱花和桃花等，我们不仅需要花的整体信息来识别它是「花」，同时还需要局部信息来确定「花」的品种。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">从这一观点出发，细粒度识别基本上就是同时使用全局信息和局部信息的分类任务。对于花的细粒度识别来说，全局信息就是用户拍摄的整张图像，而局部信息则是图像中的花或花的重要部位。这两部分信息都包括在整张图像中，我们希望模型根据整张图像预测出具体的细分类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">百度借助了知识图谱对世界上的花卉名字进行科学的科、属、种划分，建立了一个非常专业的花卉类别库。而对于其它巨量优质的花卉图像，标注人员通过权威样本库中的文字描述，并在中科院老师的帮助下，根据花卉的叶子、形状、颜色等微观特征进行挑选与标注。此外，百度还进行了标注质量的检查，标注准确率在 95% 以上。最后，这些巨量的标注数据包含了花卉的整体图像和对应的精细品种。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其实整体图像隐含地包括了全局信息与局部信息，但一般来说，局部信息同样也需要标注，而利用这些局部标注的方法即基于强监督信息的细粒度识别。不过这种局部标注是非常昂贵的，因此很多模型尝试自己学习重要的局部区域，这一类方法即基于弱监督信息的细粒度识别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">百度所采用的全卷积注意力网络是一种基于弱监督信息的细粒度识别方法，它不需要标注局部特征，并通过下文描述的全卷积注意力网络和马尔科夫决策过程自动学习重要的区域。</span></p><p style="text-align: left;"><span style="font-size: 15px;"><strong>细粒度识别</strong></span></p><p style="text-align: center;"><strong><br></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">如上所述，细粒度识别可分为强监督和弱监督两种。最开始优秀的模型一般都是基于强监督的，它们在测试集上有更好的性能。但基于强监督的方法标注成本非常高，且局部标注在遇到遮挡等情况时就会完全失去作用，因此更多的研究者开始探讨基于弱监督的细粒度识别方法。以下简单地介绍了这两种方法的实例，即基于强监督的 Part-based R-CNN 和弱监督的 Bilinear CNN。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">基于强监督信息的细粒度识别通常需要使用边界框和局部标注信息，例如 2014 年提出的 Part-based R-CNN 利用自底向上的候选区域（region proposals）计算深度卷积特征而实现细粒度识别。这种方法会学习建模局部外观，并加强局部信息之间的几何约束。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5384615384615384" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAlBX352c8hkibwza4vXWL8yLqX7K7EaAyZHKpUmyY37u6MnjVU8XBOxA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">如上所示，局部区域将从自底向上的候选区域开始（左上角），我们将基于深度卷积特征同时训练目标和局部区域。在推断阶段，所有的窗口都会由检测器评分（中间），且我们可以应用非参几何约束（底部）重新评估窗口并选择最优的目标和局部检测（右上角）。最后的步骤就是抽取局部语义信息来用于细粒度识别，并训练一个分类器预测最终类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">而基于弱监督信息的细粒度识别也有很多方法，例如 15 年提出的 Bilinear CNN，它由两个特征提取器组成，它们在每个图像位置的输出使用外积相乘，并池化以获得一个图像描述子。这种架构能以转译不变的方式对局部成对特征的交互进行建模，这对于细粒度识别非常重要。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">Bilinear CNN 所使用的外积捕捉了特征通道之间的成对相关性，因此可以建模局部特征的交互，即如果一个网络是局部检测器，那么另一个网络就是局部特征抽取器。以下展示了用于细粒度分类的 Bilinear CNN 模型，在推断过程中，一张图像将传入两个卷积神经网络 A 和 B，然后对应位置的外积相乘得出双线性向量，并传入 Softmax 层以完成分类。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img data-copyright="0" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAC0iasIBNdwvVa3IIPKpkpajaqk0Vices3rdrjnnqrIUXtiaCqbj6fo6Kw/640?wx_fmt=png" data-type="png" style="" class="" data-ratio="0.5044136191677175" data-w="793"></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span><span style="font-size: 15px;">百度所采用的细粒度识别方法也是一种使用弱监督的策略，它通过全卷积注意力网络实现类别预测，并基于强化学习调整需要注意的局部区域。</span></p><p style="text-align: center;"><strong>全卷积注意力网络</strong></p><p style="text-align: center;"><strong><br></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">尽管最近研究社区将目光投向弱监督的细粒度识别方法，但它的效果和实践上都有一些差距。百度的细粒度模型通过利用强化学习选择注意区域而大大减少了强监督所需要的图像标注量，且还能以非常高的准确度部署到应用中。他们构建一种全卷积注意网络，并根据马尔科夫决策过程确定哪些局部图像块对最终预测有帮助，因此图像的细粒度分类将同时利用全局图像和那些重要的局部图像完成预测。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">百度提出的这种架构首先基于弱监督强化学习而不需要昂贵的标注，其次它所采用的全卷积网络可以加速训练和推断过程，最后贪婪的奖励策略可以加速学习的收敛，这三点也是百度细粒度模型的显著特点。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">胡翔宇表示：「这种网络其实和人类的视觉系统非常像，在我们识别细分类别时，首先会查看整体特征而确定大概的类别，然后再仔细观察有区分度的特征确定细分类别。」百度的全卷积注意力网络（FCANs）同样首先会抽取整张图的特征进行分类，然后截取一小块特征图（Feature Map）作为当前网络注意的区域，当这样的区域是具有区分度的特征时，模型就更能正确预测出细分类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 15px;">模型架构</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">该模型主要的架构如下图所示可以分为特征抽取阶段、局部特征注意区域和分类过程。总体而言模型会先使用预训练卷积神经网络抽取图像特征，然后再结合整体图像信息和局部重要信息预测最终类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5825977301387137" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPACbkibNvAOPtJnVvZMyf4sK4EmayBxQ4k4xut74ayeDClxldLXYT2vgw/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">如上所示，在下部分的训练过程中，抽取的特征图会在所有通道上做一些截取，截取的部分就相当于模型关注的局部原始图像。如果说这个截取在最后的分类阶段中提升了预测效果，那么它就是值得注意的区域。在上部分的推断过程中，模型将利用输入图像的全局特征和所有重要的局部图像预测精细类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 15px;">特征图的抽取</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">特征图会通过全卷积神经网络进行抽取，一般我们可以选择 VGG 或 ResNet 等流行的网络架构，并在 ImageNet 数据集上预训练而能抽取一般图像的特征。胡翔宇表示：「百度很早就开始重视数据方面的建设，借助搜索方面的经验与对网页数据的理解，我们可以获取很多优质的样本。抽取特征的全卷积网络会在百度内部的数据集上预训练，它的规模要比 ImageNet 大很多，特征抽取的效果也要好一些。」</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在训练过程中，特征抽取只会运行一次，因此一张输入图像最后只会输出一组特征图。这组特征图首先可以直接用于预测图像类别，其次我们可以截取特征图的一个小部分（包含所有通道）作为模型可能需要注意的区域。这样的截取其实就近似于在输入图像上截取一个部分（感受视野），只不过直接截取特征图不需要重新抽取特征，因此会极大地减少计算量。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在推断过程中，特征图的抽取需要执行很多次，但它们都能并行地处理。直观而言，训练学习到的注意力区域其实就相当于一种掩码，它表示该区域的特征对最终预测有很重要的作用。百度并行地从输入图像抽取各个重要部分和全局的特征图，并结合它们的信息预测精细类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 15px;">注意力区域</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在抽取特征后，模型需要学习哪些局部区域对最终分类是重要的，而确定重要性的标准即局部区域对最终预测是否有帮助。在这一阶段中，注意力网络会将基本的局部卷积特征图生成一张评分图或置信图，即通过叠加的两个卷积层将特征图转换为通道数为 1 的评分图。一般第一个卷积层可使用 64 个 3×3 的卷积核，而第二个卷积层可使用 1 个 3×3 的卷积核将通道数降为 1，这一张置信图展示了模型关注的兴趣点。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">胡翔宇说：「兴趣点是网络自己学到的，而裁剪的大小是我们给定的。我们首先会给一个 8×8 的较大裁剪窗口，相当于关注较大的区域。随着迭代的进行，我们会慢慢减小裁剪窗口，这相当于关注更小的细节。」裁剪后的特征图一般需要馈送到 Softmax 层以将置信图转换为概率图。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其实在训练或推断中直接截取输入图像作为注意的局部特征有最好的效果，但这种方法在训练中计算量太大，且因为分类网络的时序预测而不能并行处理。不过在推断中可以直接使用图像的局部区域，因为后面的分类网络允许并行计算。此外，百度通过强化学习确定注意力区域，这一点将在后面讨论。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><strong><span style="font-size: 15px;">分类网络</span></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">最后的分类网络将结合全局图像信息和局部特征信息预测最终的精细类别，训练阶段和推断阶段的分类网络架构也不一样。这种架构上的区别主要在于训练阶段需要根据分类结果动态地调整注意力区域，而推断过程直接使用重要的注意力区域联合预测最终类别。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">胡翔宇表示：「在训练阶段中，前向传播的过程有点类似循环神经网络。首先模型会根据全局特征图预测图像类别，然后再结合第一个抽取的局部特征图重新预测类别，这里的方法可以简单地将前一次的 Softmax 结果与后一次的 Softmax 结果求平均。」最后，考虑加入第一个注意力区域后是否提升预测效果，我们可以确定该区域是否重要。这样继续添加新的局部注意力区域可以搜索所有重要的局部区域。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在推断中，我们可以并行地使用全卷积神经网络抽取全局特征图和所有重要局部特征图，然后馈送到 Softmax 中以分别计算出对应的分类概率。最后，我们可以采用简单的求均值方法对所有的分类器做集成处理，因此最终的分类将结合了所有的重要信息而提升精细类别的预测结果。</span></p><p style="text-align: center;"><strong> 模型训练</strong></p><p style="text-align: center;"><strong><br></strong></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">模型中最重要的部分就是选取重要的局部区域，这也称为注意力问题。百度在这一部分采取了马尔科夫决策过程，在每一个时间步中，上文所述的全卷积注意力网络可作为智能体，它将基于观察采取行动并收到奖励。在百度的模型中，全卷积注意力网络观察到的信息就是输入图像与根据注意力区域裁剪的图像，而行动对应于选择新的注意力区域。根据行动是否提升分类准确度，我们可以调整奖励来确定最优的行动或注意力区域。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在前面全卷积注意力网络的架构中，训练过程中的分类网络非常类似于循环网络，即时间步 t 的分类结果将集成之前所有时间步的分类分数。如下所示为计算最终分类分数的表达式，我们可以简单地对所有时间步上的分类分数求平均：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.22095959595959597" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAcZrY6cWoiaLZJFqgMz46VV8IcOmapkyObicwWDG4ubDYYic2LqzbbCt7A/640?wx_fmt=png" data-type="png" data-w="792" style="width: 436px;height: 96px;"></p><p style="text-align: center;"><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其中 S_t 为时间步 t 以内所有预测分数的均值，</span><img class="" data-ratio="0.20935960591133004" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPA9P1kl3RSZVoVbmhicttsDwcrAyMvO6PGX7xfQyyIe6jEpumSDoQia5iag/640?wx_fmt=png" data-type="png" data-w="406" style="width: 78px;height: 17px;"> <span style="font-size: 15px;">表示在给定分类网络的参数 θ^τ 和第 τ 个注意力区域的特征图下，在时间步τ的预测分数。智能体 FCANs 在采取了动作（选择 l^t 作为注意力区域）后可获得奖励 r^t，一般 r^t 会通过度量预测分数 S_t 和真实标注 y 之间的匹配度获得。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">陈凯表示：「在技术上，百度的细粒度识别能做到在线学习以根据用户反馈实时更新参数，但在产品价值观上，细粒度识别并不会使用用户数据在线调整模型。」因此，以下的模型训练都是在百度服务器中进行的，训练好的模型直接部署到服务器并实现云端推断。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在训练过程中，我们没有标注哪些局部图像或特征比较重要，且奖励函数不可微，因此百度采用了强化学习方法来训练注意力区域。若给定一组训练样本，我们希望能同时优化特征抽取网络的参数 θ^f、注意力网络的参数 θ^l 和分类网络的参数 θ^c，并最大化以下目标函数：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.12121212121212122" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPALvOsAMkusxYK9hAbSjMrfPX7PKoJzWI9RwO29VaLDN9Lqm6oKoAuVg/640?wx_fmt=png" data-type="png" data-w="792" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其中 L(θ_f, θ_c) 为平均交叉熵损失函数，它度量了 N 个训练样本和 T 个时间步上的平均分类损失，而最大化负的交叉熵函数即最小化分类损失。此外，给定注意力区域，分类损失将只和特征抽取网络的与分类网络的参数相关。R(θ_f, θ_l) 表示 N 个训练完本和 T 个时间步上的平均期望奖励，最大化 R 将确定最重要的注意力区域。简单来说，该函数度量了选择注意力区域 l 时获得的期望奖励 E[r]，在每一个时间步上都选择最好的注意力区域将获得最大的期望奖励。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在最大化上述目标函数的过程中，奖励策略非常重要，因为不同的奖励 r 将直接影响到注意力区域的选取，从而进一步影响分类损失。奖励策略主要体现在选择确定 r 值上，一般直观的奖励策略可以将最终分类结果作为度量整体注意力区域选择策略的标准，即如果 t=T 且 </span><img class="" data-ratio="0.12267080745341614" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPA12AiaSUic7zdc2UmjPJXWmchuiaWYm4SSvrnR0IU2MR3YhZFlKMqg8few/640?wx_fmt=png" data-type="png" data-w="644" style="width: 136px;height: 20px;"><span style="font-size: 15px;">，则 r^t =1，否则 r^t 都等于 0。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">尽管使用以上朴素的奖励策略可以通过循环的方式学习，但它可能会造成不同时间步上选择区域的困难，且导致收敛问题。因此，百度提出了一种新型奖励策略：</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.16287878787878787" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAVwGokZYIEgb6jL0JCeRiaQ9FCrTew7dibBVtkTHQ21LDmODPqMjBRlicA/640?wx_fmt=png" data-type="png" data-w="792" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">其中 </span><img class="" data-ratio="0.9710144927536232" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAEyELfPkwmnPOKLj8icgH7Bk5mZO9TuMMNpkicaJ5hkkj6r0sbX2pkXUA/640?wx_fmt=png" data-type="png" data-w="69" style="width: 17px;height: 17px;"> <span style="font-size: 15px;">表示第 n 个样本在第 t 个时间步上的分类损失。如果图像在第一个时间步就分类正确，那么它能立即收到一个奖励 1，这相当于在仅使用全图特征的情况下能正确分类。当我们奖励不同的注意力区域时，我们要求当前时间步需要分类正确，且同时分类损失相比上一时间步有降低。如果不满足这两个条件，我们将不奖励注意力区域。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">胡翔宇解释说：「对于细粒度分类问题，裁剪出来的区域是好是坏比较容易确定。例如裁剪出一个局部细节，且它已经足够证明图像是怎样的细分类，那么我们就可以确定这个区域是有价值的。其实自然图像的信息冗余度非常大，甚至只需要 1 到 2 个细节就能帮助我们识别花的具体品种，那么我们也认为这样的策略是优秀的。」</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">最后，结合上述所有过程就可构建整个模型的前向传播与反向传播过程。如下图所示，在前向传播的过程中，我们先使用全卷积神经网络 φ 抽取输入图像的特征图，然后使用注意力网络将多通道的特征图变换到单通道的置信图π。在依据置信图对完整特征图进行裁剪后，我们就得到了注意力区域 l。将注意力区域投入到分类网络就能计算出分类结果，并且当前时间步的结果和上一时间步的结果相结合就能给出对该注意力区域的奖励。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="1.2459016393442623" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8H2Xghj59KZqesWzoictPPAibQdztnibUOYBcHprc6gvLrC9l2CJVTecdeO8NRRLh8H8toNZ37XX0vA/640?wx_fmt=png" data-type="png" data-w="793" style=""></p><p><br></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">在反向传播中，上文（2）式的目标函数需要对参数求梯度以将误差向前传，并更新参数。其中 L 是经典交叉熵损失函数，它的梯度很容易计算，而奖励的平均期望 R 是不可微的，所以百度采用了蒙特卡洛方法来逼近期望奖励 E[r] 的梯度。因此在上图（b）中，分类结果 s 将按常规实现反向传播，而期望奖励的梯度可直接调整注意力网络的参数，并向前传递调整特征卷积网络的参数。</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">百度基于全卷积注意力网络和强化学习构建的细粒度识别应用目前已经部署到了云端，模型的实现都是通过 PaddlePaddle 架构完成的。胡翔宇表示：「该应用目前部署在云端，但随着移动端硬件的发展，我们会在一些场景中将网络部署到更靠近用户的地方。但即使部署在云端，考虑到推断的性价比，我们也会采用一些模型压缩方法。这些压缩方法一般可以分为两大类，首先是采用半精度（FP16）等和硬件相关的方法，其次就是将批归一化的参数压缩到卷积层等和硬件无关的方法。」</span></p><p style="text-align: justify;line-height: 1.75em;margin-bottom: 20px;"><span style="font-size: 15px;">说到移动端的部署，百度其实有开源 PaddlePaddle Mobile 框架，陈凯表示：「PaddlePaddle 的移动端框架其实也在和移动硬件的厂商合作，它后续会加强对 NPU 等硬件的优化，包括内存管理和功耗控制等。」<img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="caret-color: rgb(62, 62, 62);color: rgb(62, 62, 62);text-align: justify;white-space: normal;font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></span></p><p><br></p><p style="max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心原创，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="871998185" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><ul id="js_hotspot_area" class="article_extend_area"></ul><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
