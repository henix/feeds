<div class="title-image"><img src="https://pic4.zhimg.com/v2-cef81a3260508c53915ec503c5fa9475_b.jpg" alt=""></div><p>导语：</p><p>可能大家都知道前段时间非常火的alpha-go战胜李世石的故事，alpha-go算法就是一个对强化学习应用的炉火纯青的大师。强化学习能够帮助alpha-go战胜人类中的顶尖棋手，那么强化学习是什么呢？能不能帮助我们进行投资呢？我们通过一个小例子介绍一下。</p><h4>1.强化学习介绍</h4><p>一般来说，机器学习分为监督学习（Supervised learning），非监督学习（Unsupervised learning）以及强化学习（Reinforcement learning）三类。与监督学习，非监督学习不同，强化学习是一种多阶段的接收环境反馈的机器学习方法。强化学习的学习目标是从环境状态到行为映射关系，从而使得系统的一系列行为从环境中获得的累计奖赏最大（损失最小）。</p><figure><noscript><img src="https://pic2.zhimg.com/v2-0eef62c54a9bfd9cfb9b7b48a135c335_b.png" data-rawwidth="480" data-rawheight="280" class="origin_image zh-lightbox-thumb" width="480" data-original="https://pic2.zhimg.com/v2-0eef62c54a9bfd9cfb9b7b48a135c335_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-0eef62c54a9bfd9cfb9b7b48a135c335_b.png" data-rawwidth="480" data-rawheight="280" class="origin_image zh-lightbox-thumb lazy" width="480" data-original="https://pic2.zhimg.com/v2-0eef62c54a9bfd9cfb9b7b48a135c335_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-0eef62c54a9bfd9cfb9b7b48a135c335_b.png"/></figure><p>强化学习的应用范围非常广泛，人们可以利用强化学习来控制直升机的飞行姿态，使得直升机可以按照既定的路线飞行；也可以利用强化学习来控制机器人，使机器人保持站立状态，不会摔倒；还可以利用强化学习学习打游戏，下围棋，学习之后的算法能够完爆人类能力；当然还可以利用强化学习来做股票交易，这里我们举一个例子</p><p>假设我们需要在一个固定的时间内买入一定数量的股票，我们希望买入的总价尽可能的小。很显然我们不能直接扔一个大单到市场里，原因有二：首先，这样直接下单会造成冲击成本。其次，就算没不考虑冲击成本，当前时间单也不一定是价格最优的，后面可能出现更低的价格。一个比较好的方案是一直下限价单，只要控制好限价单的价格，就可以比较好的完成任务。</p><p>下面通过一个例子来介绍强化学习的结构及其所需的要素。</p><h4>2.强化学习结构</h4><p><figure><noscript><img src="https://pic1.zhimg.com/v2-13b0c90e7e74c0edb53ea024f3068b34_b.png" data-rawwidth="570" data-rawheight="511" class="origin_image zh-lightbox-thumb" width="570" data-original="https://pic1.zhimg.com/v2-13b0c90e7e74c0edb53ea024f3068b34_r.jpg"/></noscript><img src="https://pic1.zhimg.com/v2-13b0c90e7e74c0edb53ea024f3068b34_b.png" data-rawwidth="570" data-rawheight="511" class="origin_image zh-lightbox-thumb lazy" width="570" data-original="https://pic1.zhimg.com/v2-13b0c90e7e74c0edb53ea024f3068b34_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-13b0c90e7e74c0edb53ea024f3068b34_b.png"/></figure>强化学习结构<br/>强化学习系统接受环境状态的输入s，系统输出相应的行为动作a。环境在系统输出动作a作用下，变迁到新的状态s′。系统接受环境新状态的输入，同时得到环境对于系统的瞬时奖惩反馈r。对于强化学习系统来讲，其目标是学习一个行为策略π：S→A，其中，S是环境状态空间，A是动作空间。使系统选择的动作能够获得环境奖赏的累计值最大（也就是损失函数最小）。<br/></p><figure><noscript><img src="https://pic4.zhimg.com/v2-7c4d61c93f05eb41e4d58d85f93de067_b.png" data-rawwidth="558" data-rawheight="511" class="origin_image zh-lightbox-thumb" width="558" data-original="https://pic4.zhimg.com/v2-7c4d61c93f05eb41e4d58d85f93de067_r.jpg"/></noscript><img src="https://pic4.zhimg.com/v2-7c4d61c93f05eb41e4d58d85f93de067_b.png" data-rawwidth="558" data-rawheight="511" class="origin_image zh-lightbox-thumb lazy" width="558" data-original="https://pic4.zhimg.com/v2-7c4d61c93f05eb41e4d58d85f93de067_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-7c4d61c93f05eb41e4d58d85f93de067_b.png"/></figure><p>买入股票强化学习结构<br/>在本文例子中，强化学习系统接受环境状态的输入s，也就是剩余的订单量和剩余时间，系统输出相应的最佳动作a，也就是最佳的限价单价格。随着限价单的执行，剩余的订单量会减少，而且剩余时间也会减少，环境就变迁到了新的状态s‘。系统接收新的环境状态输入s’，同时将上一个限价单的执行成本，成交成本(成交部分)作为系统的瞬时奖惩反馈r也反馈给强化学习系统。对于强化学习系统来说，目标就是学习一个多阶段的行为策略π：S→A。系统能够根据剩余订单量以及剩余时间，确定当前的最佳的限价单价格，从而使全部订单成交成本最低。</p><h4>3. 强化学习状态到动作的学习机制</h4><p>强化学习的目的是学习一个状态到动作的映射，这一节介绍下强化学习的一种学习策略——动态规划。<br/>现在我们把买入股票问题细化为要在5分钟内买入5手某股票。每隔一分钟，我们可以查看一次现在的状态，修改限价单的价格，环境状态确定为剩余时间以及未成交股票手数。动作为当前状态下限价单价格。强化学习的目标是在规定的时间内买到所有的股票，并且付出的成本最小。</p><p>我们的状态空间：</p><p><figure><noscript><img src="https://pic3.zhimg.com/v2-148284016c110bdd55fbcfc8861e2a62_b.png" data-rawwidth="650" data-rawheight="127" class="origin_image zh-lightbox-thumb" width="650" data-original="https://pic3.zhimg.com/v2-148284016c110bdd55fbcfc8861e2a62_r.jpg"/></noscript><img src="https://pic3.zhimg.com/v2-148284016c110bdd55fbcfc8861e2a62_b.png" data-rawwidth="650" data-rawheight="127" class="origin_image zh-lightbox-thumb lazy" width="650" data-original="https://pic3.zhimg.com/v2-148284016c110bdd55fbcfc8861e2a62_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-148284016c110bdd55fbcfc8861e2a62_b.png"/></figure>动作空间：<br/></p><figure><noscript><img src="https://pic2.zhimg.com/v2-21a5aa9bf824f6ee159eb6ede4d836bd_b.png" data-rawwidth="648" data-rawheight="33" class="origin_image zh-lightbox-thumb" width="648" data-original="https://pic2.zhimg.com/v2-21a5aa9bf824f6ee159eb6ede4d836bd_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-21a5aa9bf824f6ee159eb6ede4d836bd_b.png" data-rawwidth="648" data-rawheight="33" class="origin_image zh-lightbox-thumb lazy" width="648" data-original="https://pic2.zhimg.com/v2-21a5aa9bf824f6ee159eb6ede4d836bd_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-21a5aa9bf824f6ee159eb6ede4d836bd_b.png"/></figure><p>学习的目标就是状态空间到动作空间的对应关系，例如在剩余4分钟，还剩余5手的情况下，应该以当前价格下限价单还是以当前价格 + 2个点差的价格下限价单。</p><p>因为我们的目标是让成交价格更低，可以让成交金额作为我们的损失函数（也就是环境奖赏的累计值），很显然，在某一个状态中，我们只需要选取损失函数最小的动作即可，损失函数用L函数表示。损失函数中参数中前两个变量是一组描述状态空间的变量，最后一个变量是描述动作状态的变量。L(5,5,0)表示剩余5分钟，剩余5手，按照当前价格执行的损失函数值。BL函数表示采用最优动作之后的损失函数，BL(5,5)表示L(5,5,-2)到L(5,5,2)中的最小值。</p><p>剩余5分钟，剩余5手，按照当前价格下限价单，一分钟之后可能的状态可能有多种，可能是剩余4分钟，剩余5手，也可能是剩余4分钟，剩余0手。我们用<img src="https://www.zhihu.com/equation?tex=P_%7BSS%27%7D%5E%7Ba%7D+" alt="P_{SS&#39;}^{a} " eeimg="1"/>表示从S状态，采用动作a，到达S‘状态的概率。<img src="https://www.zhihu.com/equation?tex=r_%7BSS%27%7D%5E%7Ba%7D+" alt="r_{SS&#39;}^{a} " eeimg="1"/>表示从S状态，采用动作a，到达S’状态的瞬时奖惩反馈。也就是成交的数量与成交价格的乘积。</p><figure><noscript><img src="https://pic3.zhimg.com/v2-9dac27adc281ccfe4d1dd5b006fe612a_b.png" data-rawwidth="314" data-rawheight="169" class="content_image" width="314"/></noscript><img src="https://pic3.zhimg.com/v2-9dac27adc281ccfe4d1dd5b006fe612a_b.png" data-rawwidth="314" data-rawheight="169" class="content_image lazy" width="314" data-actualsrc="https://pic3.zhimg.com/v2-9dac27adc281ccfe4d1dd5b006fe612a_b.png"/></figure><p>从上式可以看出，要想知道5分钟，5手时的最优动作，我们需要知道4分钟，0-5手情况下的最优动作以及最小损失。很显然，这种情况下，我们可以利用动态规划的方法帮助我们解决这个问题。</p><p>我们先确定0分钟，0-5手的损失函数，由于0分钟表示时间已经用完，损失函数可以直接按照固定价格执行，表示对于超时的惩罚，比如可以使用当日涨停价成交，或者按照当日的最高价成交等，或者直接设置为无穷大。之后通过0分钟时的信息，更新1分钟时的信息，直到5分钟时的信息，从而得到目前的最优策略。</p><p><figure><noscript><img src="https://pic2.zhimg.com/v2-0d60f31e1ab3f4dd877e6b457bdf6305_b.png" data-rawwidth="937" data-rawheight="451" class="origin_image zh-lightbox-thumb" width="937" data-original="https://pic2.zhimg.com/v2-0d60f31e1ab3f4dd877e6b457bdf6305_r.jpg"/></noscript><img src="https://pic2.zhimg.com/v2-0d60f31e1ab3f4dd877e6b457bdf6305_b.png" data-rawwidth="937" data-rawheight="451" class="origin_image zh-lightbox-thumb lazy" width="937" data-original="https://pic2.zhimg.com/v2-0d60f31e1ab3f4dd877e6b457bdf6305_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-0d60f31e1ab3f4dd877e6b457bdf6305_b.png"/></figure>需要注意的是：强化学习提供的是一套框架，他的学习对象是环境到动作的映射。它的学习方式就是通过损失函数来决定最优动作。有多种损失函数的更新方法，常用的损失函数更新方法有动态规划方法（上面粗略介绍了），Monte Carlo采样法，时间差分学习法。大家可以根据自己的需要进行学习。</p><p>到JoinQuant查看策略并与作者交流讨论：<a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//www.joinquant.com/post/2663%3Ff%3Dzh" target="_blank" rel="nofollow noreferrer">【量化课堂】强化学习入门</a></p>