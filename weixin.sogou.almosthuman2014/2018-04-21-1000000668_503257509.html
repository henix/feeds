<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>IJCAI 2018 | 海康威视Oral论文：分层式共现网络，实现更好的动作识别和检测</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1525110987&amp;src=3&amp;ver=1&amp;signature=zvKw2e*4FnEj-zDhL3Iecm**yVkLQyN1kvPfmxDX50hQMV6a7wdRfepEOv6h3DB5aOxJszJl1I9KgpTLMzylFvTQoeGrcbvM-xrBTs9uDB4UmogkbB2cIrxhPEv8fHCX5di8BnhTjJNC1tVEF9isFqKHuATKTAnMsO0Z743k-Aw=">原文</a></p>
<div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        <div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    IJCAI 2018 | 海康威视Oral论文：分层式共现网络，实现更好的动作识别和检测                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-21</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="font-size: 14px;text-align: justify;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12.000000953674316px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><span style="font-size: 12px;"><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="text-align: justify;">Chao Li等</span></strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">参与：Panda</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 14px;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">动作识别和检测正得到计算机视觉领域越来越多的关注。近日，海康威视在 arXiv 发布了在这方面的一项实现了新的最佳表现的研究成果，该论文也是 IJCAI 2018 Oral 论文。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">动作识别和检测等对人类行为的分析是计算机视觉领域一个基础而又困难的任务，也有很广泛的应用范围，比如智能监控系统、人机交互、游戏控制和机器人。铰接式的人体姿态（也被称为骨架（skeleton））能为描述人体动作提供非常好的表征。一方面，骨架数据在背景噪声中具有固有的稳健性，并且能提供人体动作的抽象信息和高层面特征。另一方面，与 RGB 数据相比，骨架数据的规模非常小，这让我们可以设计出轻量级且硬件友好的模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本论文关注的是基于骨架的人体动作识别和检测问题（图 1）。骨架的相互作用和组合在描述动作特征上共同发挥了关键性作用。有很多早期研究都曾试图根据骨架序列来设计和提取共现特征（co-occurrence feature），比如每个关节的配对的相对位置 [Wang et al., 2014]、配对关节的空间方向 [Jin and Choi, 2012]、Cov3DJ [Hussein et al., 2013] 和 HOJ3D [Xia et al., 2012] 等基于统计的特征。另一方面，带有长短期记忆（LSTM）神经元的循环神经网络（RNN）也常被用于建模骨架的时间序列 [Shahroudy et al., 2016; Song et al., 2017; Liu et al., 2016]。尽管 LSTM 网络就是为建模长期的时间依赖关系而设计的，但由于时间建模是在原始输入空间上完成的，所以它们难以直接从骨架上学习到高层面的特征 [Sainath et al., 2015]。而全连接层则有能力聚合所有输入神经元的全局信息，进而可以学习到共现特征。[Zhu et al., 2016] 提出了一种端到端的全连接深度 LSTM 网络来根据骨架数据学习共现特征。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.4051841746248295" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTWTzYT01LnSqAVib3CsjxcicicZ3zakibGJJ1hTFiaXIwICeQib6qM2BGtwSg/640?wx_fmt=png" data-type="png" data-w="733" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 1：基于骨架的人体动作识别的工作流程</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">CNN 模型在提取高层面信息方面能力出色，并且也已经被用于根据骨架学习空间-时间特征 [Du et al., 2016; Ke et al., 2017]。这些基于 CNN 的方法可以通过将时间动态和骨架关节分别编码成行和列而将骨架序列表示成一张图像，然后就像图像分类一样将图像输入 CNN 来识别其中含有的动作。但是，在这种情况下，只有卷积核内的相邻关节才被认为是在学习共现特征。尽管感受野（receptive field）能在之后的卷积层中覆盖骨架的所有关节，但我们很难有效地从所有关节中挖掘共现特征。由于空间维度中的权重共享机制，CNN 模型无法为每个关节都学习自由的参数。这促使我们设计一个能获得所有关节的全局响应的模型，以利用不同关节之间的相关性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们提出了一种端到端的共现特征学习框架，其使用了 CNN 来自动地从骨架序列中学习分层的共现特征。我们发现一个卷积层的输出是来自所有输入通道的全局响应。如果一个骨架的每个关节都被当作是一个通道，那么卷积层就可以轻松地学习所有关节的共现。更具体而言，我们将骨架序列表示成了一个形状帧×关节×3（最后一维作为通道）的张量。我们首先使用核大小为 n×1 的卷积层独立地为每个关节学习了点层面的特征。然后我们再将该卷积层的输出转置，以将关节的维度作为通道。在这个转置运算之后，后续的层分层地聚合来自所有关节的全局特征。此外，我们引入了一种双流式的框架 [Simonyan and Zisserman, 2014] 来明确地融合骨架运动特征。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本研究工作的主要贡献总结如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们提出使用 CNN 模型来学习骨架数据的全局共现特征，研究表明这优于局部共现特征。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们设计了一种全新的端到端分层式特征学习网络，其中的特征是从点层面特征到全局共现特征逐渐聚合起来的。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们全面地使用了多人特征融合策略，这让我们的网络可以轻松地扩展用于人数不同的场景。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在动作识别和检测任务的基准上，我们提出的框架优于所有已有的当前最佳方法。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.8959537572254336" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTUvyjFFTO4PBPbTSREcD7ovC6ibUZeov4g1ibvtKH1Cib2mvXB6VOpp4Sw/640?wx_fmt=png" data-type="png" data-w="519" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 2：3×3 卷积的分解分为两个步骤。(a) 每个输入通道的空间域中的独立 2D 卷积，其中的特征是从 3×3 的临近区域局部聚合的。(b) 各个通道上逐个元素求和，其中的特征是在所有输入通道上全局地聚合。<br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="1.3231810490693738" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTMd7kGZXGKA8JS7ffIOhOaibchCaaiaX8JDV8tMW7aDvEKwxlarYXY81A/640?wx_fmt=png" data-type="png" data-w="591" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 3：我们提出的分层式共现网络（HCN：Hierarchical Co-occurrence Network）的概况。绿色模块是卷积层，其中最后一维表示输出通道的数量。后面的「/2」表示卷积之后附带的最大池化层，步幅为 2。转置层是根据顺序参数重新排列输入张量的维度。conv1、conv5、conv6 和 fc7 之后附加了 ReLU 激活函数以引入非线性。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="0.5006765899864682" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTj7hlYXyCjuoHtlvLpXBDdAiax67MrL5DAodypYJgcYicGeAalFxIZkLA/640?wx_fmt=png" data-type="png" data-w="739" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 4：用于多人特征融合的后期融合（late fusion）图。最大、平均和连接操作在表现和泛化性能上得到了评估。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="0.32895816242821985" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT2TYac0ockE5ubLwM7fbar3icnpLiau96IZgsjN7mY1HaxG5VFHP6n3nQ/640?wx_fmt=png" data-type="png" data-w="1219" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 5：时间动作检测框架。图 3 描述了其中的骨干网络。还有两个子网络分别用于时间上提议的分割和动作分类。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="0.44900662251655626" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTH1aHEib5k0ntuCLEIiaqibqUnSHct0icibEEeDicvdBsPonXSzqhEvcuaJgA/640?wx_fmt=png" data-type="png" data-w="755" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 2：在 NTU RGB+D 数据集上的动作分类表现。CS 和 CV 分别表示 cross-subject 和 cross-view 的设置。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="0.42398884239888424" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTtDqX5QgYJlZFToaf1rxl1UyhBezDyclU12zUZ3pRPXiafOukaJwIibQw/640?wx_fmt=png" data-type="png" data-w="717" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 3：在 SBU 数据集上的动作分类表现。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="1.0364864864864864" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTe4LKlI9gybn1BelquHUQ1nYOCHdHOngO9Ps7dicjHibGfDDhM1VR0yrQ/640?wx_fmt=png" data-type="png" data-w="740" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 6：在 NTU RGB+D 数据集上的 cross-subject 设置中，在每个类别上 HCN 相对于 HCN-local 的准确度变化。为了清楚简明，这里只给出了变化超过 1% 的类别。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: left;line-height: 1.75em;"><strong><span style="font-size: 14px;">论文：使用分层聚合实现用于动作识别和检测的基于骨架数据的共现特征学习（Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="0.1894807821982468" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT1JI07qf1cAeNDnV7aibGmOVqvCZLN7em9xohR8Kvx7y4bicEMXic8B41g/640?wx_fmt=png" data-type="png" data-w="1483" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1804.06055</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">摘要：</span></strong><span style="font-size: 14px;">随着大规模骨架数据集变得可用，基于骨架的人体动作识别近来也受到了越来越多的关注。解决这一任务的最关键因素在于两方面：用于关节共现的帧内表征和用于骨架的时间演化的帧间表征。我们在本论文中提出了一种端到端的卷积式共现特征学习框架。这些共现特征是用一种分层式的方法学习到的，其中不同层次的环境信息（contextual information）是逐渐聚合的。首先独立地编码每个节点的点层面的信息。然后同时在空间域和时间域将它们组合成形义表征。具体而言，我们引入了一种全局空间聚合方案，可以学习到优于局部聚合方法的关节共现特征。此外，我们还将原始的骨架坐标与它们的时间差异整合成了一种双流式的范式。实验表明，我们的方法在 NTU RGB+D、SBU Kinect Interaction 和 PKU-MMD 等动作识别和检测基准上的表现能稳定地优于其它当前最佳方法。</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" width="48px" style="color: rgb(62, 62, 62);font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="257408642" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                        
                        <div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>

        
        <div class="rich_media_area_extra">

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_friend_cmt_area" style="display:none">
              
              
              
            </div>

                        <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_area" style="display:none">
            </div>
                    </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div><div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    IJCAI 2018 | 海康威视Oral论文：分层式共现网络，实现更好的动作识别和检测                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-21</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="font-size: 14px;text-align: justify;">arXiv</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12.000000953674316px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><span style="font-size: 12px;"><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="text-align: justify;">Chao Li等</span></strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">参与：Panda</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 14px;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">动作识别和检测正得到计算机视觉领域越来越多的关注。近日，海康威视在 arXiv 发布了在这方面的一项实现了新的最佳表现的研究成果，该论文也是 IJCAI 2018 Oral 论文。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">动作识别和检测等对人类行为的分析是计算机视觉领域一个基础而又困难的任务，也有很广泛的应用范围，比如智能监控系统、人机交互、游戏控制和机器人。铰接式的人体姿态（也被称为骨架（skeleton））能为描述人体动作提供非常好的表征。一方面，骨架数据在背景噪声中具有固有的稳健性，并且能提供人体动作的抽象信息和高层面特征。另一方面，与 RGB 数据相比，骨架数据的规模非常小，这让我们可以设计出轻量级且硬件友好的模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本论文关注的是基于骨架的人体动作识别和检测问题（图 1）。骨架的相互作用和组合在描述动作特征上共同发挥了关键性作用。有很多早期研究都曾试图根据骨架序列来设计和提取共现特征（co-occurrence feature），比如每个关节的配对的相对位置 [Wang et al., 2014]、配对关节的空间方向 [Jin and Choi, 2012]、Cov3DJ [Hussein et al., 2013] 和 HOJ3D [Xia et al., 2012] 等基于统计的特征。另一方面，带有长短期记忆（LSTM）神经元的循环神经网络（RNN）也常被用于建模骨架的时间序列 [Shahroudy et al., 2016; Song et al., 2017; Liu et al., 2016]。尽管 LSTM 网络就是为建模长期的时间依赖关系而设计的，但由于时间建模是在原始输入空间上完成的，所以它们难以直接从骨架上学习到高层面的特征 [Sainath et al., 2015]。而全连接层则有能力聚合所有输入神经元的全局信息，进而可以学习到共现特征。[Zhu et al., 2016] 提出了一种端到端的全连接深度 LSTM 网络来根据骨架数据学习共现特征。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.4051841746248295" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTWTzYT01LnSqAVib3CsjxcicicZ3zakibGJJ1hTFiaXIwICeQib6qM2BGtwSg/640?wx_fmt=png" data-type="png" data-w="733" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 1：基于骨架的人体动作识别的工作流程</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">CNN 模型在提取高层面信息方面能力出色，并且也已经被用于根据骨架学习空间-时间特征 [Du et al., 2016; Ke et al., 2017]。这些基于 CNN 的方法可以通过将时间动态和骨架关节分别编码成行和列而将骨架序列表示成一张图像，然后就像图像分类一样将图像输入 CNN 来识别其中含有的动作。但是，在这种情况下，只有卷积核内的相邻关节才被认为是在学习共现特征。尽管感受野（receptive field）能在之后的卷积层中覆盖骨架的所有关节，但我们很难有效地从所有关节中挖掘共现特征。由于空间维度中的权重共享机制，CNN 模型无法为每个关节都学习自由的参数。这促使我们设计一个能获得所有关节的全局响应的模型，以利用不同关节之间的相关性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们提出了一种端到端的共现特征学习框架，其使用了 CNN 来自动地从骨架序列中学习分层的共现特征。我们发现一个卷积层的输出是来自所有输入通道的全局响应。如果一个骨架的每个关节都被当作是一个通道，那么卷积层就可以轻松地学习所有关节的共现。更具体而言，我们将骨架序列表示成了一个形状帧×关节×3（最后一维作为通道）的张量。我们首先使用核大小为 n×1 的卷积层独立地为每个关节学习了点层面的特征。然后我们再将该卷积层的输出转置，以将关节的维度作为通道。在这个转置运算之后，后续的层分层地聚合来自所有关节的全局特征。此外，我们引入了一种双流式的框架 [Simonyan and Zisserman, 2014] 来明确地融合骨架运动特征。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本研究工作的主要贡献总结如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们提出使用 CNN 模型来学习骨架数据的全局共现特征，研究表明这优于局部共现特征。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们设计了一种全新的端到端分层式特征学习网络，其中的特征是从点层面特征到全局共现特征逐渐聚合起来的。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们全面地使用了多人特征融合策略，这让我们的网络可以轻松地扩展用于人数不同的场景。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在动作识别和检测任务的基准上，我们提出的框架优于所有已有的当前最佳方法。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.8959537572254336" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTUvyjFFTO4PBPbTSREcD7ovC6ibUZeov4g1ibvtKH1Cib2mvXB6VOpp4Sw/640?wx_fmt=png" data-type="png" data-w="519" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 2：3×3 卷积的分解分为两个步骤。(a) 每个输入通道的空间域中的独立 2D 卷积，其中的特征是从 3×3 的临近区域局部聚合的。(b) 各个通道上逐个元素求和，其中的特征是在所有输入通道上全局地聚合。<br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="1.3231810490693738" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTMd7kGZXGKA8JS7ffIOhOaibchCaaiaX8JDV8tMW7aDvEKwxlarYXY81A/640?wx_fmt=png" data-type="png" data-w="591" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 3：我们提出的分层式共现网络（HCN：Hierarchical Co-occurrence Network）的概况。绿色模块是卷积层，其中最后一维表示输出通道的数量。后面的「/2」表示卷积之后附带的最大池化层，步幅为 2。转置层是根据顺序参数重新排列输入张量的维度。conv1、conv5、conv6 和 fc7 之后附加了 ReLU 激活函数以引入非线性。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="0.5006765899864682" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTj7hlYXyCjuoHtlvLpXBDdAiax67MrL5DAodypYJgcYicGeAalFxIZkLA/640?wx_fmt=png" data-type="png" data-w="739" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 4：用于多人特征融合的后期融合（late fusion）图。最大、平均和连接操作在表现和泛化性能上得到了评估。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="0.32895816242821985" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT2TYac0ockE5ubLwM7fbar3icnpLiau96IZgsjN7mY1HaxG5VFHP6n3nQ/640?wx_fmt=png" data-type="png" data-w="1219" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 5：时间动作检测框架。图 3 描述了其中的骨干网络。还有两个子网络分别用于时间上提议的分割和动作分类。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="0.44900662251655626" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTH1aHEib5k0ntuCLEIiaqibqUnSHct0icibEEeDicvdBsPonXSzqhEvcuaJgA/640?wx_fmt=png" data-type="png" data-w="755" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 2：在 NTU RGB+D 数据集上的动作分类表现。CS 和 CV 分别表示 cross-subject 和 cross-view 的设置。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="0.42398884239888424" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTtDqX5QgYJlZFToaf1rxl1UyhBezDyclU12zUZ3pRPXiafOukaJwIibQw/640?wx_fmt=png" data-type="png" data-w="717" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 3：在 SBU 数据集上的动作分类表现。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></em></p><p><img class="" data-copyright="0" data-ratio="1.0364864864864864" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTe4LKlI9gybn1BelquHUQ1nYOCHdHOngO9Ps7dicjHibGfDDhM1VR0yrQ/640?wx_fmt=png" data-type="png" data-w="740" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 6：在 NTU RGB+D 数据集上的 cross-subject 设置中，在每个类别上 HCN 相对于 HCN-local 的准确度变化。为了清楚简明，这里只给出了变化超过 1% 的类别。</span></em></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: left;line-height: 1.75em;"><strong><span style="font-size: 14px;">论文：使用分层聚合实现用于动作识别和检测的基于骨架数据的共现特征学习（Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation）</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="0.1894807821982468" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT1JI07qf1cAeNDnV7aibGmOVqvCZLN7em9xohR8Kvx7y4bicEMXic8B41g/640?wx_fmt=png" data-type="png" data-w="1483" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1804.06055</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">摘要：</span></strong><span style="font-size: 14px;">随着大规模骨架数据集变得可用，基于骨架的人体动作识别近来也受到了越来越多的关注。解决这一任务的最关键因素在于两方面：用于关节共现的帧内表征和用于骨架的时间演化的帧间表征。我们在本论文中提出了一种端到端的卷积式共现特征学习框架。这些共现特征是用一种分层式的方法学习到的，其中不同层次的环境信息（contextual information）是逐渐聚合的。首先独立地编码每个节点的点层面的信息。然后同时在空间域和时间域将它们组合成形义表征。具体而言，我们引入了一种全局空间聚合方案，可以学习到优于局部聚合方法的关节共现特征。此外，我们还将原始的骨架坐标与它们的时间差异整合成了一种双流式的范式。实验表明，我们的方法在 NTU RGB+D、SBU Kinect Interaction 和 PKU-MMD 等动作识别和检测基准上的表现能稳定地优于其它当前最佳方法。</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" width="48px" style="color: rgb(62, 62, 62);font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="257408642" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
