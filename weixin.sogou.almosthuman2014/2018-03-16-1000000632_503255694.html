<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>ICASSP 2018 | 阿里巴巴语音交互智能团队：基于线性网络的语音合成说话人自适应</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1522019393&amp;src=3&amp;ver=1&amp;signature=s1hQLqmZz2ApNWPEkpD6aTYl0cn5KC25U0xSpGhemZynruELpzATT3LSm6Am6B8fzNu5HHwVdY5yXn55Y0vIR9hAi*Lw00ZTkuZ825G7aYLzTSZZtO1mKudcgyTtoviSGqjTmJTdmK0q3vzlE1-krxVlJJRljrAN*nQnZ86ld3g=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    ICASSP 2018 | 阿里巴巴语音交互智能团队：基于线性网络的语音合成说话人自适应                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-16</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;font-size: 16px;white-space: normal;line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="color:#ffffff;"><span style="background-color: rgb(117, 117, 118);">机器之心发布</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">作者：</span></strong><span style="font-size: 12px;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-family: 宋体;">黄智颖</span><span style="font-family: Calibri;">/Zhiying Huang</span><span style="font-family: 宋体;">，卢恒</span><span style="font-family: Calibri;">/Heng Lu</span><span style="font-family: 宋体;">，雷鸣</span><span style="font-family: Calibri;">/Ming Lei</span><span style="font-family: 宋体;">，鄢志杰</span><span style="font-family: Calibri;">/Zhijie Yan</span></span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><span style="color: rgb(136, 136, 136);font-size: 14px;text-align: justify;background-color: rgb(255, 255, 255);">语音领域的顶会 ICASSP 2018 将于 4 月 15-20 日在加拿大阿尔伯塔卡尔加里市举行。据机器之心了解，国内科技巨头阿里巴巴语音交互智能团队有 5 篇论文被此大会接收。本文对论文《Linear networks based speaker adaptation for speech synthesis》做了编译介绍。</span></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><span style="color: rgb(136, 136, 136);font-size: 14px;text-align: justify;background-color: rgb(255, 255, 255);"><br></span></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color:#888888;"><span style="font-size: 14px;background-color: rgb(255, 255, 255);">欢迎大家向机器之心推荐优秀的 ICASSP 2018 相关论文。</span></span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p><strong><span style="font-size: 14px;">论文：Linear networks based speaker adaptation for speech synthesis</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.21187800963081863" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGx0McTSA1Sjmps8oC2LOIbTYDibqhVygXLqG8KZ1zicstj2pGFppzW7nA/640?wx_fmt=png" data-type="png" data-w="1246" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="text-align: justify;font-size: 14px;color: rgb(123, 12, 0);">原文链接：https://arxiv.org/abs/1803.02445</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">摘要：</span></strong><span style="font-size: 14px;">说话人自适应算法利用说话人少量语料来建立说话人自适应语音合成系统，该系统能够合成令人满意的语音。在本文中，我们提出了基于线性网络的语音合成说话人自适应算法。该算法对每个说话人学习特定的线性网络，从而获得属于目标说话人的声学模型。通过该算法，使用 200 句目标说话人的自适应语料训练的说话人自适应系统能够获得和使用 1000 句训练的说话人相关系统相近的合成效果。</span></p><p><br></p><p style="text-align: center;"><strong>研究背景</strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">对于一个目标说话人，如果他（她）拥有充足的训练数据，那么我们便可以建立一个说话人相关的声学模型，基于该声学模型的系统称之为说话人相关的语音合成系统。利用该系统，我们能够合成和目标说话人声音很像的语音。但是，大多数时候，目标说话人没有充足的数据，这使得合成出来的语音效果不太理想。利用说话人自适应算法，能够基于比较有限的数据来获得较好的语音合成系统，该类算法节省了大量的录音、转录和检查工作，使得建立新的声音的代价变得很小。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本文中，我们提出了基于线性网络（Linear Network, LN）的语音合成说话人自适应算法。该算法通过在源说话人声学模型的层间插入线性网络，然后利用目标说话人的数据来更新该线性网络和神经网络的输出层，从而能够获得属于目标说话人的声学模型。另外，一种基于低秩分解（low-rank plus diagonal，LRPD）的模型压缩算法被应用于线性网络。实验发现，当数据量较少的时候，通过 LRPD 来移除一些冗余的参数，从而能够使得系统合成的声音更加稳定。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;"><strong>算法描述</strong></p><p style="text-align: center;"><strong><br></strong></p><p>	</p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本文中，源说话人声学模型是一个基于多任务（multi-task）DNN-BLSTM 的声学模型，见 Fig. 1 左侧。声学模型的输入为语音学特征，输出为声学特征。声学特征包括梅尔倒谱系数等。实验证明，在声学模型的底层使用深层神经网络（Deep Neural Network，DNN）可以获得更好的底层特征，并且收敛速度上相比于不使用 DNN 更快。在输出层上，不同的声学特征使用各自的输出层，它们仅共享声学模型的隐层。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p>	</p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">基于线性网络的自适应算法首先被提出于语音识别领域，它的系统结构见 Fig. 1 右侧。根据线性网络插入的位置不同，它可以被分为线性输入网络（Linear Input Network，LIN）、线性隐层网络（Linear Hidden Network，LHN）和线性输出网络（Linear Output Network，LON）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.9813242784380306" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGXs0T810khibzaOhGMp9XmUlYDDxeI0AKqic66fCQrUMeicLO8KKoiaeMBQ/640?wx_fmt=png" data-type="png" data-w="589" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">当线性网络被插入到声学模型的第</span><img class="" data-copyright="0" data-ratio="1.2666666666666666" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGHTkNg4etebiaOmMsDPKWTvE4rD1hAuYrR62HGgFQvPHwa4T1RwyDY6A/640?wx_fmt=png" data-type="png" data-w="30" style="width: 20px;height: 25px;"><span style="font-size: 14px;">和</span><img class="" data-copyright="0" data-ratio="0.41025641025641024" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGykxx4NlO7bMwOfoGUicJThVTdo1yiaz7F33d3Hr7nJiahM30g705eoIuA/640?wx_fmt=png" data-type="png" data-w="78" style="width: 31px;height: 20px;"><span style="font-size: 14px;">层之间时，线性网络的输出</span><img class="" data-copyright="0" data-ratio="1.2777777777777777" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGt3ZAO3R0ibqGCIUkzYmc4yej9kf1eBaV7kviazsUBsklziagGyrZibTKyg/640?wx_fmt=png" data-type="png" data-w="36" style="width: 20px;height: 25px;"><span style="font-size: 14px;">为：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.24793388429752067" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGnia03FMCotfa0bI0RSC1ryicRAl5BNAweHU7vIBGZB09juBjc6B3KZQw/640?wx_fmt=png" data-type="png" data-w="242" style="width: 172px;height: 42px;"></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">其中，</span><img class="" data-copyright="0" data-ratio="1.2" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGpcQvbEjUqdTaIAzmWaYeGWLkZHMOU4WIzE2UqF7bia4jlzX4mCbzrRg/640?wx_fmt=png" data-type="png" data-w="40" style="width: 21px;height: 24px;"><span style="font-size: 14px;">表示第</span><img class="" data-copyright="0" data-ratio="1.2666666666666666" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGHTkNg4etebiaOmMsDPKWTvE4rD1hAuYrR62HGgFQvPHwa4T1RwyDY6A/640?wx_fmt=png" data-type="png" data-w="30" style="width: 20px;height: 25px;"><span style="font-size: 14px;">层的输出，</span><img class="" data-copyright="0" data-ratio="0.8" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGQ3RvhVlibKWviaYOabQXYCExibcKhpJdHGsWnGxO7fxZW2wE1YqMWicdlQ/640?wx_fmt=png" data-type="png" data-w="50" style="width: 21px;height: 20px;"><span style="font-size: 14px;">表示说话人相关的线性变换矩阵，</span><img class="" data-copyright="0" data-ratio="1.0952380952380953" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGbUib4C9XScmrpibF0CfeWhX5Q8yt4ErVVtpK0v3BpluysKOplf4t48gw/640?wx_fmt=png" data-type="png" data-w="42" style="width: 21px;height: 24px;"><span style="font-size: 14px;">表示说话人相关的偏置矢量。模型训练流程如下：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1）	将线性网络插入至源说话人声学模型特定位置。此时，</span><img class="" data-copyright="0" data-ratio="0.8" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGQ3RvhVlibKWviaYOabQXYCExibcKhpJdHGsWnGxO7fxZW2wE1YqMWicdlQ/640?wx_fmt=png" data-type="png" data-w="50" style="width: 20px;height: 20px;"><span style="font-size: 14px;">被初始化为单位矩阵，</span><img class="" data-copyright="0" data-ratio="1.0952380952380953" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGbUib4C9XScmrpibF0CfeWhX5Q8yt4ErVVtpK0v3BpluysKOplf4t48gw/640?wx_fmt=png" data-type="png" data-w="42" style="width: 20px;height: 22px;"><span style="font-size: 14px;">的所有元素都初始化为 0。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2）	利用目标说话人的数据来更新线性网络中的参数</span><img class="" data-copyright="0" data-ratio="0.8" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGQ3RvhVlibKWviaYOabQXYCExibcKhpJdHGsWnGxO7fxZW2wE1YqMWicdlQ/640?wx_fmt=png" data-type="png" data-w="50" style="width: 20px;height: 20px;">和<img class="" data-copyright="0" data-ratio="1.0952380952380953" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGbUib4C9XScmrpibF0CfeWhX5Q8yt4ErVVtpK0v3BpluysKOplf4t48gw/640?wx_fmt=png" data-type="png" data-w="42" style="width: 20px;height: 22px;"><span style="font-size: 14px;">，直到收敛。此时，保持声学模型中的其它层参数固定不变。最后，获得目标说话人的声学模型。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.406570841889117" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cG7Qha1FKibeQ4ZkL8kOWp0Vaqic9oR2GV3znH6hvDhUt5q7DW0AhMBN1Q/640?wx_fmt=png" data-type="png" data-w="487" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p>	</p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">LRPD 算法主要被应用于线性网络的模型压缩。在语音识别中，基于 LRPD 的线性网络（LRPD-LN）能够减少普通线性网络（Full-LN）82% 的模型参数量，并且性能几乎不出现下降。LRPD 算法利用对角矩阵和低秩矩阵来表达 Full-LN 中的</span><img class="" data-copyright="0" data-ratio="0.8" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGQ3RvhVlibKWviaYOabQXYCExibcKhpJdHGsWnGxO7fxZW2wE1YqMWicdlQ/640?wx_fmt=png" data-type="png" data-w="50" style="width: 20px;height: 20px;"><span style="font-size: 14px;">：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.12" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGw1n1MdQfu4QHx87LVy4h2a6mftaBDe9Ah6L71EC0nsGvZlBcmr0Dgg/640?wx_fmt=png" data-type="png" data-w="450" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p>	</p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">其中，</span><img class="" data-copyright="0" data-ratio="0.5" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGAhohWOoaCAD0qUaib8sicSP8UxPibIicu3oNwxfbDBH8rXKK2j9WViawleg/640?wx_fmt=png" data-type="png" data-w="96" style="width: 36px;height: 20px;">和 <img class="" data-copyright="0" data-ratio="0.4" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGh7N2WwOqxBrfOKypB9P4bDTP3ejEia9O5Vm8xJGkmW9Mic6Zic6N33Zwg/640?wx_fmt=png" data-type="png" data-w="90" style="width: 41px;height: 20px;"><span style="font-size: 14px;">分别表示</span><img class="" data-copyright="0" data-ratio="0.48484848484848486" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGvrq5yLVtqWL5WosZqh5P6dG5cfj1OCDPxBKCJg089m5AOjTb7PvTfA/640?wx_fmt=png" data-type="png" data-w="66" style="width: 42px;height: 20px;"><span style="font-size: 14px;">和</span><img class="" data-copyright="0" data-ratio="0.3939393939393939" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cG9eJfEb1MpThfaEH9T8THvEhtOJVAQ8YDqYEIwF2hxSB2aHgibpOuibbA/640?wx_fmt=png" data-type="png" data-w="66" style="width: 44px;height: 20px;"><span style="font-size: 14px;">的矩阵，</span><img class="" data-copyright="0" data-ratio="0.6756756756756757" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGQJsJtjSQQ88jR39BibrvMheubTKbEicBTDZEbdZqrAfUnhJuxUl5nkTw/640?wx_fmt=png" data-type="png" data-w="74" style="width: 31px;height: 20px;"><span style="font-size: 14px;">为对角矩阵。可以看到，Full-LN 中的模型参数量为</span><img class="" data-copyright="0" data-ratio="1.1666666666666667" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGq2rlfY9WzPI9OibwV0h1UlhD5IKgic9MSJy7dznyeCmVqWdzQ92VHc9w/640?wx_fmt=png" data-type="png" data-w="36" style="width: 20px;height: 24px;"><span style="font-size: 14px;">，LRPD-LN 的模型参数量为</span><img class="" data-copyright="0" data-ratio="0.2" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGA5cWMcIk1oHHgHIiaSiclatOA3R4kzBibxiaTlOsmFRZQUHFiccnvkldpJA/640?wx_fmt=png" data-type="png" data-w="270" style="width: 76px;height: 20px;"><span style="font-size: 14px;">。通过实验证明，由于 LRPD-LN 所需要更新的参数量特别少，因此在目标说话人数据量有限的情况下能够获得较 Full-LN 更加稳定的合成声音。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;"><strong>实验</strong></p><p style="text-align: center;"><strong><br></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本文提出的算法，在中文数据集上进行实验，该数据集包含 3 个说话人，每个说话人有 5000 句话，时长约 5h。数据集中语音的采样率为 16k，特征提取中的窗长和窗移分别为 25ms 和 5ms。分别用 A-male、B- female 和 C-female 来命名这三个说话人。本实验中，源说话人声学模型训练过程所使用的句子数为 5000。为了对比不同句子数目下的合成效果，目标说话人的自适应数据集对应的句子数从 50 到 1000 不等。在自适应数据集之外，我们取 200 句话作为开发集，取 20 句话作为测试集（用于主观打分）。为了分析性别对自适应效果的影响，进行了三对源说话人-目标说话人之间的实验：女生-女生、男生-女生和女生-男生。另外，使用客观度量和主观测听两种方式来衡量模型的性能。客观度量主要包括：Mel-Cepstral Distortion (MCD)、root mean squared error (RMSE) of F0、unvoiced/voiced (U/V) prediction errors 和开发集的 MSE。主观测听主要是对系统合成的声音样本进行自然度和相似度上的打分——mean opinion score (MOS)。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.6497109826589595" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cG3NsXoo6aLbc5jajFVwVvcWkfzibK1ZFGId8brYvErD5YfmYUtW1laSQ/640?wx_fmt=png" data-type="png" data-w="865" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p>	</p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">以女生-女生（C-female – B-female）为例，Fig. 3 显示了不同自适应句子数目和客观度量之间的关系曲线图。其中，SD 表示说话人相关系统，OL 表示只更新源说话人声学模型输出层的说话人自适应系统，OL+Full-LN 和 OL+LRPD-LN 分别表示基于 Full-LN 和 LRPD-LN 的说话人自适应系统。根据 Fig. 3，随着训练/自适应句子数的增加，所有系统间的客观度量趋于相近。对比 SD 和另外三个自适应系统，自适应系统的性能在相同句子数目下要更优。另外，OL+LRPD-LN 和 OL+Full-LN 相比于 OL 均出现性能上的跳变（提升），说明只更新输出层而不对其他层进行更新不能够得到较好的自适应效果。同时，当自适应句子数较少的时候，OL+Full-LN 在客观性能上要差于 OL+LRPD-LN，这是因为 OL+Full-LN 引入太多的参数量，出现过拟合问题。反之，在句子数多的时候 OL+Full-LN 在客观性能上要优于 OL+LRPD-LN，此时 OL+LRPD-LN 由于参数量少，出现欠拟合问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5130293159609121" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Jdibo5tzCWbkfL5e0v72cGNIqbSAbmJ1hHxAsiaTJ7NPDicHlPZSB0lkMgG9QkmYh3fwg9QCaicKnGg/640?wx_fmt=png" data-type="png" data-w="614" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Fig. 4 上对比了不同系统间的自然度和相似度。随着句子数的减少，SD 系统的性能出现急剧下降，OL+LRPD-LN 相比于 SD 和 OL+Full-LN 要更加稳定。与客观度量一致，在相同句子数下，OL+Full-LN 和 OL+LRPD-LN 在性能上要优于 SD。并且，OL+Full-LN 和 OL+LRPD-LN 在 200 句话的性能和 SD 在 1000 句话时的性能相近。与客观度量不同，OL+LRPD-LN 在 500 句以下的时候性能上就优于 OL+Full-LN。这是因为过拟合导致合成出来的声音不稳定（虽然客观度量更优）声音的可懂度下降导致的。由此，我们依然可以得到相同的结论：当自适应句子数较少的时候，过拟合使得 OL+Full-LN 的性能变差。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;"><strong>结论</strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本文中，基于线性网络的说话人自适应算法被应用于语音合成领域，基于 LRPD 的模型压缩算法能够提高声音的稳定性。通过三对不同的源说话人-目标说话人的实验，我们发现，当自适应句子数目非常少的时候，LRPD 能够提升声音的稳定性。另外，通过提出的算法，使用 200 句目标说话人的训练语料训练的说话人自适应系统能够获得和使用 1000 句训练的说话人相关系统相近的效果。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="margin-bottom: 20px;max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心发布，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p>
                </div>
                <script nonce="1056189030" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
