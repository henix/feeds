<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>入门 | 通过 Q-learning 深入理解强化学习</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1524742198&amp;src=3&amp;ver=1&amp;signature=BE*KBY8PxkYbW-h9q-NyjX057Fq*3T7FFmDCKnSh7YdbA4JeXyv2WD7FvW3ZttjPcV39Vo6eZfkljKkcQZAyO7ZpJWNfMjVCUiq1MB8b0PS0xBDD-GQJj6S3G-IdXyQx5B7Wl7*3D-FW0ZN*j3jvYoXVyCYEbLPG5BhcNJXtf2k=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    入门 | 通过 Q-learning 深入理解强化学习                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-17</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><section style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="font-size: 14px;text-align: justify;">Medium</span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><strong style="color: rgb(136, 136, 136);font-size: 12px;font-family: inherit;text-decoration: inherit;max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">Thomas Simonini</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="color: rgb(136, 136, 136);font-size: 12px;font-family: inherit;text-decoration: inherit;max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：</strong><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">Geek AI、刘晓坤</span></strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 14px;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">本文将带你学习经典强化学习算法 Q-learning 的相关知识。在这篇文章中，你将学到：（1）Q-learning 的概念解释和算法详解；（2）通过 Numpy 实现 Q-learning。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>故事案例：骑士和公主</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="1.002" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OaYHLJsR0cf6e4OVDKxNtKnLSJh1ic42lf9MGH104rgAnYdX8u6pntlg/640?wx_fmt=png" data-type="png" data-w="500" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">假设你是一名骑士，并且你需要拯救上面的地图里被困在城堡中的公主。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你每次可以移动一个方块的距离。敌人是不能移动的，但是如果你和敌人落在了同一个方块中，你就会死。你的目标是以尽可能快的路线走到城堡去。这可以使用一个「按步积分」系统来评估。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你在每一步都会失去 1 分（每一步失去的分数帮助智能体训练的更快）</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果碰到了一个敌人，你会失去 100 分，并且训练 episode 结束。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果进入到城堡中，你就获胜了，获得 100 分。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">那么问题来了：如何才能够创建这样的智能体呢？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">下面我将介绍第一个策略。假设智能体试图走遍每一个方块，并且将其着色。绿色代表「安全」，红色代表「不安全」。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="1.002" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OpX1WXM1a289Fiau1phNcWcBAic1rxNv6JV4NQBdstg4Mn37AGAPcZuEw/640?wx_fmt=png" data-type="png" data-w="500" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">同样的地图，但是被着色了，用于显示哪些方块是可以被安全访问的。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">接着，我们告诉智能体只能选择绿色的方块。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但问题是，这种策略并不是十分有用。当绿色的方块彼此相邻时，我们不知道选择哪个方块是最好的。所以，智能体可能会在寻找城堡的过程中陷入无限的循环。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>Q-Table 简介</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">下面我将介绍第二种策略：创建一个表格。通过它，我们可以为每一个状态（state）上进行的每一个动作（action）计算出最大的未来奖励（reward）的期望。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">得益于这个表格，我们可以知道为每一个状态采取的最佳动作。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">每个状态（方块）允许四种可能的操作：左移、右移、上移、下移。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="1.0081799591002045" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OgSda96AoO4oibJVbHsicp4z4X9FBflLboqZZ5U8Qnb6L9Yt1wcEbKx6A/640?wx_fmt=png" data-type="png" data-w="489" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">「0」代表不可能的移动（如果你在左上角，你不可能向左移动或者向上移动！）</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在计算过程中，我们可以将这个网格转换成一个表。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这种表格被称为 Q-table（「Q」代表动作的「质量」）。每一列将代表四个操作（左、右、上、下），行代表状态。每个单元格的值代表给定状态和相应动作的最大未来奖励期望。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.317" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OUA0Xfib9dL2s5cKTG0zszs9wROtliaj3L6yZF3icdpqdSWic0IXdicL7Sug/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">每个 Q-table 的分数将代表在给定最佳策略的状态下采取相应动作获得的最大未来奖励期望。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为什么我们说「给定的策略」呢？这是因为我们并不实现这些策略。相反，我们只需要改进 Q-table 就可以一直选择最佳的动作。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">将这个 Q-table 想象成一个「备忘纸条」游戏。得益于此，我们通过寻找每一行中最高的分数，可以知道对于每一个状态（Q-table 中的每一行）来说，可采取的最佳动作是什么。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">太棒了！我解决了这个城堡问题！但是，请等一下... 我们如何计算 Q-table 中每个元素的值呢？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为了学习到 Q-table 中的每个值，我们将使用 Q-learning 算法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>Q-learning 算法：学习动作值函数（action value function）</strong></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">动作值函数（或称「Q 函数」）有两个输入：「状态」和「动作」。它将返回在该状态下执行该动作的未来奖励期望。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.1856763925729443" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0Of6CwnTgng0JGRJIDXSDicr2j39eZx1I6aCHkf9Hyia6yI1Cyphx2hTag/640?wx_fmt=png" data-type="png" data-w="1131" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们可以把 Q 函数视为一个在 Q-table 上滚动的读取器，用于寻找与当前状态关联的行以及与动作关联的列。它会从相匹配的单元格中返回 Q 值。这就是未来奖励的期望。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.21448999046711154" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OMCSWDPeP0AXtagywjpQ90Phc1wTRcFV6lHNNDOQfOdahGFGURKpxJQ/640?wx_fmt=png" data-type="png" data-w="1049" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在我们探索环境（environment）之前，Q-table 会给出相同的任意的设定值（大多数情况下是 0）。随着对环境的持续探索，这个 Q-table 会通过迭代地使用 Bellman 方程（动态规划方程）更新 Q(s,a) 来给出越来越好的近似。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">Q-learning 算法流程</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="1.292626728110599" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0O0Q1ZmmcCaAaYH13kjR1MnzS1ibBzxqLMqT1BYDRBJCGY7BFIdUGR6DA/640?wx_fmt=png" data-type="png" data-w="434" style=""></p><p><img class="" data-copyright="0" data-ratio="0.18375" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0O5OxwvL6vmtJGSlIGZwarSIBv0aftXKwP8fmlgia6j5yM8ozkbybIDicg/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">Q-learning 算法的伪代码</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">步骤 1：初始化 Q 值。我们构造了一个 m 列（m = 动作数 )，n 行（n = 状态数）的 Q-table，并将其中的值初始化为 0。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="1.091093117408907" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0O5YFSUCBfnuhic9ZF8U6nribicF2Stxg4NJ2WVl8RPibC7SMAic19QiaAUjaA/640?wx_fmt=png" data-type="png" data-w="494" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">步骤 2：在整个生命周期中（或者直到训练被中止前），步骤 3 到步骤 5 会一直被重复，直到达到了最大的训练次数（由用户指定）或者手动中止训练。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">步骤 3：选取一个动作。在基于当前的 Q 值估计得出的状态 s 下选择一个动作 a。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但是……如果每个 Q 值都等于零，我们一开始该选择什么动作呢？在这里，我们就可以看到探索/利用（exploration/exploitation）的权衡有多重要了。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">思路就是，在一开始，我们将使用 epsilon 贪婪策略：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们指定一个探索速率「epsilon」，一开始将它设定为 1。这个就是我们将随机采用的步长。在一开始，这个速率应该处于最大值，因为我们不知道 Q-table 中任何的值。这意味着，我们需要通过随机选择动作进行大量的探索。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">生成一个随机数。如果这个数大于 epsilon，那么我们将会进行「利用」（这意味着我们在每一步利用已经知道的信息选择动作）。否则，我们将继续进行探索。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在刚开始训练 Q 函数时，我们必须有一个大的 epsilon。随着智能体对估算出的 Q 值更有把握，我们将逐渐减小 epsilon。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5408805031446541" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0O3dpbVNaD7CaMpDYDjiajxWNHWrzefx9lcHpn1AuR2aNicnm5crSS0ocA/640?wx_fmt=png" data-type="png" data-w="954" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">步骤 4-5：评价！采用动作 a 并且观察输出的状态 s' 和奖励 r。现在我们更新函数 Q（s，a）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们采用在步骤 3 中选择的动作 a，然后执行这个动作会返回一个新的状态 s' 和奖励 r。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">接着我们使用 Bellman 方程去更新 Q（s，a）：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.2039072039072039" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OcbjjXUTzyYIUdaEP4UP2zo2dHhXoIeq9xc7CcrAKuhja2sPh3jLzCQ/640?wx_fmt=png" data-type="png" data-w="1638" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如下方代码所示，更新 Q（state，action）：</span></p><p style="text-align: justify;line-height: 1.75em;"><br></p><pre style="box-sizing: border-box;margin-top: 0px;margin-bottom: 0px;padding: 0px;font-size: 16px;color: rgb(62, 62, 62);line-height: inherit;font-variant-ligatures: normal;orphans: 2;widows: 2;background-color: rgb(255, 255, 255);"><code class="python language-python hljs" style="box-sizing: border-box;margin-right: 2px;margin-left: 2px;padding: 0.5em;font-size: 14px;color: rgb(169, 183, 198);line-height: 18px;border-top-left-radius: 0px;border-top-right-radius: 0px;border-bottom-right-radius: 0px;border-bottom-left-radius: 0px;background-color: rgb(40, 43, 46);font-family: Consolas, Inconsolata, Courier, monospace;display: block;overflow-x: auto;letter-spacing: 0px;word-wrap: normal !important;word-break: normal !important;overflow-y: auto !important;background-position: initial initial;background-repeat: initial initial;">New Q value = <br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">   Current Q value + <br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">   lr * [Reward + discount_rate * (highest Q value between possible actions <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">from</span> the new state s’ ) — Current Q value ]<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"></code></pre><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">让我们举个例子：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.6733333333333333" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OO0HR3z7NJ1qxGiahZfQuicpHxV9eDib6yvkRrFpn72KickD7NiaicqKuk5mg/640?wx_fmt=png" data-type="png" data-w="300" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一块奶酪 = +1</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">两块奶酪 = +2</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一大堆奶酪 = +10（训练结束）</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">吃到了鼠药 = -10（训练结束）</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">步骤 1：初始化 Q-table</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5662847790507365" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OAVsd0j6xULWPyopG24rD1TuOSUtzNxRACDrvtlJozAVTYQstlqwjBw/640?wx_fmt=png" data-type="png" data-w="611" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">初始化之后的 Q-table</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">步骤 2：选择一个动作。从起始点，你可以在向右走和向下走其中选择一个。由于有一个大的 epsilon 速率（因为我们至今对于环境一无所知），我们随机地选择一个。例如向右走。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.73" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OhThWicQREmfLV7nEsC21ZjqY80X7KASbWicfcWAoRG8Vvq77tkyHwfGA/640?wx_fmt=png" data-type="png" data-w="300" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5707154742096506" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0O9icxr1ta3F9zYN0JHiaaxGXyrL2hK6yZ6L0synwxhbhPsGRiboUPQYb2Q/640?wx_fmt=png" data-type="png" data-w="601" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">我们随机移动（例如向右走）</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们发现了一块奶酪（+1），现在我们可以更新开始时的 Q 值并且向右走，通过 Bellman 方程实现。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">步骤 4-5：更新 Q 函数</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.2039072039072039" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OcbjjXUTzyYIUdaEP4UP2zo2dHhXoIeq9xc7CcrAKuhja2sPh3jLzCQ/640?wx_fmt=png" data-type="png" data-w="1638" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.1895" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OqClIgVLkVhLI0xib8dQWsqVfc7JBptTnKloovRwwWRmBwZvYVC1IRfA/640?wx_fmt=png" data-type="png" data-w="2000" style=""></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先，我们计算 Q 值的改变量 ΔQ(start, right)。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">接着我们将初始的 Q 值与 ΔQ(start, right) 和学习率的积相加。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">可以将学习率看作是网络有多快地抛弃旧值、生成新值的度量。如果学习率是 1，新的估计值会成为新的 Q 值，并完全抛弃旧值。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5535714285714286" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9MYTrlCV1B0KCe8hgg5Q0OmD9zQuz4Wvs6YAHdLtm99TWPUhjD1ibGfz7H1rVcxnFlsWbrqroE9Uw/640?wx_fmt=png" data-type="png" data-w="616" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">更新后的 Q-table</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">太好了！我们刚刚更新了第一个 Q 值。现在我们要做的就是一次又一次地做这个工作直到学习结束。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>实现 Q-learning 算法</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">既然我们知道了它是如何工作的，我们将一步步地实现 Q-learning 算法。代码的每一部分都在下面的 Jupyter notebook 中直接被解释了。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你可以在我的深度强化学习课程 repo 中获得代码。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">项目地址：https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Q%20Learning%20with%20FrozenLake.ipynb</span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>回顾</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Q-learning 是一个基于值的强化学习算法，利用 Q 函数寻找最优的「动作—选择」策略。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">它根据动作值函数评估应该选择哪个动作，这个函数决定了处于某一个特定状态以及在该状态下采取特定动作的奖励期望值。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">目的：最大化 Q 函数的值（给定一个状态和动作时的未来奖励期望）。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Q-table 帮助我们找到对于每个状态来说的最佳动作。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">通过选择所有可能的动作中最佳的一个来最大化期望奖励。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Q 作为某一特定状态下采取某一特定动作的质量的度量。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">函数 Q（state，action）→返回在当前状态下采取该动作的未来奖励期望。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这个函数可以通过 Q-learning 算法来估计，使用 Bellman 方程迭代地更新 Q（s，a）</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在我们探索环境之前：Q-table 给出相同的任意的设定值→ 但是随着对环境的持续探索→Q 给出越来越好的近似。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">就是这些了！不要忘记自己去实现代码的每一部分——试着修改已有的代码是十分重要的。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">试着增加迭代次数，改变学习率，并且使用一个更复杂的环境（例如：8*8 方格的 Frozen-lake）。祝你玩的开心！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">下次，我将探讨深度 Q-learning，它是 2015 年深度增强学习最大的突破之一。并且，我将训练一个打「毁灭战士（Doom）」游戏、能杀死敌人的智能体！（敬请期待机器之心的更新～）</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" width="48px" style="font-size: 14px;background-color: rgb(255, 255, 255);color: rgb(62, 62, 62);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">原文链接：https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe</span></em></span></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="font-size: 16px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="1659517323" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
