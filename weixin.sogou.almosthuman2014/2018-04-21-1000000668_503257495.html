<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>教程 | 深度Q学习：一步步实现能玩《毁灭战士》的智能体</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1525110987&amp;src=3&amp;ver=1&amp;signature=zvKw2e*4FnEj-zDhL3Iecm**yVkLQyN1kvPfmxDX50hQMV6a7wdRfepEOv6h3DB5aOxJszJl1I9KgpTLMzylFvTQoeGrcbvM-xrBTs9uDB6ziqDd9y*V08NX2XBtRTEOPMqc4TY38TjibsLLkKdg0BFCG5u*G9quilgznj0E8oA=">原文</a></p>
<div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        <div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    教程 | 深度Q学习：一步步实现能玩《毁灭战士》的智能体                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-21</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自Medium</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12.000000953674316px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">Thomas Simonini</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">参与：Panda</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 14px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;">近年来，深度强化学习已经取得了有目共睹的成功。机器之心也曾发布过很多介绍强化学习基本理论和前沿进展的文章，比如《<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650722774&amp;idx=1&amp;sn=9d1ba901077fd3902a410e409e6211a9&amp;chksm=871b15a8b06c9cbe00ac088a504d566789498787e0bab859e1888a7c5334ebf1525a1255bc43&amp;scene=21#wechat_redirect" target="_blank">专题 | 深度强化学习综述：从 AlphaGo 背后的力量到学习资源分享（附论文）</a>》。近日，深度学习工程师 Thomas Simonini 在 freeCodeCamp 上发表了介绍深度强化学习的系列文章，已发布的三篇分别介绍了强化学习基本概念、<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650741014&amp;idx=3&amp;sn=8f82248e34cf62ef7b0d12b66a893d95&amp;chksm=871add68b06d547e766f4470e8be2f698fea03d2ab7a896101003122bac1d23724e6a1957a87&amp;scene=21#wechat_redirect" target="_blank">Q 学习</a>以及在《毁灭战士》游戏上开发智能体的过程。机器之心已经编译介绍了其中第二篇，本文为第三篇，前两篇可参阅：</span></span></p><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 14px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;"><br></span></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 14px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;"><span style="font-size: 14px;text-align: justify;">https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419 </span></span></span></p></li><li><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650741014&amp;idx=3&amp;sn=8f82248e34cf62ef7b0d12b66a893d95&amp;chksm=871add68b06d547e766f4470e8be2f698fea03d2ab7a896101003122bac1d23724e6a1957a87&amp;scene=21#wechat_redirect" target="_blank"><span style="max-width: 100%;font-size: 14px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;"><span style="font-size: 14px;text-align: justify;">通过 Q-learning 深入理解强化学习</span></span></span></a></p></li></ul></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">上一次，我们介绍了 Q 学习：这种算法可以得到 Q-table，并且智能体可将其用于寻找给定状态的最佳动作。但正如我们所见，当状态空间很大时，求取和更新 Q-table 的效果会很差。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本文是深度强化学习系列博客文章的第三篇。我们将在本文中介绍创造一个深度 Q 网络的过程。我们不会使用 Q-table，我们会实现一个神经网络，其以状态为输入，然后基于该状态为每个动作逼近 Q 值。在这种模型的帮助下，我们将创造一个能玩《毁灭战士》（Doom）的智能体！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.6974789915966386" src="https://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTUgdOgtnxE074NFIeEA4SfcazPHvOWD04jfvdy1dYZ20c7gSbSonHvw/640?wx_fmt=gif" data-type="gif" data-w="119" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">我们的 DQN 智能体</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在这篇文章中，你将学习到：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">什么是深度 Q 学习（DQL）？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">使用 DQL 的最佳策略是什么？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如何处理有时间限制的问题？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为什么要使用经历重放（experience replay）？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">DQL 背后的数学</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如何用 TensorFlow 实现它？</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>让 Q 学习变「深度」</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在上一篇文章中，我们使用 Q 学习算法创造了一个能玩《Frozen Lake》的智能体。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们实现了用于创建和更新 Q-table 的 Q 学习函数。你可以将其看作是一个参考表，能帮助我们找到一个给定状态下一个动作的最大预期未来奖励。这个策略很不错——但却难以扩大规模。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">比如我们今天要完成的目标。我们将创造一个能玩《毁灭战士》的智能体。《毁灭战士》是一个有很大的状态空间的环境（有数百万个不同状态）。为这样的环境创建和更新 Q-table 根本不行。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">针对这种情况的最好想法是创建一个神经网络，使之能在给定状态下逼近每个动作的不同 Q 值。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.7525" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTSNYdjeBwY9thEWiaLSj9nYX5txqfEiaickHVYtI3Via4sY4U8dREJRvMrA/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>深度 Q 学习的工作方式</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这是我们的深度 Q 学习的架构：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.194" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT22mfNamjOZLueiclzEaAZiaiaL8yGWopISkekcQ4rg0GdjbkiczLF5Lz3g/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">看起来很复杂，但我会一步步地解释这个架构。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们的深度 Q 神经网络以 4 帧一组为输入。这些帧会通过该网络，然后为给定状态的每个可能动作输出一个 Q 值的向量。我们需要取这个向量的最大 Q 值来找到我们最好的动作。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一开始的时候，这个智能体的表现真的很差。但随着时间的推移，它能越来越好地将帧（状态）与最佳的动作关联起来。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">预处理部分</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="0.4185714285714286" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTlO79EnGumnSj7U1ic44ooT9URO0FSK1kMts7ju3nhztV0SELWyB2eLA/640?wx_fmt=png" data-type="png" data-w="700" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">预处理是很重要的步骤。我们希望降低状态的复杂度，从而减少训练所需的计算时间。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先，我们可以将每个状态灰度化。颜色并不能提供重要的信息（在我们的情况中，我们只需要找到敌人并杀死敌人，而我们不需要颜色就能找到它们）。这能节省很多资源，因为我们将 3 个颜色通道（RGB）变成了 1 个通道（灰度）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后我们对帧进行裁剪。在我们的情况下，可以看到屋顶实际没什么用。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后我们降低帧的尺寸，再将 4 个子帧堆叠到一起。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">时间有限的问题</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Arthur Juliani 在他的文章中对此给出了很好的解释，参阅：https://goo.gl/ZU2Z9a。他有个很巧妙的想法：使用 LSTM 神经网络来处理这个问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但是，我觉得对初学者来说，使用堆叠的帧更好。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你可能会问的第一个问题是：我们为什么要把帧堆叠到一起？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们把帧堆叠到一起的原因是这有助于我们处理时间有限的问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">比如，在《乒乓球》（Pong）游戏中，你会看到这样的帧：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5325" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTR6IWiarNXS1mibgW8qrEBMkD6lg7uywpUCBS0MrwVicXB47ed2q7ugjNQ/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你能告诉我球会往哪边走吗？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">不能，因为一帧不足以体现运动情况。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但如果我再添加另外 3 帧呢？这里你就能看到球正往右边运动：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.16625" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT9Ra95hplwzlm8UWAfYDZIygnB5KiaBUjsHTqlvNexCMEn8RwDUYBatQ/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">对于我们的《毁灭战士》智能体而言，道理也是一样。如果我们一次只为其提供 1 帧，它就没有运动的概念。如果它没法确定目标的移动方向和速度，它又怎能做出正确的决定呢？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">使用卷积网络</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这些帧会在 3 个卷积层中得到处理。这些层让你能利用图像之中的空间关系。另外，因为帧是堆叠在一起的，所以你可以利用这些帧之间的一些空间属性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果你对卷积不熟悉，可以阅读 Adam Geitgey 这篇出色的直观介绍：https://goo.gl/6Dl7EA。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这里的每个卷积层都使用 ELU 作为激活函数。事实证明，ELU 是一种用于卷积层的优良激活函数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们使用了一个带有 ELU 激活函数的万全连接层和一个输出层（一个带有线性激活函数的完全连接层），其可以得到每个动作的 Q 值估计。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">经历重放：更高效地利用已观察过的经历</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">经历重放（Experience Replay）有助于我们完成两件事：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">避免忘记之前的经历</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">降低经历之间的相关性</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我将会解释这两个概念。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这部分内容和插图的灵感来自 Udacity 的「深度学习基础」纳米学位课程中的深度 Q 学习章节。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">避免忘记之前的经历</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们有个大问题：权重的可变性，因为动作和状态之间有很高的相关性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">回忆一下我们在第一篇文章中介绍的强化学习过程：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.455" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT75ic80iaWGenuO7pTAibZbSwoKdWkEkhoDOYoSmNEs2l4P6icJAhSc0n5A/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在每个时间步骤，我们都会收到一个元组（状态，动作，奖励，新状态）。我们从中学习（将该元组输入我们的神经网络），然后丢弃这个经历。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们的问题是为神经网络提供来自与环境交互的序列样本。因为在它使用新经历覆写时，往往会忘记之前的经历。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">比如，如果我们先在第一关，然后到了第二关（这两者完全不一样），那么我们的智能体就会忘记在第一关的做法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.4459279038718291" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT46picf8tJFCXsSXcLiaRzLD7THJEccVr7g3iaYSTicURbXiaW1PNHf68ssg/640?wx_fmt=png" data-type="png" data-w="749" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">在学习水中关卡的玩法后，我们的智能体会忘记第一关的玩法。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">因此，多次学习之前的经历会更加有效。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们的解决方案：创建一个「回放缓冲（replay buffer）」。这能在存储经历元组的同时与环境进行交互，然后我们可以采样少部分元组以输入我们的神经网络。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你可以将回放缓冲看作是一个文件夹，其中每个表格都是一个经历元组。你可以通过与环境的交互向其提供信息。然后你可以随机取某个表格来输入神经网络。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.123" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT5fjvRibAQXtqib7YnhjE0TPytfTTec4ibIf7SvXibUBda4wzuAyQ5zPZ3A/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这能防止网络只学习它刚刚做的事情。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">降低经历之间的相关性</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们还有另一个问题——我们知道每个动作都会影响下一个状态。这会输出一个高度相关的经历元组序列。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果我们按顺序训练这个网络，我们的智能体就有被这种相关性效应影响的风险。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">通过随机采样回放缓冲，我们可以打破这种相关性。这能防止动作值发生灾难性的震荡或发散。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">举个例子能让人更好地理解。比如说我们在玩一个第一人称的射击游戏，其中的怪可能出现在左边，也可能出现在右边。我们的智能体的目标是用枪打怪。它有两把枪和两个动作：射击左边或射击右边。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.56" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTWviaOFbUlqSv8pznJtlyxvZ9vydUicvjvUIOIib00USic6AQMjkJjgsNyw/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">这张表表示 Q 值的近似。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们学习了有顺序的经历。假如我们知道：如果我们击中了一个怪，下一个怪出现在同一方向的概率是 70%。在我们的情况中，这就是我们的经历元组之间的相关性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">让我们开始训练。我们的智能体看到了右边的怪，然后使用右边的枪射击了它。这做对了！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后下一个怪也来自右边（有 70% 的概率），然后智能体又会射击右边的枪。再次成功，很好！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后一次又一次亦复如是……</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.56" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTNPrEyAicVzibrFcFhUKogPLkK8VZNHMM5YdribQPIVeyiapLRn3ayibUyJg/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">红色的枪是所采取的动作。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">问题是，这种方法会增大在整个状态空间中使用右边的枪的值。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.56" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTP4BSIwQDHalw0iasK5XLibP1lqzftNLIBa0WjX1o0sFLUZK4dOkIdGuw/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">我们可以看到怪在左边而射击右边的枪是正例（即使这不合理）。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果我们的智能体没有看到很多左边出怪的样本（因为只有 30% 的可能性在左边），那么我们的智能体最后就只会选择右边的枪，而不管从那边出怪。这根本不合理。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.56" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTzrjFBLo5LGuXlzYrHA6WOw4muNZsXibWAh8BZWliciciad52Beb3cD5Gvg/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">即使左边出怪，我们的智能体也会开右边的枪。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们有两种解决这一问题的策略。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先，我们必须停止在学习的同时与环境进行交互。我们应该尝试不同的情况，随机地玩玩以探索状态空间。我们可以将这些经历保存在回放缓冲之中。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后，我们可以回忆这些经历并从中学习。在那之后，再回去调整更新过的价值函数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这样，我们才会有更好的样本集。我们才能根据这些样本生成模式，以任何所需的顺序回忆它们。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这有助于避免过于关注状态空间的一个区域。这能防止不断强化同一个动作。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这种方法可以被视为一种形式的监督学习。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在未来的文章中，我们还将介绍使用「优先的经历重放」。这让我们可以更加频繁地为神经网络提供罕见或「重要的」元组。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>我们的深度 Q 学习算法</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先来点数学运算。回想一下我们使用贝尔曼方程在给定状态和动作下更新 Q 值的方法：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.203" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTqSWqPFUVKs4jvOSKmkIDvjR5jibZpLsOnl5Wp1asickyI1gtOakdQC4A/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在我们的这个案例中，我们希望更新神经网络的权重以减小误差。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">通过求取我们的 Q_target（来自下一个状态的最大可能值）和 Q_value（我们当前预测的 Q 值）之间的差异，可以计算误差（TD 误差）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.292" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTBBnqhgyff4gWn2DAenYMtr9ibHZ4RMgOFkvOcLvkiaWBl0iaXzvVPic0IQ/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><pre style="box-sizing: border-box;margin-top: 0px;margin-bottom: 0px;padding: 0px;font-size: 16px;color: rgb(62, 62, 62);line-height: inherit;font-variant-ligatures: normal;orphans: 2;widows: 2;background-color: rgb(255, 255, 255);"><code class="python language-python hljs" style="box-sizing: border-box;margin-right: 2px;margin-left: 2px;padding: 0.5em;font-size: 14px;color: rgb(169, 183, 198);line-height: 18px;border-top-left-radius: 0px;border-top-right-radius: 0px;border-bottom-right-radius: 0px;border-bottom-left-radius: 0px;background-color: rgb(40, 43, 46);font-family: Consolas, Inconsolata, Courier, monospace;display: block;overflow-x: auto;letter-spacing: 0px;word-wrap: normal !important;word-break: normal !important;overflow-y: auto !important;background-position: initial initial;background-repeat: initial initial;">Initialize Doom Environment E<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">Initialize replay Memory M <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">with</span> capacity N (= finite capacity)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">Initialize the DQN weights w<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> episode <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> max_episode:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    s = Environment state<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> steps <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> max_steps:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Choose action a <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">from</span> state s using epsilon greedy.<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Take action a, get r (reward) <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">and</span> s<span class="hljs-string" style="box-sizing: border-box;font-size: inherit;color: rgb(238, 220, 112);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">' (next state)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Store experience tuple &lt;s, a, r, s'</span>&gt; <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> M<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         s = s<span class="hljs-string" style="box-sizing: border-box;font-size: inherit;color: rgb(238, 220, 112);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">' (state = new_state)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Get random minibatch of exp tuples from M<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Set Q_target = reward(s,a) +  γmaxQ(s'</span>)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Update w =  α(Q_target - Q_value) *  ∇w Q_value</code></pre><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这个算法中发生着两个过程：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们采样我们执行动作的环境并将所观察到的经历元组存储在回放记忆（replay memory）中。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">随机选择小批量的元组，并使用梯度下降更新步骤从中学习。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>实现我们的深度 Q 神经网络</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">现在我们知道其工作方式了，我们可以一步步地实现它了。我们在这个 Jupyter 笔记中介绍了代码的每个步骤和每一部分：https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/DQN%20Doom/Deep%20Q%20learning%20with%20Doom.ipynb。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一步步完成之后，你就创造出了一个能学习玩《毁灭战士》的智能体！</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-style: italic;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">原文链接：https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8</span></span></span></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="2110378925" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                        
                        <div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>

        
        <div class="rich_media_area_extra">

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_friend_cmt_area" style="display:none">
              
              
              
            </div>

                        <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_area" style="display:none">
            </div>
                    </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div><div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    教程 | 深度Q学习：一步步实现能玩《毁灭战士》的智能体                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-21</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border: 0px currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自Medium</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12.000000953674316px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">Thomas Simonini</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="font-family: inherit;text-decoration: inherit;max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">参与：Panda</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"></blockquote><blockquote style="max-width: 100%;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 14px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;">近年来，深度强化学习已经取得了有目共睹的成功。机器之心也曾发布过很多介绍强化学习基本理论和前沿进展的文章，比如《<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650722774&amp;idx=1&amp;sn=9d1ba901077fd3902a410e409e6211a9&amp;chksm=871b15a8b06c9cbe00ac088a504d566789498787e0bab859e1888a7c5334ebf1525a1255bc43&amp;scene=21#wechat_redirect" target="_blank">专题 | 深度强化学习综述：从 AlphaGo 背后的力量到学习资源分享（附论文）</a>》。近日，深度学习工程师 Thomas Simonini 在 freeCodeCamp 上发表了介绍深度强化学习的系列文章，已发布的三篇分别介绍了强化学习基本概念、<a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650741014&amp;idx=3&amp;sn=8f82248e34cf62ef7b0d12b66a893d95&amp;chksm=871add68b06d547e766f4470e8be2f698fea03d2ab7a896101003122bac1d23724e6a1957a87&amp;scene=21#wechat_redirect" target="_blank">Q 学习</a>以及在《毁灭战士》游戏上开发智能体的过程。机器之心已经编译介绍了其中第二篇，本文为第三篇，前两篇可参阅：</span></span></p><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 14px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;"><br></span></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 14px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;"><span style="font-size: 14px;text-align: justify;">https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419 </span></span></span></p></li><li><p style="max-width: 100%;min-height: 1em;text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><a href="http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650741014&amp;idx=3&amp;sn=8f82248e34cf62ef7b0d12b66a893d95&amp;chksm=871add68b06d547e766f4470e8be2f698fea03d2ab7a896101003122bac1d23724e6a1957a87&amp;scene=21#wechat_redirect" target="_blank"><span style="max-width: 100%;font-size: 14px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="font-size: 14px;text-align: justify;"><span style="font-size: 14px;text-align: justify;">通过 Q-learning 深入理解强化学习</span></span></span></a></p></li></ul></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">上一次，我们介绍了 Q 学习：这种算法可以得到 Q-table，并且智能体可将其用于寻找给定状态的最佳动作。但正如我们所见，当状态空间很大时，求取和更新 Q-table 的效果会很差。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本文是深度强化学习系列博客文章的第三篇。我们将在本文中介绍创造一个深度 Q 网络的过程。我们不会使用 Q-table，我们会实现一个神经网络，其以状态为输入，然后基于该状态为每个动作逼近 Q 值。在这种模型的帮助下，我们将创造一个能玩《毁灭战士》（Doom）的智能体！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: center;"><img class="" data-copyright="0" data-ratio="0.6974789915966386" src="https://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTUgdOgtnxE074NFIeEA4SfcazPHvOWD04jfvdy1dYZ20c7gSbSonHvw/640?wx_fmt=gif" data-type="gif" data-w="119" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">我们的 DQN 智能体</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在这篇文章中，你将学习到：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">什么是深度 Q 学习（DQL）？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">使用 DQL 的最佳策略是什么？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如何处理有时间限制的问题？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为什么要使用经历重放（experience replay）？</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">DQL 背后的数学</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如何用 TensorFlow 实现它？</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>让 Q 学习变「深度」</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在上一篇文章中，我们使用 Q 学习算法创造了一个能玩《Frozen Lake》的智能体。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们实现了用于创建和更新 Q-table 的 Q 学习函数。你可以将其看作是一个参考表，能帮助我们找到一个给定状态下一个动作的最大预期未来奖励。这个策略很不错——但却难以扩大规模。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">比如我们今天要完成的目标。我们将创造一个能玩《毁灭战士》的智能体。《毁灭战士》是一个有很大的状态空间的环境（有数百万个不同状态）。为这样的环境创建和更新 Q-table 根本不行。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">针对这种情况的最好想法是创建一个神经网络，使之能在给定状态下逼近每个动作的不同 Q 值。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.7525" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTSNYdjeBwY9thEWiaLSj9nYX5txqfEiaickHVYtI3Via4sY4U8dREJRvMrA/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>深度 Q 学习的工作方式</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这是我们的深度 Q 学习的架构：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.194" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT22mfNamjOZLueiclzEaAZiaiaL8yGWopISkekcQ4rg0GdjbkiczLF5Lz3g/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">看起来很复杂，但我会一步步地解释这个架构。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们的深度 Q 神经网络以 4 帧一组为输入。这些帧会通过该网络，然后为给定状态的每个可能动作输出一个 Q 值的向量。我们需要取这个向量的最大 Q 值来找到我们最好的动作。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一开始的时候，这个智能体的表现真的很差。但随着时间的推移，它能越来越好地将帧（状态）与最佳的动作关联起来。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">预处理部分</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong></p><p><img class="" data-copyright="0" data-ratio="0.4185714285714286" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTlO79EnGumnSj7U1ic44ooT9URO0FSK1kMts7ju3nhztV0SELWyB2eLA/640?wx_fmt=png" data-type="png" data-w="700" style=""></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">预处理是很重要的步骤。我们希望降低状态的复杂度，从而减少训练所需的计算时间。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先，我们可以将每个状态灰度化。颜色并不能提供重要的信息（在我们的情况中，我们只需要找到敌人并杀死敌人，而我们不需要颜色就能找到它们）。这能节省很多资源，因为我们将 3 个颜色通道（RGB）变成了 1 个通道（灰度）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后我们对帧进行裁剪。在我们的情况下，可以看到屋顶实际没什么用。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后我们降低帧的尺寸，再将 4 个子帧堆叠到一起。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">时间有限的问题</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">Arthur Juliani 在他的文章中对此给出了很好的解释，参阅：https://goo.gl/ZU2Z9a。他有个很巧妙的想法：使用 LSTM 神经网络来处理这个问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但是，我觉得对初学者来说，使用堆叠的帧更好。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你可能会问的第一个问题是：我们为什么要把帧堆叠到一起？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们把帧堆叠到一起的原因是这有助于我们处理时间有限的问题。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">比如，在《乒乓球》（Pong）游戏中，你会看到这样的帧：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5325" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTR6IWiarNXS1mibgW8qrEBMkD6lg7uywpUCBS0MrwVicXB47ed2q7ugjNQ/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你能告诉我球会往哪边走吗？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">不能，因为一帧不足以体现运动情况。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但如果我再添加另外 3 帧呢？这里你就能看到球正往右边运动：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.16625" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT9Ra95hplwzlm8UWAfYDZIygnB5KiaBUjsHTqlvNexCMEn8RwDUYBatQ/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">对于我们的《毁灭战士》智能体而言，道理也是一样。如果我们一次只为其提供 1 帧，它就没有运动的概念。如果它没法确定目标的移动方向和速度，它又怎能做出正确的决定呢？</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">使用卷积网络</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这些帧会在 3 个卷积层中得到处理。这些层让你能利用图像之中的空间关系。另外，因为帧是堆叠在一起的，所以你可以利用这些帧之间的一些空间属性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果你对卷积不熟悉，可以阅读 Adam Geitgey 这篇出色的直观介绍：https://goo.gl/6Dl7EA。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这里的每个卷积层都使用 ELU 作为激活函数。事实证明，ELU 是一种用于卷积层的优良激活函数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们使用了一个带有 ELU 激活函数的万全连接层和一个输出层（一个带有线性激活函数的完全连接层），其可以得到每个动作的 Q 值估计。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">经历重放：更高效地利用已观察过的经历</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">经历重放（Experience Replay）有助于我们完成两件事：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">避免忘记之前的经历</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">降低经历之间的相关性</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我将会解释这两个概念。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这部分内容和插图的灵感来自 Udacity 的「深度学习基础」纳米学位课程中的深度 Q 学习章节。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">避免忘记之前的经历</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们有个大问题：权重的可变性，因为动作和状态之间有很高的相关性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">回忆一下我们在第一篇文章中介绍的强化学习过程：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.455" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT75ic80iaWGenuO7pTAibZbSwoKdWkEkhoDOYoSmNEs2l4P6icJAhSc0n5A/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在每个时间步骤，我们都会收到一个元组（状态，动作，奖励，新状态）。我们从中学习（将该元组输入我们的神经网络），然后丢弃这个经历。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们的问题是为神经网络提供来自与环境交互的序列样本。因为在它使用新经历覆写时，往往会忘记之前的经历。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">比如，如果我们先在第一关，然后到了第二关（这两者完全不一样），那么我们的智能体就会忘记在第一关的做法。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.4459279038718291" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT46picf8tJFCXsSXcLiaRzLD7THJEccVr7g3iaYSTicURbXiaW1PNHf68ssg/640?wx_fmt=png" data-type="png" data-w="749" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">在学习水中关卡的玩法后，我们的智能体会忘记第一关的玩法。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">因此，多次学习之前的经历会更加有效。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们的解决方案：创建一个「回放缓冲（replay buffer）」。这能在存储经历元组的同时与环境进行交互，然后我们可以采样少部分元组以输入我们的神经网络。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">你可以将回放缓冲看作是一个文件夹，其中每个表格都是一个经历元组。你可以通过与环境的交互向其提供信息。然后你可以随机取某个表格来输入神经网络。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.123" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzT5fjvRibAQXtqib7YnhjE0TPytfTTec4ibIf7SvXibUBda4wzuAyQ5zPZ3A/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这能防止网络只学习它刚刚做的事情。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">降低经历之间的相关性</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们还有另一个问题——我们知道每个动作都会影响下一个状态。这会输出一个高度相关的经历元组序列。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果我们按顺序训练这个网络，我们的智能体就有被这种相关性效应影响的风险。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">通过随机采样回放缓冲，我们可以打破这种相关性。这能防止动作值发生灾难性的震荡或发散。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">举个例子能让人更好地理解。比如说我们在玩一个第一人称的射击游戏，其中的怪可能出现在左边，也可能出现在右边。我们的智能体的目标是用枪打怪。它有两把枪和两个动作：射击左边或射击右边。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.56" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTWviaOFbUlqSv8pznJtlyxvZ9vydUicvjvUIOIib00USic6AQMjkJjgsNyw/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">这张表表示 Q 值的近似。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们学习了有顺序的经历。假如我们知道：如果我们击中了一个怪，下一个怪出现在同一方向的概率是 70%。在我们的情况中，这就是我们的经历元组之间的相关性。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">让我们开始训练。我们的智能体看到了右边的怪，然后使用右边的枪射击了它。这做对了！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后下一个怪也来自右边（有 70% 的概率），然后智能体又会射击右边的枪。再次成功，很好！</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后一次又一次亦复如是……</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.56" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTNPrEyAicVzibrFcFhUKogPLkK8VZNHMM5YdribQPIVeyiapLRn3ayibUyJg/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">红色的枪是所采取的动作。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">问题是，这种方法会增大在整个状态空间中使用右边的枪的值。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.56" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTP4BSIwQDHalw0iasK5XLibP1lqzftNLIBa0WjX1o0sFLUZK4dOkIdGuw/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">我们可以看到怪在左边而射击右边的枪是正例（即使这不合理）。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如果我们的智能体没有看到很多左边出怪的样本（因为只有 30% 的可能性在左边），那么我们的智能体最后就只会选择右边的枪，而不管从那边出怪。这根本不合理。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.56" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTzrjFBLo5LGuXlzYrHA6WOw4muNZsXibWAh8BZWliciciad52Beb3cD5Gvg/640?wx_fmt=png" data-type="png" data-w="800" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">即使左边出怪，我们的智能体也会开右边的枪。</span></em></span><br><span style="font-size: 14px;"></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们有两种解决这一问题的策略。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先，我们必须停止在学习的同时与环境进行交互。我们应该尝试不同的情况，随机地玩玩以探索状态空间。我们可以将这些经历保存在回放缓冲之中。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">然后，我们可以回忆这些经历并从中学习。在那之后，再回去调整更新过的价值函数。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这样，我们才会有更好的样本集。我们才能根据这些样本生成模式，以任何所需的顺序回忆它们。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这有助于避免过于关注状态空间的一个区域。这能防止不断强化同一个动作。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这种方法可以被视为一种形式的监督学习。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在未来的文章中，我们还将介绍使用「优先的经历重放」。这让我们可以更加频繁地为神经网络提供罕见或「重要的」元组。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>我们的深度 Q 学习算法</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">首先来点数学运算。回想一下我们使用贝尔曼方程在给定状态和动作下更新 Q 值的方法：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.203" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTqSWqPFUVKs4jvOSKmkIDvjR5jibZpLsOnl5Wp1asickyI1gtOakdQC4A/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在我们的这个案例中，我们希望更新神经网络的权重以减小误差。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">通过求取我们的 Q_target（来自下一个状态的最大可能值）和 Q_value（我们当前预测的 Q 值）之间的差异，可以计算误差（TD 误差）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.292" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8GkZEft8vap2wf7yxZnZzTBBnqhgyff4gWn2DAenYMtr9ibHZ4RMgOFkvOcLvkiaWBl0iaXzvVPic0IQ/640?wx_fmt=png" data-type="png" data-w="1000" style=""></p><pre style="box-sizing: border-box;margin-top: 0px;margin-bottom: 0px;padding: 0px;font-size: 16px;color: rgb(62, 62, 62);line-height: inherit;font-variant-ligatures: normal;orphans: 2;widows: 2;background-color: rgb(255, 255, 255);"><code class="python language-python hljs" style="box-sizing: border-box;margin-right: 2px;margin-left: 2px;padding: 0.5em;font-size: 14px;color: rgb(169, 183, 198);line-height: 18px;border-top-left-radius: 0px;border-top-right-radius: 0px;border-bottom-right-radius: 0px;border-bottom-left-radius: 0px;background-color: rgb(40, 43, 46);font-family: Consolas, Inconsolata, Courier, monospace;display: block;overflow-x: auto;letter-spacing: 0px;word-wrap: normal !important;word-break: normal !important;overflow-y: auto !important;background-position: initial initial;background-repeat: initial initial;">Initialize Doom Environment E<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">Initialize replay Memory M <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">with</span> capacity N (= finite capacity)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">Initialize the DQN weights w<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> episode <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> max_episode:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    s = Environment state<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">    <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">for</span> steps <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> max_steps:<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Choose action a <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">from</span> state s using epsilon greedy.<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Take action a, get r (reward) <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">and</span> s<span class="hljs-string" style="box-sizing: border-box;font-size: inherit;color: rgb(238, 220, 112);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">' (next state)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Store experience tuple &lt;s, a, r, s'</span>&gt; <span class="hljs-keyword" style="box-sizing: border-box;font-size: inherit;color: rgb(248, 35, 117);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">in</span> M<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         s = s<span class="hljs-string" style="box-sizing: border-box;font-size: inherit;color: rgb(238, 220, 112);line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">' (state = new_state)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;"><br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Get random minibatch of exp tuples from M<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Set Q_target = reward(s,a) +  γmaxQ(s'</span>)<br style="box-sizing: border-box;font-size: inherit;color: inherit;line-height: inherit;word-wrap: inherit !important;word-break: inherit !important;">         Update w =  α(Q_target - Q_value) *  ∇w Q_value</code></pre><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这个算法中发生着两个过程：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们采样我们执行动作的环境并将所观察到的经历元组存储在回放记忆（replay memory）中。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">随机选择小批量的元组，并使用梯度下降更新步骤从中学习。</span></p></li></ul><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong>实现我们的深度 Q 神经网络</strong></span></p><p style="text-align: center;line-height: 1.75em;"><span style="font-size: 16px;"><strong><br></strong></span></p><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 14px;">现在我们知道其工作方式了，我们可以一步步地实现它了。我们在这个 Jupyter 笔记中介绍了代码的每个步骤和每一部分：https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/DQN%20Doom/Deep%20Q%20learning%20with%20Doom.ipynb。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">一步步完成之后，你就创造出了一个能学习玩《毁灭战士》的智能体！</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="color: rgb(62, 62, 62);font-size: 14px;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;" width="48px"></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-style: italic;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">原文链接：https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8</span></span></span></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);font-size: 16px;white-space: normal;background-color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(62, 62, 62);white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="2110378925" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx3d171e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
