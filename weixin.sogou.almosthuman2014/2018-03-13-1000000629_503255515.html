<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>CVPR 2018 | 新研究提出深度残差等价映射：由正脸加强侧脸识别效果</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1521766745&amp;src=3&amp;ver=1&amp;signature=99gzbjxvb0Nvmp8T5dvdOP0Wt0h9*UhenSWaXoWP1hwfOdOAys*bLWsG5I2X4RP-FzCVpSX0eT*j8nc*Ns1PstJH1W3ZnUmFQyipPVRnCx8kueu9vURfq5JLrP1X-tZ5vZG-2VOUwzns-1xQ9qxuwjxN00Z7yfQz4ACzNwYRLNU=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    CVPR 2018 | 新研究提出深度残差等价映射：由正脸加强侧脸识别效果                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-03-13</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><section style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自arXiv</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="text-align: center;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">作者：</span></strong><span style="color: rgb(136, 136, 136);"><strong><span style="font-size: 12px;">Kaidi Cao等</span></strong><strong><span style="font-size: 12px;"></span></strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">参与：李诗萌、白妤昕、思源</span></strong></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 14px;text-align: justify;">由于类别样本不均衡，人脸检测只在正脸识别上有优秀的表现，它们很难识别侧脸样本。近日，香港中文大学和商汤科技等研究者提出了一种在深度表示空间中通过等变映射在正脸和侧脸间建立联系的方法，该方法的计算开销较少，但可以大大提升侧脸识别效果。</span></p></blockquote><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">引言</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">深度学习的出现大大推动了人脸识别的发展。而人脸识别的焦点倾向于以正脸附近为中心，然而在不受限的环境中进行人脸识别，并不能保证其结果。尽管人类从正面识别侧面的表现只比从正面识别正面的表现差一点，可现存的算法在处理类似问题时准确率会下降 10% 以上。因此，姿势的变化仍旧是人脸识别应用在现实世界的重大挑战。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.6232091690544412" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8SoHjRveoUkKr6WSMfHNZHNMwax66DzVc6hYibX26uIeiaice8uYC234lzkZEClaC27z0m83F7OULuQ/640?wx_fmt=png" data-type="png" data-w="698" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>图 1：在极具挑战性的正面-侧面面部数据集上对最先进的人脸识别模型进行测试。显而易见，不同人的侧脸很容易会被进行错误匹配（假正类），而同一个人的正脸可能没法和他的侧脸匹配到，从而导致了假负类。</em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em><br></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们在图 1 中展示了最先进的人脸识别模型的错误模式。我们训练了与 [34] 中提到的一样的模型——ResNet-18 模型。这个模型在 LFW 基准数据集中的准确率高达 99.3%。尽管该模型很强大，但它还是会误匹配不同人的正脸和侧脸从而得到一些假正类结果。此外，该模型还会错配相同个体的正脸和侧脸，从而导致假负类结果。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">为什么人脸识别技术在侧脸上表现欠佳？深度学习系统很大程度上依靠数据驱动。一般而言，模型的泛化能力和数据量成正比。模型训练使用的数据集正脸和侧脸数据量不均衡，因此训练出的模型更擅长分辨正面。目前我们还没有涵盖人类所有姿势且分布均匀的数据集，因此研究人员要用其他方法解决侧脸识别问题。很多方法在识别前尽可能细致地描述面部 3D 结构，使其归一化为只含正面的图像；或采用另一个深度模型（或生成对抗网络）将人脸转正。这些方法会给整个系统增加负担。此外，面部图像，尤其是极侧面的图像很难转化为自然状态。一般而言，合成的正面图片有人工造成的遮挡或非严格表情，它们都会影响模型的性能。我们还可以采用分而治之的方法，也就是说，用独立的模型来学习特定姿态的一致性特征 [19]。但这些策略因为使用多个模型而会增加计算成本。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.5622317596566524" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8SoHjRveoUkKr6WSMfHNZH7twO2mIK7I8BlnnYnosoLTOK3bibhmjq6Ec0icic6thSCUJPBeMVyWFkw/640?wx_fmt=png" data-type="png" data-w="699" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 2：该图上部分表明个体不同姿势嵌入的深度特征。DREAM 模块对侧脸特征添加残差，还可以将其映射到正脸空间上。我们在图片下部分展示了实际的侧脸重构图和其正面映射。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">本研究假设在深度特征空间中，侧脸区域和正脸区域有关联。图 2 表明同一主体不同姿势的面部的深度表示。输入任意姿势的图像，我们可以将其特征通过添加的残差映射函数映射到正脸的特征空间上。这一理论和特征等变性的概念很接近，通过特征等变性发现，可以通过转换输入图得到深度学习层的表示。有趣的是，这样的转换可以通过基于数据的映射函数学习到，而映射函数之后还可以应用于控制输入图的表示，以达到想要的转换。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们由此受到启发，开发了深度残差等变映射（DREAM）模块。该模块可以在高层深度特征空间中将正脸和侧脸进行转换。该模块自适应地将残差添加到输入表示中，将侧脸转换为标准姿势使识别变得更为简单。为了适应任意姿势的面部输入，我们引入软门控机制（soft gate）自适应地控制残差量，这样在输入正面姿势的情况下，为保结果不变，会给极侧的姿势加入更多残差。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">从概念上讲，我们的这项工作与脸部正向化（frontalization）有关，因此我们的方法也可应用于除图像空间的正向化的其他问题。我们从实验中观察到：从侧脸特征向正脸特征转化比图像级的正向化效果更好，也就是说，在图像合成问题上该方法对负影响更为敏感。据我们所知，我们所做的这项研究，是第一次尝试在深度特征空间进行侧面到正面的转换。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">DREAM 模块的吸引力在以下方面：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ol class=" list-paddingleft-2" style="list-style-type: decimal;"><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">该模块实施简单。具体来说，DREAM 模块是一个简单有效的门控残差分支。它可以通过将这个模块拼接到基础网络，集成到现有的卷积神经网络架构中，无需改变面部表示的原始维度，还可以用标准反向传播进行端到端的训练。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">该模块权重较轻，它在基础的模型上添加的参数很少，因此无须太多计算资源。以 ResNet-18 为例，该模块只增加了 0.3% 的参数和 1.6% 的时间成本。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">基础网络识别近正面图片效果很好，且该模块能帮助基础网络在识别极端姿势的面部时取得更好的表现。该方法并不需要更详细的数据，面部数据的标准化也是以大多数现有的针对面部识别的研究为基础实现的。</span></p></li></ol><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">深度残差等变映射</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.474964234620887" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8SoHjRveoUkKr6WSMfHNZHq71N4oSbZf58JmKUXxXEfnF8qESTKThqb7zrkwVZrG2z3DTCVibiaZcQ/640?wx_fmt=png" data-type="png" data-w="699" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 3：DREAM 模块的设计思路非常简单，而且该模块易于添加到已存的 CNN 中。该模块可以轻松地将残差添加到输入表示中，将侧脸转换为标准姿势，使识别变得简单。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们描述了三种使用 DREAM 的方法。三种实验的方法比较将在后面的实验部分提出。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">拼接。部署 DREAM 模块最方便的办法是直接将模块「拼接」到训练好的 CNN 中。特别是在给定基本网络架构的时候，我们可以在不改变任何原始模型学习参数的情况下，将 DREAM 模块拼在基础网络最后的特征层中。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.648493543758967" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8SoHjRveoUkKr6WSMfHNZH9tGMBYUGhn85vdnkeiaGsnkXqY1ibQ6ibic51EdNw5mk7h9QCPPkhpuX2Q/640?wx_fmt=png" data-type="png" data-w="697" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">图 4：深度特征可视化。第一行和第三行展示的是重构侧脸原始特征。第二行和第四行描绘的是通过 DREAM 模块映射重构特征。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">端到端。我们提出的这个轻权重的模块也可以端到端的方式和主干 CNN 一起训练。给定一个简单的基础网络，我们将 DREAM 模块插入该网络，并直接用随机初始参数训练新网络。如果这个 CNN 不够简单或是之前训练过，我们可以使用现有的面部识别损失（例如，验证损失（verification）、识别损失（identification）等）用端到端的方法训练 DREAM 模块时对 CNN 进行微调。我们将这种策略命名为「end2end」。使用这种策略，模型在侧脸识别方面的表现无法保证，因为 DREAM 模块可能无法分辨正面和侧面，原因是在模块训练过程中没有具体地将一张脸的正面和侧面配对。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">端到端+重新训练。我们先将 CNN 和 DREAM 同时训练，再用成对的正面侧面的数据对 DREAM 模块进行有针对性地训练。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.13286713286713286" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8SoHjRveoUkKr6WSMfHNZHoFtvlGktnssyT7GnvqjIpjFUHKB9JticGLTtzaE8VBkQCDeBlldV9yw/640?wx_fmt=png" data-type="png" data-w="1001" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">表 1：有正面侧面设置的 CFP 数据训练得到的结果。等错误率（EER）如表所示，EER 值更低表示结果更好，加粗的是每一行中最好的结果。</span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p><img class="" data-copyright="0" data-ratio="0.8141592920353983" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8SoHjRveoUkKr6WSMfHNZH8FRKtSYxtQ11YXOpNYQrIXvrB8akcdRe1IQo1b2M6A7GicPjic2aqBuw/640?wx_fmt=png" data-type="png" data-w="678" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em style="font-size: 12px;"><span style="color: rgb(136, 136, 136);">图 6：在偏航空间用朴素 ResNet-18 和 ResNet-18+DREAM 预测得到的假正类率和假负类率间进行的比较。（a）和（b）表现了用不同偏航角（y_1，y_2）成对人脸数据预测得到的假正类率和假负类率的热力图。0.63→0.53 的意思是使用 DREAM 后，人脸偏转角度的余弦相似度从 0.63 降到了 0.53。</span></em><br></p><p style="text-align: justify;line-height: 1.75em;"><em style="font-size: 12px;"><span style="color: rgb(136, 136, 136);"><br></span></em></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">论文：Pose-Robust Face Recognition via Deep Residual Equivariant Mapping</span></strong></p><p><img class="" data-copyright="0" data-ratio="0.25880551301684535" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8SoHjRveoUkKr6WSMfHNZHwTbibRtHynI6o1eESTNQIjQDprKo659UwA0J79WBgYviaFzykW7MMC5Q/640?wx_fmt=png" data-type="png" data-w="1306" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(123, 12, 0);">论文链接：https://arxiv.org/abs/1803.00839</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"> </span></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">摘要：</span></strong><span style="font-size: 14px;">深度学习的发展使人脸识别取得了非凡的成就。然而，现在的许多人脸识别模型识别侧面的性能，尤其相较于识别正面，表现仍有不足。主要原因之一在于训练数据中正面数据和侧面数据分布不均匀——正脸数据比侧脸数据要多得多。此外，由于存在姿态的大范围变化，几何学意义上的不可变也是模型学习深度特征表示的难点之一。在本研究中，我们假设在正脸和侧脸间存在固有的映射关系，因此，在深度表示空间中可以通过等变映射在正脸和侧脸间建立联系。在构建映射的过程中，我们建立了新的深度残差等变映射（DREAM）模块，该模块可以自适应地在输入深度特征表示中添加残差连接，使侧脸表示转换为标准姿势，以简化识别。对许多强大的深度网络而言，包括 ResNet 模型，DREAM 模块在无需增强数据中侧脸部分的情况下，大大增强了模型在侧脸识别方面的表现。该模块易于使用，而且运行中计算开销较少。</span><img class="" data-copyright="0" data-ratio="0.3287671232876712" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9IcHbFIoLic1VEVWUYDcOQOd6kYzKSNx7GpKhf1OMhgW30B8WEsyibXYuvBogNHE5TQTpUQGLsWmeQ/640?wx_fmt=png" data-type="png" data-w="73" style="height: 17px;width: 51px;"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);text-align: justify;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p>
                </div>
                <script nonce="711433613" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
