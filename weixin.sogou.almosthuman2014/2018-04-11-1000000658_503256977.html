<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>观点 | 从信息论的角度理解与可视化神经网络</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1524279532&amp;src=3&amp;ver=1&amp;signature=wkek0COu6xUzfDaXM62yjBbx0KZeZsfp4FUcMZABGLPMHLFbct1DAoJqxistM7f5HIWJtxLr0TvDPm8TCN5h8V8v-MsUUQf8FbUrb5vCaTjRaF4itSjvry4Bn*uUC0PSG2Bhhjbar4jPuU9yNw4btqPup9D-LYuiQ8Rh88Fnlbs=">原文</a></p>
<div id="img-content">
                
                <h2 class="rich_media_title" id="activity-name">
                    观点 | 从信息论的角度理解与可视化神经网络                                    </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                            <em id="post-date" class="rich_media_meta rich_media_meta_text">2018-04-11</em>

                                        <a class="rich_media_meta rich_media_meta_link rich_media_meta_nickname" href="##" id="post-user">机器之心</a>
                    <span class="rich_media_meta rich_media_meta_text rich_media_meta_nickname">机器之心</span>


                    <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                        <div class="profile_inner">
                            <strong class="profile_nickname">机器之心</strong>
                            <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                            <p class="profile_meta">
                            <label class="profile_meta_label">微信号</label>
                            <span class="profile_meta_value">almosthuman2014</span>
                            </p>

                            <p class="profile_meta">
                            <label class="profile_meta_label">功能介绍</label>
                            <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                            </p>
                            
                        </div>
                        <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                            <i class="profile_arrow arrow_out"></i>
                            <i class="profile_arrow arrow_in"></i>
                        </span>
                    </div>
                </div>
                                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " id="js_content">
                    

                    

                    
                    
                    <section style="font-size: 16px;white-space: normal;max-width: 100%;color: rgb(62, 62, 62);background-color: rgb(255, 255, 255);line-height: 28.4444px;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自TowardsDataScience</span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="text-align: center;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：</strong></span><strong><span style="font-size: 12px;color: rgb(136, 136, 136);">Mukul Malik</span></strong><strong><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></strong><strong><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></strong><strong><span style="font-size: 12px;color: rgb(136, 136, 136);"></span></strong></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;font-family: inherit;text-decoration: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</span></strong></p><p style="text-align: center;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：</strong></span><strong><span style="font-size: 12px;color: rgb(136, 136, 136);">Pedro</span></strong><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">、思源</strong></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="white-space: normal;max-width: 100%;min-height: 1em;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 14px;">信息论在机器学习中非常重要，但我们通常熟知的是信息论中交叉熵等模型度量方法。最近很多研究者将信息论作为研究深度方法的理论依据，而本文的目标不是要去理解神经网络背后的数学概念，而是要在信息论的视角下可视化与解读深度神经网络。</span></p></blockquote><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「Information: the negative reciprocal value of probability.」—克劳德 香农</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">编码器-解码器</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><br></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;color: rgb(136, 136, 136);">编码器-解码器架构绝不仅仅是组合在一起的两个卷积神经网络或者循环神经网络！事实上它们甚至都可以不是神经网络！</span></p></blockquote><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.75" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1q4JVfASicZ8ZIGDD3nAxnAsySZQnK4ZkAia0zsjm0zXibD44W8kk6ic10g/640?wx_fmt=png" data-type="png" data-w="1024"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">最初从信息论的概念来说，编码器仅仅用于压缩信息而解码器可以扩展编码过的信息（https://www.cs.toronto.edu/~hinton/science.pdf）。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">而对于机器学习来说，解码和编码的过程都不是无损的，也就是说总有一些信息会丢失。编码器编码后的输出被称为上下文向量，同时它也是解码器的输入。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">常用的编码器-解码器框架配置有两种：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">解码器是编码器的逆函数。在这种设定下，解码器要尽可能地复原原始信息。它通常被用于数据去噪，这种设定有一个特殊的名字，叫做自编码器。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">编码器是一个压缩算法而解码器是一个生成算法。它用来将上下文信息从一种格式转换到另一种格式。</span></p></li></ol><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">应用示例：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ul class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">自编码器：编码器把英文文本压缩成一个向量。解码器根据这个向量生成原始的英文文本。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">编码器-解码器：编码器把英文文本压缩成一个向量。解码器根据这个向量生成原始英文文本的法语译文。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">编码器-解码器：编码器把英文文本压缩成一个向量。解码器根据文本内容生成一幅图片。</span></p></li></ul></blockquote><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">信息论</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">现在，如果我说每一个神经网络本身都是一个编码器-解码器框架；对大多数人来说，这听起来非常荒诞，但我们可以重新思考一下这个观点。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们用 X 来表示输入层，用 Y 来表示（训练集中）真实的标签或者说类别。现在我们已经知道神经网络要寻找到 X 和 Y 之间潜在的函数关系。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">因此 X 可以被视为 Y 的高熵分布。高熵是因为 X 除了 Y 的信息外还包含有许多其它的信息。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><blockquote style="white-space: normal;"><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">示例：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">「这个男孩很棒」包含了足以让我们明白其包含「positive」情感信息（二元分类）。</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但是，它也包含了如下的其它信息：</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 这是一个特定的男孩</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 这仅仅是一个男孩</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">3. 句子使用的时态是现在时</span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">现在这句话低信息熵的表示可以为「positive」，而这同样也是输出信息，我们待会再来讨论这个问题。</span></p></blockquote><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">现在想象一下每一个隐藏层都是一个单变量 H，这样多层网络就可以被表示为 H_0, H_1 ….. H_{n-1}。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">现在每一层都是一个变量，同时整个神经网络就变成了一个马尔科夫链，因为马尔科夫链中的每一个变量都仅仅依赖于前一个变量。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">所以本质上来说每一个层都以不同的抽象形式构建不同部分的信息。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">下图展示了以马尔科夫链的形式可视化神经网络。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.75" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1WyIbYGW1oMiaxPRaNssHD7qicGPVXibBykZChZlcV0m2BJXc9EzLuHrUg/640?wx_fmt=png" data-type="png" data-w="1024"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">最后一层 Y_ 应该产生一个低熵的结果（同最初的标签或者说类别'Y'相关）。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">根据信息瓶颈理论，在获取 Y_ 的过程中，输入的信息 X 经过 H 个隐藏层的压缩最终保留同 Y 最相关的信息。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">互信息</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"></span></strong></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.75" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1jgwfDFVpRjbuYp6Vjfgib7jbEsHRlnYfIe4lrnhrcGMicxeo8vPGubiaw/640?wx_fmt=png" data-type="png" data-w="1024"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">I(X,Y) = H(X)—H(X|Y)</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如上所示为互信息的表达式，其中 H 代表信息熵，H(X) 代表变量 X 的信息熵。而 H(X|Y) 表示给定 Y 时 X 的条件熵，或 H(X|Y) 表明了在 Y 已知的情况下从 X 中移除的不确定性。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">互信息的性质：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">当信息沿着马尔科夫链移动时互信息只会减少。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">对于再参数化来说互信息是恒定的，也就是说打乱一个隐藏层中的数值不会改变输出。</span></p></li></ol><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">反思瓶颈</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在神经网络的马尔科夫表达中，每一层都变成了部分信息。在信息论中，这些部分信息通常被视为是对相关信息的连续提炼。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">另一个看待这个问题的视角是：输入先被编码然后被解码为输出。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.75" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1aicIeF1s9CqteOu8Rsn2kfhNg3oicxr15EGPWXeqARmGqiaNwiak2aoe7Q/640?wx_fmt=png" data-type="png" data-w="1024"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">那么，对于足够多的隐藏层：</span><br></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">神经网络采样的复杂度由最后一个隐藏层编码的互信息决定。</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">准确度由最后一个隐藏层解码后的互信息决定。</span></p></li></ol><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.6577060931899642" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1CKxLRySusjuwL7rqeR6jvtYOnPLHe0JpaEtEvXwckKVezliaj7iaJyTA/640?wx_fmt=png" data-type="png" data-w=""></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><em style="font-size: 12px;"><span style="color: rgb(136, 136, 136);">采样复杂度是模型为了达到一定程度的准确性所需要的训练样本数和种类。</span></em><br><span style="font-size: 14px;"></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">训练阶段的互信息</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们计算了以下内容之间的互信息：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">1. 隐藏层和输入</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">2. 隐藏层和输出</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="1" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1Rnp8R13WlG8yLzYku6PlCNcugGLdXtfWGRkJDZJWDpQmgYZ5q5zDtw/640?wx_fmt=png" data-type="png" data-w="800"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;color: rgb(136, 136, 136);"><em>初始状态</em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;color: rgb(136, 136, 136);"><em><br></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在一开始，我们随机初始化网络的权重。因此网络对于正确的输出一无所知。经过连续的隐层，关于输入的互信息逐渐减少，同时隐层中关于输出的信息也同样保持了一个相对比较低的值。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="1" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1xXQcdWoSr6gyNuU4GZFqAdfSRvwhlBxGeefQbTDBHMONYJa5jw0eQQ/640?wx_fmt=png" data-type="png" data-w="800"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">压缩阶段</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">随着训练的进行，上图中的点开始向上移动，说明网络获得了关于输出的信息。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">但是上图的点同时也开始向右侧移动，说明靠后的隐层中关于输入的信息在增加。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">训练阶段所需要的时间是最长。点的密度也需要最大化，上图的点最终都聚集在了右上角。这说明输入中同输出相关的信息得到了压缩。这被称为压缩阶段。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="1" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp13NJPwMbYC9mr5WiboFaHVicia137Q8dGMFj7lMIjJmMM7HOlhbibz1eiaKQ/640?wx_fmt=png" data-type="png" data-w="800"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">扩展阶段</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">在压缩阶段之后，点开始向上并且向左移动。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这说明经过连续的隐层后，神经网络丢失了一些输入信息，并且保留到最后一个隐层的内容是关于输出的最低熵信息。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">可视化</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">马尔科夫链形式下的神经网络还说明：学习发生在隐层之间。一个隐藏层含有预测输出所需要的全部信息（包括一些噪声）。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">因此我们可以使用每一层来预测输出，这帮助我们一窥这潜藏在所谓的黑箱中层与层之间的知识。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">此外，这也能够让我们获知准确预测输出所需要的隐层数目。如果模型在较早的隐层中就已经饱和了，那么我们就可以裁剪或者丢弃掉接下来的隐层。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">这些隐层通常有几百到几千不等的维度。受人类视觉系统进化本身所限，我们无法可视化超过 3 维的内容，因此我们使用了降维技术进行可视化。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">我们有不同的方法来进行降维。Cristopher Olah 有一篇博客（http://colah.github.io/posts/2014-10-Visualizing-MNIST/）很好地阐释了这些方法。在这里我不会展开介绍 t-SNE 的细节，您可以在这篇博客（https://distill.pub/2016/misread-tsne/）中获得更多信息。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">简单来说，t-SNE 试图通过保留高维空间中点在低维空间中近邻的方式来进行降维。因此它能够产生非常准确的二维和三维图像。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">如下是一个有 2 个隐层的语言模型的层级图像。</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">关于这些图像：</span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">挑选出 16 个单词</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">用最终的语言模型对每一个上述单词找到 N 个同义词（二维中 N=200，三维中 N=50）</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">得到每一个单词在每一层的向量表达</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">使用 t-SNE 得到上述单词向量的二维和三维的降维表达</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">画出降维后的表达</span></p></li></ol><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"><br></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;"></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.4826435246995995" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1q9XNgSlOzdwlrRxR5dzKWJWRkIA2icdibv6asj3OZTHXzmCA0XUlQ4Dg/640?wx_fmt=png" data-type="png" data-w="1498"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">第一层和第二层的二维图。</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"></span></em></span></p><p style="white-space: normal;"><img class="" data-copyright="0" data-ratio="0.48452883263009844" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mEmOE6byUbK7rnC1sjfp1SGZQKLkh8sJzh9NZR8LorCQibMKlxV31Ij55sotmaWviaibGTpFvOKIug/640?wx_fmt=png" data-type="png" data-w="1422"></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);">第一层和第二层的三维图。</span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><span style="font-size: 12px;"><em><span style="color: rgb(136, 136, 136);"><br></span></em></span></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;">总结</span></strong></p><p style="white-space: normal;text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 14px;"><br></span></strong></p><ol class=" list-paddingleft-2" style=""><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">大多数的深度神经网络的工作原理都类似于解码器-编码器架构；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">压缩阶段耗费了大部分的训练时间；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">隐层的学习是自底向上的；</span></p></li><li><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 14px;">经过压缩阶段，神经网络丢弃掉的输入信息越多，输出结果就越准确（清除掉不相关的输入信息）。</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" width="48px" style="font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;visibility: visible !important;width: 48px !important;"></p></li></ol><p style="white-space: normal;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="white-space: normal;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><br></p><p style="white-space: normal;max-width: 100%;min-height: 1em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;text-align: justify;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：editor@jiqizhixin.com</span></strong></p><p style="white-space: normal;max-width: 100%;min-height: 1em;font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p><br></p>
                </div>
                <script nonce="740216844" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg/page_mp_article_improve_winwx31619e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc" id="js_preview_reward" style="display:none;">
                    <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                    <p>
                        <a class="reward_access" id="js_preview_reward_link" href="##"><span class="icon-reward"></span>赞赏</a>

                    </p>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div><div class="rich_media_tool" id="js_toobar3">
                
                                
                                            <div id="js_read_area3" class="media_tool_meta tips_global meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_primary tips_global meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                <a id="js_report_article3" style="display:none;" class="media_tool_meta tips_global meta_extra" href="##">投诉</a>

            </div><div class="rich_media_tool" id="js_sg_bar">
                
                                
                                
            </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
