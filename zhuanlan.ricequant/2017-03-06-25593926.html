<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Grid2dLSTM 构建高抽象的多因子股票时间序列预测模型</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/25593926">原文</a></p>
<div class="title-image"><img src="https://pic3.zhimg.com/v2-c8d33b80c32772e7211c94ba55cb4468_r.jpg" alt=""></div><h2>1前置假设</h2><p>这里假设股票市场的对数收益率在泛泛的情况下假想为一个弱平稳时间序列。</p><ul><li><p>Stacked LSTM</p></li></ul><p>Stacked LSTM通过堆砌多层LSTM的抽取下层LSTM的hidden_state的信息获得实例训练算法，这个取得了很好的效果提升，但是受限于tanh，这里使用的LSTM模型在超过5层之后的需要消耗的计算资源过大。我这里使用一个3层的Stacked LSTM模型进行。</p><ul><li><p>Grid2dLSTM </p></li></ul><p>Grid LSTM 是吸收Stacked LSTM和Multidimensional LSTM两种LSTM单元间网络结构形成的新算法设计。相对于Stacked结构，Grid2d使用维度概念代替Stacked里面的深度概念(层数)，将深度构建为另一个维度的序列，即转换多层LSTM为x，y维度的多维度LSTM网格结构。实验显示，Grid相对于Stacked效果有明显提升。由于计算资源有限，我只测试了3层Tied 和 3层No Tied的网络结构。</p><h2>2 模型数据</h2><p>对HS300成份股使用连续59交易日多因子数据进行many to one 训练。</p><h2>3 数据对比<br></h2><p>1) 单层LSTM的样本外准确率为0.66</p><img src="https://pic1.zhimg.com/v2-21ae720817ad09fcc71075cf9f280b0e_r.png" data-rawwidth="481" data-rawheight="329"><br><p>2) 3层Stacked LSTM的样本外准确率为0.78</p><img src="https://pic3.zhimg.com/v2-58fd80ed00e47adebc75fea405252982_r.png" data-rawwidth="493" data-rawheight="329"><br><p>3) 3 层Grid2dLSTM样本外准确率约为为 0.7 (由于使用CPU这里模型迭代只进行了3000次，未达到最终收敛状态)</p><img src="https://pic2.zhimg.com/v2-f34a4cde956b362486577427802e4faa_r.png" data-rawwidth="493" data-rawheight="329"><br><p>4) 3层Tied Grid2dLSTM模型的样本外准确率为 0.86 (由于使用CPU这里模型迭代只进行了3000次，未达到最终收敛状态)</p><img src="https://pic1.zhimg.com/v2-ed13d60d3d763d30c1c395c363cdb33e_r.png" data-rawwidth="493" data-rawheight="329">5) 6层<br><img src="https://pic2.zhimg.com/v2-b63f741a8435bf42b4e6e5fc1c98b5b1_r.png" data-rawwidth="493" data-rawheight="329">6）9层<img src="https://pic3.zhimg.com/v2-360e0bee687751086a9e975163b74c9f_r.png" data-rawwidth="493" data-rawheight="329"><p><a href="https://www.ricequant.com/community/topic/2753/" data-title="运行实例" class="">运行示例</a></p><br><code lang="python">def LSTM(H_upper, H_lower, m, omega):
    H = tf.concat([H_upper, H_lower], axis=1)
    g_u = tf.sigmoid(tf.add(tf.matmul(a=H, b=omega['W_u']), omega['b_u']))
    g_f = tf.sigmoid(tf.add(tf.matmul(a=H, b=omega['W_f']), omega['b_f']))
    g_o = tf.sigmoid(tf.add(tf.matmul(a=H, b=omega['W_o']), omega['b_o']))
    g_c = tf.tanh(tf.add(tf.matmul(a=H, b=omega['W_c']), omega['b_c']))
    m_new = tf.add(tf.multiply(m,g_f), tf.multiply(g_c,g_u))
    h_new = tf.tanh(tf.multiply(m_new, g_o))
    return h_new, m_new 

def scan_Grid2dLSTM(initializer, elems):
    x_i = elems
    
    # layer 1
    I = initializer[0]
    h_1_A_prev = initializer[1]
    m_1_A_prev = initializer[2]
    m_1_B_prev = initializer[3]
    
    I_x_i = tf.matmul(x_i, I)
    h_1_A, m_1_A = LSTM(I_x_i, h_1_A_prev, m_1_A_prev, W_1_A)
    h_1_B, m_1_B = LSTM(I_x_i, h_1_A, m_1_B_prev, W_1_B)
    
    # layer 2
    h_2_A_prev = initializer[4]
    m_2_A_prev = initializer[5]
    
    h_2_A, m_2_A = LSTM(h_1_B, h_2_A_prev, m_2_A_prev, W_2_A)
    h_2_B, m_2_B = LSTM(h_1_B, h_2_A, m_1_B, W_2_B)
    
    # layer 3
    h_3_A_prev = initializer[6]
    m_3_A_prev = initializer[7]
    
    h_3_A, m_3_A = LSTM(h_2_B, h_3_A_prev, m_3_A_prev, W_3_A)
    h_3_B, m_3_B = LSTM(h_2_B, h_3_A, m_2_B, W_3_B)
    
    return (I, h_1_A, m_1_A, m_1_B, h_2_A, m_2_A, h_3_A, m_3_A, h_3_B)
</code>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
