<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>ICLR 2018 | CMU&amp;谷歌大脑提出新型问答模型QANet：仅使用卷积和自注意力，性能大大优于RNN</title>
</head>
<body>
<p><a href="https://mp.weixin.qq.com/s?timestamp=1527695441&amp;src=3&amp;ver=1&amp;signature=sz35w1tHE*ocopopMwKokTgGvVsDFfdKVgscyaotBzTiG2zy-DGDjTW7qG3v1DHYqfzmEJ-7IVfIsCCuZwC-m5aUDl1Ne-JmgXXinWRWKeo1lIpdnPcuPywA6jgtiFNSTlCS*S7i0tWWzXpLqsXR-9loDnhEHMZE8768JPbL0iI=">原文</a></p>
<div id="js_top_ad_area" class="top_banner"></div><div class="rich_media_inner">
                        
        
        <div id="page-content" class="rich_media_area_primary">
            
                        
            <div id="img-content">

                
                <h2 class="rich_media_title" id="activity-name">
                    
                    <script nonce="87712946" type="text/javascript">
                        if(/(iPhone|iPad|iPod|iOS)/i.test(navigator.userAgent)){
                            document.write("<span class='rich_media_title_ios'>ICLR 2018 | CMU&amp;谷歌大脑提出新型问答模型QANet：仅使用卷积和自注意力，性能大大优于RNN");
                        }else{
                            document.write("ICLR 2018 | CMU&amp;谷歌大脑提出新型问答模型QANet：仅使用卷积和自注意力，性能大大优于RNN");
                        }
                    </script>

                                                                                </h2>
                <div id="meta_content" class="rich_media_meta_list">
                                                                                <span class="rich_media_meta rich_media_meta_nickname" id="profileBt">
                      <a href="javascript:void(0);">
                        机器之心                      </a>
                      <div id="js_profile_qrcode" class="profile_container" style="display:none;">
                          <div class="profile_inner">
                              <strong class="profile_nickname">机器之心</strong>
                              <img class="profile_avatar" id="js_profile_qrcode_img" src="" alt="">

                              <p class="profile_meta">
                              <label class="profile_meta_label">微信号</label>
                              <span class="profile_meta_value">almosthuman2014</span>
                              </p>

                              <p class="profile_meta">
                              <label class="profile_meta_label">功能介绍</label>
                              <span class="profile_meta_value">专业的人工智能媒体和产业服务平台</span>
                              </p>
                              
                          </div>
                          <span class="profile_arrow_wrp" id="js_profile_arrow_wrp">
                              <i class="profile_arrow arrow_out"></i>
                              <i class="profile_arrow arrow_in"></i>
                          </span>
                      </div>
                    </span>


                    <em id="publish_time" class="rich_media_meta rich_media_meta_text"></em>





                </div>
                
                
                
                
                                                
                                                                
                
                <div class="rich_media_content " lang='="en"' id="js_content">
                    

                    

                    
                    
                    <p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><section style="max-width: 100%;color: rgb(51, 51, 51);"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><section data-id="85660" data-custom="rgb(117, 117, 118)" data-color="rgb(117, 117, 118)" style="max-width: 100%;border-width: 0px;border-style: initial;border-color: currentcolor;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;"><section style="margin-top: 2em;padding-top: 0.5em;padding-bottom: 0.5em;max-width: 100%;border-style: solid none;font-family: inherit;text-decoration: inherit;border-top-color: rgb(204, 204, 204);border-bottom-color: rgb(204, 204, 204);border-top-width: 1px;border-bottom-width: 1px;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="margin-top: -1.2em;max-width: 100%;min-height: 1em;text-align: center;line-height: 1.75em;border-width: initial;border-style: initial;border-color: currentcolor;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(255, 255, 255);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;background-color: rgb(117, 117, 118);box-sizing: border-box !important;word-wrap: break-word !important;">选自<span style="max-width: 100%;font-size: 15px;text-align: justify;box-sizing: border-box !important;word-wrap: break-word !important;">arXiv</span></span></span></p><section data-style="white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);" style="padding: 16px 16px 10px;max-width: 100%;font-family: inherit;box-sizing: border-box !important;word-wrap: break-word !important;"><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">作者：Adams Wei Yu等</strong></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">机器之心编译</strong></span></span></p><p style="max-width: 100%;min-height: 1em;text-align: center;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(136, 136, 136);box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">参与：Geek AI、路</strong></span></span></p></section></section></section></section></section></section></section></section></section></section></section></section><p style="text-align: justify;line-height: 1.75em;"><br></p><blockquote><p><span style="font-size: 15px;text-align: justify;color: rgb(136, 136, 136);">近日，来自卡内基梅隆大学和谷歌大脑的研究者在 arXiv 上发布论文，提出一种新型问答模型 QANet，该模型去除了该领域此前常用的循环神经网络部分，仅使用卷积和自注意力机制，性能大大优于此前最优的模型。</span><br></p></blockquote><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">1 引言</span></strong></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">人们对机器阅读理解和自动问答任务的兴趣与日俱增。在过去的几年中，端到端的模型在许多具有挑战性的数据集上显示出非常好的结果，取得了显著的进步。最成功的模型通常会利用两个关键的组成部分：（1）处理序列化输入的循环模型，（2）处理长期交互的注意力组件。Seo 等人（2016）提出的 Bidirectional Attention Flow（BiDAF）模型是这两部分的一个成功组合，该模型在 SQuAD 数据集（Rajpurkar 等人在 2016 年发布）上获得了很好的效果。这些模型有一个缺点，即由于它们的循环性质导致训练和推断都十分缓慢，特别是对于长文本来说。高昂的训练开销不仅导致了很长的实验周期，限制了研究者进行迅速的迭代，还妨碍了模型被用于大型数据集。与此同时，缓慢的推断阻碍了机器阅读理解系统在实时应用中的部署。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">本论文中，为了使机器更迅速地进行理解，研究者提出去除这些模型的循环特性，仅使用卷积和自注意力机制作为构建编码器的模块，它们分别编码查询（query）和语境（context）。接着研究者通过标准的注意力机制来学习语境和问题之间的交互（Xiong et al., 2016; Seo et al., 2016; Bahdanau et al., 2015）。在最终解码出每个点作为答案区间的起始点和终点的概率之前，得到的数据表征被再一次用研究者提出的无循环特性（recurrency-free）的编码器进行编码。本论文研究者将这个架构称为 QANet，如图 1 所示。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">该模型设计背后的关键动机是：卷积能够捕获文本的局部结构，而自注意力机制能够学习到每一对词语之间的全局相互作用。额外的「语境-查询」注意力机制是一个用于为上下文段落中的每一个位置构建 query-aware 语境向量的标准模块，这些向量将在随后的建模层中使用。该架构的前馈特性大大加快了模型的速度。在 SQuAD 数据集上进行的实验中，本论文提出的模型训练速度提升到相应 RNN 模型的 3 到 13 倍，推断速度提升到 4 到 9 倍。如果进行一个简单的对比，该模型可以在 3 小时训练时间内达到和 BiDAF 模型（Seo 等人在 2016 年提出）同样的准确率（77.0 F1 值），而后者则需要花费 15 小时。模型的加速还让研究者通过更多的迭代来训练模型，从而得到比其它有竞争力的模型更好的结果。例如，如果使本文提出的模型训练 18 个小时，则它会在开发集上获得 82.7 的 F1 值，这比 Seo 等人在 2016 年提出的模型好得多，并且与目前公布的最好结果达到了同等水平。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">由于该模型运行速度很快，因此相较于其他模型，我们可以用多得多的数据训练它。为了进一步改进模型，研究者提出了一种补充性数据增强技术来改善训练数据。这种技术通过将原始的英文句子翻译成另一种语言然后再翻译回英语来改写例句，这不仅增加了训练实例的数量，还增强了措辞的多样性。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">在 SQuAD 数据集上，用增强数据训练的 QANet 在测试集上获得了 84.6 的 F1 值，这相比 Hu 等人（2017）公布的最佳结果——81.8 有了很大提升。研究者还进行了模型简化测试（ablation test），来证明该模型中每一个组件的有效性。本论文的贡献如下：</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">提出了一种高效的阅读理解模型，它完全建立在卷积和自注意力机制之上。据了解，本论文作者是这项研究的开创者。这个组合（卷积和自注意力机制）保持了良好的准确率，同时与相应的 RNN 模型相比，在每一轮迭代中，该模型的训练速度最高达到前者的 13 倍，推断速度最高达到前者的 9 倍。速度提升使得该模型成为扩展到更大数据集的最佳候选方案。</span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">为了在 SQuAD 数据集上提升训练结果，研究者提出了一种新的数据增强技术，通过改写句子来丰富训练数据。这使得 QANet 模型获得了比目前的最佳模型更高的准确率。</span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.768" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9WbG0M4C9hajwibUfR6hsNTENB84gNJBEpEIRGRzRSF44LUSuhxSmExw/640?wx_fmt=png" data-type="png" data-w="625" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 1：QANet 架构概览（左图），包含多个编码器模块。研究者在整个模型中使用相同的编码器模块（右图），仅仅在每个模块中改变卷积层的数量。研究者在编码器模块的每一层之间使用了层归一化和残差连接技术，还在语境和问题编码器以及三个输出编码器之间共享权重。研究者在每一个编码器层的起始处加入一个位置编码，正如 Vaswani 等人在 2017 年定义的那样，它是由波长不同的正弦函数和余弦函数组成的。编码器结构内部的位置编码后面的每一个子层（卷积、自注意力机制或前馈网络其中之一）都被包装在一个残差模块中。</span></em></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.47050754458161864" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9icRtbkCnlMaZY10RfqicZ96yibyTicbyHUwuIuK2XrkBGGmeoI0En3d34w/640?wx_fmt=png" data-type="png" data-w="729" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">图 2：以法语作为中间语言（以 English-French-English 形式改写）的数据增强过程图示。其中，k 是集束宽度，即 NMT 系统生成的译文数量。</span></em></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.2109090909090909" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9Hl91ELSjXcKBuia8EPiarnicSiboONaAwBrBGNYqibpJppXn2ZgEeeibUYWA/640?wx_fmt=png" data-type="png" data-w="825" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 1：原句和改写后的句子中的答案对比。</span></em></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;">4 实验</span></strong></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.8770370370370371" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9ib33RTB4R5VDF3icZwcpPDHnR0wOxNCdWHhkcoJkuwgwBfxEfrhhkTdQ/640?wx_fmt=png" data-type="png" data-w="675" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 2：在 SQuAD 数据集上不同模型的性能。</span></em></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.11306532663316583" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9kIUMrp8B9Q51f7r6JsItCw54fXUd2ZzTjStzhJs3mu5oG1epibIb5wg/640?wx_fmt=png" data-type="png" data-w="796" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 3：QANet 模型和基于 RNN 的模型在 SQuAD 数据集上的运行速度对比，所有模型的批大小都是 32。「RNN-x-y」表示一个有 x 个层、每层包含 y 个隐藏单元的 RNN 网络。这里使用的 RNN 模型是双向 LSTM 模型。运行速度使用「批/秒」（batches/second）这个单位来衡量，值越高，速度越快。</span></em></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.16398243045387995" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9jC9fM2dc006xkrqhdtV6VXZY36dZ2DKp2IR5Ol6ibUE9plh66t22fRg/640?wx_fmt=png" data-type="png" data-w="683" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 4：QANet 模型和 BiDAF 模型（Seo et al., 2016）在 SQuAD 数据集上的速度对比。</span></em></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.3894348894348894" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9yqGbCrjDgBibhd2WpGEAWZ65l79bPyYUGatdKJ3XuqclWiccrtibiazAgQ/640?wx_fmt=png" data-type="png" data-w="814" style=""></p><p style="text-align: justify;line-height: 1.75em;"><em><span style="font-size: 12px;color: rgb(136, 136, 136);">表 5：对数据增强和 QANet 模型中其它部分的模型简化测试。这里所展示的结果是在开发集上取得的。对于包含「data augmentation」条目的行，「×N」表示数据被增大到原始规模的 N 倍，而括号中的比率表示采样率，即原始数据、「英语-法语-英语」和「英语-德语-英语」数据的采样比例。</span></em></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.5319926873857403" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9GMD7SyY1paxHiaSVGdKzicha2amdznQyBXlz5sicU2n2SicNUJWDcH1lQA/640?wx_fmt=png" data-type="png" data-w="547" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>表 6：在 SQuAD 对抗样本测试集上的 F1 值。</em></span></p><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p><p><img class="" data-copyright="0" data-ratio="0.37099494097807756" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9GwCMBpjcgHYpCZUY5cjl8h68P9g61bebyJ0VegmKvYoMLnP62LxRYQ/640?wx_fmt=png" data-type="png" data-w="593" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>表 7：在 TriviaQA 数据集的 Wikipedia domain 上，不同的单段阅读模型在开发集上的性能。注：*表示测试集的结果。</em></span></p><p><br></p><p><img class="" data-copyright="0" data-ratio="0.10978260869565218" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9OOwib6CAUkYUTaI2DWsYO8s1LbA2OjOm7dxoZR2clcIlx6BewcGY7vQ/640?wx_fmt=png" data-type="png" data-w="920" style=""></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span><span style="color: rgb(136, 136, 136);font-size: 12px;"><em>表 8：QANet 模型和基于 RNN 的模型在 TriviaQA 维基百科数据集上的运行速度对比，所有的批大小都是 32。「RNN-x-y」表示一个有 x 个层、每层包含 y 个隐藏单元的 RNN 网络。这里使用的 RNN 是双向 LSTM。处理的速度以「批/秒」这个单位衡量，值越高，速度越快。</em></span></p><p><br></p><p style="text-align: left;line-height: 1.75em;"><strong><span style="font-size: 15px;">论文：QANET: COMBINING LOCAL CONVOLUTION WITH GLOBAL SELF-ATTENTION FOR READING COMPREHENSION</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><strong><span style="font-size: 15px;"><br></span></strong></p><p><img class="" data-copyright="0" data-ratio="0.3848396501457726" data-s="300,640" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9KjL0m9M38B0wgDCYPrwQ9RKgxSM5qOtClkMp2dic3B1cGOIXrOaskD4TVoWdIyYBQqbcxvKwAHdw/640?wx_fmt=png" data-type="png" data-w="686" style=""></p><p><br></p><ul class=" list-paddingleft-2" style="list-style-type: disc;"><li><p style="text-align: left;line-height: 1.75em;"><span style="font-size: 15px;color: rgb(123, 12, 0);">论文链接：https://openreview.net/pdf?id=B14TlG-RW</span></p></li><li><p style="text-align: left;"><span style="font-size: 15px;text-align: justify;color: rgb(123, 12, 0);">项目地址：https://github.com/hengruo/QANet-pytorch</span></p></li></ul><p><br></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;">摘要：目前的端到端机器阅读和问答（Q&amp;A）模型主要基于带注意力机制的循环神经网络（RNN）。尽管它们取得了一定程度的成功，但由于 RNN 的序列特性，这些模型的训练速度和推断速度通常较慢。我们提出了一个名为 QANet 的新型问答系统框架，它不再需要循环网络：其编码器仅仅由卷积和自注意力机制构成，卷积可以对局部相互作用建模，而自注意力机制可以对全局相互作用建模。在 SQuAD 数据集上，QANet 模型的训练速度提升到对应的 RNN 模型的 3 到 13 倍、推断速度提升到 4 到 9 倍，并且取得了和循环模型同等的准确率。速度提升使得我们能够使用更多的数据训练模型。因此，我们将 QANet 模型和使用神经机器翻译模型回译得到的数据结合了起来。在 SQuAD 数据集上，我们使用增强的数据训练的模型在测试集上获得了 84.6 的 F1 值，这远远优于目前公开的最佳模型 81.8 的 F1 值。</span><img class="" data-ratio="0.3287671232876712" src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8Zfpicd40EribGuaFicDBCRH6IOu1Rnc4T3W3J1wE0j6kQ6GorRSgicib0fmNrj3yzlokup2jia9Z0YVeA/640?wx_fmt=png" data-type="png" data-w="73" style="font-size: 15px;box-sizing: border-box !important;word-wrap: break-word !important;width: 47px;visibility: visible !important;height: 20px;" width="48px"></p><p style="max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><strong style="max-width: 100%;line-height: 25.6px;font-family: 微软雅黑;font-size: 14px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;color: rgb(62, 62, 62);line-height: 25.6px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(127, 127, 127);box-sizing: border-box !important;word-wrap: break-word !important;">本文为机器之心编译，<strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;">转载请联系本公众号获得授权</span></strong></span></strong>。</span></strong><br style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);"><span style="max-width: 100%;color: rgb(127, 127, 127);line-height: 25.6px;font-family: 微软雅黑;box-sizing: border-box !important;word-wrap: break-word !important;">✄------------------------------------------------</span></p><p style="margin-bottom: 5px;max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;white-space: normal;caret-color: rgb(51, 51, 51);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">加入机器之心（全职记者/实习生）：hr@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;white-space: normal;caret-color: rgb(51, 51, 51);background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">投稿或寻求报道：<strong style="max-width: 100%;color: rgb(62, 62, 62);font-size: 18px;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;color: rgb(217, 33, 66);line-height: 1.6;font-size: 12px;box-sizing: border-box !important;word-wrap: break-word !important;">content</span></strong>@jiqizhixin.com</span></strong></p><p style="max-width: 100%;min-height: 1em;color: rgb(51, 51, 51);letter-spacing: 0.544px;white-space: normal;background-color: rgb(255, 255, 255);font-size: 18px;font-family: 微软雅黑;text-align: center;line-height: 1.75em;box-sizing: border-box !important;word-wrap: break-word !important;"><strong style="max-width: 100%;box-sizing: border-box !important;word-wrap: break-word !important;"><span style="max-width: 100%;font-size: 12px;color: rgb(217, 33, 66);line-height: 1.6;box-sizing: border-box !important;word-wrap: break-word !important;">广告&amp;商务合作：bd@jiqizhixin.com</span></strong></p><p style="text-align: justify;line-height: 1.75em;"><span style="font-size: 15px;"></span></p>
                </div>
                <script nonce="87712946" type="text/javascript">
                    var first_sceen__time = (+new Date());

                    if ("" == 1 && document.getElementById('js_content')) {
                        document.getElementById('js_content').addEventListener("selectstart",function(e){ e.preventDefault(); });
                    }

                    
                    (function(){
                        if (navigator.userAgent.indexOf("WindowsWechat") != -1){
                            var link = document.createElement('link');
                            var head = document.getElementsByTagName('head')[0];
                            link.rel = 'stylesheet';
                            link.type = 'text/css';
                            link.href = "//res.wx.qq.com/mmbizwap/zh_CN/htmledition/style/page/appmsg_new/winwx3de35e.css";
                            head.appendChild(link);
                        }
                    })();
                </script>
                
                
                                
                <div class="ct_mpda_wrp" id="js_sponsor_ad_area" style="display:none;"></div>

                
                                <div class="reward_area tc appmsg_card_context" id="js_preview_reward" style="display:none;">
                    <div class="reward_inner">
                        <p id="js_preview_reward_wording" class="tips_global reward_tips" style="display:none;"></p>
                        <p>
                            <a class="reward_access" id="js_preview_reward_link" href="##">赞赏</a>
                        </p>
                    </div>
                </div>
                <div class="reward_qrcode_area reward_area tc" id="js_preview_reward_qrcode" style="display:none;">
                    <p class="tips_global">长按二维码向我转账</p>
                    <p id="js_preview_reward_ios_wording" class="reward_tips" style="display:none;"></p>
                    <span class="reward_qrcode_img_wrp"><img class="reward_qrcode_img" src="//res.wx.qq.com/mmbizwap/zh_CN/htmledition/images/pic/appmsg/pic_reward_qrcode.2x3534dd.png"></span>
                    <p class="tips_global">受苹果公司新规定影响，微信 iOS 版的赞赏功能被关闭，可通过二维码转账支持公众号。</p>
                </div>
                            </div>
                                        
                                

                        
            <ul id="js_hotspot_area" class="article_extend_area"></ul>


            
                        <div class="rich_media_tool" id="js_toobar3">

                                            <div id="js_read_area3" class="media_tool_meta tips_global_primary meta_primary" style="display:none;">阅读 <span id="readNum3"></span></div>

                <span style="display:none;" class="media_tool_meta meta_extra meta_praise" id="like3">
                    <i class="icon_praise_gray"></i><span class="praise_num" id="likeNum3"></span>
                </span>

                

            </div>


                        <div class="rich_media_tool" id="js_sg_bar">

                                
            </div>
                    </div>

        <div class="rich_media_area_primary sougou" id="sg_tj" style="display:none"></div>


        
        <div class="rich_media_area_extra">
            
            <div id="js_share_appmsg">
            </div>

            
                        <div class="mpda_bottom_container" id="js_bottom_ad_area"></div>
                        
            <div id="js_iframetest" style="display:none;"></div>
                        
                        
            <div class="rich_media_extra rich_media_extra_discuss" id="js_cmt_container" style="display:none">
              

              
              <div class="discuss_mod" id="js_friend_cmt_area" style="display:none">
                
                
                
              </div>

                            <div class="discuss_mod" id="js_cmt_area" style="display:none">
              </div>
                          </div>
        </div>

        
        <div id="js_pc_qr_code" class="qr_code_pc_outer" style="display:none;">
            <div class="qr_code_pc_inner">
                <div class="qr_code_pc">
                    <img id="js_pc_qr_code_img" class="qr_code_pc_img">
                    <p>微信扫一扫<br>关注该公众号</p>
                </div>
            </div>
        </div>
    </div>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
