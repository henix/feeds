<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>浅析至强RNN可微分神经计算机(DNC)-2</title>
</head>
<body>
<p><a href="https://zhuanlan.zhihu.com/p/27964341">原文</a></p>
<div class="title-image"><img src="https://pic4.zhimg.com/v2-d6969cdc7775316040ea4fbd57b9e84f_r.jpg" alt=""></div><p>增加外记忆存储器的RNN对于诸如语言建模和机器翻译等具有非常理想的应用前景，当然也可以被用于广泛的模式识别和情感处理，也是当下机器学习和量化投资的一个热点。然而，随着模型时间序列长度的增加，导致这些模型在物理内存和计算资源方面的消耗增加迅速，也就是时间和空间复杂性导致这些模型很难被大规模实用化，比如我们对过去几年全A股市场的进行分钟线量级的数据扫描学习。在论文[1]中介绍了一种被称为稀疏访问存储器（Sparse Access Memory，SAM）通过稀疏化矩阵压缩RNN时空复杂度的计算框架。论文介绍了一种通过稀疏的读写控制来训练具有大量外部记忆的NTM(神经图灵机)，并在附录里面介绍了如何将这种算法移植到DNC中。利用网络中的有效数据结构，通过k-d trees和LSH处理读写头控制的方法，在保证保持精度的前提下显著加速了训练过程。按照论文里面的描述，这种稀疏性的计算框架可以保持精度的同时压缩大约3000倍左右的物理内存消耗并提高大约1000倍的计算速度，这极具实用价值。原始的DNC（见上篇博客）极其消耗计算资源，对一个32*200参数1193长度的序列使用两个至强10核CPU训练达到收敛大约需要一周的时间，这样的计算速度显然不能用来处理A股市场的中频数据。形象一点说就是我用DNC模型训练5年的上证50成份股就需要64G内存训练几天的时间，这就很难使用DNC模型对A股市场进行大规模扫描训练了。</p><p><br></p><img src="https://pic4.zhimg.com/v2-05e360ba1c5eb4f2c7467925a12a7723_r.png" data-rawwidth="908" data-rawheight="278"><p>图1 论文1中提供的性能稀疏版DNC计算效率。</p><p>下面简单介绍如何使用稀疏读写头控制构建SDNC (Sparse Differentiable Neural Computer)</p><p>如同DNC一样，控制器与外存储器之间的通过读写头交换信息流，每时间步，控制器接受上一个时间步读取记忆向量和此时刻外部输入，经过计算发出此时刻隐藏状态矩阵。隐藏状态矩阵通过线性变换产生控制器输出向量和接口控制向量。外存储器接受接口控制向量产生此时刻的外存储器读取向量，读取向量与控制器输出向量线性组合生成DNC在时刻最终输出向量。</p><p><br></p><h2><b>更新写头</b></h2><p>更新记忆矩阵也是由稀疏的写头控制完成的。通过 <equation>M_t \leftarrow (1-R_t) \odot M_{t-1} + A_t </equation> 其中擦除矩阵 <equation>R_t = \omega_t^w e_t^T</equation>  和新写入矩阵 <equation>A_t = \omega_t^w\alpha_t^T</equation> 。其中写头控制 <equation>\omega_t^w</equation> 被约束为包含K个非零条目的向量。这种稀疏设计是基于一个简单的理念，写头控制控制写头写入新的记忆在最近（前一个时间步 <equation>t-1</equation> ）读取记忆的位置或者截至在该时刻 最少被访问的位置写入新的记忆。不同于DNC中的设计，这里假设N的数值非常大，比如记忆规模为 <equation>1e4</equation> 外存储器记住1W组记忆，这个时候写头控制不再是基于内容和动态内存允许机制选择位置添加新的记忆这意味着O(N)复杂度，而是在 <equation>K+1</equation> 个位置中选择位置写入新的记忆。这 <equation>K+1</equation> 个位置由于上一步读取位置和时刻 <equation>t</equation> 最少被访问也就是最没有价值的记忆位置组成。 <equation>K+1</equation> 位置由 <equation>K</equation> 个读取位置和一个最少访问位置组成。</p><p>不同于DNC中写头控制由基于内容寻址和动态内存允许的更新控制机制，SDNC中使用一个外加的指示器控制更新写头控制。 <equation>\omega_t^W = \alpha_t (\gamma_t \omega_{t-1}^R + (1-\gamma_t) \mathbb{I}_t^U)</equation> 写头控制更新由先前读取位置和最少被访问位置控制。对于RNN信息流程而言，在读头操作为稀疏也就是读头控制 <equation>\omega_{t-1}^R </equation>  为K个非0值得稀疏向量的时候，写头操作也相应的变为稀疏。其中先前读取位置直接使用加法增加写头控制向量相应位置权重。最少访问位置则是通过指示器 <equation>\mathbb{I}_t^U</equation>  更新，指示器在被写入信息之前置0。</p><p>指示器如下定义，如果这里有多个最小值也就是在时刻 <equation>t</equation> 有多个最少访问位置，则任意选择。</p><p><equation>\mathbb{I_t^{U}(i)} =\begin{cases} 1 &amp; if\quad U_t(i) = min \quad \mathop{U_t(j)}\limits_{j=1,...,N}\\ 0 &amp; otherwise \end{cases}</equation></p><p>论文[1]这里介绍两种计算最少使用位置的方法，这里只使用 <equation>U_T(i)</equation> 为发生不可忽略的内存访问的时间步累加和确定。 <equation>U_T(i) = T-max\{t: \omega_t^w(i)+\omega_t^R(i) &gt; \delta\}</equation> ， <equation>\delta</equation> 是通常选择为0.005的调谐参数。这时擦除矩阵 <equation>R_t = \mathbb{I}_t^U \mathop{1}^T</equation> 通过稀疏化写入操作的时间和空间复杂度在前向和后向传播过程中是恒定的。</p><h2><b>更新读头：读取外存储器记忆信息流</b></h2><p>在DNC中读取记忆向量由读头控制， <equation>r_t^i = M_t^T\omega_t^{r,i}</equation> 其中 <equation>r_t^i \in \mathbb{R^W}</equation> 表示在时间步 <equation>t</equation> 在第 <equation>i</equation> 个读头读取的记忆向量， <equation>M\in \mathbb{R}^{N \times W}</equation> 是记忆矩阵， <equation>\omega_t^{r,i} \in \mathbb{R}^N</equation> 为写头控制。为了简便起见，我们这里去掉表示读头标签 <equation>i</equation> ，则读头读取记忆公式变为 <equation>r_t = M_t^T\omega_t^{r}</equation> 这种等同于 <equation>r_t = \sum_{i=1}^N \omega_t^R(i) M_t(i)</equation>  注意这里的 <equation>(i)</equation> 表示索引，也就是 <equation> \omega_t^R(i) </equation> 表示列向量中的一个元素，同样的 <equation>M_t(i)</equation> 表示一行矩阵元素。稀疏的读头通过使用搜索树技术定义一个新的索引 <equation>s_i</equation> 这里索引将原本是 <equation>N</equation> 个数值操作变为 <equation>K</equation> 数值操作，既 <equation>r_t = \sum_{i=1}^K \omega_t^R(s_i) M_t(s_i)</equation> 其中 <equation>s_i</equation> 是由‘搜索技术’获取的位置索引。例如假设 <equation>N=512, \ k=8</equation> 通过稀疏读头的公式操作， 这里读头通过读头控制获取的记忆信息流就从基于 <equation>N</equation> 位置或 <equation>N</equation> 组记忆获取变成基于 <equation>K</equation> 组记忆获取了。</p><p>注意上述操作是通过使用最近邻数据结构来计算 <equation>K</equation> 个索引，在提取记忆信息流程操作的时候将 <equation>M_t</equation> 由密集矩阵变为稀疏矩阵，其中只有K个被索引的位置保留记忆内容，其余位置全部设置为0。通过使用最近邻数据结构的使用将算法的时间复杂度从 <equation>O(N) \ \rightarrow \ (log N)</equation> 通过使用有效的稀疏矩阵格式，如Compressed Sparse Rows (CSR)，稀疏读头可以在恒定的时间和空间中计算以及反向传播梯度。</p><h2><b>更新读头控制</b></h2><p>DNC中的时间记忆链(temporal memory linkage)是用于关联和调用以时间顺序写入的存储器位置信息的检索系统。通过 <equation>L_t\left[0,1\right]^{N\times N}</equation> 实现，其中 <equation>L_t\left[i ,j\right]</equation> 表示在位置 <equation>j</equation> 之后写入的位置 <equation>i</equation> 的程度。 <equation>L_t</equation> 通过使用优先级权重 <equation>p_t</equation> 更新，其中 <equation>p_t(i)</equation> 表示记忆矩阵位置(行) <equation>i</equation> 时刻 <equation>t</equation>  的写入程度，为一个相对矩阵各行位置的比较值。</p><p><equation>p_0=0\\ p_t = (1-\sum\omega_t^W(i))p_{t-1} + \omega_t^W\\ L_0=0\\ L_t(i,j) = \begin {cases} 0 &amp; \\ (1-\omega_t^W(i) - \omega_t^W(j))L_{t-1}(i,j) + \omega_t^W(i)p_{t-1}(j) \end {cases}\\ f_t^r = L_t \omega_{t-1}^r\\ b_t^r = L_t^T \omega_{t-1}^r</equation></p><p>写头控制通过时间记忆链计算在位置 <equation>i</equation>  之前的写入顺序信息和向前的写入顺序信息和基于余弦相似性的内容寻址更新，三种寻址机制通过使用一个 <equation>softmax</equation>  函数约束模式选择控制。这种计算方式需要 <equation>O(N^2)</equation> 的内存空间。通过使用修改的稀疏时间记忆链以及稀疏矩阵存储格式 CSR(Compressed Sparse Row format) 计算的空间复杂度被压缩到 <equation>O(K_L \times K_L)</equation> 。</p><p>时间记忆链 <equation>L_t</equation> 被拆分成两个对应前向读头控制和后向读头控制的稀疏矩阵 <equation>N_t \&amp; P_t</equation> 。 通过使用CSR存储格式 ，稀疏版本的 <equation>L_t</equation> 使用如下公式更新。</p><p><equation>N_0 = 0\\ P_0=0\\ N_t(i,j) = (1-\omega_t^W(i))N_{t-1}(i,j) + \omega_t^W(i) p_{t-1}(j)\\ P_t(i,j) = (1-\omega_t^W(j))N_{t-1}(i,j) + \omega_t^W(j) p_{t-1}(i)\\ f_t^r = N_t \omega_{t-1}^r\\ b_t^r = P_t^T \omega_{t-1}^r</equation></p><p>其中 <equation>N_t, P_t \in [0,1]^{N \times N}</equation> 并且， <equation>N_t</equation> 中 <equation>K_L</equation> 行非0， <equation>P_t</equation> 中 <equation>K_L</equation> 列非0 。<b>Compressed sparse row(CSR)</b>把行的信息压缩存储了，只显式保留每行第一个非零元素的位置。 <equation>p_t(p_{t-1})</equation> 是一个稀疏向量，由 <equation>N_t, P_t</equation> 的更新公式可以推导矩阵 <equation>\omega_t^W p_{t-1}</equation> 最多有 <equation>K_L \times K_L</equation> 个非0项，反推循环可见 <equation>N_t, P_t</equation> 非0项位置由 <equation>p_{t-1}</equation> 决定，也就是由写头控制 <equation>\omega_{t-1}^W</equation> 决定。</p><h2><b>BPTT传播</b></h2><p>考虑到记忆增强型神经网络的空间复杂度，使用BPTT算法优化的时候，在时刻 <equation>t</equation> 梯度需要基于 <equation>M_t</equation>  来计算。简单实现是需要在每个时间步<equation>t</equation>计算记忆，这导致O(NT)的空间开销会严重限制记忆矩阵规模大小和处理序列长度。这可以通过修改BPTT算法进行压缩，因为每个时间步模型只进行O(1)的时间计算，所以可以将模型按照时间步骤展开到O(T)的空间中，通过O(1)的计算时间完成计算。</p><p>另外通过使用查询向量搜索记忆矩阵的时候，通过使用近似最近邻索引(approximate nearest neighbor index ANN)搜索外存储器寻找K个最近访问位置。对于N行的记忆矩阵，ANN可以在O(log N)的时间复杂度上搜索K个最近读取位置。</p><h2><b>CSR附录：</b></h2><p>稀疏矩阵(Sparse Matrix)由于有很多0，为了节省空间，一般压缩存储。通常只需要保存非零元素及其位置即可。如有稀疏矩阵如下：</p><p><equation>\left[ \begin{matrix} 1 &amp; 0 &amp; 0 &amp; 2 &amp; 0\\ 3 &amp; 4 &amp; 0 &amp; 5 &amp; 0 \\ 6 &amp; 0 &amp; 7 &amp; 8 &amp; 0 \\ 0 &amp; 0 &amp; 10 &amp; 11 &amp;0 \\ 0 &amp; 0&amp; 0&amp; 0 &amp;12 \end{matrix} \right] </equation></p><p>AA: 按行顺序存储各个非零元素  AA: 1 2 3 4 5 6 7 8 9 10 11 12 </p><p>JR: 记录对应元素所在的列的列号 JR: 1 4 1 2 4 1 3 4 5 3 4 5  </p><p>JC: 记录每行第一个元素在AA中的位置 JC: 1 3 6 10 12 </p><p><br></p><p>引用</p><p>[1] Rae J, Hunt J J, Danihelka I, et al. Scaling memory-augmented neural networks with sparse reads and writes[C]//Advances in Neural Information Processing Systems. 2016: 3621-3629.</p><p>[2] Graves A, Wayne G, Reynolds M, et al. Hybrid computing using a neural network with dynamic external memory[J]. Nature, 2016, 538(7626): 471-476.</p>
<script async defer="defer" src="https://www.googletagmanager.com/gtag/js?id=UA-7909075-5"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){ dataLayer.push(arguments); }
gtag('js', new Date());
gtag('config', 'UA-7909075-5');
</script>
<script>
var _hmt = _hmt || [];
</script>
<script async defer="defer" src="https://hm.baidu.com/hm.js?e3d40295e416616ddc21287da9646d31"></script>
</body>
</html>
