<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:media="http://search.yahoo.com/mrss/">
<channel>
<title>TiDB 的后花园</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/</link>
<description></description>
<language>zh-cn</language>
<lastBuildDate>Wed, 06 Mar 2019 11:16:49 +0800</lastBuildDate>
<item>
<title>The (Near) Future of Database | TiDB DevCon 2019</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-03-05-58337623.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/58337623&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-fb9770b0a00ac9c1c60940b79a1ead2e_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;作者： &lt;a class=&quot;member_mention&quot; href=&quot;http://www.zhihu.com/people/5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hash=&quot;5940b1ec1c21a3538c6cfcf5711a75a6&quot; data-hovercard=&quot;p$b$5940b1ec1c21a3538c6cfcf5711a75a6&quot;&gt;@Ed Huang&lt;/a&gt; &lt;/p&gt;&lt;blockquote&gt;在 TiDB DevCon 2019 上，我司联合创始人兼 CTO 黄东旭分享了对数据库行业大趋势以及未来数据库技术的看法。以下是演讲实录，enjoy~&lt;/blockquote&gt;&lt;p&gt;&lt;br&gt;大家今天在这里看到了 TiDB 社区用户实践分享和我们自己的一些技术进展和展望，还有非常好玩的 Demo Show，正好在大会结束之前，我想跟大家聊一聊我心目中未来的 Database 应该是一个什么样子。&lt;/p&gt;&lt;p&gt;其实我们并不是一个特别擅长发明名词的公司，我记得我们第一次去用 HTAP 这个词的时候，应该是 2016 左右。在使用 HTAP 这个词的时候，我们市场部同事还跟我们说 HTAP 这个词从来没人用过，都是论文里的词，大家都不知道，你把你们公司的产品定位改成这个别人都不知道怎么办？我们后来仔细想，还是觉得 HTAP 这个方向是一个更加适合我们的方向，所以还是选了 HTAP 这个词。现在很欣喜的看到现在各种友商、后来的一些数据库，都开始争相说 HTAP，就是说得到了同行的认可。&lt;/p&gt;&lt;p&gt;那么在 HTAP 的未来应该是一个什么样子，我希望能够在今年这个 Talk 里面先说一说，但是这个题目起的有点不太谦虚，所以我特地加了一个「Near」， 分享一下这一年、两年、三年我们想做什么，和对行业大趋势的展望。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f3d324b3d1524d612e6bf5561f1b3aff_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-f3d324b3d1524d612e6bf5561f1b3aff_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-f3d324b3d1524d612e6bf5561f1b3aff_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-f3d324b3d1524d612e6bf5561f1b3aff_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-f3d324b3d1524d612e6bf5561f1b3aff_b.jpg&quot;&gt;&lt;figcaption&gt;图 1 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;今天我们的分享的一个主题就是：「我们只做用户想要的东西，并不是要去做一个完美的东西」。&lt;/b&gt;其实很多工程师包括我们自己，都会有一个小小的心理的洁癖，就是想要做一个超级快、超级牛的东西，但是做出来一个数据库，单机跑分一百万 TPS ，其实用户实际业务就需要 3000，然后所有的用户还会说我需要这些东西，比如需要 Scalability（弹性扩展）， Super Large 的数据量，最好是我的业务一行代码都不用改，而且 ACID 能够完全的满足，怎么踹都踹不坏，机器坏了可以高可用，业务层完全不用动， 另外可以在跑 OLTP 的同时，完全不用担心任何资源隔离地跑 OLAP（这里不是要说大家的愿望不切实际，而是非常切实际，我们也觉得数据库本身就应该是这样的。所以大家记住这几个要点，然后慢慢看 TiDB 到底是不是朝着这个方向发展的）。&lt;b&gt;本质上来说用户的需求就是「大一统」。看过《魔戒》的同学都知道这句话 ：ONE RING TO RULE THEM ALL，就是一套解决方案去解决各种问题。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;过去很多，包括一些行业的大佬之前说在各种环境下都要出一个数据库来解决特定的一个问题，但是其实看上去我们想走的方案还是尽可能在一个平台里面，尽可能大范围去解决用户的问题。因为不同的产品之间去做数据的交互和沟通，其实是蛮复杂的。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c26c20f2e457a5f808f1c582f677bff4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;366&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-c26c20f2e457a5f808f1c582f677bff4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-c26c20f2e457a5f808f1c582f677bff4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;366&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-c26c20f2e457a5f808f1c582f677bff4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-c26c20f2e457a5f808f1c582f677bff4_b.jpg&quot;&gt;&lt;figcaption&gt;图 2 理想中的「赛道」&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这张图（图 2）什么意思呢？就是很多人设计系统的时候，总是会陷入跑分思维，就是说这个东西在实验室或者说在一个特定的 Workload 下，跑得巨快无比。如果大家去看一下大概 2000 年以后关于数据库的论文，很多在做一个新的模型或者新的系统的时候，都会说 TPCC 能够跑到多大，然后把 Oracle 摁在地上摩擦，这样的论文有很多很多很多。但是大家回头看看 Oracle 还是王者。所以大多数实验室的产品和工程师自己做的东西都会陷入一个问题，就是想象中的我的赛道应该是一个图 2 那样的，但实际上用户的业务环境是下面这样的（图 3）。很多大家在广告上看到特别牛的东西，一放到生产环境或者说放到自己的业务场景里面就不对了，然后陷入各种各样的比较和纠结的烦恼之中。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-023adf3dfa5cffb86664271077cc3368_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-023adf3dfa5cffb86664271077cc3368_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-023adf3dfa5cffb86664271077cc3368_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-023adf3dfa5cffb86664271077cc3368_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-023adf3dfa5cffb86664271077cc3368_b.jpg&quot;&gt;&lt;figcaption&gt;图 3 实际上用户的业务环境&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;TiDB 的定位或者说我们想做的事情，并不是在图 2 那样的赛道上，跑步跑得巨快，全世界没人在短跑上跑得过我，我们不想做这样。或者说，&lt;b&gt;我们其实也能跑得很快，但是并不想把所有优势资源全都投入到一个用户可能一辈子都用不到的场景之中。我们其实更像是做铁人三项的，因为用户实际应用场景可能就是一个土路。这就是为什么 TiDB 的设计放在第一位的是「稳定性」。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们一直在想能不能做一个数据库，怎么踹都踹不坏，然后所有的异常的状况，或者它的 Workload  都是可预期的。我觉得很多人远远低估了这个事情的困难程度，其实我们自己也特别低估了困难程度。大概 4 年前出来创业的时候，我们就是想做这么一个数据库出来，我跟刘奇、崔秋三个人也就三个月做出来了。但是到现在已经 4 年过去了，我们的目标跟当年还是一模一样。不忘初心，不是忘不掉，而是因为初心还没达到，怎么忘？其实把一个数据库做稳，是很难很难的。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d3b63aa08dca21530e6201731d4a087d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-d3b63aa08dca21530e6201731d4a087d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d3b63aa08dca21530e6201731d4a087d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-d3b63aa08dca21530e6201731d4a087d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-d3b63aa08dca21530e6201731d4a087d_b.jpg&quot;&gt;&lt;figcaption&gt;图 4 近年来硬件的发展&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;而且我们这个团队的平均年龄可能也就在二十到三十岁之间，为什么我们如此年轻的一个团队，能够去做数据库这么古老的一件事情。其实也是得益于整个 IT 行业这几年非常大的发展。&lt;/b&gt;图 4 是这几年发展起来的 SSD，内存越来越大，万兆的网卡，还有各种各样的多核的 CPU，虚拟化的技术，让过去很多不可能的事情变成了可能。&lt;/p&gt;&lt;p&gt;举一个例子吧，比如极端一点，大家可能在上世纪八九十年代用过这种 5 寸盘、3 寸盘，我针对这样的磁盘设计一个数据结构，现在看上去是个笑话是吧？因为大家根本没有人用这样的设备了。在数据库这个行业里面很多的假设，在现在新的硬件的环境下其实都是不成立的。比如说，为什么 B-Tree 就一定会比 LSM-Tree 要快呢？不一定啊，我跑到 Flash 或者 NVMe SSD 、Optane 甚至未来的持久化内存这种介质上，那数据结构设计完全就发生变化了。过去可能需要投入很多精力去做的数据结构，现在暴力就好了。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-89eb2823f90288c70655dfcdcd73125e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-89eb2823f90288c70655dfcdcd73125e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-89eb2823f90288c70655dfcdcd73125e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-89eb2823f90288c70655dfcdcd73125e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-89eb2823f90288c70655dfcdcd73125e_b.jpg&quot;&gt;&lt;figcaption&gt;图 5 近年来软件变革&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;同时在软件上也发生了很多很多的变革，图 5 左上角是 &lt;b&gt;Wisckey&lt;/b&gt; 那篇论文里的一个截图，还有一些分布式系统上的新的技术，比如 2014 年 Diego 发表了 &lt;b&gt;Raft&lt;/b&gt; 这篇论文，另外 &lt;b&gt;Paxos&lt;/b&gt; 这几年在各种新的分布式系统里也用得越来越多。&lt;/p&gt;&lt;p&gt;&lt;b&gt;所以我觉得这几年我们赶上了一个比较好的时代，就是不管是软件还是硬件，还是分布式系统理论上，都有了一些比较大突破，所以我们基础才能够打得比较好。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d8d0e1f4e1b80983f699f7635135cc53_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-d8d0e1f4e1b80983f699f7635135cc53_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-d8d0e1f4e1b80983f699f7635135cc53_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-d8d0e1f4e1b80983f699f7635135cc53_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-d8d0e1f4e1b80983f699f7635135cc53_b.jpg&quot;&gt;&lt;figcaption&gt;图 6  Data Type&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;除了有这样的新的硬件和软件之外，我觉得在业务场景上也在发生一些比较大变化。过去，可能十年前就是我刚开始参加工作的时候，线上的架构基本就是在线和离线两套系统，在线是 Oracle 和 MySQL，离线是一套 Hadoop 或者一个纯离线的数据仓库。&lt;b&gt;但最近这两年越来越多的业务开始强调敏捷、微服务和中台化，于是产生了一个新的数据类型，就是 warm data，它需要像热数据这样支持 transaction、支持实时写入，但是需要海量的数据都能存在这个平台上实时查询， 并不是离线数仓这种业务。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;所以对 warm data 来说，过去在 TiDB 之前，其实是并没有太好的办法去很优雅的做一层大数据中台架构的，&lt;b&gt;「the missing part of modern data processing stack」，就是在 warm data 这方面，TiDB 正好去补充了这个位置，所以才能有这么快的增长。&lt;/b&gt;当然这个增长也是得益于 MySQL 社区的流行。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7d3300eb18100a7d6e7cdbb2cfc21533_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-7d3300eb18100a7d6e7cdbb2cfc21533_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-7d3300eb18100a7d6e7cdbb2cfc21533_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-7d3300eb18100a7d6e7cdbb2cfc21533_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-7d3300eb18100a7d6e7cdbb2cfc21533_b.jpg&quot;&gt;&lt;figcaption&gt;图 7 应用举例&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;想象一下，我们如果在过去要做这样很简单的业务（图 7），比如在美国的订单库跟在中国的订单库可能都是在不同的数据库里，用户库可能是另外一个库，然后不同的业务可能是操作不同的库。如果我想看看美国的消费者里面有哪些在中国有过消费的，就是这么一条 SQL。过去如果没有像 TiDB 这样的东西，大家想象这个东西该怎么做？&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6776f579b5ce641184910e1789153e55_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-6776f579b5ce641184910e1789153e55_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6776f579b5ce641184910e1789153e55_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-6776f579b5ce641184910e1789153e55_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-6776f579b5ce641184910e1789153e55_b.jpg&quot;&gt;&lt;figcaption&gt;图 8 过去的解决方案&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;假如说这两边的数据量都特别大，然后已经分库分表了。过去可能只能第二天才可以看到前一天的数据，因为中间比如说一个 T+1  要做一个 ETL 到一个 data ware house 里。或者厉害一点的架构师可能会说，我可以做一套实时的 OLAP 来做这个事情，怎么做呢？比如说 MySQL 中间通过一个 MQ 再通过 Hadoop 做一下 ETL，然后再导到 Hadoop 上做一个冷的数据存储，再在上面去跑一个 OLAP 做实时的分析。先不说这个实时性到底有多「实时」，大家仔细算一算，这套架构需要的副本数有多少，比如 M 是我的业务数，N 是每一个系统会存储的 Replica，拍脑袋算一下就是下面这个数字（图 9 中的 &lt;b&gt;R&lt;/b&gt; ）。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-073e12330f3c3fc606dac9675eac20f4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-073e12330f3c3fc606dac9675eac20f4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-073e12330f3c3fc606dac9675eac20f4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-073e12330f3c3fc606dac9675eac20f4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-073e12330f3c3fc606dac9675eac20f4_b.jpg&quot;&gt;&lt;figcaption&gt;图 9 过去解决方案里需要的 Replica 数量&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以大家其实一开始在过去说，TiDB 这个背后这么多 Replica  不好，但其实你想想，你自己在去做这个业务的时候，大家在过去又能怎么样呢？所以我觉得 TiDB 在这个场景下去统一一个中台，是一个大的趋势。今天在社区实践分享上也看到很多用户都要提到了 TiDB 在中台上非常好的应用。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9d14f25e82c4d71a1b03c112fdca85e2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-9d14f25e82c4d71a1b03c112fdca85e2_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-9d14f25e82c4d71a1b03c112fdca85e2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-9d14f25e82c4d71a1b03c112fdca85e2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-9d14f25e82c4d71a1b03c112fdca85e2_b.jpg&quot;&gt;&lt;figcaption&gt;图 10 现在的解决方案 &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;i&gt;回顾完行业和应用场景近年来的一些变化之后，我们再说说未来。假设要去做一个面向未来的数据库，会使用哪些技术？&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;1. Log is the new database&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;第一个大的趋势就是日志，「log is the new database」 这句话应该也是业界的一个共识吧。现在如果有一个分布式数据库的复制协议，还是同步一个逻辑语句过去，或者做 binlog 的复制，那其实还算比较 low 的。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3a905c012be42e4a4b11f39a2c66dbc2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-3a905c012be42e4a4b11f39a2c66dbc2_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-3a905c012be42e4a4b11f39a2c66dbc2_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-3a905c012be42e4a4b11f39a2c66dbc2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-3a905c012be42e4a4b11f39a2c66dbc2_b.jpg&quot;&gt;&lt;figcaption&gt;图 11  Log is the new database&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;上面图 11 左半部分是 Hyper，它是慕尼黑工业大学的一个实验性数据库项目，它做了一些分析，第一个柱形是正常的 SQL 语句的执行时间，比如说直接把一语句放到另外一个库里去执行，耗时这么多。第二个柱形是用逻辑日志去存放，耗时大概能快 23%，第三个柱形能看到如果是存放物理日志能快 56%。所以大家仔细想想，&lt;b&gt;TiDB 的架构里的 TiFlash 其实同步的是 Raft 日志，而并不是同步 Binlog 或者其他的。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上面图 11 右半部分是 Aurora，它的架构就不用说了，同步的都是 redo log 。其实他的好处也很明显，也比较直白，就是 I/O 更小，网络传输的 size 也更小，所以就更快。&lt;/p&gt;&lt;p&gt;然后在这一块 TiDB 跟传统的数据库有点不一样的就是，其实如果很多同学对 TiDB 的基础架构不太理解的话就觉得， Raft 不是一个一定要有 Index 或者说是一定强顺序的一个算法吗？那为什么能做到这样的乱序的提交？&lt;b&gt;其实 TiDB 并不是单 Raft 的架构，而是一个多 Raft 的架构，I/O 可以发生在任何一个 Raft Group 上。&lt;/b&gt;传统的单机型数据库，就算你用更好的硬件都不可能达到一个线性扩展，因为无论怎么去做，都是这么一个架构不可改变。比如说我单机上 Snapshot  加 WAL，不管怎么写， 总是在 WAL  后面加，I/O 总是发生在这。但 TiDB 的 I/O 是分散在多个 Raft Group、多个机器上，这是一个很本质的变化，这就是为什么在一些场景下，TiDB 能够获取更好的吞吐。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;2. Vectorized&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;第二个大趋势是全面的向量化。向量化是什么意思？我举个简单的例子。比如我要去算一个聚合，从一个表里面去求某一列的总量数据，如果我是一个行存的数据库，我只能把这条记录的 C 取出来，然后到下一条记录，再取再取再取，整个 Runtime 的开销也好，还有去扫描、读放大的每一行也好，都是很有问题的。但是如果在内存里面已经是一个列式存储，是很紧凑的结构的话，那会是非常快的。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e313503a2abd357f2ade60e7fa90ac59_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-e313503a2abd357f2ade60e7fa90ac59_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-e313503a2abd357f2ade60e7fa90ac59_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-e313503a2abd357f2ade60e7fa90ac59_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-e313503a2abd357f2ade60e7fa90ac59_b.jpg&quot;&gt;&lt;figcaption&gt;图 12 TiDB 向量化面临的挑战&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这里面其实也有一些挑战。我们花了大概差不多 2018 年一年的时间去做向量化的改造，其实还挺难的。为什么？首先 TiDB SQL 引擎是用了 Volcano 模型，这个模型很简单，就是遍历一棵物理计划的树，不停的调 Next，每一次 Next 都是调用他的子节点的 Next，然后再返回结果。这个模型有几个问题：第一是每一次都是拿一行，导致 CPU 的 L1、L2 这样的缓存利用率很差，就是说没有办法利用多 CPU 的 Cache。第二，在真正实现的时候，它内部的架构是一个多级的虚函数调用。大家知道虚函数调用在 Runtime  本身的开销是很大的，在《&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//cidrdb.org/cidr2005/papers/P19.pdf&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;MonetDB/X100: Hyper-Pipelining Query Execution&lt;/a&gt;》里面提到，在跑 TPC-H 的时候，Volcano 模型在 MySQL 上跑，大概有 90% 的时间是花在 MySQL 本身的 Runtime  上，而不是真正的数据扫描。所以这就是 Volcano 模型一个比较大的问题。第三，如果使用一个纯静态的列存的数据结构，大家知道列存特别大问题就是它的更新是比较麻烦的， 至少过去在 TiFlash 之前，没有一个列存数据库能够支持做增删改查。那在这种情况下，怎么保证数据的新鲜？这些都是问题。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-90389114fad37e709d0a869c444ae8a6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-90389114fad37e709d0a869c444ae8a6_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-90389114fad37e709d0a869c444ae8a6_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-90389114fad37e709d0a869c444ae8a6_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-90389114fad37e709d0a869c444ae8a6_b.jpg&quot;&gt;&lt;figcaption&gt;图 13 TiDB SQL 引擎向量化&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;TiDB 已经迈出了第一步，我们已经把 TiDB SQL 引擎的 Volcano 模型，从一行一行变成了一个 Chunk 一个 Chunk，每个 Chunk 里面是一个批量的数据，所以聚合的效率会更高。而且在 TiDB 这边做向量化之外，我们还会把这些算子推到 TiKV 来做，然后在 TiKV 也会变成一个全向量化的执行器的框架。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;3. Workload Isolation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;另外一个比较大的话题，是 Workload Isolation。今天我们在演示的各种东西都有一个中心思想，就是怎么样尽可能地把 OLTP 跟 OLAP 隔离开。这个问题在业界也有不同的声音，包括我们的老前辈 Google Spanner，他们其实是想做一个新的数据结构，来替代 Bigtable-Like SSTable 数据结构，这个数据结构叫 Ressi，大家去看 2018 年 《Spanner: Becoming a SQL System》这篇 Paper 就能看到。它其实表面上看还是行存，但内部也是一个 Chunk 变成列存这样的一个结构。但我们觉得即使是换一个新的数据结构，也没有办法很好做隔离，因为毕竟还是在一台机器上，在同一个物理资源上。最彻底的隔离是物理隔离。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0add903697fd5befb078c2be36f9869e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-0add903697fd5befb078c2be36f9869e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-0add903697fd5befb078c2be36f9869e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-0add903697fd5befb078c2be36f9869e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-0add903697fd5befb078c2be36f9869e_b.jpg&quot;&gt;&lt;figcaption&gt;图 14 TiFlash 架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们在 TiFlash 用了好几种技术来去保证数据是更新的。一是增加了 Raft Leaner，二是我们把 TiDB 的 MVCC 也实现在了 TiFlash 的内部。第三在 TiFlash 这边接触了更新（的过程），在 TiFlash 内部还有一个小的 Memstore，来处理更新的热数据结果，最后查询的时候，是列存跟内存里的行存去 merge 并得到最终的结果。&lt;b&gt;TiFlash 的核心思想就是通过 Raft 的副本来做物理隔离。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个有什么好处呢？这是我们今天给出的答案，但是背后的思考，到底是什么原因呢？为什么我们不能直接去同步一个 binlog 到另外一个 dedicate 的新集群上（比如 TiFlash 集群），而一定要走 Raft log？&lt;b&gt;最核心的原因是，我们认为 Raft log 的同步可以水平扩展的。&lt;/b&gt;因为 TiDB 内部是 Mult-Raft 架构，Raft log 是发生在每一个 TiKV 节点的同步上。大家想象一下，如果中间是通过 Kafka 沟通两边的存储引擎，那么实时的同步会受制于中间管道的吞吐。比如图 14 中绿色部分一直在更新，另一边并发写入每秒两百万，但是中间的 Kafka 集群可能只能承载 100 万的写入，那么就会导致中间的 log 堆积，而且下游的消费也是不可控的。&lt;b&gt;而通过 Raft 同步， Throughput 可以根据实际存储节点的集群大小，能够线性增长。这是一个特别核心的好处。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4. SIMD&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;说完了存储层，接下来说一说执行器。TiDB 在接下来会做一个很重要的工作，就是全面地 leverage  SIMD 的计算。我先简单科普一下 SIMD 是什么。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-64c80ccd7111821bdbd01a9bc222e0d4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-64c80ccd7111821bdbd01a9bc222e0d4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-64c80ccd7111821bdbd01a9bc222e0d4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-64c80ccd7111821bdbd01a9bc222e0d4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-64c80ccd7111821bdbd01a9bc222e0d4_b.jpg&quot;&gt;&lt;figcaption&gt;图 15 SIMD 原理举例（1/2）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;如图 15，在做一些聚合的时候，有这样一个函数，我要去做一个求和。正常人写程序，他就是一个 for 循环，做累加。但是在一个数据库里面，如果有一百亿条数据做聚合，每一次执行这条操作的时候，CPU 的这个指令是一次一次的执行，数据量特别大或者扫描的行数特别多的时候，就会很明显的感受到这个差别。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0e40ad4b5155b8533dc2943d04f195d8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-0e40ad4b5155b8533dc2943d04f195d8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-0e40ad4b5155b8533dc2943d04f195d8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-0e40ad4b5155b8533dc2943d04f195d8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-0e40ad4b5155b8533dc2943d04f195d8_b.jpg&quot;&gt;&lt;figcaption&gt;图 16 SIMD 原理举例（2/2）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;现代的 CPU 会支持一些批量的指令，比如像 _mm_add_epi32，可以一次通过一个32 位字长对齐的命令，批量的操作 4 个累加。看上去只是省了几个 CPU 的指令，但如果是在一个大数据量的情况下，基本上能得到 4 倍速度的提升。&lt;/p&gt;&lt;p&gt;&lt;b&gt;顺便说一句，有一个很大的趋势是 I/O 已经不是瓶颈了&lt;/b&gt;，大家一定要记住我这句话。再过几年，如果想去买一块机械磁盘，除了在那种冷备的业务场景以外，我相信大家可能都要去定制一块机械磁盘了。未来一定 I/O 不会是瓶颈，那瓶颈会是什么？CPU。&lt;b&gt;我们怎么去用新的硬件，去尽可能的把计算效率提升，这个才是未来我觉得数据库发展的重点。&lt;/b&gt;比如说我怎么在数据库里 leverage GPU 的计算能力，因为如果 GPU 用的好，其实可以很大程度上减少计算的开销。所以，如果在单机 I/O 这些都不是问题的话，下一个最大问题就是怎么做好分布式，这也是为什么我们一开始就选择了一条看上去更加困难的路：我要去做一个 Share-nothing 的数据库，并不是像 Aurora 底下共享一个存储。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;5. Dynamic Data placement&lt;/b&gt;&lt;/h2&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-06cea5f8e5ccb0907b854efc391fab72_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-06cea5f8e5ccb0907b854efc391fab72_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-06cea5f8e5ccb0907b854efc391fab72_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic3.zhimg.com/v2-06cea5f8e5ccb0907b854efc391fab72_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-06cea5f8e5ccb0907b854efc391fab72_b.jpg&quot;&gt;&lt;figcaption&gt;图 17 Dynamic Data placement (1/2)分库分表方案与 TiDB 对比&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在今天大家其实看不到未来十年数据增长是怎样的，回想十年前大家能想到现在我们的数据量有这么大吗？不可能的。所以新的架构或者新的数据库，一定要去面向我们未知的 Scale 做设计。比如大家想象现在有业务 100T 的数据，目前看可能还挺大的，但是有没有办法设计一套方案去解决 1P、2P 这样数据量的架构？&lt;b&gt;在海量的数据量下，怎么把数据很灵活的分片是一个很大的学问。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;为什么分库分表在对比 TiDB 的时候，我们会觉得分库分表是上一代的方案。这个也很好理解，核心的原因是分库分表的 Router 是静态的。如果出现分片不均衡，比如业务可能按照 User ID 分表，但是发现某一地方/某一部分的 User ID 特别多，导致数据不均衡了，这时 TiDB 的架构有什么优势呢？就是 TiDB 彻底把分片这个事情，从数据库里隔离了出来，放到了另外一个模块里。&lt;b&gt;分片应该是根据业务的负载、根据数据的实时运行状态，来决定这个数据应该放在哪儿。这是传统的静态分片不能相比的，不管传统的用一致性哈希，还是用最简单的对机器数取模的方式去分片（都是不能比的）。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在这个架构下，甚至未来我们还能让 AI 来帮忙。把分片操作放到 PD 里面，它就像一个 DBA 一样，甚至预测 Workload 给出数据分布操作。比如课程报名数据库系统，系统发现可能明天会是报名高峰，就事先把数据给切分好，放到更好的机器上。这在传统方案下是都需要人肉操作，其实这些事情都应该是自动化的。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-681904c633e1d90a74b890241d125243_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-681904c633e1d90a74b890241d125243_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-681904c633e1d90a74b890241d125243_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-681904c633e1d90a74b890241d125243_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-681904c633e1d90a74b890241d125243_b.jpg&quot;&gt;&lt;figcaption&gt;图 18 Dynamic Data placement (2/2)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;Dynamic Data placement 好处首先是让事情变得更 flexible ，对业务能实时感知和响应。&lt;/b&gt;另外还有一点，为什么我们有了 Dynamic Placement 的策略，还要去做 Table Partition（&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487986%26idx%3D1%26sn%3Dcc0d28d9776bc50ede7a9fc4aa403208%26chksm%3Deb163698dc61bf8e602fe61d12376c5d951a71c1568b3cd253e0f4d410bf7918c5c2fadf01ce%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;今天上午申砾也提到了&lt;/a&gt;&lt;/u&gt;）？Table Partition 在背后实现其实挺简单的。相当于业务这边已经告诉我们数据应该怎么分片比较好，我们还可以做更多针对性的优化。这个 Partition 指的是逻辑上的 Partition ，是可能根据你的业务相关的，比如说我这张表，就是存着 2018 年的数据，虽然我在底下还是 TiDB 这边，通过 PD 去调度，但是我知道你 Drop 这个 Table 的时候，一定是 Drop 这些数据，所以这样会更好，而且更加符合用户的直觉。&lt;/p&gt;&lt;p&gt;但这样架构仍然有比较大的挑战。当然这个挑战在静态分片的模型上也都会有。比如说围绕着这个问题，我们一直在去尝试解决怎么更快的发现数据的热点，比如说我们的调度器，如果最好能做到，比如突然来个秒杀业务，我们马上就发现了，就赶紧把这块数据挪到好的机器上，或者把这块数据赶紧添加副本，再或者把它放到内存的存储引擎里。这个事情应该是由数据库本身去做的。所以为什么我们这么期待 AI 技术能够帮我们，是因为虽然在 TiDB 内部，用了很多规则和方法来去做这个事情，但我们不是万能的。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;6. Storage and Computing Seperation&lt;/b&gt;&lt;/h2&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3efdf2d6f9f1d840b597e946b5067bdb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-3efdf2d6f9f1d840b597e946b5067bdb_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-3efdf2d6f9f1d840b597e946b5067bdb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-3efdf2d6f9f1d840b597e946b5067bdb_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-3efdf2d6f9f1d840b597e946b5067bdb_b.jpg&quot;&gt;&lt;figcaption&gt;图 19 存储计算分离&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;还有大的趋势是存储计算分离。我觉得现在业界有一个特别大的问题，就是把存储计算分离给固化成了某一个架构的特定一个指代，比如说只有长的像 Aurora 那样的架构才是存储计算分离。那么 TiDB 算存储计算分离吗？我觉得其实算。&lt;b&gt;或者说存储计算分离本质上带来的好处是什么？就是我们的存储依赖的物理资源，跟计算所依赖的物理资源并不一样。这点其实很重要。&lt;/b&gt;就用 TiDB 来举例子，比如计算可能需要很多 CPU，需要很多内存来去做聚合，存储节点可能需要很多的磁盘和 I/O，如果全都放在一个组件里 ，调度器就会很难受：我到底要把这个节点作为存储节点还是计算节点？其实在这块，可以让调度器根据不同的机型（来做决定），是计算型机型就放计算节点，是存储型机型就放存储节点。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;7. Everything is Pluggable&lt;/b&gt;&lt;/h2&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4e0d358737e58907be9656672a189810_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-4e0d358737e58907be9656672a189810_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-4e0d358737e58907be9656672a189810_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-4e0d358737e58907be9656672a189810_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-4e0d358737e58907be9656672a189810_b.jpg&quot;&gt;&lt;figcaption&gt;图 20 Everything is Pluggable&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;今天由于时间关系没有给大家演示的&lt;b&gt;插件平台&lt;/b&gt;。未来 TiDB 会变成一个更加灵活的框架，像图 20 中 TiFlash 是一个 local storage，我们其实也在秘密研发一个新的存储的项目叫 Unitstore，可能明年的 DevCon 就能看到它的 Demo 了。在计算方面，每一层我们未来都会去对外暴露一个非常抽象的接口，能够去 leverage 不同的系统的好处。今年我其实很喜欢的一篇 Paper 是 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247486921%26idx%3D2%26sn%3Dbcc1787a8107ec84a8d264fa196b0bf8%26chksm%3Deb162aa3dc61a3b5030e114ac1c871ed8841886d495934b33f3a8f8706858c691a4ffdb2404a%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;F1 Query&lt;/a&gt;&lt;/u&gt; 这篇论文，基本表述了我对一个大规模的分布式系统的期待，架构的切分非常漂亮。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;8. Distributed Transaction&lt;/b&gt;&lt;/h2&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_b.jpg&quot;&gt;&lt;figcaption&gt;图 21 Distributed Transaction（1/2）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;说到分布式事务，我也分享一下我的观点。&lt;b&gt;目前看上去，ACID 事务肯定是必要的。&lt;/b&gt;我们仍然还没有太多更好的办法，除了 Google 在这块用了原子钟，Truetime 非常牛，我们也在研究各种新型的时钟的技术，但是要把它推广到整个开源社区也不太可能。当然，时间戳，不管是用硬件还是软件分配，仍然是我们现在能拥有最好的东西， 因为如果要摆脱中心事务管理器，时间戳还是很重要的。&lt;b&gt;所以在这方面的挑战就会变成：怎么去减少两阶段提交带来的网络的 round-trips？或者如果有一个时钟的 PD 服务，怎么能尽可能的少去拿时间戳？&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;375&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-920818bc4fecd4594bbabd798d942661_b.jpg&quot;&gt;&lt;figcaption&gt;图 22 Distributed Transaction（2/2）&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们在这方面的理论上有一些突破，我们把 Percolator 模型做了一些优化，能够在数学上证明，可以少拿一次时钟。虽然我们目前还没有在 TiDB 里去实现，但是我们已经把数学证明的过程已经开源出来了，我们用了 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/tla-plus/blob/master/OptimizedCommitTS/OptimizedCommitTS.tla&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TLA+ 这个数学工具去做了证明&lt;/a&gt;。此外在 PD 方面，我们也在思考是不是所有的事务都必须跑到 PD 去拿时间戳？其实也不一定，我们在这上面也已有一些想法和探索，但是现在还没有成型，这个不剧透了。另外我觉得还有一个非常重要的东西，就是 Follower Read。很多场景读多写少，读的业务压力很多时候是要比写大很多的，Follower Read 能够帮我们线性扩展读的性能，而且在我们的模型上，因为没有时间戳 ，所以能够在一些特定情况下保证不会去牺牲一致性。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;9. Cloud-Native Architecture&lt;/b&gt;&lt;/h2&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9915e481920680369a625c8df2099170_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-9915e481920680369a625c8df2099170_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9915e481920680369a625c8df2099170_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;370&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic1.zhimg.com/v2-9915e481920680369a625c8df2099170_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-9915e481920680369a625c8df2099170_b.jpg&quot;&gt;&lt;figcaption&gt;图 23 Cloud-Native&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;另外一点就是 Cloud-Native。刚刚中午有一个社区小伙伴问我，你们为什么不把多租户做在 TiDB 的系统内部？&lt;b&gt;我想说「数据库就是数据库」，它并不是一个操作系统，不是一个容器管理平台。我们更喜欢模块和结构化更清晰的一个做事方式。&lt;/b&gt;而且 Kubernetes 在这块已经做的足够好了 ，我相信未来 K8s 会变成集群的新操作系统，会变成一个 Linux。比如说如果你单机时代做一个数据库，你会在你的数据库里面内置一个操作系统吗？肯定不会。所以这个模块抽象的边界，在这块我还是比较相信 K8s 的。《Large-scale cluster management at Google with Borg》这篇论文里面提到了一句话，BigTable 其实也跑在 Borg 上。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e590a8bf2f557d07ce9c18ab2d72bc4b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-e590a8bf2f557d07ce9c18ab2d72bc4b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e590a8bf2f557d07ce9c18ab2d72bc4b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;939&quot; data-rawheight=&quot;372&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;939&quot; data-original=&quot;https://pic4.zhimg.com/v2-e590a8bf2f557d07ce9c18ab2d72bc4b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-e590a8bf2f557d07ce9c18ab2d72bc4b_b.jpg&quot;&gt;&lt;figcaption&gt;图 24 TiDB 社区小伙伴的愿望列表&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当然最后，大家听完这一堆东西以后，回头看我们社区小伙伴们的愿望列表（图 24），就会发现对一下 TiDB 好像还都能对得上 :D &lt;/p&gt;&lt;p&gt;谢谢大家。&lt;/p&gt;&lt;p&gt;- END - &lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;延伸阅读 &lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57693856&quot; class=&quot;internal&quot;&gt;The Way to TiDB 3.0 and Beyond (上篇)&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57749943&quot; class=&quot;internal&quot;&gt;The Way to TiDB 3.0 and Beyond (下篇)&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/56624608&quot; class=&quot;internal&quot;&gt;2018 TiDB 社区成长足迹与小红花&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/55728943&quot; class=&quot;internal&quot;&gt;刘奇：我们最喜欢听用户说的话是「你们搞得定吗？」&lt;/a&gt;&lt;br&gt;&lt;/p&gt;&lt;blockquote&gt;1 月 19 日 &lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487846%26idx%3D1%26sn%3D5d349facbf078b19b886ccfa16b152c4%26chksm%3Deb16360cdc61bf1a29efb65e0413877e3cb31bf4e8a3e439c615ae03eeb94a937ccb23948942%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB DevCon 2019 &lt;/a&gt;在北京圆满落幕，超过 750 位热情的社区伙伴参加了此次大会。会上我们首次全面展示了全新存储引擎 Titan、新生态工具 TiFlash 以及 TiDB 在云上的进展，同时宣布 TiDB-Lightning Toolset &amp;amp; TiDB-DM 两大生态工具开源，并分享了  TiDB 3.0 的特性与未来规划，描述了我们眼中未来数据库的模样。此外，更有 11 位来自一线的 TiDB 用户为大家分享了实践经验与踩过的「坑」。同时，我们也为新晋 TiDB Committer 授予了证书，并为 2018 年最佳社区贡献个人、最佳社区贡献团队颁发了荣誉奖杯。&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-03-05-58337623</guid>
<pubDate>Tue, 05 Mar 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>这些「神秘」团队到底是做什么的？| PingCAP 招聘季</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-03-05-58058910.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/58058910&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2ab741536518ae13908e30e1899ffb01_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;过去一年在 PingCAP 全力奔跑的同时，越来越多的小伙伴开始关注我们、了解我们，我们的团队也愈加庞大，我们也期待更多对我们感兴趣的小伙伴加入我们，跟我们一起做点有意义的事情。可能有些小伙伴对我司「神秘的招聘职位」感到茫然，对我们在做的事情也没有深入的了解，&lt;b&gt;于是我们准备推出「PingCAP 招聘职位深度解读」系列文章，&lt;/b&gt;介绍 PingCAP 各个团队的小伙伴们现在在做什么、接下来的规划是什么、不同团队吸纳成员的核心需求是什么等等。&lt;br&gt;本篇将带大家速览我司各个研发团队的定位和分工，并回答一个热门问题「在 PingCAP 工作是什么样的体验？」&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;作为开源的新型分布式数据库公司，PingCAP 一直致力于探索并逐步解决分布式数据库领域的诸多问题&lt;/b&gt;，比如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如何设计和实现世界前沿的分布式 SQL 优化器，让一个复杂的 SQL 查询变的无比轻快智能；&lt;/li&gt;&lt;li&gt;如何实现一致性同步的行列格式混合的 HTAP 架构，且 AP 业务对 TP 业务几乎无干扰；&lt;/li&gt;&lt;li&gt;如何在成千上万台集群规模的情况下，实现无阻塞的表结构变更操作，而不影响任何在线的业务；&lt;/li&gt;&lt;li&gt;如何实现高效的分布式事务算法，让 ACID 事务在大规模并发的分布式存场景下依然可以高效可靠；&lt;/li&gt;&lt;li&gt;如何基于 Raft 协议实现快速稳定的数据强一致复制和自动故障恢复，确保数据安全；&lt;/li&gt;&lt;li&gt;如何设计一个高效智能的调度器，负责对上百 TB 的数据进行调度，保证系统平稳运行；&lt;/li&gt;&lt;li&gt;如何在一个 PR 提交之后，让千万级的测试 cases 在三分钟内跑完，并立即看到对数据库性能有没有显著的提升，以及混沌工程的具体实践；&lt;/li&gt;&lt;li&gt;如何在 AWS，GCP，Aliyun 等公有云上一键启动 TiDB 集群，一键伸缩上百个数据库节点，理解有状态服务在 K8s 上调度的最佳实践。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;我们研发团队的定位和分工与以上问题息息相关，或者说，是围绕着 TiDB 产品展开的。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6a32b096dd5f22d8b11ed55c02838819_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2098&quot; data-rawheight=&quot;840&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;2098&quot; data-original=&quot;https://pic2.zhimg.com/v2-6a32b096dd5f22d8b11ed55c02838819_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-6a32b096dd5f22d8b11ed55c02838819_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;2098&quot; data-rawheight=&quot;840&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;2098&quot; data-original=&quot;https://pic2.zhimg.com/v2-6a32b096dd5f22d8b11ed55c02838819_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-6a32b096dd5f22d8b11ed55c02838819_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;从上图可以看到，TiDB 集群主要包括三个核心组件：TiDB Server，TiKV Server 和 PD Server，分别用于解决计算、存储、调度这三个核心问题。此外，还有用于解决用户复杂 OLAP 需求的 TiSpark / TiFlash 组件。与之对应的，我们的内核研发团队分别是：&lt;b&gt;TiDB&lt;/b&gt; 团队、 &lt;b&gt;TiKV&lt;/b&gt; 团队和 &lt;b&gt;AP&lt;/b&gt;（Analytical Processing）团队，此外还有 &lt;b&gt;Cloud&lt;/b&gt; 团队、&lt;b&gt;EE&lt;/b&gt;（Efficiency Engineering）团队和新成立的 &lt;b&gt;QA&lt;/b&gt;（Quality Assurance）团队。&lt;/p&gt;&lt;p&gt;所以很多对 TiDB 不太了解的小伙伴看完我们的招聘页面，可能会觉得那些五（没）花（听）八（说）门（过）的研发类职位&lt;b&gt;是特别神秘的存在……吧……&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;713&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;713&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot;&gt;&lt;figcaption&gt;招聘页面上一小部分神秘部队&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;那么这些「神秘」团队到底是做什么的？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;下面就简单的介绍一下这些研发团队是做什么的吧。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 团队负责所有和 SQL 计算相关的工作以及和客户端（业务）之间的交互，包括协议解析、语法解析、查询优化、执行计算等等，这是一个承上启下的核心模块。除此之外还包括与其他数据库之间的数据迁移和同步组件，比如 TiDB 自身的 Binlog 模块以及读取 MySQL 之类数据源 Binlog 的组件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiKV 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 是一个支持事务的，数据强一致的分布式 Key-Value 存储引擎。 从产品架构图中可以看出：无论是 TiDB Server 还是 TiSpark 组件，都是从 TiKV 存取数据的，所以我们一定要保证 TiKV 的稳定和高效。TiKV 团队主要负责的就是分布式 Key-Value 存储引擎的设计和开发，分布式调度系统的设计与研发，构建分布式压力测试框架，稳定性测试框架等工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;AP 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个是一个比较新的团队，主要负责 OLAP 业务相关的产品，包括之前已经有的 TiSpark 和正在研发中的 AP 扩展引擎 TiFlash 产品。TiDB 是一款 HTAP 的产品，而加强和补齐 HTAP 中的 AP 环节主要就这个组的责任，这里包含了基于 Raft 的一致性同步列存引擎，MPP 计算引擎开发以及大数据相关产品的整合等工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 是一个 Cloud Native 的数据库，Cloud 团队的职责就是让 TiDB 更平滑、以更大的规模跑在云上。他们将 TiDB 的组件容器化，并借助 Kubernetes 进行编排与调度。其核心是 TiDB-Operator，实现了云上的快速部署、一键伸缩和故障自治愈。编排有状态的分布式服务是 Kubernetes 最有挑战的事情之一，也是这个团队最擅长解决的问题。Cloud 团队正在努力将 TiDB 构建成为一个云上的服务，即一个 Multi-tenant, Across-cloud, Fully-managed 的 DBaaS（Database as a Service）产品。&lt;/p&gt;&lt;p&gt;&lt;b&gt;EE 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这是一个非常 Hack 的团队，致力于解决研发、测试、交付、甚至公司运营中的各种效率问题。他们信仰自动化，摒弃重复性的人工劳动，发明各种 bot 帮助提高 DevOps 的效率；他们创造了强大的“薛定谔”测试平台，将混沌工程变成现实，不断挑战分布式数据库的极限；他们深入系统内核，改造 bcc/eBPF 这些最酷的工具，将操作系统的秘密暴露无遗；他们高效率定位线上的各种疑难杂症，还第一手玩到 Optane Memory 硬件——他们就是神秘的 EE 团队。&lt;/p&gt;&lt;p&gt;&lt;b&gt;QA 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每个发布的 TiDB 版本，都有数千万的测试用例来保障产品在客户生产环境下的完美工作。QA 团队开发测试工具和自动化测试框架，并引入混沌工程、人工智能技术来保障 TiDB 的数据一致性和稳定性。&lt;/p&gt;&lt;blockquote&gt;后续我们将每周更新 1-2 篇文章为大家详细介绍以上团队和相关职位。如果大家对文章有意见或建议，欢迎在微信后台留言或者发邮件到 hire@pingcap.com 告诉我们～&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;在 PingCAP 工作是什么样的体验？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这可能是很多小伙伴们最最关心的 Part。弹性工作制、零食水果、六险一金这些就不多说了，应该已经成为很多公司的标配，我们来说点有特色的：&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作内容&lt;/b&gt;&lt;/p&gt;&lt;p&gt;选择一份工作，工作内容是否有意义、有价值，你是否有兴趣投入其中，这两点至关重要。&lt;/p&gt;&lt;p&gt;在 PingCAP，你可以亲自参与打造一款代表未来数据库产品，接触核心的分布式关系数据库技术，你的每一个想法都会被重视，每一次提交都有可能给整个产品带来意想不到的变化。&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作伙伴&lt;/b&gt;&lt;/p&gt;&lt;p&gt;他们大多来自于国内外一线互联网公司，有非常出色的技术实力，作为聪明人的你一定也想和聪明的人一起工作。团队成员整体比较年轻，氛围相对轻松、自在。在这里，你可以保留自己的个性和兴趣爱好。无论你是爱好桌游、喜欢摇滚、热爱运动，都能找到与你志同道合的小伙伴，在从事喜欢的工作的同时也可以做你自己，是不是很 Cool？&lt;/p&gt;&lt;p&gt;&lt;b&gt;开源文化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们有着&lt;b&gt;活跃的开源社区&lt;/b&gt;。截止到 2019 年 3 月 1 日，TiDB+TiKV 项目在 GitHub 上的 Star 数已经达到了 21000+，拥有 350+ Contributor，社区的力量在不断壮大。TiDB-Operator、TiDB-DM、TiDB-Lightning 等生态工具陆续开源；24 篇 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiDB-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读系列文章&lt;/a&gt;&lt;/u&gt; 已经完结，&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章&lt;/a&gt;&lt;/u&gt; 已经启动 ；除了开放的线下 Infra Meetup，我们也将内部的 Paper Reading 活动放到了线上直播平台（Bilibili ID: TiDB_Robot）…… 想要了解 2018 年 TiDB 社区的成长足迹可以查看这篇文章——&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487903%26idx%3D1%26sn%3Dc14855dae7309753a7480558be80896d%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《2018 TiDB 社区成长足迹与小红花 | TiDB DevCon 2019》&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作地点&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前除北京总部之外，我们在&lt;b&gt;上海、杭州、广州、深圳、成都、硅谷&lt;/b&gt;都设立了 Office。你可以去体验北上广深的快节奏，感受经济、文化、思想的强烈碰撞，也可以去杭州、成都，在下班或午后享受片刻的宁静与悠闲，还可以去硅谷体验前沿的技术氛围；如果你喜欢美食，可以去魔都的人民广场吃炸鸡，也可以去广州品味一下正宗的粤式茶点，还可以去硅谷 Office 尝一尝正宗的西餐，当然还有成都的火锅、小酒馆等着你；甚至你还有机会 &lt;b&gt;Remote &lt;/b&gt;在家，事业家庭两相宜。&lt;/p&gt;&lt;p&gt;&lt;b&gt;需要特别说明的是，我们并不会按照工作地点来划分工作模块&lt;/b&gt;，每一个 Office 的小伙伴都在我们的核心研发模块中承担着重要角色，而且内部的跨团队和跨地域 Transfer 都非常透明，PingCAP 的整个项目协作也都是分布式的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;全方面的成长&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;入职之后，Mentor 会为你定制化培养方案，你对于所从事模块的认知会日渐深入，公司内部小伙伴的分享以及 Paper Reading、Meetup 等活动也能够帮助你对于其他知识领域有更加深刻的认识；&lt;/li&gt;&lt;li&gt;公司为每一位小伙伴提供了分享平台，支持并鼓励大家积极分享自己的想法和见解，在这个过程中，你的语言表达能力、逻辑思维能力也能得到一定程度的提升；&lt;/li&gt;&lt;li&gt;当然，如果你具备了作为 Mentor 的能力并有意向尝试 Mentor 的角色，在 PingCAP，都有机会实现。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们一直以来的理念是希望每个 PingCAP 的小伙伴都先得到个人成长，再反哺给团队和公司，每一个小伙伴都能参与到公司发展的过程中来。我们完全不担心「把你锻炼出来，却被其他公司高价挖走了」这类事情。且不说我们的薪酬本身就很有竞争力，更重要的是，我们相信一旦你喜欢上我们的理念和工作模式，你是不会舍得离开的～&lt;/p&gt;&lt;h2&gt;&lt;b&gt;加入我们吧！&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们认为优秀的工程师或多或少有以下共同特质：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A Quick Learner&lt;/li&gt;&lt;li&gt;An Earnest Curiosity&lt;/li&gt;&lt;li&gt;Faith in Open Source&lt;/li&gt;&lt;li&gt;Self-driven    &lt;/li&gt;&lt;li&gt;Get Things Done&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果你符合以上特质，欢迎进入招聘页面查看目前开放的工作机会：&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/recruit-cn/join/%23positions&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;虚位以待&lt;/a&gt;&lt;p&gt;&lt;b&gt;简历投递通道：hire@pingcap.com&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;实习生&lt;/b&gt;：公司的各项福利和学习资源对实习生全面开放，更重要的是实习生还未毕业就有机会接触工业级项目，而且实习期间表现优异者将有机会获得校招绿色通道特权。如果小伙伴们时间不够充裕，也可以先从社区 Contributor 做起，或许下一期 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D1%26sn%3D53b17f49e05af2cd832192fd67eaf75f%26chksm%3Deb1628b1dc61a1a704544f250cee480e7f6cf95cf5e84c2ccb7f0858398e685160122ed687b6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Talent Plan&lt;/a&gt;&lt;/u&gt; 的主角就是你！&lt;/p&gt;&lt;p&gt;&lt;b&gt;伯乐推荐&lt;/b&gt;：如果你身边有符合以上要求的小伙伴，也可以找我们聊一聊，推荐成功就有机会获得伯乐推荐奖励（iPad、iPhone、MacBook Pro 等等）。伯乐推荐邮件格式：[伯乐推荐] 候选人姓名-职位名称-推荐人姓名-推荐人手机号。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-03-05-58058910</guid>
<pubDate>Tue, 05 Mar 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>这些「神秘」团队到底是做什么的？| PingCAP 招聘季</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-03-01-58058910.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/58058910&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-2ab741536518ae13908e30e1899ffb01_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;过去一年在 PingCAP 全力奔跑的同时，越来越多的小伙伴开始关注我们、了解我们，我们的团队也愈加庞大，我们也期待更多对我们感兴趣的小伙伴加入我们，跟我们一起做点有意义的事情。可能有些小伙伴对我司「神秘的招聘职位」感到茫然，对我们在做的事情也没有深入的了解，&lt;b&gt;于是我们准备推出「PingCAP 招聘职位深度解读」系列文章，&lt;/b&gt;介绍 PingCAP 各个团队的小伙伴们现在在做什么、接下来的规划是什么、不同团队吸纳成员的核心需求是什么等等。&lt;br&gt;本篇将带大家速览我司各个研发团队的定位和分工，并回答一个热门问题「在 PingCAP 工作是什么样的体验？」&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;作为开源的新型分布式数据库公司，PingCAP 一直致力于探索并逐步解决分布式数据库领域的诸多问题&lt;/b&gt;，比如：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;如何设计和实现世界前沿的分布式 SQL 优化器，让一个复杂的 SQL 查询变的无比轻快智能；&lt;/li&gt;&lt;li&gt;如何实现一致性同步的行列格式混合的 HTAP 架构，且 AP 业务对 TP 业务几乎无干扰；&lt;/li&gt;&lt;li&gt;如何在成千上万台集群规模的情况下，实现无阻塞的表结构变更操作，而不影响任何在线的业务；&lt;/li&gt;&lt;li&gt;如何实现高效的分布式事务算法，让 ACID 事务在大规模并发的分布式存场景下依然可以高效可靠；&lt;/li&gt;&lt;li&gt;如何基于 Raft 协议实现快速稳定的数据强一致复制和自动故障恢复，确保数据安全；&lt;/li&gt;&lt;li&gt;如何设计一个高效智能的调度器，负责对上百 TB 的数据进行调度，保证系统平稳运行；&lt;/li&gt;&lt;li&gt;如何在一个 PR 提交之后，让千万级的测试 cases 在三分钟内跑完，并立即看到对数据库性能有没有显著的提升，以及混沌工程的具体实践；&lt;/li&gt;&lt;li&gt;如何在 AWS，GCP，Aliyun 等公有云上一键启动 TiDB 集群，一键伸缩上百个数据库节点，理解有状态服务在 K8s 上调度的最佳实践。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;我们研发团队的定位和分工与以上问题息息相关，或者说，是围绕着 TiDB 产品展开的。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;493&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-77018e92be983a55b17de2edac514616_b.jpg&quot;&gt;&lt;figcaption&gt;TiDB 产品架构&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;从上图可以看到，TiDB 集群主要包括三个核心组件：TiDB Server，TiKV Server 和 PD Server，分别用于解决计算、存储、调度这三个核心问题。此外，还有用于解决用户复杂 OLAP 需求的 TiSpark / TiFlash 组件。与之对应的，我们的内核研发团队分别是：&lt;b&gt;TiDB&lt;/b&gt; 团队、 &lt;b&gt;TiKV&lt;/b&gt; 团队和 &lt;b&gt;AP&lt;/b&gt;（Analytical Processing）团队，此外还有 &lt;b&gt;Cloud&lt;/b&gt; 团队、&lt;b&gt;EE&lt;/b&gt;（Efficiency Engineering）团队和新成立的&lt;b&gt;QA&lt;/b&gt;（Quality Assurance）团队。&lt;/p&gt;&lt;p&gt;所以很多对 TiDB 不太了解的小伙伴看完我们的招聘页面，可能会觉得那些五（没）花（听）八（说）门（过）的研发类职位&lt;b&gt;是特别神秘的存在……吧……&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;713&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;713&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-9d24925d3e6e8a37cbbd6637bb981374_b.jpg&quot;&gt;&lt;figcaption&gt;招聘页面上一小部分神秘部队&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;那么这些「神秘」团队到底是做什么的？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;下面就简单的介绍一下这些研发团队是做什么的吧。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 团队负责所有和 SQL 计算相关的工作以及和客户端（业务）之间的交互，包括协议解析、语法解析、查询优化、执行计算等等，这是一个承上启下的核心模块。除此之外还包括与其他数据库之间的数据迁移和同步组件，比如 TiDB 自身的 Binlog 模块以及读取 MySQL 之类数据源 Binlog 的组件。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiKV 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiKV 是一个支持事务的，数据强一致的分布式 Key-Value 存储引擎。 从产品架构图中可以看出：无论是 TiDB Server 还是 TiSpark 组件，都是从 TiKV 存取数据的，所以我们一定要保证 TiKV 的稳定和高效。TiKV 团队主要负责的就是分布式 Key-Value 存储引擎的设计和开发，分布式调度系统的设计与研发，构建分布式压力测试框架，稳定性测试框架等工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;AP 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个是一个比较新的团队，主要负责 OLAP 业务相关的产品，包括之前已经有的 TiSpark 和正在研发中的 AP 扩展引擎 TiFlash 产品。TiDB 是一款 HTAP 的产品，而加强和补齐 HTAP 中的 AP 环节主要就这个组的责任，这里包含了基于 Raft 的一致性同步列存引擎，MPP 计算引擎开发以及大数据相关产品的整合等工作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Cloud 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 是一个 Cloud Native 的数据库，Cloud 团队的职责就是让 TiDB 更平滑、以更大的规模跑在云上。他们将 TiDB 的组件容器化，并借助 Kubernetes 进行编排与调度。其核心是 TiDB-Operator，实现了云上的快速部署、一键伸缩和故障自治愈。编排有状态的分布式服务是 Kubernetes 最有挑战的事情之一，也是这个团队最擅长解决的问题。Cloud 团队正在努力将 TiDB 构建成为一个云上的服务，即一个 Multi-tenant, Across-cloud, Fully-managed 的 DBaaS（Database as a Service）产品。&lt;/p&gt;&lt;p&gt;&lt;b&gt;EE 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这是一个非常 Hack 的团队，致力于解决研发、测试、交付、甚至公司运营中的各种效率问题。他们信仰自动化，摒弃重复性的人工劳动，发明各种 bot 帮助提高 DevOps 的效率；他们创造了强大的“薛定谔”测试平台，将混沌工程变成现实，不断挑战分布式数据库的极限；他们深入系统内核，改造 bcc/eBPF 这些最酷的工具，将操作系统的秘密暴露无遗；他们高效率定位线上的各种疑难杂症，还第一手玩到 Optane Memory 硬件——他们就是神秘的 EE 团队。&lt;/p&gt;&lt;p&gt;&lt;b&gt;QA 团队&lt;/b&gt;&lt;/p&gt;&lt;p&gt;每个发布的 TiDB 版本，都有数千万的测试用例来保障产品在客户生产环境下的完美工作。QA 团队开发测试工具和自动化测试框架，并引入混沌工程、人工智能技术来保障 TiDB 的数据一致性和稳定性。&lt;/p&gt;&lt;blockquote&gt;后续我们将每周更新 1-2 篇文章为大家详细介绍以上团队和相关职位。如果大家对文章有意见或建议，欢迎在微信后台留言或者发邮件到 hire@pingcap.com 告诉我们～&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;在 PingCAP 工作是什么样的体验？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这可能是很多小伙伴们最最关心的 Part。弹性工作制、零食水果、六险一金这些就不多说了，应该已经成为很多公司的标配，我们来说点有特色的：&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作内容&lt;/b&gt;&lt;/p&gt;&lt;p&gt;选择一份工作，工作内容是否有意义、有价值，你是否有兴趣投入其中，这两点至关重要。&lt;/p&gt;&lt;p&gt;在 PingCAP，你可以亲自参与打造一款代表未来数据库产品，接触核心的分布式关系数据库技术，你的每一个想法都会被重视，每一次提交都有可能给整个产品带来意想不到的变化。&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作伙伴&lt;/b&gt;&lt;/p&gt;&lt;p&gt;他们大多来自于国内外一线互联网公司，有非常出色的技术实力，作为聪明人的你一定也想和聪明的人一起工作。团队成员整体比较年轻，氛围相对轻松、自在。在这里，你可以保留自己的个性和兴趣爱好。无论你是爱好桌游、喜欢摇滚、热爱运动，都能找到与你志同道合的小伙伴，在从事喜欢的工作的同时也可以做你自己，是不是很 Cool？&lt;/p&gt;&lt;p&gt;&lt;b&gt;开源文化&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们有着&lt;b&gt;活跃的开源社区&lt;/b&gt;。截止到 2019 年 3 月 1 日，TiDB+TiKV 项目在 GitHub 上的 Star 数已经达到了 21000+，拥有 350+ Contributor，社区的力量在不断壮大。TiDB-Operator、TiDB-DM、TiDB-Lightning 等生态工具陆续开源；24 篇 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiDB-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读系列文章&lt;/a&gt;&lt;/u&gt; 已经完结，&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码解析系列文章&lt;/a&gt;&lt;/u&gt; 已经启动 ；除了开放的线下 Infra Meetup，我们也将内部的 Paper Reading 活动放到了线上直播平台（Bilibili ID: TiDB_Robot）…… 想要了解 2018 年 TiDB 社区的成长足迹可以查看这篇文章——&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487903%26idx%3D1%26sn%3Dc14855dae7309753a7480558be80896d%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;《2018 TiDB 社区成长足迹与小红花 | TiDB DevCon 2019》&lt;/a&gt;&lt;/u&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;工作地点&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前除北京总部之外，我们在&lt;b&gt;上海、杭州、广州、深圳、成都、硅谷&lt;/b&gt;都设立了 Office。你可以去体验北上广深的快节奏，感受经济、文化、思想的强烈碰撞，也可以去杭州、成都，在下班或午后享受片刻的宁静与悠闲，还可以去硅谷体验前沿的技术氛围；如果你喜欢美食，可以去魔都的人民广场吃炸鸡，也可以去广州品味一下正宗的粤式茶点，还可以去硅谷 Office 尝一尝正宗的西餐，当然还有成都的火锅、小酒馆等着你；甚至你还有机会 &lt;b&gt;Remote &lt;/b&gt;在家，事业家庭两相宜。&lt;/p&gt;&lt;p&gt;&lt;b&gt;需要特别说明的是，我们并不会按照工作地点来划分工作模块&lt;/b&gt;，每一个 Office 的小伙伴都在我们的核心研发模块中承担着重要角色，而且内部的跨团队和跨地域 Transfer 都非常透明，PingCAP 的整个项目协作也都是分布式的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;全方面的成长&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;入职之后，Mentor 会为你定制化培养方案，你对于所从事模块的认知会日渐深入，公司内部小伙伴的分享以及 Paper Reading、Meetup 等活动也能够帮助你对于其他知识领域有更加深刻的认识；&lt;/li&gt;&lt;li&gt;公司为每一位小伙伴提供了分享平台，支持并鼓励大家积极分享自己的想法和见解，在这个过程中，你的语言表达能力、逻辑思维能力也能得到一定程度的提升；&lt;/li&gt;&lt;li&gt;当然，如果你具备了作为 Mentor 的能力并有意向尝试 Mentor 的角色，在 PingCAP，都有机会实现。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;我们一直以来的理念是希望每个 PingCAP 的小伙伴都先得到个人成长，再反哺给团队和公司，每一个小伙伴都能参与到公司发展的过程中来。我们完全不担心「把你锻炼出来，却被其他公司高价挖走了」这类事情。且不说我们的薪酬本身就很有竞争力，更重要的是，我们相信一旦你喜欢上我们的理念和工作模式，你是不会舍得离开的～&lt;/p&gt;&lt;h2&gt;&lt;b&gt;加入我们吧！&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们认为优秀的工程师或多或少有以下共同特质：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;A Quick Learner&lt;/li&gt;&lt;li&gt;An Earnest Curiosity&lt;/li&gt;&lt;li&gt;Faith in Open Source&lt;/li&gt;&lt;li&gt;Self-driven    &lt;/li&gt;&lt;li&gt;Get Things Done&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;如果你符合以上特质，欢迎进入招聘页面查看目前开放的工作机会：&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/recruit-cn/join/%23positions&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;虚位以待&lt;/a&gt;&lt;p&gt;&lt;b&gt;简历投递通道：hire@pingcap.com&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;实习生&lt;/b&gt;：公司的各项福利和学习资源对实习生全面开放，更重要的是实习生还未毕业就有机会接触工业级项目，而且实习期间表现优异者将有机会获得校招绿色通道特权。如果小伙伴们时间不够充裕，也可以先从社区 Contributor 做起，或许下一期 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487451%26idx%3D1%26sn%3D53b17f49e05af2cd832192fd67eaf75f%26chksm%3Deb1628b1dc61a1a704544f250cee480e7f6cf95cf5e84c2ccb7f0858398e685160122ed687b6%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Talent Plan&lt;/a&gt;&lt;/u&gt; 的主角就是你！&lt;/p&gt;&lt;p&gt;&lt;b&gt;伯乐推荐&lt;/b&gt;：如果你身边有符合以上要求的小伙伴，也可以找我们聊一聊，推荐成功就有机会获得伯乐推荐奖励（iPad、iPhone、MacBook Pro 等等）。伯乐推荐邮件格式：[伯乐推荐] 候选人姓名-职位名称-推荐人姓名-推荐人手机号。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-03-01-58058910</guid>
<pubDate>Fri, 01 Mar 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>新技术到底靠不靠谱？在中国用一下就知道了</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-03-01-58054995.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/58054995&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f75076bf8b4d44532150d93aedc45120_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;本文转载自公众号「AI前线」。&lt;/p&gt;&lt;p&gt;策划编辑｜Natalie&lt;/p&gt;&lt;p&gt;作者｜Kevin Xu&lt;/p&gt;&lt;p&gt;译者｜无明&lt;/p&gt;&lt;p&gt;编辑｜Debra&lt;/p&gt;&lt;blockquote&gt;AI 前线导读：中国科技公司是典型的早期采用者——不是因为赶时髦，而是确实有必要这么做。“中国式规模”让中国的互联网经济成为了高质量软件（特别是基础设施软件）工程的成长沃土，这在开源技术上得到了充分体现。国内开发者和企业向各大开源基金会贡献了越来越多的开源项目，而我们对国外的开源项目也产生了越来越大的影响。本文来自 PingCAP 全球战略和运营总经理 Kevin Xu，AI 前线经授权翻译。&lt;/blockquote&gt;&lt;p&gt;我的 87 岁的祖母住在沈阳郊区的一所老房子里。虽然她年岁已高，但却很有技术悟性。平常她会用三个 App 进行网购：在京东上买书，在拼多多上买水果，在淘宝上买其他东西（衬衫、围巾、洗涤剂、数独板）。&lt;/p&gt;&lt;p&gt;这三个 App 刚好是由中国电商市场的三巨头开发的，其规模远远超出了千禧一代（1980 至 1994 年出生的人群）和 Z 世代（1995 至 2009 年出生的人群）的受众总和。&lt;/p&gt;&lt;p&gt;正是这种“中国式规模”让中国的互联网经济成为高质量软件（特别是基础设施软件）工程的成长沃土。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;购物节狂欢&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;电子商务是中国互联网经济增长最快的垂直领域之一，同时也带动了数字支付和物流配送的发展。基础设施技术在这一领域经受了最为残酷的考验。“双十一”是最为典型的案例，这是由阿里巴巴提出的一个网购节日，每年的 11 月 11 日，淘宝和天猫都会如期庆祝这个节日。2017 年双十一总销售额为 253 亿美元，2018 年增长到了 308 亿美元。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;668&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;668&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-b177f01c4d03167be8a31675213afae2_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;中国第二大电子商务平台京东也有自己的年中购物节，即“618”，这是一个为期 18 天的促销活动，截止 6 月 18 日，这天刚好是京东的成立纪念日。2017 年，618 的总销售额为 176 亿美元，2018 年增长到了 284 亿美元。&lt;/p&gt;&lt;p&gt;美国亚马逊的年中购物节 Prime Day 在 2018 年和 2017 年分别创造了 41.9 亿美元和 24.1 亿美元的销售额。美国感恩节购物季在 2018 年和 2017 年的销售额分别为 178 亿美元和 196.2 亿美元。&lt;/p&gt;&lt;p&gt;对于工程师来说，有趣的不是令人瞠目结舌的销售数据，而是如何构建可以应对这些工作负载的基础设施。2017 年，阿里巴巴公布了双十一期间系统的高峰吞吐量：每秒 25.6 万笔交易和每秒 4200 万次查询。&lt;/p&gt;&lt;p&gt;不难想象，在这些促销活动期间，肯定会不可避免地出现大量的事务、查询、数据一致性问题、实时分析容量和其他难以想象的边缘情况。&lt;/p&gt;&lt;p&gt;除了这些公司，所有其他想要搭上这些促销活动顺风车的电子商务公司、所有为用户在线购物提供电子支付解决方案的银行，以及所有的物流中心和仓储中心——他们都需要有好的基础设施技术来应对新的工作负载和流量增长。&lt;/p&gt;&lt;p&gt;由于这种增长速度，以及由此产生的竞争压力，中国科技公司在采用新技术方面具有相当强的风险承受能力。&lt;/p&gt;&lt;p&gt;一家公司找到合适的产品市场，然后在不到两个月的时间内采用未经证实但很有前景的新技术为高速增长的流量提供服务，这种事情并非闻所未闻。京东在 2016 年初开始采用 Kubernetes，当时离谷歌开源 Kubernetes 还不到一年的时间，因为他们必须解决可伸缩性问题，而 OpenStack 没能帮他们实现这一目标。(京东现在拥有全球最大的 Kubernetes 集群，运行在 2 万台裸机上)&lt;/p&gt;&lt;h2&gt;&lt;b&gt;更大的规模，更大的责任&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;中国科技公司是典型的早期采用者——不是因为赶时髦，而是确实有必要这么做。中国拥有世界上最多的互联网用户（8 亿，并且还在增加当中），他们的规模（以及规模所带来的不可预知的行为）足够大，大到足以促使这些科技公司认真对待每一项技术。从这些公司生存下来的技术会变得更强大、更有弹性，也更值得被用在其他地方。&lt;/p&gt;&lt;p&gt;很多行为是不可能在构建模式下进行预测或测试的。&lt;/p&gt;&lt;p&gt;你该如何通过 Paxos 或 Raft 来模拟系统达到 100 倍查询峰值时的网络流量？当一件商品、一首歌或一段视频突然变得像病毒一样迅速传播，而所有用户都在试图访问它们，而更糟糕的是，有价值的广告收入取决于系统不能崩溃，在这种情况下，你该如何处理数据热点问题？当数据增长率为每天数 TB 时，应该如何扩展存储容量?&lt;/p&gt;&lt;p&gt;所有这些情况，在很多中国科技公司中时有发生。他们正在迅速地寻找新的解决方案，以迎接这些挑战——这为考验这些创新技术提供了一片沃土。&lt;/p&gt;&lt;p&gt;“中国式规模”已经催生了一些由中国原创的基础设施技术。去年，云原生计算基金会（CNCF）接受了其中的三个项目：Harbor、TiKV 和 DragonFly。它们的架构和用例都在之前的一篇文章（&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//softwareengineeringdaily.com/2018/12/09/chinese-open-source-software/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;softwareengineeringdaily.com&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/2018/12/09/chinese-open-source-software/&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）中做了很好的介绍。在 CNCF 生态系统之外，还有其他一些值得关注的项目。&lt;/p&gt;&lt;p&gt;&lt;b&gt;OceanBase&lt;/b&gt;&lt;/p&gt;&lt;p&gt;由蚂蚁金服开发的分布式关系数据库，最初用于支持支付宝。支付宝在中国已经无处不在。此后，OceanBase 逐渐成为阿里巴巴所有关键电子商务平台（如淘宝和天猫）的核心交易数据库。它也是一个独立的产品，南京银行就是它的用户之一。&lt;/p&gt;&lt;p&gt;2014 年以来，它经历了五次双十一的考验。可惜的是，它是一个闭源产品，在中国以外没有得到广泛采用，所以与其架构、设计或工程方面相关的英文信息并不多。&lt;/p&gt;&lt;p&gt;&lt;b&gt;TiDB&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一个开源的、兼容 MySQL 的 NewSQL 分布式数据库，由 PingCAP 于 2015 年创建。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;570&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1024&quot; data-original=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;570&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1024&quot; data-original=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-794772bb7ca5bf70b2ca91b5beac5c85_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;它采用了分层架构，SQL 处理层（左边的 TiDB 集群）和可水平伸缩的存储层（中间的 TiKV 集群）被分隔开来。（注：TiKV 也由 PingCAP 发起，但现在由 CNCF 托管）。这个设计灵感来自于谷歌的 Spanner 和基于 Spanner 构建的 F1 项目。PD（Placement Driver）集群保存元数据，提供一些负载均衡支持，并提供时间戳（作为系统事务模型的一部分）。TiSpark 集群是一个可选组件，用户可以直接基于保存在 TiKV 中的数据运行 Spark 作业。&lt;/p&gt;&lt;p&gt;目前，中国已经有几百家公司在生产环境中部署了 TiDB，如摩拜、北京银行和爱奇艺。国外也有一些大型互联网公司使用了 TiDB，如 Shopee 和 BookMyShow。&lt;/p&gt;&lt;p&gt;注：PingCAP 现已提供 TiDB 的企业版和云服务，同时也在维护开源社区版本。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Apache Kylin&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一个快速的 OLAP（在线分析处理）引擎，最初由 eBay 中国团队开发，在 2014 年贡献给 Apache 基金会，并在 2015 年底成为顶级项目。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;509&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1024&quot; data-original=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;509&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1024&quot; data-original=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-3dac063ff9f69d8d98e1eb42e48b5e65_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;Kylin 主要被用在 Hadoop 生态系统中，为数百亿行数据的分析查询带来可观的速度提升。用户先定义数据模型，然后利用 Hadoop 的分布式特性并行运行多个 MapReduce 作业，用以预构建必要的多维模型（也称为“MOLAP”）。最后，Kylin 将预先计算的模型存储在 HBase 中，供用户查询。它还使用 Zookeeper 来协调和管理这个过程的不同部分。&lt;/p&gt;&lt;p&gt;作为大数据分析引擎，Kylin 集成了 Tableau、MicroStrategy、Excel 等流行的 BI 工具。它还提供了一个 RESTful API，方便与第三方应用程序连接。除了 eBay，它还在 OPPO、百度、中国太平洋保险等公司经受过实战考验，三星和摩根大通也是它的用户。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Apache Skywalking&lt;/b&gt;&lt;/p&gt;&lt;p&gt;一个相对较新的开源应用程序性能监监控（APM）工具，用于在基于容器的环境中监控微服务。2017 年底，它成为 Apache 基金会的孵化器项目。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;507&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1024&quot; data-original=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1024&quot; data-rawheight=&quot;507&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1024&quot; data-original=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-0e9001d334f5c8f7ec878c7403f5f3f7_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;Skywalking 通过服务网格从微服务中提取指标，并利用 Jaeger 等流行工具来跟踪信息，并可以查询和分析这些指标和信息，还可以使用团队开发的 UI 进行可视化。它还提供了一个可插拔的存储接口，借助这个接口，可以将信息保存在一些流行的数据库中，比如 Elasticsearch、MySQL 和 TiDB。&lt;/p&gt;&lt;p&gt;尽管这个项目成立还不到两年，但中国的一些大型公司已经在使用它，如华为、小米和贝壳。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;中国之外&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;除了本土技术，国外的一些技术也有了“中国式规模”的味道。京东是 Prometheus、Vitesse、Jenkins 和 GitLab 等技术的用户，百度是 CockroachDB（另一个受 Spanner 启发的开源数据库，类似于 TiDB）的用户。Alluxio，一个分布式文件系统统一层，可以以内存速度运行（源自加州大学伯克利分校 AMPLab 的一个名为 Tachyon 的研究项目），也在百度、中国联通和滴滴出行等企业中得到采用。&lt;/p&gt;&lt;p&gt;中国公司不仅在大规模采用这些技术，有时候甚至直接收购它们。开源数据流平台 Apache Flink 由柏林技术大学于 2009 年创建，作为 Stratosphere 研究项目的一部分。阿里巴巴最终收购了由 Flink 创始人创办的 dataArtisan（该公司的目的是商业化 Flink）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;有价值的权衡?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;作为工程师，我们知道没有什么技术是绝对的，它们总是存在权衡。我们总是在吞吐量和延迟、数据一致性和响应时间、新特性和系统稳定性之间做出权衡。我们很少能鱼与熊掌兼得，我们也不相信把自己标榜得太高的技术。&lt;/p&gt;&lt;p&gt;市场的选择也是如此。在中国互联网经济大环境中，有一些问题一定要考虑到，特别是信息审查方面的问题。比如，对侵犯知识产权行为的法律追索仍然不太可靠，有关企业使用个人数据的监管尚处于初级阶段。&lt;/p&gt;&lt;p&gt;但如果你是一名开发者，正在寻找一些稳定可靠的技术（已经“面面俱到”的技术），那么那些已经在中国互联网环境中经受过实战考验的技术将是安全的选择。&lt;/p&gt;&lt;p&gt;如果你的团队正在构建下一个大项目，尤其是在基础设施层面，那么把这个项目交给中国的几家科技巨头公司，将会为项目带来跨越式的发展。&lt;/p&gt;&lt;p&gt;另外，你们的努力很可能也会为我的祖母带来更快乐的生活！&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;英文原文：&lt;/b&gt;&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//softwareengineeringdaily.com/2019/02/26/china-scale-the-new-sandbox-to-battle-test-innovative-technology/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic3.zhimg.com/v2-6025e530680d1bab1e937a0be1b9bb8e_180x120.jpg&quot; data-image-width=&quot;940&quot; data-image-height=&quot;470&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;China Scale: the New Sandbox to Battle-Test Innovative Technology&lt;/a&gt;&lt;p&gt;&lt;b&gt;作者介绍&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Kevin Xu 是 PingCAP 全球战略和运营总经理。他在斯坦福大学完成计算机科学与法律专业的学习。主要关注分布式系统、云原生技术、自然语言处理和开源。&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-03-01-58054995</guid>
<pubDate>Fri, 01 Mar 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>优秀的数据工程师，怎么用 Spark 在 TiDB 上做 OLAP 分析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-27-57855988.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57855988&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-903c0b831be84e0240629b5b88f04e6f_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;作者：RickyHuo&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;本文转载自公众号「大道至简bigdata」。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;原文链接：&lt;/b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/OijMYyM-7F2gbvURsfJskw&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;优秀的数据工程师，怎么用Spark在TiDB上做OLAP分析&lt;/a&gt;&lt;/p&gt;&lt;blockquote&gt;TiDB 是一款定位于在线事务处理/在线分析处理的融合型数据库产品，实现了一键水平伸缩，强一致性的多副本数据安全，分布式事务，实时 OLAP 等重要特性。&lt;br&gt;TiSpark 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品。它借助 Spark 平台，同时融合 TiKV 分布式集群的优势。直接使用 TiSpark 完成 OLAP 操作需要了解 Spark，还需要一些开发工作。&lt;b&gt;那么，有没有一些开箱即用的工具能帮我们更快速地使用 TiSpark 在 TiDB 上完成 OLAP 分析呢？&lt;/b&gt;&lt;br&gt;&lt;b&gt;目前开源社区上有一款工具 Waterdrop，可以基于 Spark，在 TiSpark 的基础上快速实现 TiDB 数据读取和 OLAP 分析。项目地址：&lt;/b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/InterestingLab/waterdrop&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/InterestingL&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ab/waterdrop&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;591&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;591&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-6718d267638bfb3b2adfff6a417cac52_b.jpg&quot;&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;使用 Waterdrop 操作 TiDB&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在我们线上有这么一个需求，从 TiDB 中读取某一天的网站访问数据，统计每个域名以及服务返回状态码的访问次数，最后将统计结果写入 TiDB 另外一个表中。 我们来看看 Waterdrop 是如何实现这么一个功能的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Waterdrop&lt;/b&gt;&lt;/p&gt;&lt;p&gt;Waterdrop 是一个非常易用，高性能，能够应对海量数据的实时数据处理产品，它构建在 Spark 之上。Waterdrop 拥有着非常丰富的插件，支持从 TiDB、Kafka、HDFS、Kudu 中读取数据，进行各种各样的数据处理，然后将结果写入 TiDB、ClickHouse、Elasticsearch 或者 Kafka 中。&lt;/p&gt;&lt;p&gt;&lt;b&gt;准备工作&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. TiDB 表结构介绍&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Input（存储访问日志的表）&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;CREATE TABLE access_log (
    domain VARCHAR(255),
    datetime VARCHAR(63),
    remote_addr VARCHAR(63),
    http_ver VARCHAR(15),
    body_bytes_send INT,
    status INT,
    request_time FLOAT,
    url TEXT
)
+-----------------+--------------+------+------+---------+-------+
| Field           | Type         | Null | Key  | Default | Extra |
+-----------------+--------------+------+------+---------+-------+
| domain          | varchar(255) | YES  |      | NULL    |       |
| datetime        | varchar(63)  | YES  |      | NULL    |       |
| remote_addr     | varchar(63)  | YES  |      | NULL    |       |
| http_ver        | varchar(15)  | YES  |      | NULL    |       |
| body_bytes_send | int(11)      | YES  |      | NULL    |       |
| status          | int(11)      | YES  |      | NULL    |       |
| request_time    | float        | YES  |      | NULL    |       |
| url             | text         | YES  |      | NULL    |       |
+-----------------+--------------+------+------+---------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;Output（存储结果数据的表）&lt;/li&gt;&lt;/ul&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;CREATE TABLE access_collect (
    date VARCHAR(23),
    domain VARCHAR(63),
    status INT,
    hit INT
)
+--------+-------------+------+------+---------+-------+
| Field  | Type        | Null | Key  | Default | Extra |
+--------+-------------+------+------+---------+-------+
| date   | varchar(23) | YES  |      | NULL    |       |
| domain | varchar(63) | YES  |      | NULL    |       |
| status | int(11)     | YES  |      | NULL    |       |
| hit    | int(11)     | YES  |      | NULL    |       |
+--------+-------------+------+------+---------+-------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;2. 安装 Waterdrop&lt;/b&gt;&lt;/p&gt;&lt;p&gt;有了 TiDB 输入和输出表之后， 我们需要安装 Waterdrop，安装十分简单，无需配置系统环境变量&lt;/p&gt;&lt;p&gt;1) 准备 Spark 环境&lt;/p&gt;&lt;p&gt;2) 安装 Waterdrop&lt;/p&gt;&lt;p&gt;3) 配置 Waterdrop&lt;/p&gt;&lt;p&gt;以下是简易步骤，具体安装可以参照 Quick Start。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;# 下载安装Spark
cd /usr/local
wget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz
tar -xvf https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz
wget
# 下载安装Waterdrop
https://github.com/InterestingLab/waterdrop/releases/download/v1.2.0/waterdrop-1.2.0.zip
unzip waterdrop-1.2.0.zip
cd waterdrop-1.2.0

vim config/waterdrop-env.sh
# 指定Spark安装路径
SPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.1.0-bin-hadoop2.7}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;实现 Waterdrop 处理流程&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们仅需要编写一个 Waterdrop 配置文件即可完成数据的读取、处理、写入。&lt;/p&gt;&lt;p&gt;Waterdrop 配置文件由四个部分组成，分别是 &lt;code&gt;Spark&lt;/code&gt;、&lt;code&gt;Input&lt;/code&gt;、&lt;code&gt;Filter&lt;/code&gt; 和 &lt;code&gt;Output&lt;/code&gt;。&lt;code&gt;Input&lt;/code&gt; 部分用于指定数据的输入源，&lt;code&gt;Filter&lt;/code&gt; 部分用于定义各种各样的数据处理、聚合，&lt;code&gt;Output&lt;/code&gt; 部分负责将处理之后的数据写入指定的数据库或者消息队列。&lt;/p&gt;&lt;p&gt;整个处理流程为 &lt;code&gt;Input&lt;/code&gt; -&amp;gt; &lt;code&gt;Filter&lt;/code&gt; -&amp;gt; &lt;code&gt;Output&lt;/code&gt;，整个流程组成了 Waterdrop 的处理流程（Pipeline）。&lt;/p&gt;&lt;blockquote&gt;以下是一个具体配置，此配置来源于线上实际应用，但是为了演示有所简化。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;Input (TiDB)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这里部分配置定义输入源，如下是从 TiDB 一张表中读取数据。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;input {
    tidb {
        database = &quot;nginx&quot;
        pre_sql = &quot;select * from nginx.access_log&quot;
        table_name = &quot;spark_nginx_input&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;Filter&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;在 Filter 部分，这里我们配置一系列的转化, 大部分数据分析的需求，都是在 Filter 完成的。Waterdrop 提供了丰富的插件，足以满足各种数据分析需求。这里我们通过 SQL 插件完成数据的聚合操作。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;filter {
    sql {
        table_name = &quot;spark_nginx_log&quot;
        sql = &quot;select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=&#39;2019-01-20&#39; group by domain, status, substring(datetime, 1, 10)&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;Output (TiDB)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;最后， 我们将处理后的结果写入 TiDB 另外一张表中。TiDB Output 是通过 JDBC 实现的。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;output {
    tidb {
        url = &quot;jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&amp;amp;characterEncoding=utf8&quot;
        table = &quot;access_collect&quot;
        user = &quot;username&quot;
        password = &quot;password&quot;
        save_mode = &quot;append&quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&lt;li&gt;Spark&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这一部分是 Spark 的相关配置，主要配置 Spark 执行时所需的资源大小以及其他 Spark 配置。&lt;br&gt;我们的 TiDB Input 插件是基于 TiSpark 实现的，而 TiSpark 依赖于 TiKV 集群和 Placement Driver (PD)。因此我们需要指定 PD 节点信息以及 TiSpark 相关配置&lt;code&gt;spark.tispark.pd.addresses&lt;/code&gt;和&lt;code&gt;spark.sql.extensions&lt;/code&gt;。&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;spark {
  spark.app.name = &quot;Waterdrop-tidb&quot;
  spark.executor.instances = 2
  spark.executor.cores = 1
  spark.executor.memory = &quot;1g&quot;
  # Set for TiSpark
  spark.tispark.pd.addresses = &quot;localhost:2379&quot;
  spark.sql.extensions = &quot;org.apache.spark.sql.TiExtensions&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;运行 Waterdrop&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们将上述四部分配置组合成我们最终的配置文件&lt;code&gt;conf/tidb.conf&lt;/code&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;spark {
    spark.app.name = &quot;Waterdrop-tidb&quot;
    spark.executor.instances = 2
    spark.executor.cores = 1
    spark.executor.memory = &quot;1g&quot;
    # Set for TiSpark
    spark.tispark.pd.addresses = &quot;localhost:2379&quot;
    spark.sql.extensions = &quot;org.apache.spark.sql.TiExtensions&quot;
}
input {
    tidb {
        database = &quot;nginx&quot;
        pre_sql = &quot;select * from nginx.access_log&quot;
        table_name = &quot;spark_table&quot;
    }
}
filter {
    sql {
        table_name = &quot;spark_nginx_log&quot;
        sql = &quot;select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=&#39;2019-01-20&#39; group by domain, status, substring(datetime, 1, 10)&quot;
    }
}
output {
    tidb {
        url = &quot;jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&amp;amp;characterEncoding=utf8&quot;
        table = &quot;access_collect&quot;
        user = &quot;username&quot;
        password = &quot;password&quot;
        save_mode = &quot;append&quot;
    }
} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;执行命令，指定配置文件，运行 Waterdrop ，即可实现我们的数据处理逻辑。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Local&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;./bin/start-waterdrop.sh --config config/tidb.conf --deploy-mode client --master &#39;local[2]&#39;&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;yarn-client&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;./bin/start-waterdrop.sh --config config/tidb.conf --deploy-mode client --master yarn&lt;/code&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;yarn-cluster&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;code&gt;./bin/start-waterdrop.sh --config config/tidb.conf --deploy-mode cluster -master yarn&lt;/code&gt;&lt;/p&gt;&lt;p&gt;如果是本机测试验证逻辑，用本地模式（Local）就可以了，一般生产环境下，都是使用&lt;code&gt;yarn-client&lt;/code&gt;或者&lt;code&gt;yarn-cluster&lt;/code&gt;模式。&lt;/p&gt;&lt;p&gt;&lt;b&gt;检查结果&lt;/b&gt;&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;mysql&amp;gt; select * from access_collect;
+------------+--------+--------+------+
| date       | domain | status | hit  |
+------------+--------+--------+------+
| 2019-01-20 | b.com  |    200 |   63 |
| 2019-01-20 | a.com  |    200 |   85 |
+------------+--------+--------+------+
2 rows in set (0.21 sec)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在这篇文章中，我们介绍了如何使用 Waterdrop 从 TiDB 中读取数据，做简单的数据处理之后写入 TiDB 另外一个表中。仅通过一个配置文件便可快速完成数据的导入，无需编写任何代码。&lt;/p&gt;&lt;p&gt;除了支持 TiDB 数据源之外，Waterdrop 同样支持 Elasticsearch，Kafka，Kudu， ClickHouse 等数据源。&lt;/p&gt;&lt;p&gt;&lt;b&gt;与此同时，我们正在研发一个重要功能，就是在 Waterdrop 中，利用 TiDB 的事务特性，实现从 Kafka 到 TiDB 流式数据处理，并且支持端（Kafka）到端（TiDB）的 Exactly-Once 数据一致性。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;希望了解 Waterdrop 和 TiDB，ClickHouse、Elasticsearch、Kafka 结合使用的更多功能和案例，可以直接进入项目主页：&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/InterestingLab/waterdrop&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/InterestingL&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ab/waterdrop&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt; ，或者联系项目负责人： Garyelephan（微信: garyelephant）、RickyHuo （微信: chodomatte1994）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-27-57855988</guid>
<pubDate>Wed, 27 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>The Way to TiDB 3.0 and Beyond (下篇)</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-26-57749943.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57749943&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-34de81f41133749c1021207829a0a288_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;本文为我司 Engineering VP 申砾在 TiDB DevCon 2019 上的演讲实录。在 &lt;u&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57693856&quot; class=&quot;internal&quot;&gt;上篇&lt;/a&gt;&lt;/u&gt; 中，申砾老师重点回顾了 TiDB 2.1 的特性，并分享了我们对「如何做好一个数据库」的看法。&lt;br&gt;本篇将继续介绍 TiDB 3.0 Beta 在稳定性、易用性、功能性上的提升，以及接下来在 Storage Layer 和 SQL Layer 的规划，enjoy~&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;593&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;593&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f9c74ae0ce4ba42ae28981d4c6b0df25_b.jpg&quot;&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;TiDB 3.0 Beta&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;2018 年年底我们开了一次用户吐槽大会，当时我们请了三个 TiDB 的重度用户，都是在生产环境有 10 套以上 TiDB 集群的用户。那次大会规则是大家不能讲 TiDB 的优点，只能讲缺点；研发同学要直面问题，不能辩解，直接提解决方案；当然我们也保护用户的安全（开个玩笑 :D），让他们放心的来吐槽。刚刚的社区实践分享也有点像吐槽大会第二季，我们也希望用户来提问题，分享他们在使用过程遇到什么坑，&lt;b&gt;因为只有直面这些问题，才有可能改进&lt;/b&gt;。所以我们在 TiDB 3.0  Beta 中有了很多改进，当然还有一些会在后续版本中去改进。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Stability at Scale&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 3.0 版本第一个目标就是「更稳定」，特别是在大规模集群、高负载的情况下保持稳定。稳定性压倒一切，如果你不稳定，用户担惊受怕，业务时断时续，后面的功能都是没有用的。所以我们希望「先把事情做对，再做快」。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 Multi-thread RaftStore&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首先来看 TiDB 3.0 一个比较亮眼的功能——多线程 Raft。我来给大家详细解释一下，为什么要做这个事情，为什么我们以前不做这个事情。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-a69f9675cd76e82972e912ad49bb47ab_b.jpg&quot;&gt;&lt;figcaption&gt;图 8 TiKV 抽象架构图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这是 TiKV 一个抽象的架构（图 8）。中间标红的图形是 RaftStore 模块，所有的 Raft Group 都在一个 TiKV 实例上，所有 Raft 状态机的驱动都是由一个叫做 RaftStore 的线程来做的，这个线程会驱动 Raft 状态机，并且将 Raft Log Append 到磁盘上，剩下的包括发消息给其他 TiKV 节点以及 Apply Raft Log 到状态机里面，都是由其他线程来做的。早期的时候，可能用户的数据量没那么大，或者吞吐表现不大的时候，其实是感知不到的。但是当吞吐量或者数据量大到一定程度，就会感觉到这里其实是一个瓶颈。虽然这个线程做的事情已经足够简单，但是因为 TiKV 上所有的 Raft Peer 都会通过一个线程来驱动自己的 Raft 状态机，所以当压力足够大的时候就会成为瓶颈。用户会看到整个 TiKV 的 CPU 并没有用满，但是为什么吞吐打不上去了？&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-77d07947c8c853d0c742042221aea3b4_b.jpg&quot;&gt;&lt;figcaption&gt;图 9 TiDB 3.0 Multi-thread RaftStore&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;因此在 TiDB 3.0 中做了一个比较大的改进，就是将 RaftStore 这个线程，由一个线程变成一个线程池， TiKV 上所有 Raft Peer 的 Raft 状态机驱动都由线程池来做，这样就能够充分利用 CPU，充分利用多核，在 Region 特别多以及写入量特别大的时候，依然能线性的提升吞吐。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;426&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;426&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-f881116575d454716095bd823634972e_b.jpg&quot;&gt;&lt;figcaption&gt;图 10 TiDB 3.0 Beta oltp_insert&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;通过上图大家可以看到，随着并发不断加大，写入是能够去线性扩展的。在早期版本中，并发到一定程度的时候，RaftStore 也会成为瓶颈，那么为什么我们之前没有做这个事情？这个优化效果这么明显，之所以之前没有做，是因为之前 Raft 这块很多时候不会成为瓶颈，而在其他地方会成为瓶颈，比如说 RocksDB 的写入或者 gRPC 可能会成为瓶颈，然后我们将 RaftStore 中的功能不断的向外拆，拆到其他线程中，或者是其他线程里面做多线程，做异步等等，随着我们的优化不断深入，用户场景下的数据量、吞吐量不断加大，我们发现 RaftStore 线程已经成为需要优化的一个点，所以我们在 3.0 中做了这个事情。而且之前保持单线程也是因为单线程简单，「先把事情做对，然后再做快」。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2 Batch Message&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第二个改进是 Batch Message。我们的组件之间通讯选择了 gRPC，首先是因为 gRPC 是 Google 出品，有人在维护他，第二是用起来很简单，也有很多功能（如流控、加密）可以用。但其实很多人吐嘈它性能比较慢，在知乎上大家也能看到各种问题，包括讨论怎么去优化他，很多人也有各种优化经验，我们也一直想怎么去优化他。以前我们用的方法是来一个 message 就通过 gRPC 发出去，虽然性能可能没有那么好，或者说性能不是他最大的亮点，但有时候调性能不能单从一个模块去考虑，应该从架构上去想，就是架构需要为性能而设计，架构上的改进往往能带来性能的质变。&lt;/p&gt;&lt;p&gt;所以我们在 TiDB 3.0 Beta 中设计了 Batch Message 。以前是一个一个消息的发，现在是按照消息的目标分队列，每个队列会有一个 Timer，当消息凑到一定个数，或者是你的 Timer 到了时间（现在应该设置的是 1ms，Batch 和这个 Timer 数量都可以调），才会将发给同一个目的地的一组消息，打成一个包，一起发过去。有了这个架构上的调整之后，我们就获得了性能上的提升。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-98d8813341bf6e5986e32c1aedd47749_b.jpg&quot;&gt;&lt;figcaption&gt;图 11 TiDB 3.0 Beta - Batch Message&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当然大家会想，会不会在并发比较低的时候变慢了？因为你凑不到足够的消息，那你就要等 Timer。其实是不会的，我们也做了一些设计，就是由对端先汇报「我当前是否忙」，如果对端不忙，那么选择一条一条的发，如果对端忙，那就可以一个 Batch 一个 Batch 的发，这是一个自适应的 Batch Message 的一套系统。图 11 右半部分是一个性能对比图，有了 Batch Message 之后，在高并发情况下吞吐提升非常快，在低并发情况下性能并没有下降。相信这个改进可以给大家带来很大的好处。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3 Titan&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第三点改进就是 Titan。CEO 刘奇在 Opening Keynote 中提到了我们新一代存储引擎 Titan，我们计划用 Titan 替换掉 RocksDB，TiDB 3.0 中已经内置了 Titan，但没有默认打开，如果大家想体验的话，可以通过配置文件去把 RocksDB 改成 Titan。我们为什么想改进 RocksDB 呢？是因为它在存储大的 Key Value 的时候，有存储空间放大和写放大严重的问题。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-d46d0ea52d5649642fe1f9f208423474_b.jpg&quot;&gt;&lt;figcaption&gt;图 12 TiDB 3.0 中内置的新存储引擎 Titan&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以我们尝试解决这个问题。当你写入的 Key Value 比较大的时候，我们会做一个检查，然后把大的 Value 放到一个 Blob File 里去，而不是放到 LSM-Tree。这样的分开存储会让 LSM-Tree 变得很小，避免了因为 LSM-Tree 比较高的时候，特别是数据量比较大时出现的比较严重的写放大问题。有了 Titan 之后，就可以解决「单个 TiKV  服务大量数据」的需求，因为之前建议 TiKV 一个实例不要高于 1T。我们后面计划单个 TiKV 实例能够支持  2T 甚至 4T 数据，让大家能够节省存储成本，并且能在 Key Value 比较大的时候，依然能获得比较好的性能。&lt;/p&gt;&lt;p&gt;除了解决写放大问题之外，其实还有一个好处就是我们可以加一个新的 API，比如 KeyExist，用来检查 Key 是否存在，因为这时 Key 和 Value 是分开存储的，我们只需要检查 Key 是否在，不需要把 Value Load 进去。或者做 Unique Key 检查时，可以不需要把 Key Value 取出来，只需要加个接口，看这个 Key 是否存在就好了，这样能够很好的提升性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.4 Robust Access Path Selection&lt;/b&gt;&lt;/p&gt;&lt;p&gt;第四点是保持查询计划稳定。这个在数据库领域其实是一个非常难的问题，我们依然没有 100% 解决这个问题，希望在 2019 年第一季度，最多到第二季度，能有一个非常好的解决方案。我们不希望当数据量变化 、写入变化、负载变化，查询计划突然变错，这个问题在线上使用过程中是灾难。那么为什么会跑着跑着变错？首先来说我们现在是一个 Cost-based optimizers，我们会参考统计信息和当前的数据的分布，来选择后面的 plan。那么数据的分布是如何获得的呢？我们是通过统计信息，比如直方图、CM Sketch来获取，这里就会出现两个问题：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;统计信息可能是不准的。统计信息毕竟是一个采样，不是全量数据，会有一些数据压缩，也会有精度上的损失。&lt;/li&gt;&lt;li&gt;随着数据不断写入，统计信息可能会落后。因为我们很难 100% 保证统计信息和数据是 Match 的。&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-15fe64f44c40a6b16af261a70c084fbe_b.jpg&quot;&gt;&lt;figcaption&gt;图 13 查询计划稳定性解决方案&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;一个非常通用的思路是， 除了依赖于 Cost Model 之外，我们还要依赖更多的 Hint，依赖于更多启发式规则去做 Access Path 裁减。举个例子：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;select * from t where a = x and b = y;
idx1(a, b)
idx2(b) -- pruned
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;大家通过直观印象来看，我们一定会选择第一个索引，而不是第二个索引，那么我们就可以把第二个索引裁掉，而不是因为统计信息落后了，然后估算出第二个索引的代价比较低，然后选择第二个索引。上面就是我们最近在做的一个事情，这里只举了一个简单的例子。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Usability&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 3.0 第二个目标是可用性，是让 TiDB 简单易用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 Query Tracing&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在 TiDB 2.0 中，大家看一个 Query 为什么慢了依赖的是 Explain，就是看查询计划，其实那个时候大家很多都看不懂，有时候看了也不知道哪有问题。后来我们在 TiDB 2.1 中支持了 Explain Analyze，这是从 PG  借鉴过来一个特性，就是我们真正的把它执行一边，然后再看看每个算子的耗时、处理的数据量，看看它到底干了一些什么事情，但其实可能还不够细，因为还没有细化到算子内部的各种操作的耗时。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4093740170e6b2eede7ab958e5c8c01b_b.jpg&quot;&gt;&lt;figcaption&gt;图 14 TiDB 3.0 - Query Tracing&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以我们又做了一个叫 Query Tracing 的东西，其实在 TiDB 2.1 之前我们已经做了一部分，在 TiDB  3.0 Beta 中做了一个收尾，就是我们可以将 Explain 结果转成一种 Tracing 格式，再通过图形化界面，把这个 Tracing 的内容展示出来，就可以看到这个算子具体干了一些什么事，每一步的消耗到底在哪里，这样就可以知道哪里有问题了。希望大家都能在 TiDB 3.0 的版本中非常直观的定位到 Query 慢的原因。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.2 Plan Management&lt;/b&gt;&lt;/p&gt;&lt;p&gt;然后第二点 Plan Management 其实也是为了 Plan 不稳定这个问题做准备的。虽然我们希望数据库能自己 100% 把 Plan 选对，但是这个是非常美好的愿望，应该还没有任何一个数据库能保证自己能 100% 的解决这个问题。那么在以前的版本中，出现问题怎么办？一种是去 Analyze 一下，很多情况下他会变好，或者说你打开自动 Analyze 这个特性，或者自动 FeedBack 这个特性，可以一定程度上变好，但是还可能过一阵统计信息又落后了，又不准了，Plan 又错了，或者由于现在 cost 模型的问题，有一些 Corner Case 处理不到，导致即使统计信息是准确的， Plan 也选不对。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-245da57e4f222d5d88dffe834e19b904_b.jpg&quot;&gt;&lt;figcaption&gt;图 15  TiDB 3.0 Beta - Plan Management&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;那么我们就需要一个兜底方案，让大家遇到这个问题时不要束手无策。一种方法是让业务去改 SQL，去加 Hint，也是可以解决的，但是跟业务去沟通可能会增加他们的使用成本或者反馈周期很长，也有可能业务本身也不愿意做这个事情。&lt;/p&gt;&lt;p&gt;另外一种是用一种在线的方式，让数据库的使用者 DBA 也能非常简单给这个 Plan 加 Hint。具体怎么做呢？我们和美团的同学一起做了一个非常好的特性叫 Plan Management，就是我们有一个 Plan 管理的模块，我们可以通过 SQL 接口给某一条 Query，某一个 Query 绑定 Plan，绑定 Hint，这时我们会对 SQL 做指纹（把 Where 条件中的一些常量变成一个通配符，然后计算出一个 SQL 的指纹），然后把这个 Hint 绑定在指纹上。一条 Query 来了之后，先解成 AST，我们再生成指纹，拿到指纹之后，Plan Hint Manager 会解析出绑定的 Plan 和 Hint，有 Plan 和 Hint 之后，我们会把 AST 中的一部分节点替换掉，接下来这个 AST 就是一个「带 Hint 的 AST」，然后扔给 Optimizer，Optimizer 就能根据 Hint 介入查询优化器以及执行计划。如果出现慢的 Query，那么可以直接通过前面的 Query Tracing 去定位，再通过 Plan Management 机制在线的给数据库手动加 Hint，来解决慢 Query 的问题。这样下来也就不需要业务人员去改 SQL。这个特性应该在 TiDB 3.0 GA 正式对外提供，现在在内部已经跑得非常好了。在这里也非常感谢美团数据库开发同学的贡献。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3 Join Reorder&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 3.0 中我们增加了 Join Reorder。以前我们有一个非常简单的 Reorder 算法，就是根据 Join 这个路径上的等值条件做了一个优先选择，现在 TiDB 3.0 Beta 已经提供了第一种 Join Reorder 算法，就是一个贪心的算法。简单来说，就是我有几个需要 Join 的表，那我先从中选择 Join 之后数据量最小的那个表（是真正根据 Join 之后的代价来选的），然后我在剩下的表中再选一个，和这个再组成一个 Join Path，这样我们就能一定程度上解决很多 Join 的问题。比如 TPC-H 上的 Q5 以前是需要手动加 Hint 才能跑出来，因为它没有选对 Join 的路径，但在 TiDB 3.0 Beta 中，已经能够自动的选择最好的 Join Path 解决这个问题了。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;429&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-f4d66d24c100bbc0313ec011d431ff55_b.jpg&quot;&gt;&lt;figcaption&gt;图 16 TiDB 3.0 Beta - Join Reorder&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们接下来还要再做一个基于动态规划的 Join Reorder 算法，很有可能会在 3.0 GA 中对外提供。 在 Join 表比较少的时候，我们用动态规划算法能保证找到最好的一个 Join 的路径，但是如果表非常多，比如大于十几个表，那可能会选择贪心的算法，因为 Join Reorder  还是比较耗时的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Functionality&lt;/b&gt;&lt;/p&gt;&lt;p&gt;说完稳定性和易用性之外，我们再看一下功能。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-4d198f0e5aeeb309d2f7d78e7b2f1087_b.jpg&quot;&gt;&lt;figcaption&gt;图 17  TiDB 3.0 Beta 新增功能&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;我们现在做了一个插件系统，因为我们发现数据库能做的功能太多了，只有我们来做其实不太可能，而且每个用户有不一样的需求，比如说这家想要一个能够结合他们的监控系统的一个模块，那家想要一个能够结合他们的认证系统做一个模块，所以我们希望有一个扩展的机制，让大家都有机会能够在一个通用的数据库内核上去定制自己想要的特性。这个插件是基于 Golang 的 Plugin 系统。如果大家有 TiDB Server 的 Binary 和自己插件的 .so，就能在启动 TiDB Server 时加载自己的插件，获得自己定制的功能。&lt;/p&gt;&lt;p&gt;图 17 还列举了一些我们正在做的功能，比如白名单，审计日志，Slow Query，还有一些在 TiDB Hackathon 中诞生的项目，我们也想拿到插件中看看是否能够做出来。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Performance&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-d523ca36fdf02a4716cb55b75dd31f5d_b.jpg&quot;&gt;&lt;figcaption&gt;图 18 TiDB 3.0 Beta - OLTP Benchmark&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;从图 18 中可以看到，我们对 TiDB 3.0 Beta 中做了这么多性能优化之后，在 OLTP 这块进步还是比较大的，比如在 SysBench 下，无论是纯读取还是写入，还是读加写，都有几倍的提升。在解决稳定性这个问题之后，我们在性能方面会投入更多的精力。因为很多时候不能把「性能」单纯的当作性能来看，很多时候慢了，可能业务就挂了，慢了就是错误。&lt;/p&gt;&lt;p&gt;当然 TiDB 3.0 中还有其他重要特性，这里就不详细展开了。（&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/docs-cn/releases/3.0beta/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 3.0 Beta Release Notes&lt;/a&gt;&lt;/u&gt; ）&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Next?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;刚才介绍是 3.0 Beta 一些比较核心的特性，我们还在继续做更多的特性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Storage Layer&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-3158c49cb166d695455c689bb3f315f8_b.jpg&quot;&gt;&lt;figcaption&gt;图 19 TiDB 存储引擎层未来规划&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;比如在存储引擎层，我们对 Raft 层还在改进，比如说刚才我提到了我们有 Raft Learner，我们已经能够极大的减少由于调度带来的 Raft Group 不可用的概率，但是把一个 Learner 提成 Voter 再把另一个 Voter 干掉的时间间隔虽然比较短，但时间间隔依然存在，所以也并不是一个 100% 安全的方案。因此我们做了 Raft Joint Consensus。以前成员变更只能一个一个来：先把 Learner 提成 Voter，再把另一个 Voter 干掉。但有了 Raft Joint Consensus 之后，就能在一次操作中执行多个 ConfChange，从而把因为调度导致的 Region 不可用的概率降为零。&lt;/p&gt;&lt;p&gt;另外我们还在做跨数据中心的部署。前面社区实践分享中来自北京银行的于振华老师提到过，他们是一个两地三中心五部分的方案。现在的 TiDB 已经有一些机制能比较不错地处理这种场景，但我们能够做更多更好的东西，比如说我们可以支持 Witness 这种角色，它只做投票，不同步数据，对带宽的需求比较少，即使机房之间带宽非常低，他可以参与投票。在其他节点失效的情况下，他可以参与选举，决定谁是 Leader。另外我们支持通过 Follower 去读数据，但写入还是要走 Leader，这样对跨机房有什么好处呢？ 就是可以读本地机房的副本，而不是一定要读远端机房那个 Leader，但是写入还是要走远端机房的 Leader，这就能极大的降低读的延迟。除此之外，还有支持链式复制，而不是都通过 Leader 去复制，直接通过本地机房复制数据。&lt;/p&gt;&lt;p&gt;之后我们还可以基于 Learner 做数据的 Backup。通过 learner 去拉一个镜像，存到本地，或者通过 Learner 拉取镜像之后的增量，做增量的物理备份。所以之后要做物理备份是通过 Learner 实时的把 TiKV 中数据做一个物理备份，包括全量和增量。当需要恢复的时候，再通过这个备份直接恢复就好了，不需要通过 SQL 导出再导入，能比较快提升恢复速度。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. SQL Layer&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;428&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-20100b2316a1675b6c484d87b43e4323_b.jpg&quot;&gt;&lt;figcaption&gt;图 20 TiDB 存储引擎层未来规划&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 SQL 层，我们还做了很多事情，比如 Optimizer 正在朝下一代做演进，它是基于最先进的 Cascades 模型。我们希望 Optimizer 能够处理任意复杂的 Query，帮大家解决从 OLTP 到 OLAP 一整套问题，甚至更复杂的问题。比如现在 TiDB 只在 TiKV 上查数据，下一步还要接入TiFlash，TiFlash 的代价或者算子其实不一样的，我们希望能够在 TiDB 上支持多个存储引擎，比如同一个 Query，可以一部分算子推到 TiFlash 上去处理，一部分算子在 TiKV 上处理，在 TiFlash 上做全表扫描，TiKV 上就做 Index 点查，最后汇总在一起再做计算。&lt;/p&gt;&lt;p&gt;我们还计划提供一个新的工具，叫 SQL Tuning Advisor。现在用户遇到了慢 Query，或者想在上线业务之前做 SQL 审核和优化建议，很多时候是人肉来做的，之后我们希望把这个过程变成自动的。&lt;/p&gt;&lt;p&gt;除此之外我们还将支持向量化的引擎，就是把这个引擎进一步做向量化。未来我们还要继续兼容最新的 MySQL 8.0 的特性 Common Table，目前计划以 MySQL 5.7 为兼容目标，和社区用户一起把 TiDB 过渡到 MySQL 8.0 兼容。&lt;/p&gt;&lt;p&gt;&lt;b&gt;说了这么多，我个人觉得，我们做一个好的数据库，有用的数据库，最重要一点是我们有大量的老师，可以向用户，向社区学习。&lt;/b&gt;不管是分享了使用 TiDB 的经验和坑也好，还是去提 Issue 报 Bug，或者是给 TiDB 提交了代码，都是在帮助我们把 TiDB 做得更好，所以在这里表示一下衷心的感谢。最后再立一个 flag，去年我们共写了 24 篇 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiDB-%25E6%25BA%2590%25E7%25A0%2581%25E9%2598%2585%25E8%25AF%25BB&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB 源码阅读文章&lt;/a&gt;，今年还会写 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiKV 源码系列文章&lt;/a&gt;&lt;/u&gt;。我们希望把项目背后只有开发同学才能理解的这套逻辑讲出来，让大家知道 TiDB 是怎样的工作的，希望今年能把这个事情做完，感谢大家。&lt;/p&gt;&lt;p&gt;延伸阅读：&lt;/p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57693856&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic3.zhimg.com/v2-8c30e2e05d268ae22671337dbb6d4a4e_180x120.jpg&quot; data-image-width=&quot;1280&quot; data-image-height=&quot;853&quot; class=&quot;internal&quot;&gt;ZoeyZhai：The Way to TiDB 3.0 and Beyond (上篇)&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-26-57749943</guid>
<pubDate>Tue, 26 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>The Way to TiDB 3.0 and Beyond (上篇)</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-26-57693856.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57693856&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-8c30e2e05d268ae22671337dbb6d4a4e_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;我司 Engineering VP 申砾在 TiDB DevCon 2019  上分享了 TiDB 产品进化过程中的思考与未来规划。本文为演讲实录&lt;b&gt;上篇&lt;/b&gt;，重点回顾了 TiDB 2.1 的特性，并分享了我们对「如何做一个好的数据库」的看法。&lt;/blockquote&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;720&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-17f0e874be7e88911e19edc8a57e822b_b.jpg&quot;&gt;&lt;figcaption&gt;我司 Engineering VP 申砾&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;感谢这么多朋友的到场，今天我会从我们的一些思考的角度来回顾过去一段时间做了什么事情，以及未来的半年到一年时间内将会做什么事情，特别是「我们为什么要做这些事情」&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;TiDB 这个产品，我们从 2015 年年中开始做，做到现在，三年半，将近四年了，从最早期的 Beta 版的时候就开始上线，到后来 RC 版本，最后在 2017 年终于发了 1.0，开始铺了一部分用户，到 2.0 的时候，用户数量就开始涨的非常快。然后我们最近发了 2.1，在 2.1 之后，我们也和各种用户去聊，跟他们聊一些使用的体验，有什么样的问题，包括对我们进行吐嘈。我们就在这些实践经验基础之上，设计了 3.0 的一些特性，以及我们的一些工作的重点。现在我们正在朝 3.0 这个版本去演进，到今天早上已经发了 &lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/docs-cn/releases/3.0beta/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;3.0 Beta 版本&lt;/a&gt;&lt;/u&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;TiDB 2.1&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;首先我们来讲 2.1，2.1 是一个非常重要的版本，这个版本我们吸取了很多用户的使用场景中看到的问题，以及特别多用户的建议。在这里我跟大家聊一聊它有哪些比较重要的特性。&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;432&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;432&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-ab298e0871d04ade7b25b6c914961bfb_b.jpg&quot;&gt;&lt;figcaption&gt;图 1 TiDB 2.1 新增重要功能&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;首先我们两个核心组件：存储引擎和计算引擎，在这两方面，我们做了一些非常重要的改进，当然这些改进有可能是用户看不到的。或者说这些改进其实我们是不希望用户能看到的，一旦你看到了，注意到这些改进的话，说明你的系统遇到这些问题了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1. Raft&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 Learner&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大家都知道 Raft 会有 Leader 和 Follower 这两个概念，Leader 来负责读写，Follower 来作为 Backup，然后随时找机会成为新的 Leader。如果你想加一个新的节点，比如说在扩容或者故障恢复，新加了一个 Follower 进来，这个时候 Raft Group 有 4 个成员， Leader、Follower 都是 Voter，都能够在写入数据时候对日志进行投票，或者是要在成员变更的时候投票的。这时一旦发生意外情况，比如网络变更或者出现网络分区，假设 2 个被隔离掉的节点都在一个物理位置上，就会导致 4 个 Voter 中 2 个不可用，那这时这个 Raft Group 就不可用了。&lt;/p&gt;&lt;p&gt;大家可能觉得这个场景并不常见，但是如果我们正在做负载均衡调度或者扩容时，一旦出现这种情况，就很有可能影响业务。所以我们加了 Learner 这个角色，Learner 的功能也是我们贡献给 etcd 这个项目的。有了 Learner 之后，我们在扩容时不会先去加一个 Follower（也就是一个 Voter），而是增加一个 Learner 的角色，它不是 Voter，所以它只会同步数据不会投票，所以无论在做数据写入还是成员变更的时候都不会算上它。当同步完所有数据时（因为数据量大的时候同步时间会比较长），拿到所有数据之后，再把它变成一个 Voter，同时再把另一个我们想下线的 Follower 下掉就好了。这样就能极大的缩短同时存在 4 个 Voter 的时间，整个 Raft Group 的可用性就得到了提升。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-12c781603546eebfc1410cc34dfa2a97_b.jpg&quot;&gt;&lt;figcaption&gt;图 2 TiDB 2.1 -  Raft Learner&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其实增加 Learner 功能不只是出于提升 Raft Group 可用性，或者说出于安全角度考虑，实际上我们也在用 Learner 来做更多的事情。比如，我们可以随便去加 Learner，然后把 Learner 变成一个只读副本，很多很重的分析任务就可以在 Learner 上去做。TiFlash 这个项目其实就是用 Learner 这个特性来增加只读副本，同时保证不会影响线上写入的延迟，因为它并不参与写入的时候投票。这样的好处是第一不影响写入延迟，第二有 Raft 实时同步数据，第三我们还能在上面快速地做很复杂的分析，同时线上 OLTP 业务有物理上的隔离。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.2 PreVote&lt;/b&gt;&lt;/p&gt;&lt;p&gt;除了 Learner 之外，我们 2.1 中默认开启了 PreVote 这个功能。&lt;/p&gt;&lt;p&gt;我们考虑一种意外情况，就是在 Raft group 中出现了网络隔离，有 1 个节点和另外 2 个节点隔离掉了，然后它现在发现「我找不到 Leader 了，Leader 可能已经挂掉了」，然后就开始投票，不断投票，但是因为它和其他节点是隔离开的，所以没有办法选举成功。它每次失败，都会把自己的 term 加 1，每次失败加 1，网络隔离发生一段时间之后，它的 term 就会很高。当网络分区恢复之后，它的选举消息就能发出去了，并且这个选举消息里面的 term 是比较高的。根据 Raft 的协议，当遇到一个 term 比较高的时候，可能就会同意发起选举，当前的 Leader 就会下台来参与选举。但是因为发生网络隔离这段时间他是没有办法同步数据的，此时它的 Raft Log 一定是落后的，所以即使它的 term 很高，也不可能被选成新的 Leader。所以这个时候经过一次选举之后，它不会成为新 Leader，只有另外两个有机会成为新的 Leader。&lt;/p&gt;&lt;p&gt;大家可以看到，这个选举是对整个 Raft Group 造成了危害：首先它不可能成为新的 Leader，第二它把原有的 Leader 赶下台了，并且在这个选举过程中是没有 Leader 的，这时的 Raft Group 是不能对外提供服务的。虽然这个时间会很短，但也可能会造成比较大的抖动。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-447138e9b1b8b343cd34247da3087b39_b.jpg&quot;&gt;&lt;figcaption&gt;图 3 TiDB 2.1 - Raft PreVote &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;所以我们有了 PreVote 这个特性。具体是这样做的（如图 3）：在进行选举之前，先用 PreVote 这套机制来进行预选举，每个成员把自己的信息，包括 term，Raft Log  Index 放进去，发给其它成员，其它成员有这个信息之后，认为「我可以选你为 Leader」，才会发起真正的选举。&lt;/p&gt;&lt;p&gt;有了 PreVote 之后，我们就可以避免这种大规模的一个节点上很多数据、很多 Raft Group、很多 Peer 的情况下突然出现网络分区，在恢复之后造成大量的 Region 出现选举，导致整个服务有抖动。 因此 PreVote 能极大的提升稳定性。&lt;/p&gt;&lt;p&gt;&lt;b&gt;2. Concurrent DDL Operation&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当然除了 Raft 这几个改进之外，TiDB 2.1 中还有一个比较大的改进，就是在 DDL 模块。这是我们 2.1 中一个比较显著的特性。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-10b9f381bce04c28c1cc6283b938a2ca_b.jpg&quot;&gt;&lt;figcaption&gt;图 4 TiDB 2.1 之前的 DDL 机制&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 2.1 之前的 DDL 整套机制是这样的（如图 4）：用户将 DDL 提交到任何一个 TiDB Server，发过来一个 DDL 语句，TiDB Server 经过一些初期的检查之后会打包成一个 DDL Job，扔到 TiKV 上一个封装好的队列中，整个集群只有一个 TiDB Server 会执行 DDL，而且只有一个线程在做这个事情。这个线程会去队列中拿到队列头的一个 Job，拿到之后就开始做，直到这个 Job 做完，即 DDL 操作执行完毕后，会再把这个 Job 扔到历史队列中，并且标记已经成功，这时 TiDB Sever 能感知到这个 DDL 操作是已经结束了，然后对外返回。前面的 Job 在执行完之前，后面的 DDL 操作是不会执行的，因而会造成一个情况： 假设前面有一个 AddIndex，比如在一个百亿行表上去 AddIndex，这个时间是非常长的，后面的 Create Table 是非常快的，但这时 Create Table 操作会被 AddIndex 阻塞，只有等到 AddIndex  执行完了，才会执行 Create Table，这个给有些用户造成了困扰，所以我们在 TiDB 2.1 中做了改进。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;430&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-acdddddaeb84c21ba055475802797705_b.jpg&quot;&gt;&lt;figcaption&gt;图 5 TiDB 2.1 - DDL 机制&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;在 TiDB 2.1 中 DDL 从执行层面分为两种（如图 5）。一种是 AddIndex 操作，即回填数据（就是把所有的数据扫出来，然后再填回去），这个操作耗时是非常长的，而且一些用户是线上场景，并发度不可能调得很高，因为在回写数据的时候，可能会对集群的写入造成压力。&lt;/p&gt;&lt;p&gt;另外一种是所有其他 DDL 操作，因为不管是 Create Table 还是加一个 Column 都是非常快的，只会修改 metadata 剩下的交给后台来做。所以我们将 AddIndex 的操作和其他有 DDL 的操作分成两个队列，每种 DDL 语句按照分类，进到不同队列中，在 DDL 的处理节点上会启用多个线程来分别处理这些队列，将比较慢的 AddIndex 的操作交给单独的一个线程来做，这样就不会出现一个  AddIndex 操作阻塞其他所有 Create Table 语句的问题了。&lt;/p&gt;&lt;p&gt;这样就提升了系统的易用性，当然我们下一步还会做进一步的并行， 比如在  AddIndex 时，可以在多个表上同时  AddIndex，或者一个表上同时 Add 多个 Index。我们也希望能够做成真正并行的一个 DDL 操作。&lt;/p&gt;&lt;p&gt;&lt;b&gt;3. Parallel Hash Aggregation&lt;/b&gt;&lt;/p&gt;&lt;p&gt;除了刚刚提到的稳定性和易用性的提升，我们在 TiDB 2.1 中，也对分析能力做了提升。我们在聚合的算子上做了两点改进。  第一点是对整个聚合框架做了优化，就是从一行一行处理的聚合模式，变成了一批一批数据处理的聚合模式，另外我们还在哈希聚合算子上做了并行。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;431&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-862d83d19a3791e0305c669041890b51_b.jpg&quot;&gt;&lt;figcaption&gt;图 6 TiDB 2.1 - Parallel Hash Aggregation&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;为什么我们要优化聚合算子？因为在分析场景下，有两类算子是非常重要的，是 Join 和聚合。Join 算子我们之前已经做了并行处理，而 TiDB 2.1 中我们进一步对聚合算子做了并行处理。在哈希聚合中，我们在一个聚合算子里启用多个线程，分两个阶段进行聚合。这样就能够极大的提升聚合的速度。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;427&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a49f81e793aab13d76c07c20784b1895_b.jpg&quot;&gt;&lt;figcaption&gt;图 7 TiDB 2.0 与 TiDB 2.1 TPC-H Benchmark 对比&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图 7 是 TiDB 2.1 发布的时候，我们做了一个 TPC-H Benchmark。实际上所有的 Query 都有提升，其中 Q17 和 Q18 提升最大。因为在 TiDB  2.0 测试时，Q17、Q18 还是一个长尾的 Query，分析之后发现瓶颈就在于聚合算子的执行。整个机器的 CPU 并不忙，但就是时间很长，我们做了 Profile 发现就是聚合的时间太长了，所以在 TiDB 2.1 中，对聚合算子做了并行，并且这个并行度可以调节。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4. Ecosystem Tools&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TiDB 2.1 发布的时我们还发布了两个工具，分别叫 TiDB-DM 和 TiDB-Lightning。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-ecosystem-tools-3/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-DM&lt;/a&gt;&lt;/u&gt;&lt;/b&gt; 全称是 TiDB Data Migration，这个工具主要用来把我们之前的 Loader 和以及 Syncer 做了产品化改造，让大家更好用，它能够做分库分表的合并，能够只同步一些表中的数据，并且它还能够对数据做一些改写，因为分库分表合并的时候，数据合到一个表中可能会冲突，这时我们就需要一种非常方便、可配置的工具来操作，而不是让用户手动的去调各种参数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/blog-cn/tidb-ecosystem-tools-2/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;TiDB-Lightning&lt;/a&gt;&lt;/u&gt;&lt;/b&gt; 这个工具是用来做全量的数据导入。之前的 Loader 也可以做全量数据导入，但是它是走的最标准的那套 SQL 的流程，需要做 SQL 的解析优化、 两阶段提交、Raft 复制等等一系列操作。但是我们觉得这个过程可以更快。因为很多用户想迁移到 TiDB 的数据不是几十 G 或者几百 G，而是几 T、几十 T、上百 T 的数据，通过传统导入的方式会非常慢。现在 TiDB-Lightning 可以直接将本地从 MySQL 或者其他库中导出的 SQL 文本，或者是 CSV 格式的文件，直接转成 RocksDB 底层的 SST file ，然后再注入到 TiKV 中，加载进去就导入成功了，能够极大的提升导入速度。当然我们还在不断的优化，希望这个速度能不断提升，将 1TB 数据的导入，压缩到一两个小时。这两个工具，有一部分用户已经用到了（&lt;u&gt;&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI3NDIxNTQyOQ%3D%3D%26mid%3D2247487797%26idx%3D1%26sn%3Dd6de39f3d0172418c682eeb7779ea5e8%26chksm%3Deb16365fdc61bf497ee794f8b99780c2ec946bd7e1ed13bd47871bfae1355d7c76b190ee7c5a%26scene%3D21%23wechat_redirect&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;并且已经正式开源&lt;/a&gt;&lt;/u&gt;）。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How to build a good database?&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们有相当多的用户正在使用 TiDB，我们在很多的场景中见到了各种各样的 Case，甚至包括机器坏掉甚至连续坏掉的情况。见了很多场景之后，我们就在想之后如何去改进产品，如何去避免在各种场景中遇到的「坑」，于是我们在更深入地思考一个问题：如何做一个好的数据库。因为做一个产品其实挺容易的，一个人两三个月也能搞一套数据库，不管是分库分表，还是类似于在 KV 上做一个 SQL，甚至做一个分布式数据库，都是可能在一个季度甚至半年之内做出来的。但是要真正做一个好的数据库，做一个成熟的数据库，做一个能在生产系统中大规模使用，并且能够让用户自己玩起来的数据库，其实里面有非常多工作要做。&lt;/p&gt;&lt;p&gt;&lt;b&gt;首先数据库最基本的是要「有用」，就是能解决用户问题&lt;/b&gt;。而要解决用户问题，第一点就是要知道用户有什么样的问题，我们就需要跟各类用户去聊，看用户的场景，一起来分析，一起来获得使用场景中真实存在的问题。所以最近我们有大量的同事，不管是交付的同事还是研发的同事，都与用户做了比较深入的访谈，聊聊用户在使用过程中有什么的问题，有什么样的需求，用户也提供各种各样的建议。我们希望 TiDB 能够很好的解决用户场景中存在的问题，甚至是用户自己暂时还没有察觉到的问题，进一步的满足用户的各种需求。&lt;/p&gt;&lt;p&gt;&lt;b&gt;第二点是「易用性」。&lt;/b&gt;就好像一辆车，手动挡的大家也能开，但其实大家现在都想开自动挡。我们希望我们的数据库是一辆自动挡的车，甚至未来是一辆无人驾驶的车，让用户不需要关心这些事情，只需要专注自己的业务就好了。所以我们会不断的优化现有的解决方案，给用户更多更好的解决方案，下一步再将这些方案自动化，让用户用更低的成本使用我们的数据库。&lt;/p&gt;&lt;p&gt;&lt;b&gt;最后一点「稳定性」也非常重要&lt;/b&gt;，就是让用户不会用着用着担惊受怕，比如半夜报警之类的事情。而且我们希望 TiDB 能在大规模数据集上、在大流量上也能保持稳定。&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;下篇将于明日推送，重点介绍 TiDB 3.0 Beta 在稳定性、易用性和功能性上的提升，以及 TiDB 在 Storage Layer 和 SQL Layer 方面的规划。&lt;/b&gt;&lt;/blockquote&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-26-57693856</guid>
<pubDate>Tue, 26 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>第一届 RustCon Asia 来了！</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-21-57330714.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57330714&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e66905c46f38af9890c7ec84896f6b63_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//rustcon.asia/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;RustCon Asia&lt;/a&gt;&lt;b&gt; 来了！由秘猿科技与 PingCAP 联合主办，亚洲第一届 Rust 大会将于 4 月 20 日在中国北京开启。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大会为期 4 天，包括 20 日全天和 21 日上午的主题演讲以及 22-23 日的多个主题 workshop 环节。其中主题演讲讲师来自于国内外资深 Rust 开发者和社区活跃贡献者；workshop 主题将覆盖到 Rust 开发入门和成熟技术栈或产品的实战操作和演示。&lt;br&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;458&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1080&quot; data-rawheight=&quot;458&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1080&quot; data-original=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5d166bfe8eb7c11d10cd47253c758589_b.jpg&quot;&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;关于 RustCon Asia&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;我们受欧洲 RustFest、美国东部 Rust Belt Rust、俄罗斯 RustRush、美国西部 RustConf 和拉美 Rust LatAm 的影响和激励，开启亚洲的第一场 Rust 大会，并期望 RustCon Asia 未来能够周期性持续举办，连接亚洲的 Rust 的开发者与全球的 Rust 社区，相互扶持，共同布道 Rust 开发语言。&lt;/p&gt;&lt;p&gt;在亚洲，我们已有不少 Rust 开发的优秀案例。一些 Rust 项目已经在生产环境中使用多年，包括中国的银行核心系统、信任链、分布式系统、网络和云服务基础设施等。&lt;/p&gt;&lt;p&gt;我们选择北京作为 RustCon Asia 的第一站，首先因为我们的组织者秘猿科技和 PingCAP 都来自中国；其次也因为我们对中国的开发者和开发社区文化特别熟悉。秘猿科技和 PingCAP 都非常重视开发者社区，除了产品本身的号召力之外，核心团队的开发者也在各种开发者社区特别活跃，持续贡献技术知识和组织多种开发者活动。&lt;/p&gt;&lt;p&gt;未来，我们将 RustCon Asia 推进到亚洲的其他国家，更好的促进当地社区与全球社区的合作和互助。&lt;/p&gt;&lt;p&gt;RustCon Asia 目前已开启讲师席位，欢迎关注官网信息，并通过 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//cfp.rustcon.asia/events/rustcon-asia&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;CFP&lt;/a&gt; 提交您的议题信息，支持中英文双语。会议其它细节我们还在逐步确定，请随时关注我们的动态。&lt;br&gt;此次大会期望能够满足你的以下期待：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;与国内社区的老友面基，与国际社区的开发者见面；&lt;/li&gt;&lt;li&gt;中英文主题演讲，并有双向同传支持；&lt;/li&gt;&lt;li&gt;实操 workshop（无同传）；&lt;/li&gt;&lt;li&gt;涵盖从新人友好到高级的技术内容；&lt;/li&gt;&lt;li&gt;匿名议题提交和筛选，以便将最优秀内容呈现给大家；&lt;/li&gt;&lt;li&gt;与生产环境中使用 Rust 的项目成员交流；&lt;/li&gt;&lt;li&gt;开放、温馨的氛围；&lt;/li&gt;&lt;li&gt;有机会与新、老朋友一起探索北京城！&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;b&gt;讲师席位和研讨会席位还在接受报名中，请于官网 CFP 处提交（支持中英文双语）：&lt;/b&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//cfp.rustcon.asia/events/rustcon-asia&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;cfp.rustcon.asia/events&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/rustcon-asia&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&lt;b&gt;大会官网&lt;/b&gt;：&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//rustcon.asia/&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;rustcon.asia/&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&lt;b&gt;中文直达&lt;/b&gt;：&lt;a href=&quot;http://link.zhihu.com/?target=http%3A//www.huodongxing.com/event/6479456003900&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;http://www.&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;huodongxing.com/event/6&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;479456003900&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;br&gt;&lt;b&gt;Twitter &lt;/b&gt;@RustConAsia&lt;br&gt;&lt;b&gt;合作咨询&lt;/b&gt;：aimee@cryptape.com&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;关于秘猿科技&lt;/b&gt;&lt;/p&gt;&lt;p&gt;杭州秘猿科技有限公司（Cryptape Co.,Ltd.）的使命是用技术创造信任，为加密经济提供基础设施和服务。公司成立于 2016 年 ，核心团队从 2011 年开始参与或主导各种区块链项目，实践经验丰富。秘猿科技具备深厚的区块链技术研发和工程实力，核心技术人员均有超过 10 年以上开发经验。公司完全自主研发了区块链基础平台 CITA，并于 2017 年开源，其创新的架构设计解决了区块链底层扩展性问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;关于 PingCAP&lt;/b&gt;&lt;/p&gt;&lt;p&gt;PingCAP 是一家开源的新型分布式数据库公司，秉承开源是基础软件的未来这一理念，PingCAP 持续扩大社区影响力，致力于前沿技术领域的创新实现。其研发的分布式关系型数据库 TiDB 项目，具备「分布式强一致性事务、在线弹性水平扩展、故障自恢复的高可用、跨数据中心多活」等核心特性，是大数据时代理想的数据库集群和云数据库解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-21-57330714</guid>
<pubDate>Thu, 21 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiDB 在摩拜单车的深度实践及应用</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-18-57047909.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/57047909&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-9f67bc657e08abc949d2ff8773e22a0b_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;blockquote&gt;&lt;b&gt;作者介绍：&lt;/b&gt;吕磊，摩拜单车高级 DBA&lt;/blockquote&gt;&lt;h2&gt;&lt;b&gt;一、业务场景&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;摩拜单车 2017 年开始将 TiDB 尝试应用到实际业务当中，根据业务的不断发展，TiDB 版本快速迭代，我们将 TiDB 在摩拜单车的使用场景逐渐分为了三个等级：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;P0 级核心业务：线上核心业务，必须单业务单集群，不允许多个业务共享集群性能，跨 AZ 部署，具有异地灾备能力。&lt;/li&gt;&lt;li&gt;P1 级在线业务：线上业务，在不影响主流程的前提下，可以允许多个业务共享一套 TiDB 集群。&lt;/li&gt;&lt;li&gt;离线业务集群：非线上业务，对实时性要求不高，可以忍受分钟级别的数据延迟。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;本文会选择三个场景，给大家简单介绍一下 TiDB 在摩拜单车的使用姿势、遇到的问题以及解决方案。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;二、订单集群（P0 级业务）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;订单业务是公司的 P0 级核心业务，以前的 Sharding 方案已经无法继续支撑摩拜快速增长的订单量，单库容量上限、数据分布不均等问题愈发明显，尤其是订单合库，单表已经是百亿级别，TiDB 作为 Sharding 方案的一个替代方案，不仅完美解决了上面的问题，还能为业务提供多维度的查询。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.1 订单 TiDB 集群的两地三中心部署架构&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;664&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;664&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-e2b22fd5998801290ebdf41f0986a9f7_b.jpg&quot;&gt;&lt;figcaption&gt;图 1  两地三中心部署架构图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;整个集群部署在三个机房，同城 A、同城 B、异地 C。由于异地机房的网络延迟较高，设计原则是尽量使 PD Leader 和 TiKV Region Leader 选在同城机房（Raft 协议只有 Leader 节点对外提供服务），我们的解决方案如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;PD 通过 Leader priority 将三个 PD server 优先级分别设置为 5 5 3。&lt;/li&gt;&lt;li&gt;将跨机房的 TiKV 实例通过 label 划分 AZ，保证 Region 的三副本不会落在同一个 AZ 内。&lt;/li&gt;&lt;li&gt;通过 label-property reject-leader 限制异地机房的 Region Leader，保证绝大部分情况下 Region 的 Leader 节点会选在同城机房 A、B。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2.2 订单集群的迁移过程以及业务接入拓扑&lt;/b&gt;&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;522&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-0e26587f4c6d3d9e3f83a04701796a93_b.jpg&quot;&gt;&lt;figcaption&gt;图 2  订单集群的迁移过程以及业务接入拓扑图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;为了方便描述，图中 Sharding-JDBC 部分称为&lt;b&gt;老 Sharding 集群&lt;/b&gt;，DBProxy 部分称为&lt;b&gt;新 Sharding 集群。&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;新 Sharding 集群按照 order_id 取模通过 DBproxy 写入各分表，解决数据分布不均、热点等问题。&lt;/li&gt;&lt;li&gt;将老 Sharding 集群的数据通过使用 DRC（摩拜自研的开源异构数据同步工具 Gravity &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/moiot/gravity&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/moiot/gravit&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;）全量+增量同步到新 Sharding 集群，并将增量数据进行打标，反向同步链路忽略带标记的流量，避免循环复制。&lt;/li&gt;&lt;li&gt;为支持上线过程中业务回滚至老 Sharding 集群，需要将新 Sharding 集群上的增量数据同步回老 Sharding 集群，由于写回老 Sharding 集群需要耦合业务逻辑，因此 DRC（Gravity）负责订阅 DBProxy-Sharding 集群的增量数放入 Kafka，由业务方开发一个消费 Kafka 的服务将数据写入到老 Sharding 集群。&lt;/li&gt;&lt;li&gt;新的 TiDB 集群作为订单合库，使用 DRC（Gravity）从新 Sharding 集群同步数据到 TiDB 中。&lt;/li&gt;&lt;li&gt;新方案中 DBProxy 集群负责 order_id 的读写流量，TiDB 合库作为 readonly 负责其他多维度的查询。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2.3 使用 TiDB 遇到的一些问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2.3.1&lt;/b&gt; &lt;b&gt;上线初期新集群流量灰度到 20% 的时候，发现 TiDB coprocessor 非常高，日志出现大量 server is busy 错误。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;问题分析：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;订单数据单表超过 100 亿行，每次查询涉及的数据分散在 1000+ 个 Region 上，根据 index 构造的 handle 去读表数据的时候需要往这些 Region 上发送很多 distsql 请求，进而导致 coprocessor 上 gRPC 的 QPS 上升。&lt;/li&gt;&lt;li&gt;TiDB 的执行引擎是以 Volcano 模型运行，所有的物理 Executor 构成一个树状结构，每一层通过调用下一层的 &lt;code&gt;Next/NextChunk()&lt;/code&gt; 方法获取结果。Chunk 是内存中存储内部数据的一种数据结构，用于减小内存分配开销、降低内存占用以及实现内存使用量统计/控制，TiDB 2.0 中使用的执行框架会不断调用 Child 的 &lt;code&gt;NextChunk&lt;/code&gt; 函数，获取一个 Chunk 的数据。每次函数调用返回一批数据，数据量由一个叫 &lt;code&gt;tidb_max_chunk_size&lt;/code&gt; 的 session 变量来控制，默认是 1024 行。订单表的特性，由于数据分散，实际上单个 Region 上需要访问的数据并不多。所以这个场景 Chunk size 直接按照默认配置（1024）显然是不合适的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;解决方案：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;升级到 2.1 GA 版本以后，这个参数变成了一个全局可调的参数，并且默认值改成了 32，这样内存使用更加高效、合理，该问题得到解决。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2.3.2&lt;/b&gt; &lt;b&gt;数据全量导入 TiDB 时，由于 TiDB 会默认使用一个隐式的自增 rowid，大量 INSERT 时把数据集中写入单个 Region，造成写入热点。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;解决方案&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;通过设置 &lt;code&gt;SHARD_ROW_ID_BITS&lt;/code&gt; (&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/docs/blob/master/sql/tidb-specific.md%23shard_row_id_bits&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pingcap/docs&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;/blob/master/sql/tidb-specific.md#shard_row_id_bits&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;)，可以把 rowid 打散写入多个不同的 Region，缓解写入热点问题：ALTER TABLE table_name SHARD_ROW_ID_BITS = 8;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;2.3.3&lt;/b&gt; &lt;b&gt;异地机房由于网络延迟相对比较高，设计中赋予它的主要职责是灾备，并不提供服务。曾经出现过一次大约持续 10s 的网络抖动，TiDB 端发现大量的 no Leader 日志，Region follower 节点出现网络隔离情况，隔离节点 term 自增，重新接入集群时候会导致 Region 重新选主，较长时间的网络波动，会让上面的选主发生多次，而选主过程中无法提供正常服务，最后可能导致雪崩。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;问题分析&lt;/b&gt;：Raft 算法中一个 Follower 出现网络隔离的场景，如下图所示。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;806&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;806&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-e8f336732b0dc5e53aa355449480369e_b.jpg&quot;&gt;&lt;figcaption&gt;图 3  Raft 算法中，Follower 出现网络隔离的场景图&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Follower C 在 election timeout 没收到心跳之后，会发起选举，并转换为 Candidate 角色。&lt;/li&gt;&lt;li&gt;每次发起选举时都会把 term 加 1，由于网络隔离，选举失败的 C 节点 term 会不断增大。&lt;/li&gt;&lt;li&gt;在网络恢复后，这个节点的 term 会传播到集群的其他节点，导致重新选主，由于 C 节点的日志数据实际上不是最新的，并不会成为 Leader，整个集群的秩序被这个网络隔离过的 C 节点扰乱，这显然是不合理的。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;解决方案：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;TiDB 2.1 GA 版本引入了 Raft PreVote 机制，该问题得到解决。&lt;/li&gt;&lt;li&gt;在 PreVote 算法中，Candidate 首先要确认自己能赢得集群中大多数节点的投票，才会把自己的 term 增加，然后发起真正的投票，其他节点同意发起重新选举的条件更严格，必须同时满足 ：&lt;/li&gt;&lt;ul&gt;&lt;li&gt;没有收到 Leader 的心跳，至少有一次选举超时。&lt;/li&gt;&lt;li&gt;Candidate 日志足够新。PreVote 算法的引入，网络隔离节点由于无法获得大部分节点的许可，因此无法增加 term，重新加入集群时不会导致重新选主。&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;三、在线业务集群（P1 级业务）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;在线业务集群，承载了用户余额变更、我的消息、用户生命周期、信用分等 P1 级业务，数据规模和访问量都在可控范围内。产出的 TiDB Binlog 可以通过 Gravity 以增量形式同步给大数据团队，通过分析模型计算出用户新的信用分定期写回 TiDB 集群。&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;729&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;729&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-f33bcb57f0cd01ad0eaad75b4ba92b0a_b.jpg&quot;&gt;&lt;figcaption&gt;图 4  在线业务集群拓扑图&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;&lt;b&gt;四、数据沙盒集群（离线业务）&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;b&gt;数据沙盒，属于离线业务集群，是摩拜单车的一个数据聚合集群。目前运行着近百个 TiKV 实例，承载了 60 多 TB 数据，由公司自研的 Gravity 数据复制中心将线上数据库实时汇总到 TiDB 供离线查询使用，同时集群也承载了一些内部的离线业务、数据报表等应用。目前集群的总写入 TPS 平均在 1-2w/s，QPS 峰值 9w/s+，集群性能比较稳定。&lt;/b&gt;该集群的设计优势有如下几点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;可供开发人员安全的查询线上数据。&lt;/li&gt;&lt;li&gt;特殊场景下的跨库联表 SQL。&lt;/li&gt;&lt;li&gt;大数据团队的数据抽取、离线分析、BI 报表。&lt;/li&gt;&lt;li&gt;可以随时按需增加索引，满足多维度的复杂查询。&lt;/li&gt;&lt;li&gt;离线业务可以直接将流量指向沙盒集群，不会对线上数据库造成额外负担。&lt;/li&gt;&lt;li&gt;分库分表的数据聚合。&lt;/li&gt;&lt;li&gt;数据归档、灾备。&lt;/li&gt;&lt;/ul&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;614&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;614&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_r.jpg&quot; data-actualsrc=&quot;https://pic3.zhimg.com/v2-130f384fa8a05990f9458325ae851dce_b.jpg&quot;&gt;&lt;figcaption&gt;图 5  数据沙盒集群拓扑图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;b&gt;4.1 遇到过的一些问题和解决方案&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1.1&lt;/b&gt; &lt;b&gt;TiDB server oom 重启&lt;/b&gt;&lt;/p&gt;&lt;p&gt;很多使用过 TiDB 的朋友可能都遇到过这一问题，当 TiDB 在遇到超大请求时会一直申请内存导致 oom, 偶尔因为一条简单的查询语句导致整个内存被撑爆，影响集群的总体稳定性。虽然 TiDB 本身有 oom action 这个参数，但是我们实际配置过并没有效果。&lt;/p&gt;&lt;p&gt;于是我们选择了一个折中的方案，也是目前 TiDB 比较推荐的方案：单台物理机部署多个 TiDB 实例，通过端口进行区分，给不稳定查询的端口设置内存限制（如图 5 中间部分的 TiDBcluster1 和 TiDBcluster2）。例：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;[tidb_servers]
tidb-01-A ansible_host=$ip_address deploy_dir=/$deploydir1 tidb_port=$tidb_port1 tidb_status_port=$status_port1
tidb-01-B ansible_host=$ip_address deploy_dir=/$deploydir2 tidb_port=$tidb_port2 tidb_status_port=$status_port2  MemoryLimit=20G 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;实际上 &lt;code&gt;tidb-01-A&lt;/code&gt;、&lt;code&gt;tidb-01-B&lt;/code&gt; 部署在同一台物理机，&lt;code&gt;tidb-01-B&lt;/code&gt; 内存超过阈值会被系统自动重启，不影响 &lt;code&gt;tidb-01-A&lt;/code&gt;。&lt;/p&gt;&lt;p&gt;TiDB 在 2.1 版本后引入新的参数 &lt;code&gt;tidb_mem_quota_query&lt;/code&gt;，可以设置查询语句的内存使用阈值，目前 TiDB 已经可以部分解决上述问题。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1.2&lt;/b&gt; &lt;b&gt;TiDB-Binlog 组件的效率问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;大家平时关注比较多的是如何从 MySQL 迁移到 TiDB，但当业务真正迁移到 TiDB 上以后，TiDB 的 Binlog 就开始变得重要起来。TiDB-Binlog 模块，包含 Pump&amp;amp;Drainer 两个组件。TiDB 开启 Binlog 后，将产生的 Binlog 通过 Pump 组件实时写入本地磁盘，再异步发送到 Kafka，Drainer 将 Kafka 中的 Binlog 进行归并排序，再转换成固定格式输出到下游。&lt;/p&gt;&lt;p&gt;使用过程中我们碰到了几个问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Pump 发送到 Kafka 的速度跟不上 Binlog 产生的速度。&lt;/li&gt;&lt;li&gt;Drainer 处理 Kafka 数据的速度太慢，导致延时过高。&lt;/li&gt;&lt;li&gt;单机部署多 TiDB 实例，不支持多 Pump。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;其实前两个问题都是读写 Kafka 时产生的，Pump&amp;amp;Drainer 按照顺序、单 partition 分别进行读&amp;amp;写，速度瓶颈非常明显，后期增大了 Pump 发送的 batch size，加快了写 Kafka 的速度。但同时又遇到一些新的问题：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;当源端 Binlog 消息积压太多，一次往 Kafka 发送过大消息，导致 Kafka oom。&lt;/li&gt;&lt;li&gt;当 Pump 高速大批写入 Kafka 的时候，发现 Drainer 不工作，无法读取 Kafka 数据。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;和 PingCAP 工程师一起排查，最终发现这是属于 sarama 本身的一个 bug，sarama 对数据写入没有阈值限制，但是读取却设置了阈值：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/Shopify/sarama/blob/master/real_decoder.go%23L88&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/Shopify/sara&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ma/blob/master/real_decoder.go#L88&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;最后的解决方案是给 Pump 和 Drainer 增加参数 Kafka-max-message 来限制消息大小。单机部署多 TiDB 实例，不支持多 Pump，也通过更新 ansible 脚本得到了解决，将 Pump.service 以及和 TiDB 的对应关系改成 Pump-8250.service，以端口区分。&lt;/p&gt;&lt;p&gt;针对以上问题，PingCAP 公司对 TiDB-Binlog 进行了重构，新版本的 TiDB-Binlog 不再使用 Kafka 存储 binlog。Pump 以及 Drainer 的功能也有所调整，Pump 形成一个集群，可以水平扩容来均匀承担业务压力。另外，原 Drainer 的 binlog 排序逻辑移到 Pump 来做，以此来提高整体的同步性能。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.1.3&lt;/b&gt; &lt;b&gt;监控问题&lt;/b&gt;&lt;/p&gt;&lt;p&gt;当前的 TiDB 监控架构中，TiKV 依赖 Pushgateway 拉取监控数据到 Prometheus，当 TiKV 实例数量越来越多，达到 Pushgateway 的内存限制 2GB 进程会进入假死状态，Grafana 监控就会变成下图的断点样子：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;568&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;568&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-5e6512fb9077015ddb64bf314a4ae141_b.jpg&quot;&gt;&lt;figcaption&gt;图 6  监控拓扑图&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;260&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;974&quot; data-original=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_b.jpg&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;974&quot; data-rawheight=&quot;260&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;974&quot; data-original=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-2729028d7fa7a264d679ba16e48496c1_b.jpg&quot;&gt;&lt;figcaption&gt;图 7  监控展示图&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;目前临时处理方案是部署多套 Pushgateway，将 TiKV 的监控信息指向不同的 Pushgateway 节点来分担流量。这个问题的最终还是要用 TiDB 的新版本（2.1.3 以上的版本已经支持），Prometheus 能够直接拉取 TiKV 的监控信息，取消对 Pushgateway 的依赖。&lt;/p&gt;&lt;p&gt;&lt;b&gt;4.2 数据复制中心 Gravity (DRC)&lt;/b&gt;&lt;/p&gt;&lt;p&gt;下面简单介绍一下摩拜单车自研的数据复制组件 Gravity（DRC）。&lt;/p&gt;&lt;p&gt;Gravity 是摩拜单车数据库团队自研的一套数据复制组件，目前已经稳定支撑了公司数百条同步通道，TPS 50000/s，80 线延迟小于 50ms，具有如下特点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;多数据源（MySQL, MongoDB, TiDB, PostgreSQL）。&lt;/li&gt;&lt;li&gt;支持异构（不同的库、表、字段之间同步），支持分库分表到合表的同步。&lt;/li&gt;&lt;li&gt;支持双活&amp;amp;多活，复制过程将流量打标，避免循环复制。&lt;/li&gt;&lt;li&gt;管理节点高可用，故障恢复不会丢失数据。&lt;/li&gt;&lt;li&gt;支持 filter plugin（语句过滤，类型过滤，column 过滤等多维度的过滤）。&lt;/li&gt;&lt;li&gt;支持传输过程进行数据转换。&lt;/li&gt;&lt;li&gt;一键全量 + 增量迁移数据。&lt;/li&gt;&lt;li&gt;轻量级，稳定高效，容易部署。&lt;/li&gt;&lt;li&gt;支持基于 Kubernetes 的 PaaS 平台，简化运维任务。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;使用场景：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;大数据总线：发送 MySQL Binlog，Mongo Oplog，TiDB Binlog 的增量数据到 Kafka 供下游消费。&lt;/li&gt;&lt;li&gt;单向数据同步：MySQL → MySQL&amp;amp;TiDB 的全量、增量同步。&lt;/li&gt;&lt;li&gt;双向数据同步：MySQL ↔ MySQL 的双向增量同步，同步过程中可以防止循环复制。&lt;/li&gt;&lt;li&gt;分库分表到合库的同步：MySQL 分库分表 → 合库的同步，可以指定源表和目标表的对应关系。&lt;/li&gt;&lt;li&gt;数据清洗：同步过程中，可通过 filter plugin 将数据自定义转换。&lt;/li&gt;&lt;li&gt;数据归档：MySQL→ 归档库，同步链路中过滤掉 delete 语句。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;Gravity 的设计初衷是要将多种数据源联合到一起，互相打通，让业务设计上更灵活，数据复制、数据转换变的更容易，能够帮助大家更容易的将业务平滑迁移到 TiDB 上面。该项目已经在 GitHub 开源，欢迎大家交流使用&lt;/b&gt;：&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/moiot/gravity&quot; class=&quot; external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/moiot/gravit&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;ellipsis&quot;&gt;&lt;/span&gt;&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;五、总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;TiDB 的出现，不仅弥补了 MySQL 单机容量上限、传统 Sharding 方案查询维度单一等缺点，而且其计算存储分离的架构设计让集群水平扩展变得更容易。业务可以更专注于研发而不必担心复杂的维护成本。未来，摩拜单车还会继续尝试将更多的核心业务迁移到 TiDB 上，让 TiDB 发挥更大价值，也祝愿 TiDB 发展的越来越好。&lt;/p&gt;&lt;p&gt;更多案例阅读：&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.pingcap.com/cases-cn/&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;案例&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-18-57047909</guid>
<pubDate>Mon, 18 Feb 2019 00:00:00 +0800</pubDate>
</item>
<item>
<title>TiKV 源码解析系列文章（二）raft-rs proposal 示例情景分析</title>
<link>https://henix.github.io/feeds/zhuanlan.newsql/2019-02-15-56820135.html</link>
<description>&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/56820135&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;title-image&quot;&gt;&lt;img src=&quot;https://pic3.zhimg.com/v2-8a7fcd6586c081fcc255b95b014946b0_b.jpg&quot; alt=&quot;&quot;&gt;&lt;/div&gt;&lt;p&gt;&lt;b&gt;作者：屈鹏&lt;/b&gt;&lt;/p&gt;&lt;p&gt;本文为 TiKV 源码解析系列的第二篇，按照计划首先将为大家介绍 TiKV 依赖的周边库 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//github.com/pingcap/raft-rs&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;raft-rs&lt;/a&gt; 。raft-rs 是 Raft 算法的 &lt;a href=&quot;http://link.zhihu.com/?target=https%3A//www.rust-lang.org/&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;Rust&lt;/a&gt;语言实现。Raft 是分布式领域中应用非常广泛的一种共识算法，相比于此类算法的鼻祖 Paxos，具有更简单、更容易理解和实现的特点。&lt;/p&gt;&lt;p&gt;分布式系统的共识算法会将数据的写入复制到多个副本，从而在网络隔离或节点失败的时候仍然提供可用性。具体到 Raft 算法中，发起一个读写请求称为一次 proposal。本文将以 raft-rs 的公共 API 作为切入点，介绍一般 proposal 过程的实现原理，让用户可以深刻理解并掌握 raft-rs API 的使用， 以便用户开发自己的分布式应用，或者优化、定制 TiKV。&lt;/p&gt;&lt;p&gt;文中引用的代码片段的完整实现可以参见 raft-rs 仓库中的 source-code 分支。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Public API 简述&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;仓库中的 &lt;code&gt;examples/five_mem_node/main.rs&lt;/code&gt; 文件是一个包含了主要 API 用法的简单示例。它创建了一个 5 节点的 Raft 系统，并进行了 100 个 proposal 的请求和提交。经过进一步精简之后，主要的类型封装和运行逻辑如下：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;struct Node {
    // 持有一个 RawNode 实例
    raft_group: Option&amp;lt;RawNode&amp;lt;MemStorage&amp;gt;&amp;gt;,
    // 接收其他节点发来的 Raft 消息
    my_mailbox: Receiver&amp;lt;Message&amp;gt;,
    // 发送 Raft 消息给其他节点
    mailboxes: HashMap&amp;lt;u64, Sender&amp;lt;Message&amp;gt;&amp;gt;,
}
let mut t = Instant::now();
// 在 Node 实例上运行一个循环，周期性地处理 Raft 消息、tick 和 Ready。
loop {
    thread::sleep(Duration::from_millis(10));
    while let Ok(msg) = node.my_mailbox.try_recv() {
        // 处理收到的 Raft 消息
        node.step(msg); 
    }
    let raft_group = match node.raft_group.as_mut().unwrap();
    if t.elapsed() &amp;gt;= Duration::from_millis(100) {
        raft_group.tick();
        t = Instant::now();
    }
    // 处理 Raft 产生的 Ready，并将处理进度更新回 Raft 中
    let mut ready = raft_group.ready();
    persist(ready.entries());  // 处理刚刚收到的 Raft Log
    send_all(ready.messages);  // 将 Raft 产生的消息发送给其他节点
    handle_committed_entries(ready.committed_entries.take());
    raft_group.advance(ready);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这段代码中值得注意的地方是：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;RawNode 是 raft-rs 库与应用交互的主要界面。要在自己的应用中使用 raft-rs，首先就需要持有一个 RawNode 实例，正如 Node 结构体所做的那样。&lt;/li&gt;&lt;li&gt;RawNode 的范型参数是一个满足 Storage 约束的类型，可以认为是一个存储了 Raft Log 的存储引擎，示例中使用的是 MemStorage。&lt;/li&gt;&lt;li&gt;在收到 Raft 消息之后，调用 &lt;code&gt;RawNode::step&lt;/code&gt; 方法来处理这条消息。&lt;/li&gt;&lt;li&gt;每隔一段时间（称为一个 tick），调用 &lt;code&gt;RawNode::tick&lt;/code&gt; 方法使 Raft 的逻辑时钟前进一步。&lt;/li&gt;&lt;li&gt;使用 &lt;code&gt;RawNode::ready&lt;/code&gt; 接口从 Raft 中获取收到的最新日志（&lt;code&gt;Ready::entries&lt;/code&gt;），已经提交的日志（&lt;code&gt;Ready::committed_entries&lt;/code&gt;），以及需要发送给其他节点的消息等内容。&lt;/li&gt;&lt;li&gt;在确保一个 Ready 中的所有进度被正确处理完成之后，调用 &lt;code&gt;RawNode::advance&lt;/code&gt; 接口。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;接下来的几节将展开详细描述。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Storage trait&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Raft 算法中的日志复制部分抽象了一个可以不断追加写入新日志的持久化数组，这一数组在 raft-rs 中即对应 Storage。使用一个表格可以直观地展示这个 trait 的各个方法分别可以从这个持久化数组中获取哪些信息：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1228&quot; data-rawheight=&quot;830&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1228&quot; data-original=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1228&quot; data-rawheight=&quot;830&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1228&quot; data-original=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_r.jpg&quot; data-actualsrc=&quot;https://pic4.zhimg.com/v2-252bf470d229f9592d04c407ff2bb5ff_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;值得注意的是，这个 Storage 中并不包括持久化 Raft Log，也不会将 Raft Log 应用到应用程序自己的状态机的接口。这些内容需要应用程序自行处理。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;&lt;code&gt;RawNode::step&lt;/code&gt; 接口&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这个接口处理从该 Raft group 中其他节点收到的消息。比如，当 Follower 收到 Leader 发来的日志时，需要把日志存储起来并回复相应的 ACK；或者当节点收到 term 更高的选举消息时，应该进入选举状态并回复自己的投票。这个接口和它调用的子函数的详细逻辑几乎涵盖了 Raft 协议的全部内容，代码较多，因此这里仅阐述在 Leader 上发生的日志复制过程。&lt;/p&gt;&lt;p&gt;当应用程序希望向 Raft 系统提交一个写入时，需要在 Leader 上调用 &lt;code&gt;RawNode::propose&lt;/code&gt; 方法，后者就会调用 &lt;code&gt;RawNode::step&lt;/code&gt;，而参数是一个类型为 &lt;code&gt;MessageType::MsgPropose&lt;/code&gt; 的消息；应用程序要写入的内容被封装到了这个消息中。对于这一消息类型，后续会调用 &lt;code&gt;Raft::step_leader&lt;/code&gt; 函数，将这个消息作为一个 Raft Log 暂存起来，同时广播到 Follower 的信箱中。到这一步，propose 的过程就可以返回了，注意，此时这个 Raft Log 并没有持久化，同时广播给 Follower 的 MsgAppend 消息也并未真正发出去。应用程序需要设法将这个写入挂起，等到从 Raft 中获知这个写入已经被集群中的过半成员确认之后，再向这个写入的发起者返回写入成功的响应。那么， 如何能够让 Raft 把消息真正发出去，并接收 Follower 的确认呢？&lt;/p&gt;&lt;p&gt;&lt;code&gt;RawNode::ready&lt;/code&gt; 和 &lt;code&gt;RawNode::advance&lt;/code&gt; 接口&lt;/p&gt;&lt;p&gt;这个接口返回一个 Ready 结构体：&lt;/p&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;&lt;span&gt;&lt;/span&gt;pub struct Ready {
    pub committed_entries: Option&amp;lt;Vec&amp;lt;Entry&amp;gt;&amp;gt;,
    pub messages: Vec&amp;lt;Message&amp;gt;,
    // some other fields...
}
impl Ready {
    pub fn entries(&amp;amp;self) -&amp;gt; &amp;amp;[Entry] {
        &amp;amp;self.entries
    }
    // some other methods...
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;一些暂时无关的字段和方法已经略去，在 propose 过程中主要用到的方法和字段分别是：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1224&quot; data-rawheight=&quot;450&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1224&quot; data-original=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1224&quot; data-rawheight=&quot;450&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1224&quot; data-original=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_r.jpg&quot; data-actualsrc=&quot;https://pic2.zhimg.com/v2-a4a87373f3b33b3643eede35de254c6d_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;对照 &lt;code&gt;examples/five_mem_node/main.rs&lt;/code&gt; 中的示例，可以知道应用程序在 propose 一个消息之后，应该调用 &lt;code&gt;RawNode::ready&lt;/code&gt; 并在返回的 Ready 上继续进行处理：包括持久化 Raft Log，将 Raft 消息发送到网络上等。&lt;/p&gt;&lt;p&gt;而在 Follower 上，也不断运行着示例代码中与 Leader 相同的循环：接收 Raft 消息，从 Ready 中收集回复并发回给 Leader……对于 propose 过程而言，当 Leader 收到了足够的确认这一 Raft Log 的回复，便能够认为这一 Raft Log 已经被确认了，这一逻辑体现在 &lt;code&gt;Raft::handle_append_response&lt;/code&gt; 之后的 &lt;code&gt;Raft::maybe_commit&lt;/code&gt; 方法中。在下一次这个 Raft 节点调用 &lt;code&gt;RawNode::ready&lt;/code&gt; 时，便可以取出这部分被确认的消息，并应用到状态机中了。&lt;/p&gt;&lt;p&gt;在将一个 Ready 结构体中的内容处理完成之后，应用程序即可调用这个方法更新 Raft 中的一些进度，包括 last index、commit index 和 apply index 等。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;&lt;code&gt;RawNode::tick&lt;/code&gt; 接口&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;这是本文最后要介绍的一个接口，它的作用是驱动 Raft 内部的逻辑时钟前进，并对超时进行处理。比如对于 Follower 而言，如果它在 tick 的时候发现 Leader 已经失联很久了，便会发起一次选举；而 Leader 为了避免自己被取代，也会在一个更短的超时之后给 Follower 发送心跳。值得注意的是，tick 也是会产生 Raft 消息的，为了使这部分 Raft 消息能够及时发送出去，在应用程序的每一轮循环中一般应该先处理 tick，然后处理 Ready，正如示例程序中所做的那样。&lt;/p&gt;&lt;h2&gt;&lt;b&gt;总结&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;最后用一张图展示在 Leader 上是通过哪些 API 进行 propose 的：&lt;/p&gt;&lt;figure&gt;&lt;noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1574&quot; data-rawheight=&quot;818&quot; class=&quot;origin_image zh-lightbox-thumb&quot; width=&quot;1574&quot; data-original=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_r.jpg&quot;&gt;&lt;/noscript&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_b.jpg&quot; data-caption=&quot;&quot; data-size=&quot;normal&quot; data-rawwidth=&quot;1574&quot; data-rawheight=&quot;818&quot; class=&quot;origin_image zh-lightbox-thumb lazy&quot; width=&quot;1574&quot; data-original=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_r.jpg&quot; data-actualsrc=&quot;https://pic1.zhimg.com/v2-e8a01c7baf43e1a7d6e1430ebb0434d8_b.jpg&quot;&gt;&lt;/figure&gt;&lt;p&gt;本期关于 raft-rs 的源码解析就到此结束了，我们非常鼓励大家在自己的分布式应用中尝试 raft-rs 这个库，同时提出宝贵的意见和建议。后续关于 raft-rs 我们还会深入介绍 Configuration Change 和 Snapshot 的实现与优化等内容，展示更深入的设计原理、更详细的优化细节，方便大家分析定位 raft-rs 和 TiKV 使用中的潜在问题。&lt;/p&gt;&lt;p&gt;更多阅读：&lt;/p&gt;&lt;a href=&quot;http://link.zhihu.com/?target=https%3A//pingcap.com/blog-cn/%23TiKV-%25E6%25BA%2590%25E7%25A0%2581%25E8%25A7%25A3%25E6%259E%2590&quot; data-draft-node=&quot;block&quot; data-draft-type=&quot;link-card&quot; data-image=&quot;https://pic2.zhimg.com/v2-60ab5bd867c2434d70c957a02a2169e1_ipico.jpg&quot; data-image-width=&quot;1200&quot; data-image-height=&quot;1200&quot; class=&quot; wrap external&quot; target=&quot;_blank&quot; rel=&quot;nofollow noreferrer&quot;&gt;博客&lt;/a&gt;&lt;p&gt;&lt;/p&gt;</description>
<author>ZoeyZhai</author>
<guid isPermaLink="false">2019-02-15-56820135</guid>
<pubDate>Fri, 15 Feb 2019 00:00:00 +0800</pubDate>
</item>
</channel>
</rss>
